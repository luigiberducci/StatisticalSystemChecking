Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.176s, episode steps: 100, steps per second: 568, episode reward: 180.378, mean reward: 1.804 [1.450, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.055, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.061s, episode steps: 100, steps per second: 1638, episode reward: 210.480, mean reward: 2.105 [1.450, 3.881], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.996, 10.110], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.068s, episode steps: 100, steps per second: 1466, episode reward: 180.968, mean reward: 1.810 [1.444, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.272, 10.123], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.075s, episode steps: 100, steps per second: 1329, episode reward: 245.259, mean reward: 2.453 [1.460, 5.068], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.346, 10.136], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.069s, episode steps: 100, steps per second: 1444, episode reward: 186.759, mean reward: 1.868 [1.446, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.837, 10.098], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.064s, episode steps: 100, steps per second: 1566, episode reward: 214.057, mean reward: 2.141 [1.582, 3.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.111, 10.440], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.069s, episode steps: 100, steps per second: 1450, episode reward: 210.572, mean reward: 2.106 [1.521, 3.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.920, 10.098], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.070s, episode steps: 100, steps per second: 1434, episode reward: 186.372, mean reward: 1.864 [1.467, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.852, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.076s, episode steps: 100, steps per second: 1318, episode reward: 191.405, mean reward: 1.914 [1.463, 3.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.274, 10.201], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.064s, episode steps: 100, steps per second: 1562, episode reward: 185.100, mean reward: 1.851 [1.432, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.637, 10.438], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.072s, episode steps: 100, steps per second: 1398, episode reward: 198.470, mean reward: 1.985 [1.529, 4.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.574, 10.235], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.079s, episode steps: 100, steps per second: 1264, episode reward: 201.944, mean reward: 2.019 [1.446, 3.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.760, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.064s, episode steps: 100, steps per second: 1565, episode reward: 203.162, mean reward: 2.032 [1.449, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.631, 10.190], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.064s, episode steps: 100, steps per second: 1565, episode reward: 183.751, mean reward: 1.838 [1.453, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.221, 10.098], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.064s, episode steps: 100, steps per second: 1562, episode reward: 188.504, mean reward: 1.885 [1.446, 3.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.613, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.064s, episode steps: 100, steps per second: 1563, episode reward: 191.030, mean reward: 1.910 [1.488, 3.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.544, 10.098], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.064s, episode steps: 100, steps per second: 1573, episode reward: 199.258, mean reward: 1.993 [1.452, 4.222], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.736, 10.098], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.063s, episode steps: 100, steps per second: 1577, episode reward: 206.594, mean reward: 2.066 [1.457, 4.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.775, 10.199], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.069s, episode steps: 100, steps per second: 1450, episode reward: 176.712, mean reward: 1.767 [1.456, 2.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.095, 10.212], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.069s, episode steps: 100, steps per second: 1456, episode reward: 193.275, mean reward: 1.933 [1.468, 3.777], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.130, 10.115], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.064s, episode steps: 100, steps per second: 1574, episode reward: 194.196, mean reward: 1.942 [1.473, 5.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.922, 10.187], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.064s, episode steps: 100, steps per second: 1567, episode reward: 190.246, mean reward: 1.902 [1.486, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.561, 10.098], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.064s, episode steps: 100, steps per second: 1573, episode reward: 204.448, mean reward: 2.044 [1.452, 4.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.475, 10.152], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.070s, episode steps: 100, steps per second: 1433, episode reward: 216.432, mean reward: 2.164 [1.450, 3.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.706, 10.098], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.069s, episode steps: 100, steps per second: 1449, episode reward: 194.163, mean reward: 1.942 [1.535, 2.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.484, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.064s, episode steps: 100, steps per second: 1559, episode reward: 184.292, mean reward: 1.843 [1.439, 2.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.488, 10.249], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.064s, episode steps: 100, steps per second: 1556, episode reward: 183.161, mean reward: 1.832 [1.455, 2.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.065, 10.195], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.064s, episode steps: 100, steps per second: 1553, episode reward: 190.860, mean reward: 1.909 [1.468, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.965, 10.098], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.064s, episode steps: 100, steps per second: 1560, episode reward: 192.386, mean reward: 1.924 [1.463, 4.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.637, 10.261], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.069s, episode steps: 100, steps per second: 1452, episode reward: 208.836, mean reward: 2.088 [1.455, 4.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.376, 10.098], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.066s, episode steps: 100, steps per second: 1514, episode reward: 213.879, mean reward: 2.139 [1.496, 4.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.647, 10.098], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.067s, episode steps: 100, steps per second: 1486, episode reward: 187.326, mean reward: 1.873 [1.460, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.801, 10.098], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.065s, episode steps: 100, steps per second: 1545, episode reward: 200.459, mean reward: 2.005 [1.462, 3.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.067, 10.098], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.065s, episode steps: 100, steps per second: 1544, episode reward: 198.556, mean reward: 1.986 [1.438, 4.303], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.865, 10.357], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.069s, episode steps: 100, steps per second: 1442, episode reward: 197.180, mean reward: 1.972 [1.446, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.171, 10.098], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.069s, episode steps: 100, steps per second: 1446, episode reward: 189.010, mean reward: 1.890 [1.432, 3.568], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.855, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.069s, episode steps: 100, steps per second: 1447, episode reward: 188.502, mean reward: 1.885 [1.472, 2.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.728, 10.098], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.064s, episode steps: 100, steps per second: 1572, episode reward: 182.229, mean reward: 1.822 [1.440, 2.757], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.159, 10.167], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.064s, episode steps: 100, steps per second: 1571, episode reward: 193.010, mean reward: 1.930 [1.537, 3.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.555, 10.338], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.086s, episode steps: 100, steps per second: 1166, episode reward: 244.818, mean reward: 2.448 [1.520, 7.692], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.613, 10.098], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.080s, episode steps: 100, steps per second: 1243, episode reward: 190.856, mean reward: 1.909 [1.450, 3.370], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.346, 10.307], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.069s, episode steps: 100, steps per second: 1449, episode reward: 186.614, mean reward: 1.866 [1.435, 2.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.953, 10.098], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.074s, episode steps: 100, steps per second: 1356, episode reward: 207.352, mean reward: 2.074 [1.446, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.919, 10.286], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.074s, episode steps: 100, steps per second: 1351, episode reward: 185.921, mean reward: 1.859 [1.470, 3.150], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.433, 10.127], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.064s, episode steps: 100, steps per second: 1568, episode reward: 209.352, mean reward: 2.094 [1.470, 4.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.208, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.069s, episode steps: 100, steps per second: 1443, episode reward: 197.035, mean reward: 1.970 [1.447, 4.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.947, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.064s, episode steps: 100, steps per second: 1568, episode reward: 191.308, mean reward: 1.913 [1.454, 3.991], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.706, 10.216], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.069s, episode steps: 100, steps per second: 1456, episode reward: 204.676, mean reward: 2.047 [1.483, 3.413], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.551, 10.222], loss: --, mae: --, mean_q: --
  4900/100000: episode: 49, duration: 0.068s, episode steps: 100, steps per second: 1462, episode reward: 185.045, mean reward: 1.850 [1.477, 3.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.322, 10.098], loss: --, mae: --, mean_q: --
  5000/100000: episode: 50, duration: 0.064s, episode steps: 100, steps per second: 1559, episode reward: 195.093, mean reward: 1.951 [1.506, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.866, 10.187], loss: --, mae: --, mean_q: --
  5100/100000: episode: 51, duration: 1.255s, episode steps: 100, steps per second: 80, episode reward: 184.051, mean reward: 1.841 [1.478, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.325, 10.191], loss: 0.457950, mae: 0.702506, mean_q: 1.613691
  5200/100000: episode: 52, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 192.159, mean reward: 1.922 [1.456, 4.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.575, 10.098], loss: 0.108104, mae: 0.327994, mean_q: 2.675313
  5300/100000: episode: 53, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 204.173, mean reward: 2.042 [1.475, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.782, 10.098], loss: 0.120396, mae: 0.331281, mean_q: 3.103650
  5400/100000: episode: 54, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 191.889, mean reward: 1.919 [1.487, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.006, 10.098], loss: 0.100332, mae: 0.307911, mean_q: 3.361777
  5500/100000: episode: 55, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 222.884, mean reward: 2.229 [1.453, 6.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.350, 10.523], loss: 0.102698, mae: 0.311664, mean_q: 3.566603
  5600/100000: episode: 56, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 188.691, mean reward: 1.887 [1.438, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.102, 10.274], loss: 0.122765, mae: 0.319460, mean_q: 3.690224
  5700/100000: episode: 57, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 188.817, mean reward: 1.888 [1.467, 3.277], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.285, 10.133], loss: 0.111892, mae: 0.312200, mean_q: 3.735033
  5800/100000: episode: 58, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 186.963, mean reward: 1.870 [1.457, 2.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.851, 10.098], loss: 0.106119, mae: 0.307899, mean_q: 3.795094
  5900/100000: episode: 59, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 188.844, mean reward: 1.888 [1.493, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.911, 10.214], loss: 0.140125, mae: 0.340335, mean_q: 3.834585
  6000/100000: episode: 60, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.323, mean reward: 1.903 [1.447, 4.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.919, 10.159], loss: 0.111967, mae: 0.319196, mean_q: 3.849220
  6100/100000: episode: 61, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 191.874, mean reward: 1.919 [1.455, 3.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.599, 10.098], loss: 0.100507, mae: 0.307504, mean_q: 3.859826
  6200/100000: episode: 62, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 196.218, mean reward: 1.962 [1.454, 3.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.292, 10.176], loss: 0.103420, mae: 0.304895, mean_q: 3.852622
  6300/100000: episode: 63, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 197.633, mean reward: 1.976 [1.453, 4.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.892, 10.098], loss: 0.115672, mae: 0.322438, mean_q: 3.863913
  6400/100000: episode: 64, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 187.948, mean reward: 1.879 [1.444, 3.459], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.142, 10.098], loss: 0.105275, mae: 0.306984, mean_q: 3.859982
  6500/100000: episode: 65, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 194.991, mean reward: 1.950 [1.452, 3.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.729, 10.258], loss: 0.110746, mae: 0.310716, mean_q: 3.844584
  6600/100000: episode: 66, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 193.903, mean reward: 1.939 [1.456, 3.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.375, 10.098], loss: 0.107013, mae: 0.312584, mean_q: 3.855424
  6700/100000: episode: 67, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 215.589, mean reward: 2.156 [1.477, 4.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.775, 10.388], loss: 0.110569, mae: 0.315781, mean_q: 3.858006
  6800/100000: episode: 68, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 185.343, mean reward: 1.853 [1.478, 2.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.953, 10.258], loss: 0.120980, mae: 0.332670, mean_q: 3.874782
  6900/100000: episode: 69, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 186.007, mean reward: 1.860 [1.474, 3.052], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.965, 10.098], loss: 0.103689, mae: 0.308029, mean_q: 3.871850
  7000/100000: episode: 70, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 191.373, mean reward: 1.914 [1.494, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.818, 10.230], loss: 0.108032, mae: 0.306610, mean_q: 3.861580
  7100/100000: episode: 71, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 191.124, mean reward: 1.911 [1.510, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.780, 10.098], loss: 0.115305, mae: 0.318570, mean_q: 3.858700
  7200/100000: episode: 72, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 184.553, mean reward: 1.846 [1.444, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.177, 10.246], loss: 0.105125, mae: 0.312408, mean_q: 3.867950
  7300/100000: episode: 73, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 209.055, mean reward: 2.091 [1.504, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.995, 10.401], loss: 0.111223, mae: 0.318648, mean_q: 3.870523
  7400/100000: episode: 74, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 196.147, mean reward: 1.961 [1.453, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.777, 10.098], loss: 0.099499, mae: 0.309275, mean_q: 3.866791
  7500/100000: episode: 75, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 189.411, mean reward: 1.894 [1.447, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.021, 10.173], loss: 0.094323, mae: 0.308134, mean_q: 3.889009
  7600/100000: episode: 76, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 208.005, mean reward: 2.080 [1.511, 3.719], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.798, 10.228], loss: 0.106089, mae: 0.311196, mean_q: 3.875771
  7700/100000: episode: 77, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 186.513, mean reward: 1.865 [1.480, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.921, 10.142], loss: 0.109921, mae: 0.320803, mean_q: 3.867355
  7800/100000: episode: 78, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 180.219, mean reward: 1.802 [1.477, 2.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.710, 10.098], loss: 0.097149, mae: 0.303741, mean_q: 3.868434
  7900/100000: episode: 79, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 192.374, mean reward: 1.924 [1.436, 6.659], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-2.825, 10.098], loss: 0.104940, mae: 0.310011, mean_q: 3.854798
  8000/100000: episode: 80, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 191.869, mean reward: 1.919 [1.490, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.988, 10.098], loss: 0.108884, mae: 0.317977, mean_q: 3.872833
  8100/100000: episode: 81, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 182.142, mean reward: 1.821 [1.443, 4.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.549, 10.207], loss: 0.099893, mae: 0.309850, mean_q: 3.866423
  8200/100000: episode: 82, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 183.930, mean reward: 1.839 [1.449, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.646, 10.098], loss: 0.118077, mae: 0.327140, mean_q: 3.874054
  8300/100000: episode: 83, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 208.747, mean reward: 2.087 [1.450, 4.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.853, 10.238], loss: 0.105038, mae: 0.313281, mean_q: 3.861153
  8400/100000: episode: 84, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 202.979, mean reward: 2.030 [1.451, 4.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.310, 10.098], loss: 0.111181, mae: 0.314921, mean_q: 3.868014
  8500/100000: episode: 85, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 204.554, mean reward: 2.046 [1.466, 5.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.516, 10.179], loss: 0.099859, mae: 0.311802, mean_q: 3.860131
  8600/100000: episode: 86, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 177.743, mean reward: 1.777 [1.464, 2.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.653, 10.170], loss: 0.117002, mae: 0.322671, mean_q: 3.869169
  8700/100000: episode: 87, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 185.026, mean reward: 1.850 [1.463, 3.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.629, 10.098], loss: 0.099068, mae: 0.301194, mean_q: 3.865352
  8800/100000: episode: 88, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 197.762, mean reward: 1.978 [1.476, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.106, 10.310], loss: 0.130691, mae: 0.326798, mean_q: 3.876442
  8900/100000: episode: 89, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 179.174, mean reward: 1.792 [1.465, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.354, 10.118], loss: 0.108958, mae: 0.317234, mean_q: 3.873462
  9000/100000: episode: 90, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 198.842, mean reward: 1.988 [1.464, 3.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.760, 10.098], loss: 0.111649, mae: 0.324915, mean_q: 3.854210
  9100/100000: episode: 91, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 190.733, mean reward: 1.907 [1.493, 3.259], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.244, 10.098], loss: 0.096478, mae: 0.303163, mean_q: 3.843304
  9200/100000: episode: 92, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 196.794, mean reward: 1.968 [1.492, 3.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.381, 10.098], loss: 0.107317, mae: 0.307674, mean_q: 3.844035
  9300/100000: episode: 93, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 193.048, mean reward: 1.930 [1.470, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.580, 10.160], loss: 0.092048, mae: 0.298640, mean_q: 3.827987
  9400/100000: episode: 94, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 193.943, mean reward: 1.939 [1.478, 3.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.340, 10.208], loss: 0.086817, mae: 0.291780, mean_q: 3.822273
  9500/100000: episode: 95, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 185.547, mean reward: 1.855 [1.460, 4.668], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.872, 10.151], loss: 0.092712, mae: 0.293142, mean_q: 3.812736
  9600/100000: episode: 96, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 221.529, mean reward: 2.215 [1.477, 6.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.559, 10.098], loss: 0.100285, mae: 0.307048, mean_q: 3.814256
  9700/100000: episode: 97, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 195.211, mean reward: 1.952 [1.455, 3.932], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.171, 10.242], loss: 0.088255, mae: 0.296540, mean_q: 3.828168
  9800/100000: episode: 98, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 189.189, mean reward: 1.892 [1.512, 2.672], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.605, 10.162], loss: 0.098094, mae: 0.304649, mean_q: 3.830287
  9900/100000: episode: 99, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 199.819, mean reward: 1.998 [1.440, 3.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.537, 10.237], loss: 0.105293, mae: 0.308151, mean_q: 3.843050
 10000/100000: episode: 100, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 195.436, mean reward: 1.954 [1.455, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.428, 10.320], loss: 0.093088, mae: 0.296687, mean_q: 3.821096
 10100/100000: episode: 101, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 183.735, mean reward: 1.837 [1.443, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.438, 10.098], loss: 0.087992, mae: 0.294222, mean_q: 3.831048
 10200/100000: episode: 102, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 190.693, mean reward: 1.907 [1.469, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.205, 10.135], loss: 0.099803, mae: 0.303582, mean_q: 3.831096
 10300/100000: episode: 103, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 182.757, mean reward: 1.828 [1.469, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.437, 10.236], loss: 0.092216, mae: 0.296798, mean_q: 3.813770
 10400/100000: episode: 104, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 192.497, mean reward: 1.925 [1.443, 5.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.816, 10.146], loss: 0.098983, mae: 0.304080, mean_q: 3.810460
 10500/100000: episode: 105, duration: 0.574s, episode steps: 100, steps per second: 174, episode reward: 185.717, mean reward: 1.857 [1.462, 3.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.905, 10.151], loss: 0.094288, mae: 0.300438, mean_q: 3.833423
 10600/100000: episode: 106, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 192.722, mean reward: 1.927 [1.460, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.270, 10.098], loss: 0.087643, mae: 0.300786, mean_q: 3.819363
 10700/100000: episode: 107, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 192.815, mean reward: 1.928 [1.451, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.648, 10.296], loss: 0.082639, mae: 0.289454, mean_q: 3.809516
 10800/100000: episode: 108, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 200.997, mean reward: 2.010 [1.472, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.228, 10.151], loss: 0.088785, mae: 0.291658, mean_q: 3.810830
 10900/100000: episode: 109, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 204.179, mean reward: 2.042 [1.436, 4.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.200, 10.128], loss: 0.084564, mae: 0.295661, mean_q: 3.813927
 11000/100000: episode: 110, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 180.083, mean reward: 1.801 [1.487, 2.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.110, 10.098], loss: 0.087283, mae: 0.292307, mean_q: 3.809967
 11100/100000: episode: 111, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 188.405, mean reward: 1.884 [1.456, 2.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.353, 10.328], loss: 0.084417, mae: 0.294846, mean_q: 3.815967
 11200/100000: episode: 112, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 189.431, mean reward: 1.894 [1.442, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.620, 10.345], loss: 0.093449, mae: 0.304023, mean_q: 3.807260
 11300/100000: episode: 113, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 190.554, mean reward: 1.906 [1.464, 3.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.214, 10.098], loss: 0.077528, mae: 0.286902, mean_q: 3.807524
 11400/100000: episode: 114, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 191.635, mean reward: 1.916 [1.443, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.657, 10.098], loss: 0.092969, mae: 0.295737, mean_q: 3.809770
 11500/100000: episode: 115, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 192.849, mean reward: 1.928 [1.472, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.338, 10.284], loss: 0.094195, mae: 0.304845, mean_q: 3.825174
 11600/100000: episode: 116, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 230.059, mean reward: 2.301 [1.548, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.091, 10.098], loss: 0.105459, mae: 0.316898, mean_q: 3.828211
 11700/100000: episode: 117, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 181.534, mean reward: 1.815 [1.437, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.689, 10.122], loss: 0.084120, mae: 0.289564, mean_q: 3.810427
 11800/100000: episode: 118, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 183.703, mean reward: 1.837 [1.433, 3.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.448, 10.258], loss: 0.081089, mae: 0.297763, mean_q: 3.824727
 11900/100000: episode: 119, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 182.841, mean reward: 1.828 [1.484, 4.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.404, 10.167], loss: 0.086163, mae: 0.294962, mean_q: 3.823506
 12000/100000: episode: 120, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: 184.980, mean reward: 1.850 [1.453, 2.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.890, 10.258], loss: 0.089392, mae: 0.294514, mean_q: 3.815060
 12100/100000: episode: 121, duration: 0.575s, episode steps: 100, steps per second: 174, episode reward: 186.002, mean reward: 1.860 [1.460, 3.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.833, 10.098], loss: 0.082712, mae: 0.291461, mean_q: 3.810626
 12200/100000: episode: 122, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 188.784, mean reward: 1.888 [1.454, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.674, 10.241], loss: 0.088485, mae: 0.297435, mean_q: 3.826456
 12300/100000: episode: 123, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: 199.788, mean reward: 1.998 [1.490, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.514, 10.204], loss: 0.097337, mae: 0.305180, mean_q: 3.816150
 12400/100000: episode: 124, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 206.735, mean reward: 2.067 [1.449, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.552, 10.098], loss: 0.089852, mae: 0.295924, mean_q: 3.808105
 12500/100000: episode: 125, duration: 0.685s, episode steps: 100, steps per second: 146, episode reward: 196.840, mean reward: 1.968 [1.510, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.839, 10.098], loss: 0.080814, mae: 0.294556, mean_q: 3.822984
 12600/100000: episode: 126, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 194.493, mean reward: 1.945 [1.465, 3.931], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.411, 10.098], loss: 0.082195, mae: 0.289396, mean_q: 3.828955
 12700/100000: episode: 127, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: 212.254, mean reward: 2.123 [1.481, 6.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.356, 10.098], loss: 0.087553, mae: 0.294886, mean_q: 3.835044
 12800/100000: episode: 128, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 188.685, mean reward: 1.887 [1.458, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.806, 10.098], loss: 0.089031, mae: 0.295555, mean_q: 3.829830
 12900/100000: episode: 129, duration: 0.606s, episode steps: 100, steps per second: 165, episode reward: 180.713, mean reward: 1.807 [1.467, 3.104], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.327, 10.098], loss: 0.088364, mae: 0.296076, mean_q: 3.822644
 13000/100000: episode: 130, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 207.186, mean reward: 2.072 [1.452, 3.633], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.496, 10.098], loss: 0.085800, mae: 0.295652, mean_q: 3.814361
 13100/100000: episode: 131, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 197.459, mean reward: 1.975 [1.451, 2.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.884, 10.264], loss: 0.093159, mae: 0.299274, mean_q: 3.828822
 13200/100000: episode: 132, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: 196.253, mean reward: 1.963 [1.467, 4.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.566, 10.322], loss: 0.086388, mae: 0.295294, mean_q: 3.841081
 13300/100000: episode: 133, duration: 0.582s, episode steps: 100, steps per second: 172, episode reward: 199.403, mean reward: 1.994 [1.470, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.689, 10.341], loss: 0.088197, mae: 0.302807, mean_q: 3.838608
 13400/100000: episode: 134, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 191.621, mean reward: 1.916 [1.453, 3.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.118, 10.098], loss: 0.089252, mae: 0.292882, mean_q: 3.825169
 13500/100000: episode: 135, duration: 0.599s, episode steps: 100, steps per second: 167, episode reward: 201.898, mean reward: 2.019 [1.488, 3.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.853, 10.098], loss: 0.083463, mae: 0.295422, mean_q: 3.835924
 13600/100000: episode: 136, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 187.848, mean reward: 1.878 [1.442, 5.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.921, 10.128], loss: 0.094650, mae: 0.303668, mean_q: 3.846494
 13700/100000: episode: 137, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 190.663, mean reward: 1.907 [1.453, 3.071], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.547, 10.098], loss: 0.096898, mae: 0.301383, mean_q: 3.854972
 13800/100000: episode: 138, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 219.282, mean reward: 2.193 [1.483, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.087, 10.098], loss: 0.093262, mae: 0.308123, mean_q: 3.866120
 13900/100000: episode: 139, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 202.012, mean reward: 2.020 [1.495, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.028, 10.098], loss: 0.083273, mae: 0.291350, mean_q: 3.850783
 14000/100000: episode: 140, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: 196.953, mean reward: 1.970 [1.493, 3.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.636, 10.098], loss: 0.080376, mae: 0.286892, mean_q: 3.845956
 14100/100000: episode: 141, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 188.338, mean reward: 1.883 [1.490, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.417, 10.098], loss: 0.095046, mae: 0.302544, mean_q: 3.858094
 14200/100000: episode: 142, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 180.483, mean reward: 1.805 [1.473, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.796, 10.225], loss: 0.094071, mae: 0.301857, mean_q: 3.851542
 14300/100000: episode: 143, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 195.208, mean reward: 1.952 [1.492, 7.073], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.119, 10.136], loss: 0.092846, mae: 0.305047, mean_q: 3.860602
 14400/100000: episode: 144, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 194.341, mean reward: 1.943 [1.449, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.658, 10.098], loss: 0.087215, mae: 0.296358, mean_q: 3.849934
 14500/100000: episode: 145, duration: 0.561s, episode steps: 100, steps per second: 178, episode reward: 181.357, mean reward: 1.814 [1.448, 3.229], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.872, 10.124], loss: 0.084341, mae: 0.300440, mean_q: 3.862382
 14600/100000: episode: 146, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 195.826, mean reward: 1.958 [1.449, 3.974], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.668, 10.206], loss: 0.085471, mae: 0.287965, mean_q: 3.825862
 14700/100000: episode: 147, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 194.843, mean reward: 1.948 [1.464, 4.065], mean action: 0.000 [0.000, 0.000], mean observation: 1.415 [-1.008, 10.098], loss: 0.084796, mae: 0.290069, mean_q: 3.841763
 14800/100000: episode: 148, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 188.206, mean reward: 1.882 [1.476, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.783, 10.235], loss: 0.073005, mae: 0.278285, mean_q: 3.819445
 14900/100000: episode: 149, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 200.292, mean reward: 2.003 [1.470, 5.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.944, 10.098], loss: 0.085380, mae: 0.294055, mean_q: 3.824709
[Info] 1-TH LEVEL FOUND: 4.28068733215332, Considering 10/90 traces
 15000/100000: episode: 150, duration: 5.085s, episode steps: 100, steps per second: 20, episode reward: 194.165, mean reward: 1.942 [1.433, 2.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.416, 10.098], loss: 0.086073, mae: 0.288546, mean_q: 3.830564
 15020/100000: episode: 151, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 59.342, mean reward: 2.967 [1.946, 5.204], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.506, 10.100], loss: 0.063580, mae: 0.268456, mean_q: 3.814053
 15109/100000: episode: 152, duration: 0.465s, episode steps: 89, steps per second: 191, episode reward: 163.916, mean reward: 1.842 [1.443, 2.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.557 [-1.212, 10.100], loss: 0.086568, mae: 0.293979, mean_q: 3.844818
 15129/100000: episode: 153, duration: 0.118s, episode steps: 20, steps per second: 170, episode reward: 60.002, mean reward: 3.000 [2.205, 4.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.169, 10.100], loss: 0.079978, mae: 0.281600, mean_q: 3.816320
 15149/100000: episode: 154, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 42.645, mean reward: 2.132 [1.601, 3.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.054, 10.123], loss: 0.114096, mae: 0.301893, mean_q: 3.862965
 15162/100000: episode: 155, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 37.952, mean reward: 2.919 [2.330, 4.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.173, 10.100], loss: 0.106639, mae: 0.304443, mean_q: 3.840796
 15251/100000: episode: 156, duration: 0.482s, episode steps: 89, steps per second: 184, episode reward: 170.741, mean reward: 1.918 [1.462, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 1.560 [-0.538, 10.100], loss: 0.083742, mae: 0.293968, mean_q: 3.849955
 15264/100000: episode: 157, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 27.246, mean reward: 2.096 [1.724, 2.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.083, 10.100], loss: 0.104848, mae: 0.299771, mean_q: 3.840384
 15353/100000: episode: 158, duration: 0.465s, episode steps: 89, steps per second: 191, episode reward: 162.262, mean reward: 1.823 [1.467, 3.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.555 [-1.430, 10.296], loss: 0.092590, mae: 0.311373, mean_q: 3.848273
 15365/100000: episode: 159, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 29.786, mean reward: 2.482 [2.238, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.630, 10.100], loss: 0.078452, mae: 0.271483, mean_q: 3.817147
 15382/100000: episode: 160, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 46.258, mean reward: 2.721 [2.063, 4.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.190, 10.100], loss: 0.089984, mae: 0.318268, mean_q: 3.863586
 15472/100000: episode: 161, duration: 0.496s, episode steps: 90, steps per second: 181, episode reward: 177.695, mean reward: 1.974 [1.485, 4.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.543 [-2.042, 10.283], loss: 0.105206, mae: 0.311571, mean_q: 3.869292
 15560/100000: episode: 162, duration: 0.472s, episode steps: 88, steps per second: 186, episode reward: 180.244, mean reward: 2.048 [1.439, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.560 [-0.833, 10.100], loss: 0.097228, mae: 0.309285, mean_q: 3.878381
 15580/100000: episode: 163, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 48.015, mean reward: 2.401 [1.994, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.254, 10.100], loss: 0.079195, mae: 0.289967, mean_q: 3.827352
 15592/100000: episode: 164, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 37.466, mean reward: 3.122 [2.295, 7.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.219, 10.100], loss: 0.127530, mae: 0.335904, mean_q: 3.908202
 15605/100000: episode: 165, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 23.847, mean reward: 1.834 [1.497, 2.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.374, 10.100], loss: 0.105879, mae: 0.297072, mean_q: 3.840160
 15695/100000: episode: 166, duration: 0.492s, episode steps: 90, steps per second: 183, episode reward: 187.361, mean reward: 2.082 [1.515, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-0.906, 10.100], loss: 0.099752, mae: 0.312638, mean_q: 3.871713
 15707/100000: episode: 167, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 28.291, mean reward: 2.358 [1.867, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.088, 10.100], loss: 0.129098, mae: 0.336407, mean_q: 3.823581
 15720/100000: episode: 168, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 32.924, mean reward: 2.533 [2.028, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.137, 10.100], loss: 0.066684, mae: 0.273340, mean_q: 3.812414
 15732/100000: episode: 169, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 24.415, mean reward: 2.035 [1.554, 2.721], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.105, 10.100], loss: 0.080798, mae: 0.298379, mean_q: 3.886018
 15745/100000: episode: 170, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 35.895, mean reward: 2.761 [2.236, 5.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.331, 10.100], loss: 0.114733, mae: 0.346086, mean_q: 3.901256
 15758/100000: episode: 171, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 33.063, mean reward: 2.543 [2.203, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.267, 10.100], loss: 0.106849, mae: 0.332023, mean_q: 3.935425
 15775/100000: episode: 172, duration: 0.092s, episode steps: 17, steps per second: 186, episode reward: 47.184, mean reward: 2.776 [2.149, 4.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-1.264, 10.100], loss: 0.086466, mae: 0.304017, mean_q: 3.865689
 15792/100000: episode: 173, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 51.157, mean reward: 3.009 [1.993, 10.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.796, 10.100], loss: 0.119099, mae: 0.308172, mean_q: 3.852279
 15880/100000: episode: 174, duration: 0.467s, episode steps: 88, steps per second: 188, episode reward: 179.899, mean reward: 2.044 [1.470, 4.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.558 [-0.902, 10.100], loss: 0.120143, mae: 0.324022, mean_q: 3.901996
 15893/100000: episode: 175, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 26.745, mean reward: 2.057 [1.696, 2.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.192, 10.100], loss: 0.072273, mae: 0.279454, mean_q: 3.841205
 15981/100000: episode: 176, duration: 0.469s, episode steps: 88, steps per second: 188, episode reward: 180.537, mean reward: 2.052 [1.533, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.566 [-0.435, 10.100], loss: 0.124023, mae: 0.329603, mean_q: 3.888376
 16070/100000: episode: 177, duration: 0.492s, episode steps: 89, steps per second: 181, episode reward: 165.397, mean reward: 1.858 [1.490, 2.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.552 [-0.854, 10.107], loss: 0.098687, mae: 0.315386, mean_q: 3.907259
 16083/100000: episode: 178, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 34.725, mean reward: 2.671 [2.133, 4.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.842, 10.100], loss: 0.108956, mae: 0.320048, mean_q: 3.878924
 16100/100000: episode: 179, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 44.510, mean reward: 2.618 [1.862, 6.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.938, 10.100], loss: 0.107717, mae: 0.315320, mean_q: 3.969612
 16113/100000: episode: 180, duration: 0.079s, episode steps: 13, steps per second: 166, episode reward: 29.037, mean reward: 2.234 [2.008, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.563, 10.100], loss: 0.088438, mae: 0.291021, mean_q: 3.865188
 16133/100000: episode: 181, duration: 0.126s, episode steps: 20, steps per second: 159, episode reward: 42.376, mean reward: 2.119 [1.598, 3.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.669, 10.100], loss: 0.088932, mae: 0.299640, mean_q: 3.928988
 16222/100000: episode: 182, duration: 0.549s, episode steps: 89, steps per second: 162, episode reward: 188.643, mean reward: 2.120 [1.514, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.555 [-1.048, 10.213], loss: 0.108711, mae: 0.323781, mean_q: 3.922755
 16312/100000: episode: 183, duration: 0.723s, episode steps: 90, steps per second: 124, episode reward: 169.712, mean reward: 1.886 [1.452, 5.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.554 [-1.061, 10.281], loss: 0.098957, mae: 0.319941, mean_q: 3.907483
 16324/100000: episode: 184, duration: 0.081s, episode steps: 12, steps per second: 149, episode reward: 30.714, mean reward: 2.560 [2.232, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.362, 10.100], loss: 0.101638, mae: 0.336210, mean_q: 3.946140
 16414/100000: episode: 185, duration: 0.523s, episode steps: 90, steps per second: 172, episode reward: 174.493, mean reward: 1.939 [1.489, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 1.557 [-0.206, 10.100], loss: 0.123548, mae: 0.313179, mean_q: 3.908412
 16431/100000: episode: 186, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 48.957, mean reward: 2.880 [2.195, 4.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.236, 10.100], loss: 0.120021, mae: 0.324703, mean_q: 3.931093
 16521/100000: episode: 187, duration: 0.498s, episode steps: 90, steps per second: 181, episode reward: 163.482, mean reward: 1.816 [1.501, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 1.547 [-1.225, 10.100], loss: 0.110121, mae: 0.328748, mean_q: 3.918154
 16541/100000: episode: 188, duration: 0.118s, episode steps: 20, steps per second: 169, episode reward: 53.049, mean reward: 2.652 [2.197, 3.499], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.726, 10.100], loss: 0.095564, mae: 0.310578, mean_q: 3.864208
 16558/100000: episode: 189, duration: 0.107s, episode steps: 17, steps per second: 158, episode reward: 53.459, mean reward: 3.145 [2.285, 7.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.399, 10.100], loss: 0.108782, mae: 0.351959, mean_q: 3.953871
 16575/100000: episode: 190, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 37.129, mean reward: 2.184 [1.759, 2.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.121, 10.100], loss: 0.102027, mae: 0.330538, mean_q: 3.915815
 16588/100000: episode: 191, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 35.597, mean reward: 2.738 [2.055, 4.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.826, 10.100], loss: 0.108081, mae: 0.318012, mean_q: 3.896419
 16676/100000: episode: 192, duration: 0.555s, episode steps: 88, steps per second: 159, episode reward: 159.815, mean reward: 1.816 [1.453, 2.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.567 [-1.088, 10.100], loss: 0.103014, mae: 0.312507, mean_q: 3.917400
 16688/100000: episode: 193, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 27.111, mean reward: 2.259 [2.079, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.307, 10.100], loss: 0.099966, mae: 0.301237, mean_q: 3.890979
 16777/100000: episode: 194, duration: 0.493s, episode steps: 89, steps per second: 181, episode reward: 170.884, mean reward: 1.920 [1.472, 5.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.554 [-0.583, 10.100], loss: 0.130068, mae: 0.333280, mean_q: 3.941406
 16867/100000: episode: 195, duration: 0.485s, episode steps: 90, steps per second: 186, episode reward: 166.752, mean reward: 1.853 [1.441, 3.312], mean action: 0.000 [0.000, 0.000], mean observation: 1.551 [-0.755, 10.453], loss: 0.103940, mae: 0.318822, mean_q: 3.922359
 16879/100000: episode: 196, duration: 0.068s, episode steps: 12, steps per second: 175, episode reward: 26.839, mean reward: 2.237 [1.864, 2.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.312, 10.100], loss: 0.126830, mae: 0.356474, mean_q: 3.945089
 16968/100000: episode: 197, duration: 0.498s, episode steps: 89, steps per second: 179, episode reward: 187.221, mean reward: 2.104 [1.472, 4.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.554 [-0.521, 10.335], loss: 0.123030, mae: 0.339538, mean_q: 3.949743
 16980/100000: episode: 198, duration: 0.081s, episode steps: 12, steps per second: 149, episode reward: 28.603, mean reward: 2.384 [1.707, 2.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.428, 10.100], loss: 0.167237, mae: 0.321648, mean_q: 3.969085
 17070/100000: episode: 199, duration: 0.488s, episode steps: 90, steps per second: 184, episode reward: 162.266, mean reward: 1.803 [1.483, 2.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.551 [-1.367, 10.195], loss: 0.110972, mae: 0.326618, mean_q: 3.925726
 17158/100000: episode: 200, duration: 0.476s, episode steps: 88, steps per second: 185, episode reward: 167.360, mean reward: 1.902 [1.465, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.571 [-0.436, 10.120], loss: 0.112735, mae: 0.325064, mean_q: 3.935137
 17170/100000: episode: 201, duration: 0.074s, episode steps: 12, steps per second: 161, episode reward: 34.040, mean reward: 2.837 [2.126, 3.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.290, 10.100], loss: 0.095271, mae: 0.314672, mean_q: 3.890450
 17260/100000: episode: 202, duration: 0.481s, episode steps: 90, steps per second: 187, episode reward: 205.981, mean reward: 2.289 [1.536, 4.495], mean action: 0.000 [0.000, 0.000], mean observation: 1.545 [-0.948, 10.272], loss: 0.129343, mae: 0.322232, mean_q: 3.924787
 17350/100000: episode: 203, duration: 0.506s, episode steps: 90, steps per second: 178, episode reward: 192.677, mean reward: 2.141 [1.509, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-1.464, 10.100], loss: 0.130457, mae: 0.323617, mean_q: 3.934693
 17439/100000: episode: 204, duration: 0.475s, episode steps: 89, steps per second: 187, episode reward: 179.475, mean reward: 2.017 [1.433, 3.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.554 [-1.101, 10.100], loss: 0.127871, mae: 0.338089, mean_q: 3.953048
 17451/100000: episode: 205, duration: 0.070s, episode steps: 12, steps per second: 170, episode reward: 23.290, mean reward: 1.941 [1.746, 2.230], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.176, 10.100], loss: 0.207853, mae: 0.368771, mean_q: 3.982219
 17464/100000: episode: 206, duration: 0.078s, episode steps: 13, steps per second: 168, episode reward: 32.031, mean reward: 2.464 [2.014, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.169, 10.100], loss: 0.124130, mae: 0.348513, mean_q: 3.924808
 17476/100000: episode: 207, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 24.181, mean reward: 2.015 [1.764, 2.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.569, 10.100], loss: 0.102030, mae: 0.332763, mean_q: 3.947402
 17489/100000: episode: 208, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 33.767, mean reward: 2.597 [2.065, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.655, 10.100], loss: 0.120762, mae: 0.330638, mean_q: 3.942992
 17579/100000: episode: 209, duration: 0.491s, episode steps: 90, steps per second: 183, episode reward: 174.805, mean reward: 1.942 [1.484, 4.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.542 [-0.696, 10.100], loss: 0.131220, mae: 0.347169, mean_q: 3.959935
 17599/100000: episode: 210, duration: 0.115s, episode steps: 20, steps per second: 174, episode reward: 54.883, mean reward: 2.744 [2.403, 3.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.257, 10.100], loss: 0.106755, mae: 0.337848, mean_q: 3.975426
 17612/100000: episode: 211, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 32.330, mean reward: 2.487 [1.873, 3.920], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.256, 10.100], loss: 0.162030, mae: 0.353182, mean_q: 3.945891
 17701/100000: episode: 212, duration: 0.485s, episode steps: 89, steps per second: 184, episode reward: 194.464, mean reward: 2.185 [1.441, 8.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.559 [-0.549, 10.100], loss: 0.128337, mae: 0.332016, mean_q: 3.947069
 17790/100000: episode: 213, duration: 0.485s, episode steps: 89, steps per second: 184, episode reward: 170.783, mean reward: 1.919 [1.477, 2.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.559 [-0.695, 10.297], loss: 0.118174, mae: 0.329038, mean_q: 3.966004
 17879/100000: episode: 214, duration: 0.489s, episode steps: 89, steps per second: 182, episode reward: 179.225, mean reward: 2.014 [1.442, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 1.556 [-0.616, 10.156], loss: 0.141236, mae: 0.337790, mean_q: 3.981806
 17892/100000: episode: 215, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 31.977, mean reward: 2.460 [2.095, 3.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.155, 10.100], loss: 0.081228, mae: 0.311128, mean_q: 3.947518
 17980/100000: episode: 216, duration: 0.463s, episode steps: 88, steps per second: 190, episode reward: 161.141, mean reward: 1.831 [1.442, 2.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.568 [-0.325, 10.100], loss: 0.138933, mae: 0.339604, mean_q: 3.973470
 17993/100000: episode: 217, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 29.342, mean reward: 2.257 [1.762, 2.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.546, 10.100], loss: 0.122227, mae: 0.334408, mean_q: 3.944102
 18083/100000: episode: 218, duration: 0.476s, episode steps: 90, steps per second: 189, episode reward: 171.654, mean reward: 1.907 [1.447, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.547 [-0.762, 10.100], loss: 0.146540, mae: 0.355002, mean_q: 3.964351
 18103/100000: episode: 219, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 51.531, mean reward: 2.577 [1.759, 5.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.082, 10.100], loss: 0.138189, mae: 0.357583, mean_q: 4.038566
 18193/100000: episode: 220, duration: 0.496s, episode steps: 90, steps per second: 181, episode reward: 168.657, mean reward: 1.874 [1.453, 2.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.540 [-0.327, 10.100], loss: 0.124220, mae: 0.338940, mean_q: 3.992355
 18282/100000: episode: 221, duration: 0.494s, episode steps: 89, steps per second: 180, episode reward: 191.396, mean reward: 2.151 [1.481, 5.482], mean action: 0.000 [0.000, 0.000], mean observation: 1.548 [-0.785, 10.100], loss: 0.134265, mae: 0.336325, mean_q: 3.981747
 18372/100000: episode: 222, duration: 0.481s, episode steps: 90, steps per second: 187, episode reward: 173.549, mean reward: 1.928 [1.482, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.552 [-0.474, 10.100], loss: 0.135822, mae: 0.344486, mean_q: 3.981367
 18461/100000: episode: 223, duration: 0.477s, episode steps: 89, steps per second: 187, episode reward: 183.647, mean reward: 2.063 [1.445, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.543 [-0.675, 10.100], loss: 0.118875, mae: 0.330149, mean_q: 3.982599
 18549/100000: episode: 224, duration: 0.504s, episode steps: 88, steps per second: 175, episode reward: 171.464, mean reward: 1.948 [1.472, 4.177], mean action: 0.000 [0.000, 0.000], mean observation: 1.579 [-0.589, 10.306], loss: 0.156881, mae: 0.359075, mean_q: 4.002373
 18562/100000: episode: 225, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 32.597, mean reward: 2.507 [1.916, 4.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.313, 10.100], loss: 0.087316, mae: 0.289391, mean_q: 3.927480
 18575/100000: episode: 226, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 30.526, mean reward: 2.348 [1.972, 2.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.248, 10.100], loss: 0.157257, mae: 0.354753, mean_q: 3.969005
 18664/100000: episode: 227, duration: 0.493s, episode steps: 89, steps per second: 180, episode reward: 170.364, mean reward: 1.914 [1.502, 3.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.558 [-0.971, 10.100], loss: 0.121393, mae: 0.324681, mean_q: 3.976269
 18752/100000: episode: 228, duration: 0.477s, episode steps: 88, steps per second: 185, episode reward: 162.796, mean reward: 1.850 [1.458, 3.105], mean action: 0.000 [0.000, 0.000], mean observation: 1.567 [-0.684, 10.198], loss: 0.118336, mae: 0.343990, mean_q: 3.957070
 18765/100000: episode: 229, duration: 0.076s, episode steps: 13, steps per second: 172, episode reward: 30.977, mean reward: 2.383 [1.991, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.233, 10.100], loss: 0.131842, mae: 0.336980, mean_q: 3.918210
 18854/100000: episode: 230, duration: 0.480s, episode steps: 89, steps per second: 185, episode reward: 180.452, mean reward: 2.028 [1.478, 3.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.559 [-0.564, 10.175], loss: 0.124143, mae: 0.331990, mean_q: 3.953362
 18943/100000: episode: 231, duration: 0.478s, episode steps: 89, steps per second: 186, episode reward: 172.766, mean reward: 1.941 [1.495, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.548 [-0.865, 10.100], loss: 0.145788, mae: 0.344505, mean_q: 3.993434
 18960/100000: episode: 232, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 38.920, mean reward: 2.289 [1.998, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.226, 10.100], loss: 0.099270, mae: 0.329185, mean_q: 3.937101
 18980/100000: episode: 233, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 49.171, mean reward: 2.459 [1.995, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.361, 10.100], loss: 0.140646, mae: 0.349748, mean_q: 3.985687
 19000/100000: episode: 234, duration: 0.113s, episode steps: 20, steps per second: 178, episode reward: 38.527, mean reward: 1.926 [1.441, 2.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.035, 10.119], loss: 0.148428, mae: 0.355254, mean_q: 3.982285
 19088/100000: episode: 235, duration: 0.482s, episode steps: 88, steps per second: 182, episode reward: 165.054, mean reward: 1.876 [1.466, 2.901], mean action: 0.000 [0.000, 0.000], mean observation: 1.571 [-0.960, 10.130], loss: 0.119096, mae: 0.329339, mean_q: 3.964493
 19178/100000: episode: 236, duration: 0.494s, episode steps: 90, steps per second: 182, episode reward: 161.971, mean reward: 1.800 [1.464, 2.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.553 [-0.376, 10.168], loss: 0.146113, mae: 0.350648, mean_q: 3.974663
 19198/100000: episode: 237, duration: 0.116s, episode steps: 20, steps per second: 173, episode reward: 51.594, mean reward: 2.580 [1.802, 4.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.172, 10.100], loss: 0.225971, mae: 0.369568, mean_q: 4.013276
 19288/100000: episode: 238, duration: 0.511s, episode steps: 90, steps per second: 176, episode reward: 190.576, mean reward: 2.118 [1.479, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.549 [-0.268, 10.190], loss: 0.097961, mae: 0.316213, mean_q: 3.956581
 19376/100000: episode: 239, duration: 0.494s, episode steps: 88, steps per second: 178, episode reward: 210.995, mean reward: 2.398 [1.706, 3.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.557 [-1.413, 10.100], loss: 0.115681, mae: 0.334145, mean_q: 3.979435
[Info] 2-TH LEVEL FOUND: 5.313349723815918, Considering 12/88 traces
 19465/100000: episode: 240, duration: 4.476s, episode steps: 89, steps per second: 20, episode reward: 202.893, mean reward: 2.280 [1.512, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.562 [-0.615, 10.254], loss: 0.124024, mae: 0.347437, mean_q: 4.002627
 19482/100000: episode: 241, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 45.763, mean reward: 2.692 [2.039, 3.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.599, 10.100], loss: 0.101973, mae: 0.329169, mean_q: 3.969967
 19492/100000: episode: 242, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 27.151, mean reward: 2.715 [2.057, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.691, 10.100], loss: 0.125433, mae: 0.324571, mean_q: 3.977169
 19509/100000: episode: 243, duration: 0.098s, episode steps: 17, steps per second: 174, episode reward: 41.994, mean reward: 2.470 [1.701, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.181, 10.100], loss: 0.153896, mae: 0.380850, mean_q: 4.041887
 19526/100000: episode: 244, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 41.436, mean reward: 2.437 [2.064, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-1.524, 10.100], loss: 0.162881, mae: 0.359124, mean_q: 4.033572
 19543/100000: episode: 245, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 38.731, mean reward: 2.278 [1.800, 3.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.236, 10.100], loss: 0.139593, mae: 0.375067, mean_q: 4.029674
 19560/100000: episode: 246, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 37.906, mean reward: 2.230 [1.724, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.175, 10.100], loss: 0.115525, mae: 0.327347, mean_q: 4.006351
 19577/100000: episode: 247, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 46.502, mean reward: 2.735 [2.483, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.290, 10.100], loss: 0.096732, mae: 0.323846, mean_q: 3.977897
 19594/100000: episode: 248, duration: 0.092s, episode steps: 17, steps per second: 186, episode reward: 52.417, mean reward: 3.083 [2.241, 6.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.409, 10.100], loss: 0.134382, mae: 0.368545, mean_q: 4.022247
 19611/100000: episode: 249, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 40.145, mean reward: 2.361 [2.025, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.332, 10.100], loss: 0.094831, mae: 0.325051, mean_q: 4.028950
 19628/100000: episode: 250, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 60.009, mean reward: 3.530 [1.679, 9.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.039, 10.100], loss: 0.139837, mae: 0.355226, mean_q: 4.073738
 19635/100000: episode: 251, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 19.206, mean reward: 2.744 [2.295, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.391, 10.100], loss: 0.183677, mae: 0.371839, mean_q: 4.056663
 19652/100000: episode: 252, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 46.029, mean reward: 2.708 [2.097, 3.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.328, 10.100], loss: 0.185902, mae: 0.353253, mean_q: 4.064805
 19669/100000: episode: 253, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 58.664, mean reward: 3.451 [2.074, 5.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.373, 10.100], loss: 0.182372, mae: 0.391571, mean_q: 4.089738
 19686/100000: episode: 254, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 47.510, mean reward: 2.795 [1.775, 7.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.112, 10.100], loss: 0.129545, mae: 0.350478, mean_q: 4.052564
 19703/100000: episode: 255, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 39.499, mean reward: 2.323 [1.811, 3.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.423, 10.100], loss: 0.111607, mae: 0.338258, mean_q: 4.034459
 19710/100000: episode: 256, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 21.658, mean reward: 3.094 [2.581, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.370, 10.100], loss: 0.189662, mae: 0.365378, mean_q: 4.115910
 19727/100000: episode: 257, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 40.581, mean reward: 2.387 [1.899, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-1.003, 10.100], loss: 0.136005, mae: 0.361844, mean_q: 4.063533
 19734/100000: episode: 258, duration: 0.045s, episode steps: 7, steps per second: 155, episode reward: 20.402, mean reward: 2.915 [2.486, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.324, 10.100], loss: 0.219440, mae: 0.364874, mean_q: 4.100057
 19751/100000: episode: 259, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 39.960, mean reward: 2.351 [1.569, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.203, 10.100], loss: 0.157546, mae: 0.367831, mean_q: 4.080688
 19768/100000: episode: 260, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 43.144, mean reward: 2.538 [2.085, 3.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.029, 10.100], loss: 0.150432, mae: 0.376210, mean_q: 4.092215
 19785/100000: episode: 261, duration: 0.089s, episode steps: 17, steps per second: 191, episode reward: 45.621, mean reward: 2.684 [2.177, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.200, 10.100], loss: 0.091154, mae: 0.329965, mean_q: 4.134727
 19802/100000: episode: 262, duration: 0.103s, episode steps: 17, steps per second: 164, episode reward: 47.926, mean reward: 2.819 [2.247, 5.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.243, 10.100], loss: 0.175492, mae: 0.387654, mean_q: 4.119495
 19819/100000: episode: 263, duration: 0.106s, episode steps: 17, steps per second: 160, episode reward: 44.447, mean reward: 2.615 [1.941, 5.197], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.311, 10.100], loss: 0.154458, mae: 0.393007, mean_q: 4.129547
 19836/100000: episode: 264, duration: 0.092s, episode steps: 17, steps per second: 185, episode reward: 41.551, mean reward: 2.444 [1.815, 5.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.176, 10.100], loss: 0.181989, mae: 0.378443, mean_q: 4.063977
 19846/100000: episode: 265, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 26.300, mean reward: 2.630 [2.217, 4.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.762, 10.100], loss: 0.114897, mae: 0.368097, mean_q: 4.000653
 19863/100000: episode: 266, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 54.443, mean reward: 3.203 [2.338, 4.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.356, 10.100], loss: 0.096725, mae: 0.320892, mean_q: 4.063202
 19880/100000: episode: 267, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 37.109, mean reward: 2.183 [1.844, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.929, 10.100], loss: 0.118942, mae: 0.359765, mean_q: 4.095621
 19897/100000: episode: 268, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 45.029, mean reward: 2.649 [1.930, 5.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.526, 10.100], loss: 0.173037, mae: 0.381623, mean_q: 4.058984
 19914/100000: episode: 269, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 39.616, mean reward: 2.330 [1.562, 5.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.305, 10.100], loss: 0.159363, mae: 0.386298, mean_q: 4.062133
 19931/100000: episode: 270, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 41.247, mean reward: 2.426 [2.123, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.799, 10.100], loss: 0.138267, mae: 0.367733, mean_q: 4.041243
 19948/100000: episode: 271, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 46.957, mean reward: 2.762 [2.277, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.128, 10.100], loss: 0.159823, mae: 0.368712, mean_q: 4.115104
 19965/100000: episode: 272, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 38.461, mean reward: 2.262 [1.834, 3.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.208, 10.100], loss: 0.147889, mae: 0.361822, mean_q: 4.110431
 19975/100000: episode: 273, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 36.457, mean reward: 3.646 [2.772, 5.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.777, 10.100], loss: 0.131160, mae: 0.358687, mean_q: 4.153852
 19992/100000: episode: 274, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 40.500, mean reward: 2.382 [2.034, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.293, 10.100], loss: 0.116215, mae: 0.355406, mean_q: 4.109105
 20009/100000: episode: 275, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 50.269, mean reward: 2.957 [2.043, 7.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.351, 10.100], loss: 0.168728, mae: 0.369829, mean_q: 4.188601
 20016/100000: episode: 276, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 21.852, mean reward: 3.122 [2.504, 4.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.348, 10.100], loss: 0.212499, mae: 0.401588, mean_q: 4.172004
 20033/100000: episode: 277, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 36.131, mean reward: 2.125 [1.805, 3.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.768, 10.100], loss: 0.179916, mae: 0.396447, mean_q: 4.169765
 20050/100000: episode: 278, duration: 0.109s, episode steps: 17, steps per second: 156, episode reward: 43.763, mean reward: 2.574 [1.728, 6.412], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.449, 10.100], loss: 0.121611, mae: 0.360935, mean_q: 4.098757
 20067/100000: episode: 279, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 44.946, mean reward: 2.644 [2.166, 4.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.345, 10.100], loss: 0.171038, mae: 0.402205, mean_q: 4.193953
 20084/100000: episode: 280, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 43.243, mean reward: 2.544 [1.876, 4.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.462, 10.100], loss: 0.133282, mae: 0.349766, mean_q: 4.160554
 20091/100000: episode: 281, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 24.842, mean reward: 3.549 [3.021, 4.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.385, 10.100], loss: 0.310209, mae: 0.433795, mean_q: 4.299444
 20108/100000: episode: 282, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 46.950, mean reward: 2.762 [2.007, 5.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.476, 10.100], loss: 0.126375, mae: 0.348327, mean_q: 4.142725
 20125/100000: episode: 283, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 35.383, mean reward: 2.081 [1.545, 4.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.100, 10.100], loss: 0.197960, mae: 0.395167, mean_q: 4.206740
 20142/100000: episode: 284, duration: 0.100s, episode steps: 17, steps per second: 169, episode reward: 49.344, mean reward: 2.903 [2.402, 3.989], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.669, 10.100], loss: 0.099580, mae: 0.336025, mean_q: 4.088394
 20159/100000: episode: 285, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 41.677, mean reward: 2.452 [1.864, 3.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.273, 10.100], loss: 0.122234, mae: 0.358386, mean_q: 4.126856
 20176/100000: episode: 286, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 44.341, mean reward: 2.608 [1.962, 3.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.890, 10.100], loss: 0.185464, mae: 0.363209, mean_q: 4.205522
 20193/100000: episode: 287, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 34.244, mean reward: 2.014 [1.647, 4.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.348, 10.100], loss: 0.147260, mae: 0.358915, mean_q: 4.170529
 20203/100000: episode: 288, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 28.160, mean reward: 2.816 [2.567, 3.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.481, 10.100], loss: 0.136798, mae: 0.381805, mean_q: 4.145469
 20220/100000: episode: 289, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 53.267, mean reward: 3.133 [2.699, 3.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.135, 10.100], loss: 0.110072, mae: 0.346388, mean_q: 4.169210
 20237/100000: episode: 290, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 37.958, mean reward: 2.233 [1.736, 4.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.247, 10.100], loss: 0.153415, mae: 0.363752, mean_q: 4.142266
 20254/100000: episode: 291, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 50.228, mean reward: 2.955 [2.354, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.274, 10.100], loss: 0.193904, mae: 0.387711, mean_q: 4.123120
 20261/100000: episode: 292, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 21.989, mean reward: 3.141 [2.489, 4.514], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.301, 10.100], loss: 0.172838, mae: 0.373885, mean_q: 4.143684
 20278/100000: episode: 293, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 51.685, mean reward: 3.040 [2.279, 5.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.139, 10.100], loss: 0.190893, mae: 0.402953, mean_q: 4.241268
 20295/100000: episode: 294, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 40.307, mean reward: 2.371 [1.791, 2.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.264, 10.100], loss: 0.126982, mae: 0.346534, mean_q: 4.150477
 20312/100000: episode: 295, duration: 0.106s, episode steps: 17, steps per second: 160, episode reward: 38.498, mean reward: 2.265 [1.827, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.207, 10.100], loss: 0.105765, mae: 0.332815, mean_q: 4.171401
 20319/100000: episode: 296, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 21.414, mean reward: 3.059 [2.625, 4.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.310, 10.100], loss: 0.114263, mae: 0.343440, mean_q: 4.147084
 20336/100000: episode: 297, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 47.802, mean reward: 2.812 [2.025, 7.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.298, 10.100], loss: 0.195294, mae: 0.402843, mean_q: 4.211779
 20353/100000: episode: 298, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 47.492, mean reward: 2.794 [1.753, 5.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.463, 10.100], loss: 0.132272, mae: 0.372545, mean_q: 4.113690
 20370/100000: episode: 299, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 43.297, mean reward: 2.547 [2.058, 3.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.213, 10.100], loss: 0.149995, mae: 0.361018, mean_q: 4.214715
 20387/100000: episode: 300, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 68.526, mean reward: 4.031 [2.155, 11.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.391, 10.100], loss: 0.179825, mae: 0.386390, mean_q: 4.240685
 20404/100000: episode: 301, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 46.927, mean reward: 2.760 [2.157, 3.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.265, 10.100], loss: 0.111569, mae: 0.340472, mean_q: 4.157520
 20421/100000: episode: 302, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 37.831, mean reward: 2.225 [1.792, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.391, 10.100], loss: 0.165576, mae: 0.380550, mean_q: 4.208016
 20438/100000: episode: 303, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 44.299, mean reward: 2.606 [2.021, 3.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.407, 10.100], loss: 0.226900, mae: 0.407695, mean_q: 4.218008
 20448/100000: episode: 304, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 31.151, mean reward: 3.115 [2.290, 4.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.582, 10.100], loss: 0.217631, mae: 0.429801, mean_q: 4.216638
 20465/100000: episode: 305, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 41.913, mean reward: 2.465 [2.065, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.243, 10.100], loss: 0.148325, mae: 0.394835, mean_q: 4.160982
 20482/100000: episode: 306, duration: 0.100s, episode steps: 17, steps per second: 171, episode reward: 49.108, mean reward: 2.889 [2.181, 4.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.822, 10.100], loss: 0.121799, mae: 0.353648, mean_q: 4.120022
 20499/100000: episode: 307, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 43.032, mean reward: 2.531 [2.065, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.221, 10.100], loss: 0.154320, mae: 0.376575, mean_q: 4.233901
 20516/100000: episode: 308, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 52.363, mean reward: 3.080 [2.308, 10.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.935, 10.100], loss: 0.135245, mae: 0.359896, mean_q: 4.171755
 20523/100000: episode: 309, duration: 0.045s, episode steps: 7, steps per second: 154, episode reward: 19.246, mean reward: 2.749 [2.284, 3.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.864, 10.100], loss: 0.110513, mae: 0.346556, mean_q: 4.011439
 20533/100000: episode: 310, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 31.249, mean reward: 3.125 [2.179, 4.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.538, 10.100], loss: 0.134611, mae: 0.371198, mean_q: 4.189652
 20540/100000: episode: 311, duration: 0.042s, episode steps: 7, steps per second: 166, episode reward: 22.808, mean reward: 3.258 [2.473, 5.454], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.227, 10.100], loss: 0.163636, mae: 0.374851, mean_q: 4.248895
 20557/100000: episode: 312, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 39.276, mean reward: 2.310 [1.952, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.533, 10.100], loss: 0.204591, mae: 0.395906, mean_q: 4.202085
 20574/100000: episode: 313, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 49.613, mean reward: 2.918 [2.354, 5.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.203, 10.100], loss: 0.209012, mae: 0.398055, mean_q: 4.294947
 20591/100000: episode: 314, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 40.244, mean reward: 2.367 [1.705, 5.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.915, 10.100], loss: 0.275604, mae: 0.429551, mean_q: 4.218072
 20608/100000: episode: 315, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 44.495, mean reward: 2.617 [2.043, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.250, 10.100], loss: 0.195987, mae: 0.405418, mean_q: 4.294807
 20625/100000: episode: 316, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 48.065, mean reward: 2.827 [2.125, 6.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.185, 10.100], loss: 0.126745, mae: 0.353408, mean_q: 4.230156
 20642/100000: episode: 317, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 46.432, mean reward: 2.731 [2.145, 4.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.478, 10.100], loss: 0.140173, mae: 0.366009, mean_q: 4.239641
 20659/100000: episode: 318, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 45.684, mean reward: 2.687 [2.118, 5.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.251, 10.100], loss: 0.183671, mae: 0.425266, mean_q: 4.260009
 20676/100000: episode: 319, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 39.328, mean reward: 2.313 [1.737, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.315, 10.100], loss: 0.134257, mae: 0.359361, mean_q: 4.151476
 20683/100000: episode: 320, duration: 0.044s, episode steps: 7, steps per second: 158, episode reward: 23.302, mean reward: 3.329 [2.883, 4.322], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.286, 10.100], loss: 0.138537, mae: 0.370744, mean_q: 4.177095
 20700/100000: episode: 321, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 45.716, mean reward: 2.689 [2.165, 3.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.423, 10.100], loss: 0.199060, mae: 0.418486, mean_q: 4.295721
 20717/100000: episode: 322, duration: 0.094s, episode steps: 17, steps per second: 180, episode reward: 39.614, mean reward: 2.330 [1.972, 3.716], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-1.144, 10.100], loss: 0.194881, mae: 0.407975, mean_q: 4.261662
 20734/100000: episode: 323, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 45.274, mean reward: 2.663 [2.220, 3.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.482, 10.100], loss: 0.267199, mae: 0.416235, mean_q: 4.203362
 20751/100000: episode: 324, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 41.316, mean reward: 2.430 [1.979, 3.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.187, 10.100], loss: 0.251415, mae: 0.397511, mean_q: 4.266124
 20758/100000: episode: 325, duration: 0.045s, episode steps: 7, steps per second: 156, episode reward: 19.403, mean reward: 2.772 [2.648, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.397, 10.100], loss: 0.208474, mae: 0.426321, mean_q: 4.360580
 20768/100000: episode: 326, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 33.332, mean reward: 3.333 [2.342, 4.805], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.523, 10.100], loss: 0.177591, mae: 0.381058, mean_q: 4.119737
 20778/100000: episode: 327, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 36.056, mean reward: 3.606 [2.585, 4.437], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.261, 10.100], loss: 0.162658, mae: 0.392489, mean_q: 4.287496
[Info] 3-TH LEVEL FOUND: 5.654399871826172, Considering 10/90 traces
 20795/100000: episode: 328, duration: 4.128s, episode steps: 17, steps per second: 4, episode reward: 40.585, mean reward: 2.387 [1.818, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.651, 10.100], loss: 0.159583, mae: 0.384520, mean_q: 4.235814
 20805/100000: episode: 329, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 47.147, mean reward: 4.715 [2.482, 8.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.546, 10.100], loss: 0.188212, mae: 0.439511, mean_q: 4.345707
 20814/100000: episode: 330, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 32.898, mean reward: 3.655 [2.653, 5.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.400, 10.100], loss: 0.147036, mae: 0.369546, mean_q: 4.172404
 20823/100000: episode: 331, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 20.467, mean reward: 2.274 [2.124, 2.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.431, 10.100], loss: 0.171597, mae: 0.394720, mean_q: 4.221502
 20830/100000: episode: 332, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 21.275, mean reward: 3.039 [2.687, 3.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.507, 10.100], loss: 0.101020, mae: 0.338836, mean_q: 4.155583
 20839/100000: episode: 333, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 25.945, mean reward: 2.883 [2.352, 3.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.403, 10.100], loss: 0.163840, mae: 0.387997, mean_q: 4.236975
 20846/100000: episode: 334, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 21.331, mean reward: 3.047 [2.387, 3.995], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.314, 10.100], loss: 0.170091, mae: 0.412468, mean_q: 4.343930
 20855/100000: episode: 335, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 32.812, mean reward: 3.646 [2.917, 4.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.482, 10.100], loss: 0.376037, mae: 0.472649, mean_q: 4.391486
 20864/100000: episode: 336, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 32.751, mean reward: 3.639 [2.560, 5.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.370, 10.100], loss: 0.219883, mae: 0.424978, mean_q: 4.121738
 20872/100000: episode: 337, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 26.943, mean reward: 3.368 [2.782, 4.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.431, 10.100], loss: 0.179299, mae: 0.464024, mean_q: 4.511712
 20881/100000: episode: 338, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 52.800, mean reward: 5.867 [2.511, 18.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.744, 10.100], loss: 0.186236, mae: 0.375918, mean_q: 4.231589
 20890/100000: episode: 339, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 33.317, mean reward: 3.702 [2.497, 5.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.452, 10.100], loss: 0.139253, mae: 0.385020, mean_q: 4.212669
 20898/100000: episode: 340, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 25.788, mean reward: 3.224 [2.547, 3.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.348, 10.100], loss: 0.135672, mae: 0.376374, mean_q: 4.334698
 20908/100000: episode: 341, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 39.382, mean reward: 3.938 [2.686, 7.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.479, 10.100], loss: 0.121990, mae: 0.348292, mean_q: 4.269263
 20917/100000: episode: 342, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 31.622, mean reward: 3.514 [2.118, 4.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.393, 10.100], loss: 0.149045, mae: 0.372113, mean_q: 4.267798
 20927/100000: episode: 343, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 29.623, mean reward: 2.962 [2.663, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.439, 10.100], loss: 0.582492, mae: 0.503112, mean_q: 4.425052
 20934/100000: episode: 344, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 26.685, mean reward: 3.812 [2.910, 5.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.407, 10.100], loss: 0.170736, mae: 0.411930, mean_q: 3.952062
 20943/100000: episode: 345, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 29.194, mean reward: 3.244 [2.872, 3.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.541, 10.100], loss: 0.352242, mae: 0.547417, mean_q: 4.571332
 20952/100000: episode: 346, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 31.048, mean reward: 3.450 [3.047, 3.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.481, 10.100], loss: 0.125549, mae: 0.362185, mean_q: 4.117109
 20960/100000: episode: 347, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 25.816, mean reward: 3.227 [2.466, 3.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.469, 10.100], loss: 0.299188, mae: 0.458982, mean_q: 4.344244
 20970/100000: episode: 348, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 38.774, mean reward: 3.877 [2.671, 5.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.672, 10.100], loss: 0.210647, mae: 0.445769, mean_q: 4.338780
 20979/100000: episode: 349, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 24.750, mean reward: 2.750 [2.340, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.413, 10.100], loss: 0.177569, mae: 0.377890, mean_q: 4.265300
 20988/100000: episode: 350, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 26.608, mean reward: 2.956 [2.575, 3.478], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.337, 10.100], loss: 0.163909, mae: 0.393576, mean_q: 4.332860
 20997/100000: episode: 351, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 25.676, mean reward: 2.853 [2.403, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.486, 10.100], loss: 0.287475, mae: 0.430483, mean_q: 4.309965
 21006/100000: episode: 352, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 29.125, mean reward: 3.236 [2.789, 4.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.409, 10.100], loss: 0.528281, mae: 0.397911, mean_q: 4.270216
 21015/100000: episode: 353, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 26.020, mean reward: 2.891 [2.501, 3.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.957, 10.100], loss: 0.307327, mae: 0.485185, mean_q: 4.302818
 21024/100000: episode: 354, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 38.089, mean reward: 4.232 [3.427, 5.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.661, 10.100], loss: 0.231018, mae: 0.488458, mean_q: 4.558639
 21033/100000: episode: 355, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 39.399, mean reward: 4.378 [2.432, 5.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.655, 10.100], loss: 0.247668, mae: 0.470711, mean_q: 4.306567
 21041/100000: episode: 356, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 28.517, mean reward: 3.565 [2.982, 4.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.415, 10.100], loss: 0.669864, mae: 0.435888, mean_q: 4.356041
 21050/100000: episode: 357, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 36.873, mean reward: 4.097 [2.449, 8.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.525, 10.100], loss: 0.207924, mae: 0.455669, mean_q: 4.411349
 21060/100000: episode: 358, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 37.571, mean reward: 3.757 [2.670, 5.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.455, 10.100], loss: 0.150286, mae: 0.365967, mean_q: 4.324551
 21067/100000: episode: 359, duration: 0.046s, episode steps: 7, steps per second: 153, episode reward: 22.694, mean reward: 3.242 [2.743, 4.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.352, 10.100], loss: 0.263395, mae: 0.415002, mean_q: 4.399628
 21076/100000: episode: 360, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 30.538, mean reward: 3.393 [2.556, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.512, 10.100], loss: 0.149680, mae: 0.381472, mean_q: 4.208136
 21083/100000: episode: 361, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 20.095, mean reward: 2.871 [2.623, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.320, 10.100], loss: 0.214294, mae: 0.393886, mean_q: 4.458972
 21092/100000: episode: 362, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 28.185, mean reward: 3.132 [2.507, 3.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.698, 10.100], loss: 0.247440, mae: 0.406203, mean_q: 4.359355
 21101/100000: episode: 363, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 31.684, mean reward: 3.520 [2.538, 7.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.372, 10.100], loss: 0.223595, mae: 0.465750, mean_q: 4.485042
 21110/100000: episode: 364, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 27.516, mean reward: 3.057 [2.506, 4.186], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.397, 10.100], loss: 0.236454, mae: 0.433492, mean_q: 4.402302
 21119/100000: episode: 365, duration: 0.051s, episode steps: 9, steps per second: 176, episode reward: 24.153, mean reward: 2.684 [2.375, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.469, 10.100], loss: 0.146958, mae: 0.393499, mean_q: 4.292665
 21128/100000: episode: 366, duration: 0.052s, episode steps: 9, steps per second: 175, episode reward: 26.178, mean reward: 2.909 [2.202, 4.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.375, 10.100], loss: 0.184868, mae: 0.419689, mean_q: 4.428200
 21137/100000: episode: 367, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 24.890, mean reward: 2.766 [2.414, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.466, 10.100], loss: 0.125718, mae: 0.353868, mean_q: 4.300670
 21146/100000: episode: 368, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 29.217, mean reward: 3.246 [2.326, 5.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.709, 10.100], loss: 0.179033, mae: 0.377409, mean_q: 4.363420
 21155/100000: episode: 369, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 30.422, mean reward: 3.380 [2.309, 4.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.361, 10.100], loss: 0.229116, mae: 0.409965, mean_q: 4.385654
 21164/100000: episode: 370, duration: 0.048s, episode steps: 9, steps per second: 186, episode reward: 29.595, mean reward: 3.288 [2.586, 4.005], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-1.697, 10.100], loss: 0.150847, mae: 0.404965, mean_q: 4.458472
 21171/100000: episode: 371, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 16.873, mean reward: 2.410 [2.142, 2.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.275, 10.100], loss: 0.253102, mae: 0.514471, mean_q: 4.478827
 21178/100000: episode: 372, duration: 0.042s, episode steps: 7, steps per second: 169, episode reward: 18.896, mean reward: 2.699 [2.396, 2.969], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.376, 10.100], loss: 0.291209, mae: 0.446812, mean_q: 4.433478
 21187/100000: episode: 373, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 24.410, mean reward: 2.712 [2.149, 3.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.491, 10.100], loss: 0.194607, mae: 0.413357, mean_q: 4.308586
 21196/100000: episode: 374, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 27.875, mean reward: 3.097 [2.634, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.490, 10.100], loss: 0.192933, mae: 0.424622, mean_q: 4.436666
 21205/100000: episode: 375, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 25.983, mean reward: 2.887 [2.160, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.473, 10.100], loss: 0.169911, mae: 0.419531, mean_q: 4.399710
 21214/100000: episode: 376, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 30.398, mean reward: 3.378 [2.194, 4.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.368, 10.100], loss: 0.187718, mae: 0.425021, mean_q: 4.423876
 21223/100000: episode: 377, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 21.312, mean reward: 2.368 [2.014, 2.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.438, 10.100], loss: 0.231067, mae: 0.457655, mean_q: 4.421849
 21232/100000: episode: 378, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 28.708, mean reward: 3.190 [2.289, 5.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.351, 10.100], loss: 0.175191, mae: 0.397294, mean_q: 4.346176
 21241/100000: episode: 379, duration: 0.053s, episode steps: 9, steps per second: 168, episode reward: 28.805, mean reward: 3.201 [2.386, 4.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.374, 10.100], loss: 0.117151, mae: 0.367034, mean_q: 4.408845
 21249/100000: episode: 380, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 27.637, mean reward: 3.455 [2.796, 4.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.438, 10.100], loss: 0.126714, mae: 0.372607, mean_q: 4.535099
 21257/100000: episode: 381, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 24.344, mean reward: 3.043 [2.690, 3.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.287, 10.100], loss: 0.177405, mae: 0.415870, mean_q: 4.384588
 21265/100000: episode: 382, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 26.101, mean reward: 3.263 [2.780, 3.636], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.388, 10.100], loss: 0.112358, mae: 0.361608, mean_q: 4.391405
 21273/100000: episode: 383, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 26.255, mean reward: 3.282 [2.704, 3.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.396, 10.100], loss: 0.182541, mae: 0.407100, mean_q: 4.412754
 21280/100000: episode: 384, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 28.911, mean reward: 4.130 [3.321, 4.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.406, 10.100], loss: 0.182049, mae: 0.425613, mean_q: 4.350452
 21288/100000: episode: 385, duration: 0.048s, episode steps: 8, steps per second: 165, episode reward: 24.147, mean reward: 3.018 [2.557, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.064, 10.100], loss: 0.136017, mae: 0.402984, mean_q: 4.461057
 21297/100000: episode: 386, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 27.340, mean reward: 3.038 [2.247, 4.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.529, 10.100], loss: 0.235799, mae: 0.415141, mean_q: 4.553934
 21304/100000: episode: 387, duration: 0.038s, episode steps: 7, steps per second: 182, episode reward: 20.935, mean reward: 2.991 [2.701, 3.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.392, 10.100], loss: 0.182011, mae: 0.424173, mean_q: 4.466573
 21313/100000: episode: 388, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 23.494, mean reward: 2.610 [2.196, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.372, 10.100], loss: 0.141721, mae: 0.404064, mean_q: 4.386252
 21321/100000: episode: 389, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 26.655, mean reward: 3.332 [2.699, 4.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.366, 10.100], loss: 0.209076, mae: 0.415938, mean_q: 4.504473
 21329/100000: episode: 390, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 32.496, mean reward: 4.062 [3.323, 5.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.494, 10.100], loss: 0.159782, mae: 0.396874, mean_q: 4.447047
 21337/100000: episode: 391, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 27.226, mean reward: 3.403 [2.685, 4.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.433, 10.100], loss: 0.193832, mae: 0.414976, mean_q: 4.375916
 21345/100000: episode: 392, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 23.155, mean reward: 2.894 [2.482, 3.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.627, 10.100], loss: 0.173276, mae: 0.434155, mean_q: 4.584189
 21353/100000: episode: 393, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 25.218, mean reward: 3.152 [2.892, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.307, 10.100], loss: 0.180460, mae: 0.417945, mean_q: 4.431267
 21362/100000: episode: 394, duration: 0.056s, episode steps: 9, steps per second: 159, episode reward: 26.714, mean reward: 2.968 [2.458, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.423, 10.100], loss: 0.318420, mae: 0.442643, mean_q: 4.494613
 21371/100000: episode: 395, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 61.673, mean reward: 6.853 [2.852, 18.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.404, 10.100], loss: 0.275484, mae: 0.427247, mean_q: 4.537448
 21381/100000: episode: 396, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 34.241, mean reward: 3.424 [2.503, 4.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.382, 10.100], loss: 0.284570, mae: 0.428694, mean_q: 4.406329
 21390/100000: episode: 397, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 29.759, mean reward: 3.307 [2.366, 4.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.403, 10.100], loss: 0.207658, mae: 0.470169, mean_q: 4.513120
 21398/100000: episode: 398, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 28.114, mean reward: 3.514 [2.624, 4.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.433, 10.100], loss: 0.242386, mae: 0.504232, mean_q: 4.559173
 21407/100000: episode: 399, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 26.470, mean reward: 2.941 [2.403, 3.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.499, 10.100], loss: 0.239563, mae: 0.448480, mean_q: 4.490797
 21416/100000: episode: 400, duration: 0.054s, episode steps: 9, steps per second: 165, episode reward: 28.876, mean reward: 3.208 [2.365, 6.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.775, 10.100], loss: 0.203124, mae: 0.456685, mean_q: 4.627130
 21425/100000: episode: 401, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 26.886, mean reward: 2.987 [2.385, 4.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.477, 10.100], loss: 0.249048, mae: 0.435306, mean_q: 4.456950
 21433/100000: episode: 402, duration: 0.053s, episode steps: 8, steps per second: 152, episode reward: 26.863, mean reward: 3.358 [2.628, 4.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.434, 10.100], loss: 0.169627, mae: 0.398361, mean_q: 4.513303
 21442/100000: episode: 403, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 37.933, mean reward: 4.215 [2.844, 6.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.489, 10.100], loss: 0.157993, mae: 0.368107, mean_q: 4.469589
 21450/100000: episode: 404, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 19.790, mean reward: 2.474 [2.311, 2.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.264, 10.100], loss: 0.208014, mae: 0.467503, mean_q: 4.699740
 21460/100000: episode: 405, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 31.555, mean reward: 3.156 [2.189, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.366, 10.100], loss: 0.171948, mae: 0.423851, mean_q: 4.568370
 21468/100000: episode: 406, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 23.655, mean reward: 2.957 [2.511, 3.199], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.488, 10.100], loss: 0.338173, mae: 0.455737, mean_q: 4.547973
 21477/100000: episode: 407, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 29.944, mean reward: 3.327 [2.580, 3.970], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.904, 10.100], loss: 0.205701, mae: 0.470509, mean_q: 4.655166
 21485/100000: episode: 408, duration: 0.047s, episode steps: 8, steps per second: 172, episode reward: 28.232, mean reward: 3.529 [2.795, 4.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.319, 10.100], loss: 0.176776, mae: 0.430833, mean_q: 4.494144
 21494/100000: episode: 409, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 27.928, mean reward: 3.103 [2.869, 3.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.439, 10.100], loss: 0.648644, mae: 0.508332, mean_q: 4.478339
 21503/100000: episode: 410, duration: 0.051s, episode steps: 9, steps per second: 178, episode reward: 27.798, mean reward: 3.089 [2.691, 4.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.356, 10.100], loss: 0.657331, mae: 0.543878, mean_q: 4.718714
 21512/100000: episode: 411, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 25.654, mean reward: 2.850 [2.176, 3.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.452, 10.100], loss: 0.376637, mae: 0.590665, mean_q: 4.430614
 21521/100000: episode: 412, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 34.008, mean reward: 3.779 [2.170, 4.938], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.434, 10.100], loss: 0.229667, mae: 0.506101, mean_q: 4.812964
 21530/100000: episode: 413, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 33.186, mean reward: 3.687 [2.634, 5.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.522, 10.100], loss: 0.329418, mae: 0.543457, mean_q: 4.721097
 21539/100000: episode: 414, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 31.601, mean reward: 3.511 [2.879, 5.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.387, 10.100], loss: 0.224781, mae: 0.429325, mean_q: 4.437800
 21548/100000: episode: 415, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 29.686, mean reward: 3.298 [2.478, 4.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.454, 10.100], loss: 0.187233, mae: 0.434726, mean_q: 4.550325
 21558/100000: episode: 416, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 25.574, mean reward: 2.557 [2.178, 3.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.372, 10.100], loss: 0.244963, mae: 0.484070, mean_q: 4.698754
 21567/100000: episode: 417, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 25.659, mean reward: 2.851 [2.426, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.378, 10.100], loss: 0.165291, mae: 0.420953, mean_q: 4.697000
[Info] 4-TH LEVEL FOUND: 7.210341930389404, Considering 10/90 traces
 21574/100000: episode: 418, duration: 4.097s, episode steps: 7, steps per second: 2, episode reward: 22.790, mean reward: 3.256 [2.738, 4.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.329, 10.100], loss: 0.770037, mae: 0.543904, mean_q: 4.660084
 21582/100000: episode: 419, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 25.558, mean reward: 3.195 [2.481, 4.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.414, 10.100], loss: 0.160987, mae: 0.396438, mean_q: 4.461349
 21588/100000: episode: 420, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 23.612, mean reward: 3.935 [3.362, 4.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.582, 10.100], loss: 0.803202, mae: 0.583441, mean_q: 4.793517
 21594/100000: episode: 421, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 17.612, mean reward: 2.935 [2.520, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.477, 10.100], loss: 0.304430, mae: 0.482788, mean_q: 4.260088
 21601/100000: episode: 422, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 25.719, mean reward: 3.674 [2.793, 5.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.384, 10.100], loss: 0.312469, mae: 0.455998, mean_q: 4.733060
 21609/100000: episode: 423, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 32.020, mean reward: 4.002 [3.041, 5.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-1.071, 10.100], loss: 0.405669, mae: 0.519162, mean_q: 4.573399
 21614/100000: episode: 424, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 16.297, mean reward: 3.259 [2.754, 4.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.446, 10.100], loss: 0.510550, mae: 0.583578, mean_q: 4.817536
 21618/100000: episode: 425, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 12.602, mean reward: 3.151 [2.835, 3.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.477, 10.100], loss: 0.322258, mae: 0.475046, mean_q: 4.436990
 21623/100000: episode: 426, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 16.837, mean reward: 3.367 [3.052, 3.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.621, 10.100], loss: 0.153617, mae: 0.450032, mean_q: 4.641228
 21628/100000: episode: 427, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 23.662, mean reward: 4.732 [3.872, 5.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.494, 10.100], loss: 0.314995, mae: 0.456576, mean_q: 4.428660
 21634/100000: episode: 428, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 36.693, mean reward: 6.116 [3.443, 10.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.735, 10.100], loss: 0.308696, mae: 0.535253, mean_q: 4.698473
 21639/100000: episode: 429, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 33.163, mean reward: 6.633 [4.309, 11.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.467, 10.100], loss: 0.175771, mae: 0.405660, mean_q: 4.434035
 21645/100000: episode: 430, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 25.708, mean reward: 4.285 [3.135, 6.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.451, 10.100], loss: 0.375926, mae: 0.563858, mean_q: 4.758140
 21649/100000: episode: 431, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: 15.127, mean reward: 3.782 [3.233, 4.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.562, 10.100], loss: 0.163938, mae: 0.401676, mean_q: 4.581463
 21656/100000: episode: 432, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 30.486, mean reward: 4.355 [4.069, 5.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.874, 10.100], loss: 0.311329, mae: 0.497872, mean_q: 4.718832
 21662/100000: episode: 433, duration: 0.034s, episode steps: 6, steps per second: 177, episode reward: 30.613, mean reward: 5.102 [3.608, 6.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.517, 10.100], loss: 0.248024, mae: 0.457312, mean_q: 4.385111
 21668/100000: episode: 434, duration: 0.041s, episode steps: 6, steps per second: 147, episode reward: 32.165, mean reward: 5.361 [4.249, 8.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.647, 10.100], loss: 0.262907, mae: 0.484977, mean_q: 4.773531
 21674/100000: episode: 435, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 29.417, mean reward: 4.903 [3.478, 6.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.612, 10.100], loss: 0.137908, mae: 0.402541, mean_q: 4.483483
 21679/100000: episode: 436, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 20.449, mean reward: 4.090 [3.397, 4.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.516, 10.100], loss: 0.317805, mae: 0.551623, mean_q: 4.879797
 21684/100000: episode: 437, duration: 0.027s, episode steps: 5, steps per second: 187, episode reward: 14.863, mean reward: 2.973 [2.689, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.516, 10.100], loss: 0.309883, mae: 0.509215, mean_q: 4.489357
 21688/100000: episode: 438, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 13.453, mean reward: 3.363 [2.604, 4.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.525, 10.100], loss: 0.401748, mae: 0.573807, mean_q: 4.640333
 21694/100000: episode: 439, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 32.599, mean reward: 5.433 [4.307, 6.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.516, 10.100], loss: 0.196075, mae: 0.481665, mean_q: 4.771840
 21699/100000: episode: 440, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 21.441, mean reward: 4.288 [3.739, 5.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.535, 10.100], loss: 0.330039, mae: 0.513327, mean_q: 4.691231
 21703/100000: episode: 441, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 13.315, mean reward: 3.329 [2.750, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.872, 10.100], loss: 0.221844, mae: 0.497320, mean_q: 4.849904
 21708/100000: episode: 442, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 26.468, mean reward: 5.294 [4.703, 6.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.578, 10.100], loss: 0.182031, mae: 0.424996, mean_q: 4.578265
 21713/100000: episode: 443, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 17.274, mean reward: 3.455 [2.995, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.570, 10.100], loss: 1.005830, mae: 0.650650, mean_q: 4.812572
 21720/100000: episode: 444, duration: 0.043s, episode steps: 7, steps per second: 164, episode reward: 28.932, mean reward: 4.133 [3.181, 5.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.407, 10.100], loss: 0.190833, mae: 0.423551, mean_q: 4.510662
 21726/100000: episode: 445, duration: 0.038s, episode steps: 6, steps per second: 160, episode reward: 19.678, mean reward: 3.280 [2.831, 4.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.504, 10.100], loss: 0.248852, mae: 0.469366, mean_q: 4.800292
 21731/100000: episode: 446, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 21.913, mean reward: 4.383 [3.384, 5.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.528, 10.100], loss: 0.144747, mae: 0.416618, mean_q: 4.689941
 21735/100000: episode: 447, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 12.563, mean reward: 3.141 [2.874, 3.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-1.444, 10.100], loss: 0.323238, mae: 0.510200, mean_q: 4.628969
 21739/100000: episode: 448, duration: 0.031s, episode steps: 4, steps per second: 130, episode reward: 10.399, mean reward: 2.600 [2.418, 2.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.461, 10.100], loss: 0.297983, mae: 0.529912, mean_q: 4.895792
 21744/100000: episode: 449, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 14.781, mean reward: 2.956 [2.749, 3.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.470, 10.100], loss: 0.166417, mae: 0.395594, mean_q: 4.618398
 21749/100000: episode: 450, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 20.652, mean reward: 4.130 [3.343, 4.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.510, 10.100], loss: 0.149057, mae: 0.394712, mean_q: 4.632810
 21754/100000: episode: 451, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 15.969, mean reward: 3.194 [2.745, 4.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.576, 10.100], loss: 0.237506, mae: 0.446641, mean_q: 4.734845
 21760/100000: episode: 452, duration: 0.034s, episode steps: 6, steps per second: 178, episode reward: 39.385, mean reward: 6.564 [3.994, 9.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.620, 10.100], loss: 0.907106, mae: 0.600473, mean_q: 4.935337
 21768/100000: episode: 453, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 24.640, mean reward: 3.080 [2.166, 4.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.425, 10.100], loss: 0.185587, mae: 0.416441, mean_q: 4.436445
 21773/100000: episode: 454, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 25.976, mean reward: 5.195 [3.860, 7.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.677, 10.100], loss: 0.376579, mae: 0.595269, mean_q: 4.737999
 21779/100000: episode: 455, duration: 0.043s, episode steps: 6, steps per second: 139, episode reward: 32.765, mean reward: 5.461 [3.252, 7.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.509, 10.100], loss: 0.797976, mae: 0.571009, mean_q: 4.881540
 21784/100000: episode: 456, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 24.614, mean reward: 4.923 [3.599, 8.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.592, 10.100], loss: 0.213974, mae: 0.415652, mean_q: 4.417977
 21788/100000: episode: 457, duration: 0.030s, episode steps: 4, steps per second: 131, episode reward: 12.935, mean reward: 3.234 [2.670, 3.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.612, 10.100], loss: 0.338068, mae: 0.505430, mean_q: 4.610586
 21793/100000: episode: 458, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 15.714, mean reward: 3.143 [2.863, 3.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.546, 10.100], loss: 0.249883, mae: 0.528409, mean_q: 4.923492
 21798/100000: episode: 459, duration: 0.034s, episode steps: 5, steps per second: 146, episode reward: 23.127, mean reward: 4.625 [3.122, 7.006], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.510, 10.100], loss: 0.324419, mae: 0.527203, mean_q: 4.694682
 21802/100000: episode: 460, duration: 0.027s, episode steps: 4, steps per second: 150, episode reward: 12.731, mean reward: 3.183 [2.664, 3.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.537, 10.100], loss: 0.285973, mae: 0.533804, mean_q: 4.901049
 21807/100000: episode: 461, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 26.601, mean reward: 5.320 [3.902, 9.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.732, 10.100], loss: 0.294558, mae: 0.480245, mean_q: 4.602708
 21815/100000: episode: 462, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 29.294, mean reward: 3.662 [2.738, 4.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.471, 10.100], loss: 0.192635, mae: 0.449008, mean_q: 4.843718
 21820/100000: episode: 463, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 19.693, mean reward: 3.939 [3.004, 6.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.511, 10.100], loss: 0.361202, mae: 0.502046, mean_q: 4.652703
 21825/100000: episode: 464, duration: 0.029s, episode steps: 5, steps per second: 171, episode reward: 14.545, mean reward: 2.909 [2.395, 4.044], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.467, 10.100], loss: 0.287405, mae: 0.458596, mean_q: 4.745348
 21830/100000: episode: 465, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 14.957, mean reward: 2.991 [2.725, 3.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.380, 10.100], loss: 0.176214, mae: 0.421008, mean_q: 4.728198
 21838/100000: episode: 466, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 22.562, mean reward: 2.820 [2.247, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.325, 10.100], loss: 0.269328, mae: 0.485656, mean_q: 4.818677
 21842/100000: episode: 467, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 14.253, mean reward: 3.563 [2.794, 4.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.877, 10.100], loss: 0.269641, mae: 0.454365, mean_q: 4.650540
 21847/100000: episode: 468, duration: 0.034s, episode steps: 5, steps per second: 146, episode reward: 15.411, mean reward: 3.082 [2.818, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.445, 10.100], loss: 1.126492, mae: 0.626447, mean_q: 4.802392
 21851/100000: episode: 469, duration: 0.028s, episode steps: 4, steps per second: 143, episode reward: 15.682, mean reward: 3.920 [3.517, 4.336], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.541, 10.100], loss: 0.285578, mae: 0.567368, mean_q: 5.229502
 21857/100000: episode: 470, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: 19.467, mean reward: 3.245 [3.065, 3.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.536, 10.100], loss: 0.444629, mae: 0.527172, mean_q: 4.604394
 21862/100000: episode: 471, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 23.287, mean reward: 4.657 [4.077, 5.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.441, 10.100], loss: 0.252838, mae: 0.466650, mean_q: 4.708766
 21869/100000: episode: 472, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 21.954, mean reward: 3.136 [2.917, 3.485], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.386, 10.100], loss: 0.683562, mae: 0.521678, mean_q: 4.573256
 21874/100000: episode: 473, duration: 0.035s, episode steps: 5, steps per second: 144, episode reward: 23.609, mean reward: 4.722 [3.303, 6.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.599, 10.100], loss: 0.265110, mae: 0.555008, mean_q: 4.922942
 21879/100000: episode: 474, duration: 0.028s, episode steps: 5, steps per second: 177, episode reward: 17.266, mean reward: 3.453 [3.292, 3.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.441, 10.100], loss: 0.497700, mae: 0.675427, mean_q: 4.595294
 21884/100000: episode: 475, duration: 0.033s, episode steps: 5, steps per second: 154, episode reward: 36.534, mean reward: 7.307 [4.953, 10.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.627, 10.100], loss: 0.469845, mae: 0.671276, mean_q: 5.048764
 21889/100000: episode: 476, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 19.876, mean reward: 3.975 [3.096, 4.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.505, 10.100], loss: 0.502881, mae: 0.529366, mean_q: 4.555892
 21894/100000: episode: 477, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 22.338, mean reward: 4.468 [3.285, 6.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.557, 10.100], loss: 0.324638, mae: 0.611752, mean_q: 5.185399
 21898/100000: episode: 478, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 10.064, mean reward: 2.516 [2.213, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.492, 10.100], loss: 0.226084, mae: 0.487694, mean_q: 4.735869
 21904/100000: episode: 479, duration: 0.041s, episode steps: 6, steps per second: 146, episode reward: 32.033, mean reward: 5.339 [4.169, 7.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.562, 10.100], loss: 0.785775, mae: 0.525997, mean_q: 4.759342
 21911/100000: episode: 480, duration: 0.047s, episode steps: 7, steps per second: 148, episode reward: 31.182, mean reward: 4.455 [3.574, 7.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.429, 10.100], loss: 0.380877, mae: 0.585072, mean_q: 4.708878
 21917/100000: episode: 481, duration: 0.041s, episode steps: 6, steps per second: 146, episode reward: 24.005, mean reward: 4.001 [3.308, 5.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.512, 10.100], loss: 0.513360, mae: 0.599573, mean_q: 4.702094
 21923/100000: episode: 482, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 26.601, mean reward: 4.433 [4.013, 4.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.493, 10.100], loss: 0.370201, mae: 0.562889, mean_q: 4.825804
 21929/100000: episode: 483, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 19.193, mean reward: 3.199 [2.510, 4.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.848, 10.100], loss: 1.522517, mae: 0.665105, mean_q: 4.879967
 21937/100000: episode: 484, duration: 0.049s, episode steps: 8, steps per second: 165, episode reward: 23.365, mean reward: 2.921 [2.603, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.261, 10.100], loss: 0.298049, mae: 0.533484, mean_q: 4.935268
 21941/100000: episode: 485, duration: 0.026s, episode steps: 4, steps per second: 152, episode reward: 17.497, mean reward: 4.374 [2.712, 6.326], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.608, 10.100], loss: 0.300598, mae: 0.526957, mean_q: 4.891335
 21945/100000: episode: 486, duration: 0.027s, episode steps: 4, steps per second: 148, episode reward: 13.002, mean reward: 3.251 [3.099, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.479, 10.100], loss: 0.482496, mae: 0.579073, mean_q: 4.638221
 21950/100000: episode: 487, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 20.282, mean reward: 4.056 [3.295, 5.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.691, 10.100], loss: 0.209837, mae: 0.449514, mean_q: 4.659340
 21955/100000: episode: 488, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 17.242, mean reward: 3.448 [3.033, 3.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.377, 10.100], loss: 0.267526, mae: 0.521004, mean_q: 4.830934
 21960/100000: episode: 489, duration: 0.035s, episode steps: 5, steps per second: 143, episode reward: 21.495, mean reward: 4.299 [3.571, 5.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.512, 10.100], loss: 0.279460, mae: 0.529121, mean_q: 4.837513
 21965/100000: episode: 490, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 24.270, mean reward: 4.854 [3.401, 6.143], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.489, 10.100], loss: 0.215174, mae: 0.402830, mean_q: 4.846179
 21969/100000: episode: 491, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: 13.324, mean reward: 3.331 [2.831, 4.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.516, 10.100], loss: 0.334285, mae: 0.561911, mean_q: 4.616661
 21975/100000: episode: 492, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 21.319, mean reward: 3.553 [3.176, 4.076], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.393, 10.100], loss: 0.504250, mae: 0.545406, mean_q: 4.831856
 21981/100000: episode: 493, duration: 0.033s, episode steps: 6, steps per second: 182, episode reward: 17.856, mean reward: 2.976 [2.703, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.377, 10.100], loss: 0.319209, mae: 0.507713, mean_q: 4.821260
 21985/100000: episode: 494, duration: 0.026s, episode steps: 4, steps per second: 151, episode reward: 14.228, mean reward: 3.557 [3.233, 3.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.539, 10.100], loss: 0.415744, mae: 0.527905, mean_q: 4.970761
 21991/100000: episode: 495, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 37.034, mean reward: 6.172 [3.920, 12.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.665, 10.100], loss: 0.192936, mae: 0.467390, mean_q: 4.581260
 21995/100000: episode: 496, duration: 0.028s, episode steps: 4, steps per second: 144, episode reward: 18.178, mean reward: 4.545 [3.174, 5.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.600, 10.100], loss: 0.196711, mae: 0.459092, mean_q: 5.005523
 22000/100000: episode: 497, duration: 0.029s, episode steps: 5, steps per second: 171, episode reward: 19.024, mean reward: 3.805 [3.526, 4.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.412, 10.100], loss: 1.105459, mae: 0.561414, mean_q: 4.864093
 22004/100000: episode: 498, duration: 0.027s, episode steps: 4, steps per second: 149, episode reward: 12.393, mean reward: 3.098 [2.726, 3.959], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.582, 10.100], loss: 0.314874, mae: 0.558553, mean_q: 5.036448
 22012/100000: episode: 499, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 33.462, mean reward: 4.183 [2.251, 12.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.559, 10.100], loss: 0.232192, mae: 0.480992, mean_q: 4.844872
 22017/100000: episode: 500, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 22.380, mean reward: 4.476 [3.481, 5.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.851, 10.100], loss: 0.158138, mae: 0.418034, mean_q: 4.869669
 22021/100000: episode: 501, duration: 0.029s, episode steps: 4, steps per second: 139, episode reward: 13.615, mean reward: 3.404 [3.067, 3.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.438, 10.100], loss: 0.149460, mae: 0.393647, mean_q: 4.640383
 22026/100000: episode: 502, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 62.177, mean reward: 12.435 [4.357, 20.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.670, 10.100], loss: 0.433274, mae: 0.510862, mean_q: 4.670761
 22031/100000: episode: 503, duration: 0.035s, episode steps: 5, steps per second: 143, episode reward: 17.356, mean reward: 3.471 [2.898, 3.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.403, 10.100], loss: 0.231151, mae: 0.527735, mean_q: 5.097906
 22036/100000: episode: 504, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 23.599, mean reward: 4.720 [3.961, 5.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.700, 10.100], loss: 1.169924, mae: 0.607342, mean_q: 4.641914
 22043/100000: episode: 505, duration: 0.040s, episode steps: 7, steps per second: 174, episode reward: 34.678, mean reward: 4.954 [3.379, 6.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.501, 10.100], loss: 0.886998, mae: 0.635768, mean_q: 5.008206
 22048/100000: episode: 506, duration: 0.032s, episode steps: 5, steps per second: 159, episode reward: 22.018, mean reward: 4.404 [3.359, 5.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.509, 10.100], loss: 0.445569, mae: 0.562497, mean_q: 4.854023
 22053/100000: episode: 507, duration: 0.034s, episode steps: 5, steps per second: 145, episode reward: 16.196, mean reward: 3.239 [2.908, 3.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.712, 10.100], loss: 0.298434, mae: 0.555348, mean_q: 4.983744
[Info] 5-TH LEVEL FOUND: 7.57151460647583, Considering 13/87 traces
 22058/100000: episode: 508, duration: 4.152s, episode steps: 5, steps per second: 1, episode reward: 18.400, mean reward: 3.680 [3.148, 4.613], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.570, 10.100], loss: 0.190098, mae: 0.478457, mean_q: 4.976144
 22061/100000: episode: 509, duration: 0.021s, episode steps: 3, steps per second: 144, episode reward: 16.753, mean reward: 5.584 [4.746, 6.797], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.611, 10.100], loss: 0.586519, mae: 0.533996, mean_q: 4.724587
 22066/100000: episode: 510, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 21.255, mean reward: 4.251 [3.124, 6.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.671, 10.100], loss: 0.371329, mae: 0.511008, mean_q: 4.904388
 22071/100000: episode: 511, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 22.698, mean reward: 4.540 [3.628, 5.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.483, 10.100], loss: 0.200156, mae: 0.483527, mean_q: 4.960710
 22076/100000: episode: 512, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 25.298, mean reward: 5.060 [4.101, 7.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.627, 10.100], loss: 0.860659, mae: 0.682734, mean_q: 4.930847
 22081/100000: episode: 513, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 25.451, mean reward: 5.090 [4.292, 6.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.648, 10.100], loss: 0.321506, mae: 0.524901, mean_q: 4.851842
 22086/100000: episode: 514, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 24.691, mean reward: 4.938 [3.529, 5.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.412, 10.100], loss: 0.833461, mae: 0.555191, mean_q: 4.663846
 22091/100000: episode: 515, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 18.690, mean reward: 3.738 [3.285, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.534, 10.100], loss: 1.813065, mae: 0.784893, mean_q: 5.329312
 22096/100000: episode: 516, duration: 0.027s, episode steps: 5, steps per second: 185, episode reward: 19.414, mean reward: 3.883 [3.105, 5.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.454, 10.100], loss: 0.419316, mae: 0.585379, mean_q: 4.801201
 22101/100000: episode: 517, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 32.988, mean reward: 6.598 [4.467, 10.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.586, 10.100], loss: 0.543207, mae: 0.675058, mean_q: 4.450287
 22106/100000: episode: 518, duration: 0.033s, episode steps: 5, steps per second: 151, episode reward: 17.213, mean reward: 3.443 [3.000, 3.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.586, 10.100], loss: 0.824312, mae: 0.620327, mean_q: 5.140124
 22111/100000: episode: 519, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 20.075, mean reward: 4.015 [3.603, 4.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.484, 10.100], loss: 0.305730, mae: 0.553418, mean_q: 5.254163
 22116/100000: episode: 520, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 24.885, mean reward: 4.977 [4.227, 5.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.492, 10.100], loss: 0.358855, mae: 0.530076, mean_q: 4.813745
 22121/100000: episode: 521, duration: 0.034s, episode steps: 5, steps per second: 145, episode reward: 23.196, mean reward: 4.639 [3.318, 7.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.522, 10.100], loss: 0.270729, mae: 0.481065, mean_q: 4.856085
 22126/100000: episode: 522, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 25.292, mean reward: 5.058 [4.060, 6.094], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.594, 10.100], loss: 0.332880, mae: 0.550898, mean_q: 4.886281
 22131/100000: episode: 523, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 26.820, mean reward: 5.364 [4.427, 6.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.610, 10.100], loss: 0.172556, mae: 0.445413, mean_q: 4.943902
 22136/100000: episode: 524, duration: 0.036s, episode steps: 5, steps per second: 141, episode reward: 21.397, mean reward: 4.279 [3.814, 5.028], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.536, 10.100], loss: 0.481492, mae: 0.502729, mean_q: 4.870013
 22141/100000: episode: 525, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 24.759, mean reward: 4.952 [2.988, 6.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.588, 10.100], loss: 0.925196, mae: 0.574300, mean_q: 5.015145
 22146/100000: episode: 526, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 16.996, mean reward: 3.399 [3.198, 3.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.596, 10.100], loss: 0.404888, mae: 0.564499, mean_q: 4.731612
 22151/100000: episode: 527, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 26.403, mean reward: 5.281 [4.498, 5.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.579, 10.100], loss: 0.215192, mae: 0.433258, mean_q: 4.956816
 22156/100000: episode: 528, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 24.104, mean reward: 4.821 [3.458, 5.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.790, 10.100], loss: 0.337836, mae: 0.531865, mean_q: 5.207461
 22161/100000: episode: 529, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 26.854, mean reward: 5.371 [4.643, 6.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.600, 10.100], loss: 0.278305, mae: 0.484811, mean_q: 4.900880
 22166/100000: episode: 530, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 29.294, mean reward: 5.859 [3.544, 9.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.544, 10.100], loss: 0.260558, mae: 0.472655, mean_q: 4.851195
 22171/100000: episode: 531, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 15.911, mean reward: 3.182 [2.921, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.939, 10.100], loss: 0.393513, mae: 0.565685, mean_q: 4.905005
 22176/100000: episode: 532, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 18.168, mean reward: 3.634 [2.839, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.590, 10.100], loss: 0.312588, mae: 0.541775, mean_q: 4.925105
 22181/100000: episode: 533, duration: 0.032s, episode steps: 5, steps per second: 155, episode reward: 26.250, mean reward: 5.250 [4.494, 7.190], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.503, 10.100], loss: 1.620692, mae: 0.633141, mean_q: 4.933291
 22186/100000: episode: 534, duration: 0.029s, episode steps: 5, steps per second: 172, episode reward: 19.014, mean reward: 3.803 [2.740, 4.504], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.650, 10.100], loss: 0.463072, mae: 0.677147, mean_q: 5.351805
 22191/100000: episode: 535, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 25.977, mean reward: 5.195 [4.266, 5.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.584, 10.100], loss: 0.243389, mae: 0.459564, mean_q: 4.566074
 22196/100000: episode: 536, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 25.514, mean reward: 5.103 [3.752, 6.183], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-1.037, 10.100], loss: 0.192785, mae: 0.470728, mean_q: 5.096847
 22201/100000: episode: 537, duration: 0.031s, episode steps: 5, steps per second: 162, episode reward: 16.574, mean reward: 3.315 [3.044, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.455, 10.100], loss: 0.502200, mae: 0.554584, mean_q: 4.833899
 22206/100000: episode: 538, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 22.586, mean reward: 4.517 [3.808, 5.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.677, 10.100], loss: 0.425031, mae: 0.575160, mean_q: 5.159746
 22211/100000: episode: 539, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 32.033, mean reward: 6.407 [4.842, 10.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.561, 10.100], loss: 0.490906, mae: 0.632047, mean_q: 5.148124
 22216/100000: episode: 540, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 27.030, mean reward: 5.406 [3.669, 8.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.672, 10.100], loss: 0.203806, mae: 0.469482, mean_q: 5.034546
 22221/100000: episode: 541, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 23.588, mean reward: 4.718 [4.064, 5.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.471, 10.100], loss: 0.509244, mae: 0.587522, mean_q: 5.030570
 22226/100000: episode: 542, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 22.679, mean reward: 4.536 [3.989, 5.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.589, 10.100], loss: 0.304649, mae: 0.538719, mean_q: 5.089615
 22231/100000: episode: 543, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 17.336, mean reward: 3.467 [2.835, 4.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.611, 10.100], loss: 0.235436, mae: 0.478905, mean_q: 4.948812
 22236/100000: episode: 544, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 20.409, mean reward: 4.082 [3.211, 5.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.593, 10.100], loss: 1.491327, mae: 0.793062, mean_q: 5.274173
 22241/100000: episode: 545, duration: 0.034s, episode steps: 5, steps per second: 149, episode reward: 22.897, mean reward: 4.579 [4.214, 5.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.461, 10.100], loss: 0.537512, mae: 0.714872, mean_q: 5.285324
 22246/100000: episode: 546, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 22.642, mean reward: 4.528 [3.697, 5.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.500, 10.100], loss: 1.198391, mae: 0.769055, mean_q: 4.462996
 22251/100000: episode: 547, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 26.357, mean reward: 5.271 [3.938, 7.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.662, 10.100], loss: 0.252881, mae: 0.505648, mean_q: 4.852546
 22256/100000: episode: 548, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 22.558, mean reward: 4.512 [3.407, 5.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.599, 10.100], loss: 0.477848, mae: 0.653223, mean_q: 5.134239
 22261/100000: episode: 549, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 27.457, mean reward: 5.491 [4.735, 6.226], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.061, 10.100], loss: 0.756314, mae: 0.640525, mean_q: 5.032884
 22266/100000: episode: 550, duration: 0.030s, episode steps: 5, steps per second: 164, episode reward: 20.035, mean reward: 4.007 [3.326, 4.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.568, 10.100], loss: 0.377328, mae: 0.599308, mean_q: 5.221186
 22271/100000: episode: 551, duration: 0.034s, episode steps: 5, steps per second: 149, episode reward: 28.250, mean reward: 5.650 [4.042, 9.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.624, 10.100], loss: 0.655772, mae: 0.680515, mean_q: 4.796388
 22276/100000: episode: 552, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 20.626, mean reward: 4.125 [3.194, 5.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.589, 10.100], loss: 0.240708, mae: 0.477516, mean_q: 4.892918
 22281/100000: episode: 553, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: 28.924, mean reward: 5.785 [3.101, 12.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.505, 10.100], loss: 0.307903, mae: 0.530459, mean_q: 5.104105
 22286/100000: episode: 554, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 22.530, mean reward: 4.506 [3.310, 5.498], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.605, 10.100], loss: 0.251932, mae: 0.499930, mean_q: 4.968773
 22291/100000: episode: 555, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: 19.697, mean reward: 3.939 [3.467, 4.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.514, 10.100], loss: 0.281350, mae: 0.484631, mean_q: 4.935583
 22296/100000: episode: 556, duration: 0.031s, episode steps: 5, steps per second: 159, episode reward: 23.721, mean reward: 4.744 [3.448, 6.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.634, 10.100], loss: 0.342514, mae: 0.575765, mean_q: 5.188284
[Info] FALSIFICATION!
 22300/100000: episode: 557, duration: 0.464s, episode steps: 4, steps per second: 9, episode reward: 1027.751, mean reward: 256.938 [6.116, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.598, 10.098], loss: 0.665231, mae: 0.597758, mean_q: 4.955281
 22305/100000: episode: 558, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 23.402, mean reward: 4.680 [3.949, 5.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.541, 10.100], loss: 0.306749, mae: 0.526535, mean_q: 5.130040
 22310/100000: episode: 559, duration: 0.039s, episode steps: 5, steps per second: 130, episode reward: 20.678, mean reward: 4.136 [3.557, 4.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.838, 10.100], loss: 0.437967, mae: 0.583253, mean_q: 4.867309
 22315/100000: episode: 560, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 23.666, mean reward: 4.733 [4.361, 5.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.523, 10.100], loss: 0.336820, mae: 0.555987, mean_q: 4.868276
 22320/100000: episode: 561, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 37.389, mean reward: 7.478 [4.505, 12.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.539, 10.100], loss: 0.446561, mae: 0.594118, mean_q: 5.145323
 22325/100000: episode: 562, duration: 0.032s, episode steps: 5, steps per second: 156, episode reward: 27.884, mean reward: 5.577 [4.043, 8.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.595, 10.100], loss: 0.368202, mae: 0.577020, mean_q: 5.113675
 22328/100000: episode: 563, duration: 0.019s, episode steps: 3, steps per second: 160, episode reward: 11.425, mean reward: 3.808 [3.619, 3.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.613, 10.100], loss: 0.587417, mae: 0.601911, mean_q: 5.014715
 22333/100000: episode: 564, duration: 0.034s, episode steps: 5, steps per second: 149, episode reward: 20.452, mean reward: 4.090 [3.455, 4.919], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.528, 10.100], loss: 1.438129, mae: 0.845935, mean_q: 5.342437
 22338/100000: episode: 565, duration: 0.030s, episode steps: 5, steps per second: 164, episode reward: 21.987, mean reward: 4.397 [3.473, 6.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-1.178, 10.100], loss: 0.519603, mae: 0.589492, mean_q: 5.201758
 22343/100000: episode: 566, duration: 0.029s, episode steps: 5, steps per second: 173, episode reward: 23.281, mean reward: 4.656 [3.774, 6.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.563, 10.100], loss: 0.733820, mae: 0.704685, mean_q: 5.409853
 22348/100000: episode: 567, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 17.806, mean reward: 3.561 [3.016, 4.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.564, 10.100], loss: 0.539764, mae: 0.654594, mean_q: 5.355721
 22353/100000: episode: 568, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 28.918, mean reward: 5.784 [4.135, 7.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.564, 10.100], loss: 0.409595, mae: 0.595571, mean_q: 5.106428
 22358/100000: episode: 569, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 18.464, mean reward: 3.693 [3.043, 4.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.515, 10.100], loss: 0.400990, mae: 0.630354, mean_q: 5.477120
 22363/100000: episode: 570, duration: 0.036s, episode steps: 5, steps per second: 137, episode reward: 19.631, mean reward: 3.926 [3.299, 4.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.563, 10.100], loss: 0.615075, mae: 0.700772, mean_q: 5.271614
 22368/100000: episode: 571, duration: 0.035s, episode steps: 5, steps per second: 141, episode reward: 16.331, mean reward: 3.266 [2.930, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.651, 10.100], loss: 0.278142, mae: 0.496109, mean_q: 5.013817
 22373/100000: episode: 572, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: 27.542, mean reward: 5.508 [4.103, 8.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.562, 10.100], loss: 0.432570, mae: 0.587322, mean_q: 5.178031
 22378/100000: episode: 573, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: 20.415, mean reward: 4.083 [3.631, 4.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.544, 10.100], loss: 0.331960, mae: 0.547531, mean_q: 5.269676
 22381/100000: episode: 574, duration: 0.021s, episode steps: 3, steps per second: 146, episode reward: 19.842, mean reward: 6.614 [5.525, 7.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.710, 10.100], loss: 0.306147, mae: 0.563517, mean_q: 5.015627
 22386/100000: episode: 575, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 20.771, mean reward: 4.154 [3.812, 4.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.502, 10.100], loss: 0.298510, mae: 0.547315, mean_q: 5.132555
 22391/100000: episode: 576, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 21.265, mean reward: 4.253 [3.517, 5.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.581, 10.100], loss: 0.332262, mae: 0.505315, mean_q: 5.198231
 22396/100000: episode: 577, duration: 0.034s, episode steps: 5, steps per second: 147, episode reward: 21.616, mean reward: 4.323 [3.381, 5.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.537, 10.100], loss: 0.229651, mae: 0.457952, mean_q: 5.047207
 22401/100000: episode: 578, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 30.041, mean reward: 6.008 [4.219, 7.700], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.582, 10.100], loss: 0.422932, mae: 0.513769, mean_q: 5.112762
 22406/100000: episode: 579, duration: 0.035s, episode steps: 5, steps per second: 143, episode reward: 21.139, mean reward: 4.228 [3.590, 4.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.530, 10.100], loss: 0.811181, mae: 0.689040, mean_q: 5.320462
 22411/100000: episode: 580, duration: 0.029s, episode steps: 5, steps per second: 171, episode reward: 22.819, mean reward: 4.564 [3.885, 5.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.499, 10.100], loss: 0.566566, mae: 0.559766, mean_q: 5.002973
 22416/100000: episode: 581, duration: 0.035s, episode steps: 5, steps per second: 145, episode reward: 24.815, mean reward: 4.963 [3.959, 6.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.576, 10.100], loss: 0.869147, mae: 0.730113, mean_q: 5.397793
 22421/100000: episode: 582, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 23.266, mean reward: 4.653 [3.271, 5.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.602, 10.100], loss: 0.388433, mae: 0.574349, mean_q: 4.884404
 22424/100000: episode: 583, duration: 0.020s, episode steps: 3, steps per second: 152, episode reward: 23.892, mean reward: 7.964 [5.756, 12.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.720, 10.100], loss: 0.318313, mae: 0.633389, mean_q: 5.554572
 22429/100000: episode: 584, duration: 0.029s, episode steps: 5, steps per second: 171, episode reward: 24.090, mean reward: 4.818 [3.360, 7.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.582, 10.100], loss: 0.415674, mae: 0.616870, mean_q: 5.302373
 22434/100000: episode: 585, duration: 0.037s, episode steps: 5, steps per second: 135, episode reward: 19.754, mean reward: 3.951 [3.364, 4.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-1.076, 10.100], loss: 0.245651, mae: 0.492952, mean_q: 5.035627
 22439/100000: episode: 586, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 21.642, mean reward: 4.328 [3.064, 6.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.490, 10.100], loss: 0.335409, mae: 0.571086, mean_q: 5.198851
 22444/100000: episode: 587, duration: 0.031s, episode steps: 5, steps per second: 164, episode reward: 29.510, mean reward: 5.902 [4.840, 7.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.580, 10.100], loss: 0.453496, mae: 0.569726, mean_q: 5.093419
 22449/100000: episode: 588, duration: 0.030s, episode steps: 5, steps per second: 167, episode reward: 17.678, mean reward: 3.536 [2.345, 4.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.791, 10.100], loss: 1.053029, mae: 0.704538, mean_q: 5.363073
 22454/100000: episode: 589, duration: 0.037s, episode steps: 5, steps per second: 136, episode reward: 29.550, mean reward: 5.910 [3.767, 8.792], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.538, 10.100], loss: 0.310342, mae: 0.577057, mean_q: 5.182863
 22459/100000: episode: 590, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: 32.670, mean reward: 6.534 [4.414, 11.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.539, 10.100], loss: 0.426003, mae: 0.620912, mean_q: 5.204484
 22462/100000: episode: 591, duration: 0.020s, episode steps: 3, steps per second: 148, episode reward: 15.254, mean reward: 5.085 [4.994, 5.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.651, 10.100], loss: 0.235732, mae: 0.489724, mean_q: 5.257325
 22467/100000: episode: 592, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: 20.885, mean reward: 4.177 [2.994, 5.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.779, 10.100], loss: 0.343468, mae: 0.600422, mean_q: 5.203447
 22472/100000: episode: 593, duration: 0.030s, episode steps: 5, steps per second: 168, episode reward: 20.053, mean reward: 4.011 [3.567, 4.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.602, 10.100], loss: 0.283015, mae: 0.516057, mean_q: 5.046064
 22477/100000: episode: 594, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: 20.339, mean reward: 4.068 [3.173, 5.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.489, 10.100], loss: 0.725913, mae: 0.682140, mean_q: 5.275815
[Info] Complete ISplit Iteration
[Info] Levels: [4.2806873, 5.3133497, 5.6544, 7.210342, 7.5715146, 8.067875]
[Info] Cond. Prob: [0.1, 0.12, 0.1, 0.1, 0.13, 0.08]
[Info] Error Prob: 1.2480000000000004e-06

 22482/100000: episode: 595, duration: 4.330s, episode steps: 5, steps per second: 1, episode reward: 19.156, mean reward: 3.831 [2.911, 5.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.563, 10.100], loss: 0.391813, mae: 0.587173, mean_q: 5.090544
 22582/100000: episode: 596, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 186.266, mean reward: 1.863 [1.481, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.362, 10.161], loss: 155.694626, mae: 1.441303, mean_q: 5.592927
 22682/100000: episode: 597, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 197.098, mean reward: 1.971 [1.448, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.517, 10.245], loss: 0.959264, mae: 0.801693, mean_q: 5.342289
 22782/100000: episode: 598, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 196.840, mean reward: 1.968 [1.462, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.587, 10.445], loss: 309.445251, mae: 1.764662, mean_q: 5.946889
 22882/100000: episode: 599, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 202.507, mean reward: 2.025 [1.515, 3.741], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.860, 10.098], loss: 155.253815, mae: 1.310272, mean_q: 5.868568
 22982/100000: episode: 600, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 178.708, mean reward: 1.787 [1.445, 2.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.149, 10.154], loss: 0.862020, mae: 0.755681, mean_q: 5.483757
 23082/100000: episode: 601, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 201.450, mean reward: 2.014 [1.432, 4.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.384, 10.098], loss: 0.999999, mae: 0.793957, mean_q: 5.420213
 23182/100000: episode: 602, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 215.238, mean reward: 2.152 [1.460, 3.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.292, 10.098], loss: 0.911565, mae: 0.780811, mean_q: 5.325237
 23282/100000: episode: 603, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 193.404, mean reward: 1.934 [1.441, 4.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.807, 10.098], loss: 0.885352, mae: 0.754577, mean_q: 5.295777
 23382/100000: episode: 604, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 195.595, mean reward: 1.956 [1.443, 3.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.666, 10.098], loss: 155.085785, mae: 1.254915, mean_q: 5.715071
 23482/100000: episode: 605, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 176.770, mean reward: 1.768 [1.481, 2.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.360, 10.098], loss: 0.885486, mae: 0.773997, mean_q: 5.312103
 23582/100000: episode: 606, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 189.449, mean reward: 1.894 [1.435, 3.974], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.790, 10.098], loss: 154.887192, mae: 1.077604, mean_q: 5.363327
 23682/100000: episode: 607, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 202.064, mean reward: 2.021 [1.454, 4.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.634, 10.098], loss: 1.117251, mae: 0.928895, mean_q: 5.574891
 23782/100000: episode: 608, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 181.387, mean reward: 1.814 [1.439, 2.926], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.378, 10.163], loss: 308.059570, mae: 1.496754, mean_q: 5.565452
 23882/100000: episode: 609, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 192.380, mean reward: 1.924 [1.484, 3.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.700, 10.321], loss: 155.911407, mae: 1.662693, mean_q: 6.263223
 23982/100000: episode: 610, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 200.293, mean reward: 2.003 [1.484, 3.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.754, 10.470], loss: 155.038193, mae: 1.303787, mean_q: 5.866085
 24082/100000: episode: 611, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 185.942, mean reward: 1.859 [1.453, 2.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-2.108, 10.124], loss: 0.991973, mae: 0.791321, mean_q: 5.440330
 24182/100000: episode: 612, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 219.936, mean reward: 2.199 [1.494, 10.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.004, 10.375], loss: 155.039673, mae: 1.272106, mean_q: 5.681044
 24282/100000: episode: 613, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 187.600, mean reward: 1.876 [1.453, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.370, 10.164], loss: 309.106812, mae: 1.719810, mean_q: 6.064559
 24382/100000: episode: 614, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 192.761, mean reward: 1.928 [1.475, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.422, 10.409], loss: 154.956284, mae: 1.247250, mean_q: 5.673401
 24482/100000: episode: 615, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 193.045, mean reward: 1.930 [1.434, 3.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.199, 10.296], loss: 0.866712, mae: 0.738805, mean_q: 5.296784
 24582/100000: episode: 616, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 182.438, mean reward: 1.824 [1.461, 2.807], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.825, 10.098], loss: 0.911722, mae: 0.735565, mean_q: 5.227017
 24682/100000: episode: 617, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 213.307, mean reward: 2.133 [1.471, 5.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.791, 10.334], loss: 0.830504, mae: 0.716129, mean_q: 5.169865
 24782/100000: episode: 618, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 196.865, mean reward: 1.969 [1.467, 6.335], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.221, 10.098], loss: 154.849380, mae: 1.143049, mean_q: 5.399754
 24882/100000: episode: 619, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 184.330, mean reward: 1.843 [1.437, 3.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.776, 10.098], loss: 463.082489, mae: 2.230769, mean_q: 6.180236
 24982/100000: episode: 620, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 225.224, mean reward: 2.252 [1.436, 7.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.754, 10.427], loss: 154.746872, mae: 1.219962, mean_q: 5.595078
 25082/100000: episode: 621, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 199.463, mean reward: 1.995 [1.459, 6.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.776, 10.343], loss: 1.016575, mae: 0.747157, mean_q: 5.261136
 25182/100000: episode: 622, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 215.567, mean reward: 2.156 [1.467, 5.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.238, 10.372], loss: 614.384155, mae: 2.612904, mean_q: 6.110803
 25282/100000: episode: 623, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 197.385, mean reward: 1.974 [1.494, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.165, 10.168], loss: 155.191879, mae: 1.492156, mean_q: 5.803166
 25382/100000: episode: 624, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 181.504, mean reward: 1.815 [1.463, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.604, 10.109], loss: 308.421600, mae: 1.720478, mean_q: 6.058353
 25482/100000: episode: 625, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.383, mean reward: 1.904 [1.455, 3.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.986, 10.098], loss: 154.501083, mae: 1.087099, mean_q: 5.328281
 25582/100000: episode: 626, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 198.868, mean reward: 1.989 [1.467, 4.604], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.178, 10.099], loss: 307.648407, mae: 1.529195, mean_q: 5.488975
 25682/100000: episode: 627, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 182.784, mean reward: 1.828 [1.443, 2.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.870, 10.098], loss: 154.825180, mae: 1.609338, mean_q: 5.962127
 25782/100000: episode: 628, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 190.667, mean reward: 1.907 [1.457, 2.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.355, 10.098], loss: 307.633698, mae: 1.456055, mean_q: 5.236611
 25882/100000: episode: 629, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 219.511, mean reward: 2.195 [1.455, 4.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.968, 10.098], loss: 307.768250, mae: 1.537340, mean_q: 5.402517
 25982/100000: episode: 630, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 197.210, mean reward: 1.972 [1.485, 3.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.273, 10.098], loss: 155.308167, mae: 1.423359, mean_q: 5.547498
 26082/100000: episode: 631, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 211.572, mean reward: 2.116 [1.518, 3.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.595, 10.113], loss: 154.576477, mae: 1.113882, mean_q: 5.127116
 26182/100000: episode: 632, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 200.024, mean reward: 2.000 [1.437, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.394, 10.264], loss: 0.874964, mae: 0.699049, mean_q: 4.991396
 26282/100000: episode: 633, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 181.812, mean reward: 1.818 [1.466, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.352, 10.227], loss: 0.678653, mae: 0.611843, mean_q: 4.759675
 26382/100000: episode: 634, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 184.523, mean reward: 1.845 [1.447, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.905, 10.148], loss: 307.429779, mae: 1.330685, mean_q: 4.923933
 26482/100000: episode: 635, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 197.104, mean reward: 1.971 [1.433, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.004, 10.098], loss: 307.321289, mae: 1.596519, mean_q: 5.385405
 26582/100000: episode: 636, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 199.735, mean reward: 1.997 [1.463, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.698, 10.098], loss: 1.010064, mae: 0.709024, mean_q: 4.830128
 26682/100000: episode: 637, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 252.882, mean reward: 2.529 [1.462, 5.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.680, 10.098], loss: 0.630937, mae: 0.534525, mean_q: 4.540243
 26782/100000: episode: 638, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 198.484, mean reward: 1.985 [1.462, 5.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.316, 10.098], loss: 154.279724, mae: 0.971785, mean_q: 4.664407
 26882/100000: episode: 639, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 191.577, mean reward: 1.916 [1.484, 3.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.784, 10.117], loss: 154.117249, mae: 0.956644, mean_q: 4.716237
 26982/100000: episode: 640, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 178.111, mean reward: 1.781 [1.451, 2.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.276, 10.140], loss: 0.509545, mae: 0.488987, mean_q: 4.403142
 27082/100000: episode: 641, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 202.897, mean reward: 2.029 [1.479, 3.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.459, 10.098], loss: 0.386790, mae: 0.445393, mean_q: 4.223093
 27182/100000: episode: 642, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 209.693, mean reward: 2.097 [1.497, 4.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.652, 10.098], loss: 0.299645, mae: 0.405393, mean_q: 4.124142
 27282/100000: episode: 643, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 201.768, mean reward: 2.018 [1.474, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.957, 10.288], loss: 0.214606, mae: 0.378266, mean_q: 4.067271
 27382/100000: episode: 644, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 220.603, mean reward: 2.206 [1.458, 5.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.469, 10.354], loss: 0.200907, mae: 0.357918, mean_q: 3.969138
 27482/100000: episode: 645, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 196.056, mean reward: 1.961 [1.433, 2.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.080, 10.271], loss: 0.134141, mae: 0.334833, mean_q: 3.909725
 27582/100000: episode: 646, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 218.164, mean reward: 2.182 [1.478, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.313, 10.175], loss: 0.160489, mae: 0.345560, mean_q: 3.927806
 27682/100000: episode: 647, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 195.668, mean reward: 1.957 [1.487, 4.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.353, 10.265], loss: 0.151217, mae: 0.344119, mean_q: 3.930180
 27782/100000: episode: 648, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 205.725, mean reward: 2.057 [1.502, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.089, 10.098], loss: 0.126326, mae: 0.328627, mean_q: 3.934902
 27882/100000: episode: 649, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 203.210, mean reward: 2.032 [1.444, 4.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.240, 10.098], loss: 0.139173, mae: 0.334382, mean_q: 3.918854
 27982/100000: episode: 650, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 176.138, mean reward: 1.761 [1.461, 2.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.207, 10.098], loss: 0.165447, mae: 0.349292, mean_q: 3.951468
 28082/100000: episode: 651, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 186.832, mean reward: 1.868 [1.459, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.473, 10.295], loss: 0.142896, mae: 0.337321, mean_q: 3.926264
 28182/100000: episode: 652, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 185.402, mean reward: 1.854 [1.463, 2.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.031, 10.098], loss: 0.119289, mae: 0.319840, mean_q: 3.917672
 28282/100000: episode: 653, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 206.934, mean reward: 2.069 [1.456, 3.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.614, 10.105], loss: 0.150065, mae: 0.335372, mean_q: 3.910632
 28382/100000: episode: 654, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 191.838, mean reward: 1.918 [1.470, 5.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.197, 10.098], loss: 0.154707, mae: 0.341445, mean_q: 3.934641
 28482/100000: episode: 655, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 193.582, mean reward: 1.936 [1.451, 3.684], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.008, 10.153], loss: 0.142341, mae: 0.329948, mean_q: 3.913851
 28582/100000: episode: 656, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 203.120, mean reward: 2.031 [1.491, 3.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.855, 10.098], loss: 0.162827, mae: 0.338935, mean_q: 3.924986
 28682/100000: episode: 657, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 237.606, mean reward: 2.376 [1.442, 7.334], mean action: 0.000 [0.000, 0.000], mean observation: 1.447 [-1.610, 10.403], loss: 0.130550, mae: 0.324270, mean_q: 3.936565
 28782/100000: episode: 658, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 198.234, mean reward: 1.982 [1.468, 4.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.453, 10.098], loss: 0.128165, mae: 0.336527, mean_q: 3.933067
 28882/100000: episode: 659, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 176.840, mean reward: 1.768 [1.471, 2.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.643, 10.132], loss: 0.145522, mae: 0.336093, mean_q: 3.933784
 28982/100000: episode: 660, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 192.561, mean reward: 1.926 [1.462, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.537, 10.098], loss: 0.129738, mae: 0.331202, mean_q: 3.947506
 29082/100000: episode: 661, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 187.421, mean reward: 1.874 [1.465, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.220, 10.183], loss: 0.153062, mae: 0.340375, mean_q: 3.940396
 29182/100000: episode: 662, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 182.759, mean reward: 1.828 [1.462, 3.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.487, 10.241], loss: 0.129824, mae: 0.326382, mean_q: 3.946494
 29282/100000: episode: 663, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 200.182, mean reward: 2.002 [1.488, 3.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.523, 10.307], loss: 0.127143, mae: 0.322901, mean_q: 3.919923
 29382/100000: episode: 664, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 180.575, mean reward: 1.806 [1.448, 2.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.122, 10.221], loss: 0.118729, mae: 0.319240, mean_q: 3.910966
 29482/100000: episode: 665, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 190.222, mean reward: 1.902 [1.455, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.043, 10.165], loss: 0.113411, mae: 0.321609, mean_q: 3.909361
 29582/100000: episode: 666, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 188.958, mean reward: 1.890 [1.465, 5.687], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.131, 10.098], loss: 0.124667, mae: 0.324184, mean_q: 3.930716
 29682/100000: episode: 667, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 192.866, mean reward: 1.929 [1.443, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.498, 10.124], loss: 0.130267, mae: 0.331660, mean_q: 3.940627
 29782/100000: episode: 668, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 190.371, mean reward: 1.904 [1.469, 2.800], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.561, 10.275], loss: 0.125848, mae: 0.326247, mean_q: 3.934598
 29882/100000: episode: 669, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 195.172, mean reward: 1.952 [1.448, 4.469], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.331, 10.164], loss: 0.129276, mae: 0.326138, mean_q: 3.931624
 29982/100000: episode: 670, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 184.614, mean reward: 1.846 [1.469, 4.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.784, 10.352], loss: 0.114524, mae: 0.316589, mean_q: 3.909247
 30082/100000: episode: 671, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 195.076, mean reward: 1.951 [1.456, 4.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.145, 10.252], loss: 0.129544, mae: 0.328418, mean_q: 3.914527
 30182/100000: episode: 672, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 193.765, mean reward: 1.938 [1.473, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.880, 10.339], loss: 0.116432, mae: 0.324144, mean_q: 3.922857
 30282/100000: episode: 673, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 185.049, mean reward: 1.850 [1.464, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.359, 10.098], loss: 0.111079, mae: 0.314576, mean_q: 3.899995
 30382/100000: episode: 674, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 199.431, mean reward: 1.994 [1.455, 3.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.735, 10.156], loss: 0.101863, mae: 0.308464, mean_q: 3.892084
 30482/100000: episode: 675, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 202.517, mean reward: 2.025 [1.500, 3.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.898, 10.098], loss: 0.098092, mae: 0.310051, mean_q: 3.906674
 30582/100000: episode: 676, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 199.858, mean reward: 1.999 [1.452, 4.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.276, 10.171], loss: 0.110586, mae: 0.318420, mean_q: 3.916628
 30682/100000: episode: 677, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 206.436, mean reward: 2.064 [1.438, 4.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.693, 10.177], loss: 0.120213, mae: 0.326169, mean_q: 3.906614
 30782/100000: episode: 678, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 194.599, mean reward: 1.946 [1.475, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.891, 10.177], loss: 0.123460, mae: 0.327772, mean_q: 3.918143
 30882/100000: episode: 679, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 204.963, mean reward: 2.050 [1.518, 4.703], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.762, 10.098], loss: 0.115569, mae: 0.323094, mean_q: 3.919265
 30982/100000: episode: 680, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 207.814, mean reward: 2.078 [1.471, 8.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.113, 10.363], loss: 0.108925, mae: 0.313032, mean_q: 3.905716
 31082/100000: episode: 681, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 177.030, mean reward: 1.770 [1.445, 2.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.216, 10.148], loss: 0.107005, mae: 0.308696, mean_q: 3.891695
 31182/100000: episode: 682, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 193.058, mean reward: 1.931 [1.441, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.828, 10.140], loss: 0.104302, mae: 0.303300, mean_q: 3.891542
 31282/100000: episode: 683, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 199.372, mean reward: 1.994 [1.448, 3.366], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.051, 10.098], loss: 0.106181, mae: 0.310549, mean_q: 3.906188
 31382/100000: episode: 684, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 182.650, mean reward: 1.827 [1.455, 2.861], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.648, 10.127], loss: 0.138985, mae: 0.331834, mean_q: 3.891942
 31482/100000: episode: 685, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 226.690, mean reward: 2.267 [1.534, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.170, 10.098], loss: 0.102328, mae: 0.314761, mean_q: 3.895203
 31582/100000: episode: 686, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 198.762, mean reward: 1.988 [1.456, 3.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.933, 10.098], loss: 0.128541, mae: 0.327517, mean_q: 3.916547
 31682/100000: episode: 687, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 181.378, mean reward: 1.814 [1.492, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.311, 10.098], loss: 0.103032, mae: 0.304468, mean_q: 3.874620
 31782/100000: episode: 688, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 179.404, mean reward: 1.794 [1.436, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.039, 10.098], loss: 0.107205, mae: 0.303852, mean_q: 3.877209
 31882/100000: episode: 689, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 190.715, mean reward: 1.907 [1.471, 3.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.511, 10.130], loss: 0.091833, mae: 0.300110, mean_q: 3.889244
 31982/100000: episode: 690, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 206.267, mean reward: 2.063 [1.550, 3.524], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.712, 10.098], loss: 0.093785, mae: 0.294615, mean_q: 3.863210
 32082/100000: episode: 691, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 183.164, mean reward: 1.832 [1.467, 2.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.104, 10.115], loss: 0.111285, mae: 0.314768, mean_q: 3.882814
 32182/100000: episode: 692, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 186.835, mean reward: 1.868 [1.458, 3.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.626, 10.098], loss: 0.088708, mae: 0.293784, mean_q: 3.840469
 32282/100000: episode: 693, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.653, mean reward: 1.907 [1.430, 4.617], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.771, 10.259], loss: 0.097531, mae: 0.298729, mean_q: 3.866647
 32382/100000: episode: 694, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 207.785, mean reward: 2.078 [1.509, 4.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.860, 10.136], loss: 0.095838, mae: 0.301013, mean_q: 3.851461
[Info] 1-TH LEVEL FOUND: 4.3295512199401855, Considering 10/90 traces
 32482/100000: episode: 695, duration: 4.558s, episode steps: 100, steps per second: 22, episode reward: 205.361, mean reward: 2.054 [1.473, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.659, 10.098], loss: 0.097343, mae: 0.300475, mean_q: 3.849144
 32518/100000: episode: 696, duration: 0.186s, episode steps: 36, steps per second: 194, episode reward: 125.737, mean reward: 3.493 [2.346, 5.610], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.408, 10.100], loss: 0.087227, mae: 0.288914, mean_q: 3.840834
 32609/100000: episode: 697, duration: 0.476s, episode steps: 91, steps per second: 191, episode reward: 169.772, mean reward: 1.866 [1.474, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-0.796, 10.100], loss: 0.116181, mae: 0.316403, mean_q: 3.869592
 32650/100000: episode: 698, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 142.121, mean reward: 3.466 [1.768, 5.890], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-1.137, 10.100], loss: 0.114888, mae: 0.315347, mean_q: 3.863486
 32734/100000: episode: 699, duration: 0.445s, episode steps: 84, steps per second: 189, episode reward: 156.546, mean reward: 1.864 [1.442, 2.801], mean action: 0.000 [0.000, 0.000], mean observation: 1.626 [-0.604, 10.237], loss: 0.123987, mae: 0.319508, mean_q: 3.891736
 32768/100000: episode: 700, duration: 0.186s, episode steps: 34, steps per second: 182, episode reward: 70.979, mean reward: 2.088 [1.463, 4.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.047 [-0.158, 10.108], loss: 0.116477, mae: 0.314485, mean_q: 3.887918
 32864/100000: episode: 701, duration: 0.488s, episode steps: 96, steps per second: 197, episode reward: 209.067, mean reward: 2.178 [1.511, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.708, 10.100], loss: 0.125381, mae: 0.333412, mean_q: 3.922268
 32900/100000: episode: 702, duration: 0.189s, episode steps: 36, steps per second: 190, episode reward: 103.514, mean reward: 2.875 [1.707, 4.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.290, 10.100], loss: 0.128428, mae: 0.327274, mean_q: 3.888761
 32931/100000: episode: 703, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 87.674, mean reward: 2.828 [1.890, 7.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.178, 10.100], loss: 0.136354, mae: 0.340390, mean_q: 3.904704
 33023/100000: episode: 704, duration: 0.449s, episode steps: 92, steps per second: 205, episode reward: 208.579, mean reward: 2.267 [1.498, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.528 [-0.455, 10.100], loss: 0.131938, mae: 0.342590, mean_q: 3.940144
 33057/100000: episode: 705, duration: 0.199s, episode steps: 34, steps per second: 171, episode reward: 91.919, mean reward: 2.704 [1.841, 4.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.205, 10.100], loss: 0.159918, mae: 0.356535, mean_q: 3.955577
 33149/100000: episode: 706, duration: 0.460s, episode steps: 92, steps per second: 200, episode reward: 191.685, mean reward: 2.084 [1.464, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.536 [-0.794, 10.403], loss: 0.123743, mae: 0.335131, mean_q: 3.938406
 33241/100000: episode: 707, duration: 0.457s, episode steps: 92, steps per second: 201, episode reward: 188.639, mean reward: 2.050 [1.486, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.527 [-1.232, 10.100], loss: 0.135417, mae: 0.342630, mean_q: 3.954380
 33325/100000: episode: 708, duration: 0.499s, episode steps: 84, steps per second: 168, episode reward: 174.999, mean reward: 2.083 [1.518, 2.868], mean action: 0.000 [0.000, 0.000], mean observation: 1.606 [-0.746, 10.160], loss: 0.120663, mae: 0.332518, mean_q: 3.969496
 33361/100000: episode: 709, duration: 0.212s, episode steps: 36, steps per second: 170, episode reward: 230.858, mean reward: 6.413 [3.278, 15.836], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-1.001, 10.100], loss: 0.150491, mae: 0.353811, mean_q: 3.996835
 33397/100000: episode: 710, duration: 0.209s, episode steps: 36, steps per second: 172, episode reward: 91.644, mean reward: 2.546 [1.919, 3.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-1.065, 10.100], loss: 0.148087, mae: 0.368581, mean_q: 3.995511
 33433/100000: episode: 711, duration: 0.189s, episode steps: 36, steps per second: 191, episode reward: 106.984, mean reward: 2.972 [1.861, 4.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.194, 10.100], loss: 0.176122, mae: 0.358797, mean_q: 4.013810
 33530/100000: episode: 712, duration: 0.511s, episode steps: 97, steps per second: 190, episode reward: 174.869, mean reward: 1.803 [1.472, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.487 [-0.687, 10.139], loss: 0.224997, mae: 0.387037, mean_q: 4.033293
 33571/100000: episode: 713, duration: 0.234s, episode steps: 41, steps per second: 175, episode reward: 160.824, mean reward: 3.923 [2.680, 10.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-1.079, 10.100], loss: 0.410287, mae: 0.421267, mean_q: 4.058984
 33662/100000: episode: 714, duration: 0.581s, episode steps: 91, steps per second: 157, episode reward: 177.613, mean reward: 1.952 [1.445, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 1.546 [-0.291, 10.227], loss: 0.171390, mae: 0.366248, mean_q: 4.018553
 33698/100000: episode: 715, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 81.427, mean reward: 2.262 [1.637, 3.609], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.219, 10.100], loss: 0.173032, mae: 0.386769, mean_q: 4.037937
 33739/100000: episode: 716, duration: 0.229s, episode steps: 41, steps per second: 179, episode reward: 194.804, mean reward: 4.751 [3.026, 9.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-1.177, 10.100], loss: 0.232153, mae: 0.408844, mean_q: 4.114811
 33830/100000: episode: 717, duration: 0.487s, episode steps: 91, steps per second: 187, episode reward: 194.963, mean reward: 2.142 [1.441, 4.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.546 [-1.698, 10.392], loss: 0.299542, mae: 0.418111, mean_q: 4.109632
 33921/100000: episode: 718, duration: 0.514s, episode steps: 91, steps per second: 177, episode reward: 215.305, mean reward: 2.366 [1.435, 7.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.535 [-0.628, 10.100], loss: 0.327648, mae: 0.421092, mean_q: 4.125812
 33962/100000: episode: 719, duration: 0.209s, episode steps: 41, steps per second: 196, episode reward: 129.072, mean reward: 3.148 [1.563, 5.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.352, 10.100], loss: 0.181465, mae: 0.369571, mean_q: 4.088353
 33998/100000: episode: 720, duration: 0.198s, episode steps: 36, steps per second: 182, episode reward: 98.906, mean reward: 2.747 [1.831, 5.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.016 [-0.487, 10.100], loss: 0.136221, mae: 0.363197, mean_q: 4.142434
 34034/100000: episode: 721, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 82.847, mean reward: 2.301 [1.467, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.485, 10.198], loss: 0.191230, mae: 0.398491, mean_q: 4.162355
 34130/100000: episode: 722, duration: 0.497s, episode steps: 96, steps per second: 193, episode reward: 185.670, mean reward: 1.934 [1.553, 3.196], mean action: 0.000 [0.000, 0.000], mean observation: 1.485 [-0.923, 10.100], loss: 0.211037, mae: 0.386726, mean_q: 4.137613
 34164/100000: episode: 723, duration: 0.181s, episode steps: 34, steps per second: 188, episode reward: 75.734, mean reward: 2.227 [1.518, 3.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.192, 10.141], loss: 0.170822, mae: 0.382365, mean_q: 4.133762
 34200/100000: episode: 724, duration: 0.194s, episode steps: 36, steps per second: 185, episode reward: 156.802, mean reward: 4.356 [2.569, 9.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.002 [-0.319, 10.100], loss: 0.201630, mae: 0.397129, mean_q: 4.180107
 34234/100000: episode: 725, duration: 0.177s, episode steps: 34, steps per second: 193, episode reward: 94.932, mean reward: 2.792 [1.632, 4.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.125, 10.100], loss: 0.233685, mae: 0.395703, mean_q: 4.160580
 34318/100000: episode: 726, duration: 0.457s, episode steps: 84, steps per second: 184, episode reward: 173.094, mean reward: 2.061 [1.490, 4.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.604 [-0.850, 10.100], loss: 0.223500, mae: 0.406369, mean_q: 4.229649
 34414/100000: episode: 727, duration: 0.505s, episode steps: 96, steps per second: 190, episode reward: 210.023, mean reward: 2.188 [1.572, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.498 [-0.713, 10.493], loss: 0.244674, mae: 0.412908, mean_q: 4.224657
 34498/100000: episode: 728, duration: 0.462s, episode steps: 84, steps per second: 182, episode reward: 186.277, mean reward: 2.218 [1.452, 5.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.616 [-0.472, 10.380], loss: 0.192700, mae: 0.404372, mean_q: 4.188941
 34589/100000: episode: 729, duration: 0.490s, episode steps: 91, steps per second: 186, episode reward: 173.189, mean reward: 1.903 [1.462, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.548 [-0.372, 10.337], loss: 0.209172, mae: 0.410002, mean_q: 4.243349
 34685/100000: episode: 730, duration: 0.516s, episode steps: 96, steps per second: 186, episode reward: 179.798, mean reward: 1.873 [1.470, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.491 [-0.228, 10.100], loss: 0.258305, mae: 0.426695, mean_q: 4.268384
 34719/100000: episode: 731, duration: 0.167s, episode steps: 34, steps per second: 204, episode reward: 77.825, mean reward: 2.289 [1.507, 3.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.544, 10.122], loss: 0.317231, mae: 0.435747, mean_q: 4.262417
 34760/100000: episode: 732, duration: 0.214s, episode steps: 41, steps per second: 191, episode reward: 141.188, mean reward: 3.444 [1.867, 6.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.479, 10.100], loss: 0.300320, mae: 0.437671, mean_q: 4.270685
 34796/100000: episode: 733, duration: 0.187s, episode steps: 36, steps per second: 192, episode reward: 95.217, mean reward: 2.645 [1.697, 6.341], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.732, 10.100], loss: 0.180140, mae: 0.407554, mean_q: 4.287894
 34888/100000: episode: 734, duration: 0.499s, episode steps: 92, steps per second: 185, episode reward: 190.810, mean reward: 2.074 [1.527, 5.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.527 [-0.892, 10.210], loss: 0.211775, mae: 0.395720, mean_q: 4.307246
 34924/100000: episode: 735, duration: 0.192s, episode steps: 36, steps per second: 187, episode reward: 97.863, mean reward: 2.718 [1.856, 4.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.633, 10.100], loss: 0.223025, mae: 0.421505, mean_q: 4.287081
 35020/100000: episode: 736, duration: 0.506s, episode steps: 96, steps per second: 190, episode reward: 175.423, mean reward: 1.827 [1.450, 2.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.489 [-0.554, 10.195], loss: 0.193965, mae: 0.391817, mean_q: 4.286776
 35056/100000: episode: 737, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 153.411, mean reward: 4.261 [2.221, 9.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.493, 10.100], loss: 0.276523, mae: 0.427853, mean_q: 4.343736
 35147/100000: episode: 738, duration: 0.459s, episode steps: 91, steps per second: 198, episode reward: 189.426, mean reward: 2.082 [1.517, 3.262], mean action: 0.000 [0.000, 0.000], mean observation: 1.550 [-0.476, 10.428], loss: 0.190286, mae: 0.394961, mean_q: 4.294021
 35178/100000: episode: 739, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 83.444, mean reward: 2.692 [1.915, 3.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.127, 10.100], loss: 0.229175, mae: 0.426867, mean_q: 4.386901
 35262/100000: episode: 740, duration: 0.434s, episode steps: 84, steps per second: 194, episode reward: 190.165, mean reward: 2.264 [1.481, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.606 [-1.039, 10.316], loss: 0.227067, mae: 0.414161, mean_q: 4.286941
 35296/100000: episode: 741, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 71.872, mean reward: 2.114 [1.560, 2.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.181, 10.100], loss: 0.226243, mae: 0.435309, mean_q: 4.370040
 35332/100000: episode: 742, duration: 0.198s, episode steps: 36, steps per second: 182, episode reward: 125.372, mean reward: 3.483 [2.389, 6.837], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-2.380, 10.100], loss: 0.261195, mae: 0.422703, mean_q: 4.385474
 35368/100000: episode: 743, duration: 0.193s, episode steps: 36, steps per second: 187, episode reward: 88.550, mean reward: 2.460 [1.627, 5.158], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-1.178, 10.100], loss: 0.177141, mae: 0.402866, mean_q: 4.384358
 35399/100000: episode: 744, duration: 0.168s, episode steps: 31, steps per second: 185, episode reward: 82.908, mean reward: 2.674 [1.748, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.778, 10.100], loss: 0.144603, mae: 0.369759, mean_q: 4.309869
 35496/100000: episode: 745, duration: 0.515s, episode steps: 97, steps per second: 188, episode reward: 183.324, mean reward: 1.890 [1.461, 4.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.480 [-0.977, 10.214], loss: 0.214325, mae: 0.434616, mean_q: 4.390793
 35530/100000: episode: 746, duration: 0.163s, episode steps: 34, steps per second: 208, episode reward: 76.198, mean reward: 2.241 [1.521, 6.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.027 [-0.521, 10.106], loss: 0.207014, mae: 0.410072, mean_q: 4.400428
 35621/100000: episode: 747, duration: 0.460s, episode steps: 91, steps per second: 198, episode reward: 179.525, mean reward: 1.973 [1.485, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 1.536 [-0.873, 10.251], loss: 0.208950, mae: 0.409010, mean_q: 4.367953
 35655/100000: episode: 748, duration: 0.189s, episode steps: 34, steps per second: 180, episode reward: 70.549, mean reward: 2.075 [1.526, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.510, 10.100], loss: 0.251990, mae: 0.442436, mean_q: 4.384309
 35689/100000: episode: 749, duration: 0.169s, episode steps: 34, steps per second: 201, episode reward: 66.685, mean reward: 1.961 [1.475, 3.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.555, 10.263], loss: 0.252869, mae: 0.417147, mean_q: 4.384688
 35773/100000: episode: 750, duration: 0.452s, episode steps: 84, steps per second: 186, episode reward: 171.272, mean reward: 2.039 [1.456, 3.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.610 [-1.309, 10.100], loss: 0.260725, mae: 0.428878, mean_q: 4.344164
 35804/100000: episode: 751, duration: 0.158s, episode steps: 31, steps per second: 196, episode reward: 72.795, mean reward: 2.348 [1.707, 6.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.497, 10.100], loss: 0.199537, mae: 0.408233, mean_q: 4.438417
 35835/100000: episode: 752, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 189.204, mean reward: 6.103 [1.825, 23.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-1.104, 10.100], loss: 0.230769, mae: 0.405576, mean_q: 4.415088
 35871/100000: episode: 753, duration: 0.181s, episode steps: 36, steps per second: 199, episode reward: 125.599, mean reward: 3.489 [2.118, 9.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.403, 10.100], loss: 0.234977, mae: 0.418354, mean_q: 4.413243
 35967/100000: episode: 754, duration: 0.484s, episode steps: 96, steps per second: 198, episode reward: 193.183, mean reward: 2.012 [1.484, 4.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.483 [-0.856, 10.100], loss: 0.378850, mae: 0.480156, mean_q: 4.453536
 36003/100000: episode: 755, duration: 0.213s, episode steps: 36, steps per second: 169, episode reward: 108.502, mean reward: 3.014 [2.177, 4.153], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-1.425, 10.100], loss: 0.469525, mae: 0.526945, mean_q: 4.612434
 36039/100000: episode: 756, duration: 0.175s, episode steps: 36, steps per second: 205, episode reward: 108.194, mean reward: 3.005 [2.052, 8.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.173, 10.100], loss: 0.307839, mae: 0.502308, mean_q: 4.372368
 36075/100000: episode: 757, duration: 0.195s, episode steps: 36, steps per second: 184, episode reward: 121.966, mean reward: 3.388 [2.068, 6.508], mean action: 0.000 [0.000, 0.000], mean observation: 1.994 [-0.852, 10.100], loss: 0.222619, mae: 0.438882, mean_q: 4.413143
 36159/100000: episode: 758, duration: 0.451s, episode steps: 84, steps per second: 186, episode reward: 184.607, mean reward: 2.198 [1.440, 4.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.603 [-0.205, 10.100], loss: 0.402639, mae: 0.481843, mean_q: 4.487323
 36251/100000: episode: 759, duration: 0.493s, episode steps: 92, steps per second: 186, episode reward: 171.922, mean reward: 1.869 [1.464, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.533 [-1.118, 10.201], loss: 0.423035, mae: 0.500439, mean_q: 4.527156
 36282/100000: episode: 760, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 89.650, mean reward: 2.892 [2.074, 3.921], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.307, 10.100], loss: 0.439969, mae: 0.486847, mean_q: 4.563120
 36313/100000: episode: 761, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 92.526, mean reward: 2.985 [1.731, 4.311], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.294, 10.100], loss: 0.239112, mae: 0.428550, mean_q: 4.499734
 36349/100000: episode: 762, duration: 0.198s, episode steps: 36, steps per second: 182, episode reward: 149.092, mean reward: 4.141 [2.872, 6.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-1.606, 10.100], loss: 0.265566, mae: 0.456160, mean_q: 4.580585
 36383/100000: episode: 763, duration: 0.198s, episode steps: 34, steps per second: 172, episode reward: 86.465, mean reward: 2.543 [1.670, 3.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.140, 10.100], loss: 0.229694, mae: 0.431962, mean_q: 4.578422
 36419/100000: episode: 764, duration: 0.196s, episode steps: 36, steps per second: 184, episode reward: 134.160, mean reward: 3.727 [2.292, 8.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.993 [-0.962, 10.100], loss: 0.601936, mae: 0.549667, mean_q: 4.580601
 36515/100000: episode: 765, duration: 0.599s, episode steps: 96, steps per second: 160, episode reward: 180.719, mean reward: 1.882 [1.438, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.490 [-0.649, 10.283], loss: 0.354573, mae: 0.485944, mean_q: 4.598217
 36611/100000: episode: 766, duration: 0.516s, episode steps: 96, steps per second: 186, episode reward: 180.482, mean reward: 1.880 [1.478, 2.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.490 [-0.548, 10.278], loss: 0.319718, mae: 0.480563, mean_q: 4.622174
 36647/100000: episode: 767, duration: 0.205s, episode steps: 36, steps per second: 175, episode reward: 81.993, mean reward: 2.278 [1.481, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.941, 10.115], loss: 0.446348, mae: 0.477923, mean_q: 4.648118
 36739/100000: episode: 768, duration: 0.556s, episode steps: 92, steps per second: 165, episode reward: 182.318, mean reward: 1.982 [1.466, 2.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-0.872, 10.100], loss: 0.386717, mae: 0.495231, mean_q: 4.562684
 36773/100000: episode: 769, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 76.703, mean reward: 2.256 [1.737, 3.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-0.179, 10.100], loss: 0.334253, mae: 0.464702, mean_q: 4.673664
 36814/100000: episode: 770, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 94.711, mean reward: 2.310 [1.742, 4.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.328, 10.100], loss: 0.314466, mae: 0.477629, mean_q: 4.631276
 36850/100000: episode: 771, duration: 0.206s, episode steps: 36, steps per second: 175, episode reward: 112.717, mean reward: 3.131 [2.047, 5.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.345, 10.100], loss: 0.447786, mae: 0.486277, mean_q: 4.581469
 36946/100000: episode: 772, duration: 0.515s, episode steps: 96, steps per second: 186, episode reward: 178.917, mean reward: 1.864 [1.491, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.491 [-0.982, 10.100], loss: 0.306371, mae: 0.457156, mean_q: 4.605027
 37038/100000: episode: 773, duration: 0.576s, episode steps: 92, steps per second: 160, episode reward: 185.331, mean reward: 2.014 [1.482, 3.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.526 [-0.656, 10.100], loss: 0.222804, mae: 0.431061, mean_q: 4.607646
 37074/100000: episode: 774, duration: 0.205s, episode steps: 36, steps per second: 175, episode reward: 113.363, mean reward: 3.149 [1.834, 5.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-0.468, 10.100], loss: 0.411850, mae: 0.479567, mean_q: 4.641528
 37166/100000: episode: 775, duration: 0.505s, episode steps: 92, steps per second: 182, episode reward: 166.662, mean reward: 1.812 [1.453, 2.929], mean action: 0.000 [0.000, 0.000], mean observation: 1.533 [-0.992, 10.118], loss: 0.339741, mae: 0.479478, mean_q: 4.635367
 37200/100000: episode: 776, duration: 0.165s, episode steps: 34, steps per second: 206, episode reward: 79.038, mean reward: 2.325 [1.852, 3.876], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.913, 10.100], loss: 0.363306, mae: 0.493380, mean_q: 4.709478
 37296/100000: episode: 777, duration: 0.523s, episode steps: 96, steps per second: 184, episode reward: 179.649, mean reward: 1.871 [1.470, 2.855], mean action: 0.000 [0.000, 0.000], mean observation: 1.494 [-0.447, 10.269], loss: 0.343287, mae: 0.461206, mean_q: 4.659185
 37327/100000: episode: 778, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 76.506, mean reward: 2.468 [1.797, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.659, 10.100], loss: 0.235332, mae: 0.456466, mean_q: 4.625693
 37411/100000: episode: 779, duration: 0.447s, episode steps: 84, steps per second: 188, episode reward: 194.544, mean reward: 2.316 [1.470, 7.275], mean action: 0.000 [0.000, 0.000], mean observation: 1.604 [-1.137, 10.100], loss: 0.408426, mae: 0.503174, mean_q: 4.691006
 37503/100000: episode: 780, duration: 0.505s, episode steps: 92, steps per second: 182, episode reward: 184.615, mean reward: 2.007 [1.485, 4.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.523 [-1.479, 10.149], loss: 0.329634, mae: 0.465812, mean_q: 4.607878
 37587/100000: episode: 781, duration: 0.451s, episode steps: 84, steps per second: 186, episode reward: 179.372, mean reward: 2.135 [1.489, 4.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.605 [-0.444, 10.209], loss: 0.287086, mae: 0.450568, mean_q: 4.597694
 37683/100000: episode: 782, duration: 0.512s, episode steps: 96, steps per second: 188, episode reward: 179.516, mean reward: 1.870 [1.448, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.486 [-0.872, 10.100], loss: 0.308047, mae: 0.432576, mean_q: 4.577807
 37779/100000: episode: 783, duration: 0.517s, episode steps: 96, steps per second: 186, episode reward: 180.182, mean reward: 1.877 [1.437, 2.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.495 [-0.593, 10.259], loss: 0.407882, mae: 0.479753, mean_q: 4.638665
 37871/100000: episode: 784, duration: 0.501s, episode steps: 92, steps per second: 184, episode reward: 174.372, mean reward: 1.895 [1.432, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.540 [-0.512, 10.260], loss: 0.276766, mae: 0.443206, mean_q: 4.573362
[Info] 2-TH LEVEL FOUND: 8.65021800994873, Considering 10/90 traces
 37963/100000: episode: 785, duration: 4.581s, episode steps: 92, steps per second: 20, episode reward: 164.001, mean reward: 1.783 [1.451, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.539 [-1.325, 10.198], loss: 0.286552, mae: 0.448560, mean_q: 4.578008
 37987/100000: episode: 786, duration: 0.130s, episode steps: 24, steps per second: 184, episode reward: 84.872, mean reward: 3.536 [2.225, 5.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.123, 10.100], loss: 0.232646, mae: 0.435752, mean_q: 4.475977
 38006/100000: episode: 787, duration: 0.127s, episode steps: 19, steps per second: 150, episode reward: 60.308, mean reward: 3.174 [2.709, 3.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.270, 10.100], loss: 0.515895, mae: 0.466648, mean_q: 4.548465
 38030/100000: episode: 788, duration: 0.122s, episode steps: 24, steps per second: 196, episode reward: 99.749, mean reward: 4.156 [2.879, 8.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.336, 10.100], loss: 0.471399, mae: 0.471775, mean_q: 4.547261
 38047/100000: episode: 789, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 54.810, mean reward: 3.224 [2.514, 4.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.174, 10.100], loss: 0.191032, mae: 0.432318, mean_q: 4.579585
 38070/100000: episode: 790, duration: 0.122s, episode steps: 23, steps per second: 188, episode reward: 77.698, mean reward: 3.378 [2.135, 4.992], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.869, 10.100], loss: 0.562501, mae: 0.519114, mean_q: 4.652527
 38093/100000: episode: 791, duration: 0.144s, episode steps: 23, steps per second: 159, episode reward: 72.483, mean reward: 3.151 [1.892, 8.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.752, 10.100], loss: 0.239667, mae: 0.447608, mean_q: 4.604385
 38119/100000: episode: 792, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 89.278, mean reward: 3.434 [2.095, 8.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.421, 10.100], loss: 0.565537, mae: 0.500426, mean_q: 4.720162
 38139/100000: episode: 793, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 59.485, mean reward: 2.974 [2.301, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.292, 10.100], loss: 0.434123, mae: 0.504667, mean_q: 4.630476
 38158/100000: episode: 794, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 58.541, mean reward: 3.081 [2.462, 4.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.252, 10.100], loss: 0.392083, mae: 0.500467, mean_q: 4.683731
 38175/100000: episode: 795, duration: 0.097s, episode steps: 17, steps per second: 176, episode reward: 79.078, mean reward: 4.652 [2.405, 10.768], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.256, 10.100], loss: 0.487382, mae: 0.554860, mean_q: 4.705566
 38196/100000: episode: 796, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 65.569, mean reward: 3.122 [2.386, 4.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.186, 10.100], loss: 0.311398, mae: 0.468283, mean_q: 4.707733
 38221/100000: episode: 797, duration: 0.125s, episode steps: 25, steps per second: 200, episode reward: 84.137, mean reward: 3.365 [1.826, 5.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.202, 10.100], loss: 0.315559, mae: 0.479878, mean_q: 4.745305
 38248/100000: episode: 798, duration: 0.151s, episode steps: 27, steps per second: 179, episode reward: 133.410, mean reward: 4.941 [3.669, 9.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.453, 10.100], loss: 0.333549, mae: 0.476903, mean_q: 4.632640
 38271/100000: episode: 799, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 60.649, mean reward: 2.637 [2.022, 4.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.316, 10.100], loss: 0.364218, mae: 0.496678, mean_q: 4.846784
 38288/100000: episode: 800, duration: 0.102s, episode steps: 17, steps per second: 166, episode reward: 61.901, mean reward: 3.641 [2.732, 5.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.219, 10.100], loss: 0.242037, mae: 0.460581, mean_q: 4.566663
 38308/100000: episode: 801, duration: 0.119s, episode steps: 20, steps per second: 169, episode reward: 60.523, mean reward: 3.026 [2.029, 4.236], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.157, 10.100], loss: 0.394539, mae: 0.452893, mean_q: 4.658653
 38328/100000: episode: 802, duration: 0.094s, episode steps: 20, steps per second: 213, episode reward: 55.085, mean reward: 2.754 [2.124, 4.593], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.350, 10.100], loss: 0.251565, mae: 0.485891, mean_q: 4.617908
 38355/100000: episode: 803, duration: 0.134s, episode steps: 27, steps per second: 202, episode reward: 102.943, mean reward: 3.813 [2.399, 12.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.403, 10.100], loss: 0.237601, mae: 0.449771, mean_q: 4.624584
 38372/100000: episode: 804, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 66.028, mean reward: 3.884 [2.859, 5.679], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.652, 10.100], loss: 0.334918, mae: 0.496402, mean_q: 4.708059
 38393/100000: episode: 805, duration: 0.105s, episode steps: 21, steps per second: 201, episode reward: 95.819, mean reward: 4.563 [2.436, 14.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.434, 10.100], loss: 0.371886, mae: 0.480281, mean_q: 4.788564
 38416/100000: episode: 806, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 72.101, mean reward: 3.135 [2.382, 4.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.846, 10.100], loss: 0.573058, mae: 0.505595, mean_q: 4.761900
 38436/100000: episode: 807, duration: 0.174s, episode steps: 20, steps per second: 115, episode reward: 57.843, mean reward: 2.892 [2.465, 3.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-2.142, 10.100], loss: 0.499344, mae: 0.460528, mean_q: 4.745692
 38461/100000: episode: 808, duration: 0.202s, episode steps: 25, steps per second: 123, episode reward: 113.799, mean reward: 4.552 [2.260, 8.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.433, 10.100], loss: 0.296242, mae: 0.493900, mean_q: 4.627708
 38481/100000: episode: 809, duration: 0.134s, episode steps: 20, steps per second: 149, episode reward: 58.299, mean reward: 2.915 [2.333, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.384, 10.100], loss: 0.273236, mae: 0.461498, mean_q: 4.756000
 38505/100000: episode: 810, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 83.174, mean reward: 3.466 [2.833, 4.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.408, 10.100], loss: 0.258994, mae: 0.479292, mean_q: 4.781838
 38528/100000: episode: 811, duration: 0.116s, episode steps: 23, steps per second: 198, episode reward: 103.580, mean reward: 4.503 [2.140, 12.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.423, 10.100], loss: 0.352313, mae: 0.456234, mean_q: 4.678802
 38554/100000: episode: 812, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 89.613, mean reward: 3.447 [2.333, 6.010], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.224, 10.100], loss: 0.424601, mae: 0.510463, mean_q: 4.833503
 38574/100000: episode: 813, duration: 0.165s, episode steps: 20, steps per second: 122, episode reward: 79.225, mean reward: 3.961 [2.683, 5.253], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-1.049, 10.100], loss: 0.332518, mae: 0.458712, mean_q: 4.711135
 38595/100000: episode: 814, duration: 0.156s, episode steps: 21, steps per second: 134, episode reward: 69.333, mean reward: 3.302 [2.589, 4.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.947, 10.100], loss: 0.197275, mae: 0.416420, mean_q: 4.863896
 38619/100000: episode: 815, duration: 0.206s, episode steps: 24, steps per second: 117, episode reward: 79.343, mean reward: 3.306 [2.206, 5.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.444, 10.100], loss: 0.292398, mae: 0.456643, mean_q: 4.728750
 38642/100000: episode: 816, duration: 0.145s, episode steps: 23, steps per second: 158, episode reward: 63.975, mean reward: 2.782 [2.071, 3.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-1.488, 10.100], loss: 0.339530, mae: 0.494743, mean_q: 4.805282
 38662/100000: episode: 817, duration: 0.127s, episode steps: 20, steps per second: 158, episode reward: 63.892, mean reward: 3.195 [2.553, 5.521], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.250, 10.100], loss: 0.298562, mae: 0.500745, mean_q: 4.737239
 38682/100000: episode: 818, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 68.044, mean reward: 3.402 [2.801, 4.194], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.446, 10.100], loss: 0.605307, mae: 0.506569, mean_q: 4.800153
 38706/100000: episode: 819, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 85.409, mean reward: 3.559 [2.485, 5.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.321, 10.100], loss: 0.543053, mae: 0.520207, mean_q: 4.837461
 38726/100000: episode: 820, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 62.770, mean reward: 3.139 [2.488, 4.232], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.253, 10.100], loss: 0.244719, mae: 0.438629, mean_q: 4.753182
 38749/100000: episode: 821, duration: 0.133s, episode steps: 23, steps per second: 173, episode reward: 60.999, mean reward: 2.652 [2.198, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.894, 10.100], loss: 0.325649, mae: 0.498345, mean_q: 4.776244
 38775/100000: episode: 822, duration: 0.152s, episode steps: 26, steps per second: 171, episode reward: 106.762, mean reward: 4.106 [2.156, 14.787], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.489, 10.100], loss: 0.304395, mae: 0.463874, mean_q: 4.783381
 38795/100000: episode: 823, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 69.361, mean reward: 3.468 [2.507, 4.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.200, 10.100], loss: 0.308483, mae: 0.459241, mean_q: 4.721160
 38818/100000: episode: 824, duration: 0.135s, episode steps: 23, steps per second: 171, episode reward: 58.093, mean reward: 2.526 [2.117, 3.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.818, 10.100], loss: 0.196172, mae: 0.428515, mean_q: 4.702933
 38835/100000: episode: 825, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 59.576, mean reward: 3.504 [2.407, 5.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.555, 10.100], loss: 0.335765, mae: 0.446710, mean_q: 4.818439
 38852/100000: episode: 826, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 88.757, mean reward: 5.221 [3.343, 8.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.341, 10.100], loss: 0.596241, mae: 0.534600, mean_q: 4.962554
 38878/100000: episode: 827, duration: 0.159s, episode steps: 26, steps per second: 163, episode reward: 114.172, mean reward: 4.391 [2.936, 8.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.462, 10.100], loss: 0.255744, mae: 0.462386, mean_q: 4.824734
 38899/100000: episode: 828, duration: 0.130s, episode steps: 21, steps per second: 161, episode reward: 63.329, mean reward: 3.016 [1.582, 5.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.177], loss: 0.388701, mae: 0.506921, mean_q: 4.951949
 38925/100000: episode: 829, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 90.811, mean reward: 3.493 [2.162, 4.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.518, 10.100], loss: 0.254709, mae: 0.455710, mean_q: 4.800900
 38951/100000: episode: 830, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 128.296, mean reward: 4.934 [3.341, 7.318], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.785, 10.100], loss: 0.444320, mae: 0.520847, mean_q: 4.942070
 38976/100000: episode: 831, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 161.489, mean reward: 6.460 [3.971, 11.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.835, 10.100], loss: 0.558463, mae: 0.541115, mean_q: 4.933228
 38995/100000: episode: 832, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 94.199, mean reward: 4.958 [3.783, 7.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.736, 10.100], loss: 0.431575, mae: 0.539519, mean_q: 4.967473
 39014/100000: episode: 833, duration: 0.108s, episode steps: 19, steps per second: 176, episode reward: 96.765, mean reward: 5.093 [3.756, 8.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.821, 10.100], loss: 0.285527, mae: 0.504958, mean_q: 4.900915
 39033/100000: episode: 834, duration: 0.136s, episode steps: 19, steps per second: 140, episode reward: 120.450, mean reward: 6.339 [3.483, 25.727], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.317, 10.100], loss: 0.364405, mae: 0.531744, mean_q: 5.115193
 39059/100000: episode: 835, duration: 0.194s, episode steps: 26, steps per second: 134, episode reward: 94.041, mean reward: 3.617 [2.384, 5.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.298, 10.100], loss: 0.413057, mae: 0.505381, mean_q: 4.990797
 39086/100000: episode: 836, duration: 0.213s, episode steps: 27, steps per second: 127, episode reward: 112.455, mean reward: 4.165 [1.901, 9.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.086, 10.100], loss: 0.343549, mae: 0.501839, mean_q: 5.000991
 39113/100000: episode: 837, duration: 0.169s, episode steps: 27, steps per second: 160, episode reward: 199.342, mean reward: 7.383 [1.951, 54.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.218, 10.100], loss: 1.898323, mae: 0.673281, mean_q: 5.029308
 39133/100000: episode: 838, duration: 0.136s, episode steps: 20, steps per second: 147, episode reward: 66.214, mean reward: 3.311 [2.989, 3.904], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.847, 10.100], loss: 0.406546, mae: 0.569359, mean_q: 5.006800
 39157/100000: episode: 839, duration: 0.168s, episode steps: 24, steps per second: 143, episode reward: 68.405, mean reward: 2.850 [1.849, 5.422], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.202, 10.100], loss: 0.481321, mae: 0.557573, mean_q: 5.107202
 39182/100000: episode: 840, duration: 0.168s, episode steps: 25, steps per second: 149, episode reward: 137.576, mean reward: 5.503 [3.411, 7.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.539, 10.100], loss: 0.580936, mae: 0.555042, mean_q: 5.105011
[Info] FALSIFICATION!
 39188/100000: episode: 841, duration: 0.225s, episode steps: 6, steps per second: 27, episode reward: 1043.742, mean reward: 173.957 [4.403, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.872 [-0.350, 9.139], loss: 1.682356, mae: 0.643126, mean_q: 5.088115
 39209/100000: episode: 842, duration: 0.149s, episode steps: 21, steps per second: 141, episode reward: 61.435, mean reward: 2.925 [2.186, 4.633], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.139, 10.100], loss: 735.384705, mae: 3.897529, mean_q: 7.172912
 39232/100000: episode: 843, duration: 0.161s, episode steps: 23, steps per second: 143, episode reward: 140.624, mean reward: 6.114 [2.285, 24.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.137, 10.100], loss: 671.676697, mae: 3.109239, mean_q: 6.364230
 39251/100000: episode: 844, duration: 0.128s, episode steps: 19, steps per second: 148, episode reward: 105.395, mean reward: 5.547 [2.910, 8.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.803, 10.100], loss: 1.266274, mae: 0.984621, mean_q: 5.359666
 39276/100000: episode: 845, duration: 0.161s, episode steps: 25, steps per second: 155, episode reward: 110.606, mean reward: 4.424 [2.694, 11.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.314, 10.100], loss: 0.447724, mae: 0.631693, mean_q: 5.264202
 39303/100000: episode: 846, duration: 0.171s, episode steps: 27, steps per second: 158, episode reward: 82.938, mean reward: 3.072 [1.710, 6.231], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.046, 10.100], loss: 570.532715, mae: 1.919511, mean_q: 5.560682
 39328/100000: episode: 847, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 104.545, mean reward: 4.182 [2.203, 19.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.554, 10.100], loss: 1.962751, mae: 1.492665, mean_q: 6.415286
 39348/100000: episode: 848, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 64.835, mean reward: 3.242 [2.216, 4.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.280, 10.100], loss: 0.967444, mae: 0.833202, mean_q: 5.542722
 39369/100000: episode: 849, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 189.381, mean reward: 9.018 [3.432, 46.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.761, 10.100], loss: 2.525392, mae: 0.771032, mean_q: 5.584950
 39396/100000: episode: 850, duration: 0.143s, episode steps: 27, steps per second: 188, episode reward: 86.332, mean reward: 3.197 [2.163, 6.024], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.219, 10.100], loss: 0.704738, mae: 0.675824, mean_q: 5.370963
 39423/100000: episode: 851, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 96.966, mean reward: 3.591 [2.051, 8.420], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.327, 10.100], loss: 2.810809, mae: 1.425537, mean_q: 6.681103
 39444/100000: episode: 852, duration: 0.105s, episode steps: 21, steps per second: 199, episode reward: 86.467, mean reward: 4.117 [3.228, 5.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.432, 10.100], loss: 2.674304, mae: 0.874351, mean_q: 5.750793
 39470/100000: episode: 853, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 67.916, mean reward: 2.612 [1.729, 4.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.283, 10.100], loss: 1.157780, mae: 0.745048, mean_q: 5.645828
 39497/100000: episode: 854, duration: 0.146s, episode steps: 27, steps per second: 184, episode reward: 75.769, mean reward: 2.806 [1.771, 4.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.086, 10.100], loss: 0.671320, mae: 0.654576, mean_q: 5.544846
 39516/100000: episode: 855, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 75.884, mean reward: 3.994 [2.910, 5.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.534, 10.100], loss: 0.668584, mae: 0.650296, mean_q: 5.496984
 39536/100000: episode: 856, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 55.412, mean reward: 2.771 [2.354, 3.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.264, 10.100], loss: 1.313561, mae: 0.713037, mean_q: 5.582740
 39556/100000: episode: 857, duration: 0.097s, episode steps: 20, steps per second: 207, episode reward: 59.537, mean reward: 2.977 [2.214, 4.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.334, 10.100], loss: 769.752075, mae: 2.905553, mean_q: 6.652276
 39579/100000: episode: 858, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 93.287, mean reward: 4.056 [2.160, 8.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.501, 10.100], loss: 4.028896, mae: 1.173714, mean_q: 5.854847
 39604/100000: episode: 859, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 153.494, mean reward: 6.140 [3.952, 10.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.567, 10.100], loss: 2.969363, mae: 0.933145, mean_q: 5.866538
 39628/100000: episode: 860, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 75.471, mean reward: 3.145 [2.531, 4.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.527, 10.100], loss: 0.950348, mae: 0.756474, mean_q: 5.693359
 39652/100000: episode: 861, duration: 0.136s, episode steps: 24, steps per second: 176, episode reward: 94.799, mean reward: 3.950 [2.681, 5.554], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.922, 10.100], loss: 640.475403, mae: 2.160564, mean_q: 5.988514
 39679/100000: episode: 862, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 88.328, mean reward: 3.271 [1.748, 5.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.214, 10.100], loss: 2.182716, mae: 1.472140, mean_q: 6.461509
 39696/100000: episode: 863, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 66.579, mean reward: 3.916 [2.527, 5.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.325, 10.100], loss: 3.242071, mae: 0.887046, mean_q: 5.738796
 39722/100000: episode: 864, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 104.994, mean reward: 4.038 [3.259, 4.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.334, 10.100], loss: 590.997925, mae: 2.545293, mean_q: 6.752398
 39746/100000: episode: 865, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 98.856, mean reward: 4.119 [2.917, 5.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.927, 10.100], loss: 0.807479, mae: 0.742623, mean_q: 5.692550
 39765/100000: episode: 866, duration: 0.095s, episode steps: 19, steps per second: 199, episode reward: 72.185, mean reward: 3.799 [2.346, 6.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.393, 10.100], loss: 2.627021, mae: 0.795284, mean_q: 5.999142
 39782/100000: episode: 867, duration: 0.102s, episode steps: 17, steps per second: 167, episode reward: 90.932, mean reward: 5.349 [3.369, 8.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.411, 10.100], loss: 2.220514, mae: 0.798456, mean_q: 6.048207
 39802/100000: episode: 868, duration: 0.123s, episode steps: 20, steps per second: 162, episode reward: 71.158, mean reward: 3.558 [2.493, 5.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.399, 10.100], loss: 0.919119, mae: 0.713664, mean_q: 5.735040
 39825/100000: episode: 869, duration: 0.113s, episode steps: 23, steps per second: 203, episode reward: 61.312, mean reward: 2.666 [2.205, 3.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.562, 10.100], loss: 667.920105, mae: 2.229733, mean_q: 6.082719
 39848/100000: episode: 870, duration: 0.138s, episode steps: 23, steps per second: 167, episode reward: 80.118, mean reward: 3.483 [2.158, 4.763], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-1.364, 10.100], loss: 4.093774, mae: 1.861346, mean_q: 7.359338
 39872/100000: episode: 871, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 68.846, mean reward: 2.869 [1.787, 4.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.241, 10.100], loss: 1.104567, mae: 0.859780, mean_q: 5.586750
 39898/100000: episode: 872, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 90.648, mean reward: 3.486 [2.675, 5.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.303, 10.100], loss: 591.335754, mae: 2.535799, mean_q: 6.713018
 39925/100000: episode: 873, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 77.854, mean reward: 2.883 [1.632, 4.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.197, 10.100], loss: 1.077924, mae: 0.963413, mean_q: 6.040229
 39945/100000: episode: 874, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 92.387, mean reward: 4.619 [2.883, 10.292], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.402, 10.100], loss: 0.532485, mae: 0.727325, mean_q: 5.913146
[Info] Complete ISplit Iteration
[Info] Levels: [4.329551, 8.650218, 9.522885]
[Info] Cond. Prob: [0.1, 0.1, 0.46]
[Info] Error Prob: 0.004600000000000001

 39971/100000: episode: 875, duration: 4.242s, episode steps: 26, steps per second: 6, episode reward: 73.831, mean reward: 2.840 [2.165, 4.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.388, 10.100], loss: 1.305886, mae: 0.873138, mean_q: 6.123327
 40071/100000: episode: 876, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 214.716, mean reward: 2.147 [1.457, 3.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.033, 10.098], loss: 154.965942, mae: 1.335940, mean_q: 6.327160
 40171/100000: episode: 877, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 182.125, mean reward: 1.821 [1.478, 2.631], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.216, 10.226], loss: 155.429672, mae: 1.339623, mean_q: 6.230989
 40271/100000: episode: 878, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 198.147, mean reward: 1.981 [1.453, 3.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.467, 10.223], loss: 308.451416, mae: 1.934086, mean_q: 6.685034
 40371/100000: episode: 879, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 224.914, mean reward: 2.249 [1.458, 4.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.225, 10.344], loss: 155.830154, mae: 1.330344, mean_q: 6.215372
 40471/100000: episode: 880, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 196.210, mean reward: 1.962 [1.435, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.921, 10.237], loss: 307.526184, mae: 1.897389, mean_q: 6.440629
 40571/100000: episode: 881, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 516.284, mean reward: 5.163 [1.485, 102.752], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.010, 10.588], loss: 307.530640, mae: 1.998202, mean_q: 6.484546
 40671/100000: episode: 882, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 201.019, mean reward: 2.010 [1.508, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.969, 10.174], loss: 4.101969, mae: 0.920661, mean_q: 6.083049
 40771/100000: episode: 883, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 193.726, mean reward: 1.937 [1.483, 3.285], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.230, 10.212], loss: 155.987808, mae: 1.384098, mean_q: 6.212943
 40871/100000: episode: 884, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 194.829, mean reward: 1.948 [1.464, 3.722], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.062, 10.335], loss: 1.211461, mae: 0.751359, mean_q: 5.777983
 40971/100000: episode: 885, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 189.064, mean reward: 1.891 [1.467, 4.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.390, 10.164], loss: 1.305506, mae: 0.710279, mean_q: 5.669231
 41071/100000: episode: 886, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 205.235, mean reward: 2.052 [1.502, 4.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.714, 10.326], loss: 4.136824, mae: 0.807739, mean_q: 5.570761
 41171/100000: episode: 887, duration: 0.531s, episode steps: 100, steps per second: 189, episode reward: 196.163, mean reward: 1.962 [1.485, 3.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.237, 10.098], loss: 1.749975, mae: 0.747762, mean_q: 5.550352
 41271/100000: episode: 888, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 190.310, mean reward: 1.903 [1.442, 3.253], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.631, 10.297], loss: 1.146197, mae: 0.714851, mean_q: 5.495270
 41371/100000: episode: 889, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 189.548, mean reward: 1.895 [1.481, 4.577], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.006, 10.387], loss: 309.566254, mae: 1.953290, mean_q: 6.215866
 41471/100000: episode: 890, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 184.472, mean reward: 1.845 [1.455, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.281, 10.098], loss: 154.833679, mae: 1.280385, mean_q: 5.922220
 41571/100000: episode: 891, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 205.937, mean reward: 2.059 [1.451, 4.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.538, 10.414], loss: 1.620071, mae: 0.786363, mean_q: 5.588434
 41671/100000: episode: 892, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 207.163, mean reward: 2.072 [1.490, 4.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.005, 10.098], loss: 309.484222, mae: 1.879367, mean_q: 6.113413
 41771/100000: episode: 893, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 188.195, mean reward: 1.882 [1.462, 2.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.589, 10.123], loss: 154.787155, mae: 1.305559, mean_q: 6.009844
 41871/100000: episode: 894, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 185.484, mean reward: 1.855 [1.462, 3.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.736, 10.216], loss: 156.151108, mae: 1.314852, mean_q: 5.882311
 41971/100000: episode: 895, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 195.544, mean reward: 1.955 [1.475, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.938, 10.098], loss: 155.955200, mae: 1.362888, mean_q: 6.021789
 42071/100000: episode: 896, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 209.422, mean reward: 2.094 [1.503, 3.760], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.005, 10.383], loss: 155.754318, mae: 1.120473, mean_q: 5.708901
 42171/100000: episode: 897, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 190.401, mean reward: 1.904 [1.454, 3.213], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.614, 10.098], loss: 1.635573, mae: 0.939705, mean_q: 5.866246
 42271/100000: episode: 898, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 193.326, mean reward: 1.933 [1.469, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.163, 10.098], loss: 307.476044, mae: 1.779465, mean_q: 6.189220
 42371/100000: episode: 899, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 195.970, mean reward: 1.960 [1.480, 3.178], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.632, 10.164], loss: 156.465668, mae: 1.395674, mean_q: 5.989432
 42471/100000: episode: 900, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 205.132, mean reward: 2.051 [1.478, 4.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.649, 10.148], loss: 2.929562, mae: 0.814598, mean_q: 5.595123
 42571/100000: episode: 901, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 199.278, mean reward: 1.993 [1.451, 3.965], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.089, 10.275], loss: 1.200730, mae: 0.697257, mean_q: 5.535508
 42671/100000: episode: 902, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 188.633, mean reward: 1.886 [1.458, 3.813], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.087, 10.289], loss: 154.230072, mae: 1.199382, mean_q: 5.832623
 42771/100000: episode: 903, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 185.578, mean reward: 1.856 [1.468, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.832, 10.098], loss: 154.353073, mae: 1.303940, mean_q: 5.682048
 42871/100000: episode: 904, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 193.511, mean reward: 1.935 [1.447, 2.776], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.369, 10.098], loss: 1.747270, mae: 0.711725, mean_q: 5.487497
 42971/100000: episode: 905, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 184.907, mean reward: 1.849 [1.468, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.432, 10.098], loss: 2.786958, mae: 0.731502, mean_q: 5.370897
 43071/100000: episode: 906, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 190.845, mean reward: 1.908 [1.482, 2.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.060, 10.098], loss: 2.613008, mae: 0.717026, mean_q: 5.349165
 43171/100000: episode: 907, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 183.865, mean reward: 1.839 [1.483, 2.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.668, 10.224], loss: 156.704559, mae: 1.361856, mean_q: 5.622173
 43271/100000: episode: 908, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 192.647, mean reward: 1.926 [1.465, 3.522], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.557, 10.098], loss: 4.046713, mae: 0.769426, mean_q: 5.295693
 43371/100000: episode: 909, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 188.675, mean reward: 1.887 [1.443, 3.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.315, 10.098], loss: 3.144260, mae: 0.770357, mean_q: 5.283730
 43471/100000: episode: 910, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 197.380, mean reward: 1.974 [1.464, 3.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.819, 10.252], loss: 2.856023, mae: 0.713250, mean_q: 5.094452
 43571/100000: episode: 911, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 194.016, mean reward: 1.940 [1.478, 3.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.168, 10.098], loss: 155.799377, mae: 1.177883, mean_q: 5.353972
 43671/100000: episode: 912, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 190.343, mean reward: 1.903 [1.439, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.278, 10.327], loss: 1.303162, mae: 0.700641, mean_q: 4.980986
 43771/100000: episode: 913, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 209.917, mean reward: 2.099 [1.481, 5.232], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.766, 10.323], loss: 1.225629, mae: 0.594572, mean_q: 4.902729
 43871/100000: episode: 914, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 193.881, mean reward: 1.939 [1.449, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.614, 10.098], loss: 1.347935, mae: 0.585132, mean_q: 4.905833
 43971/100000: episode: 915, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 202.347, mean reward: 2.023 [1.470, 3.041], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.285, 10.352], loss: 1.032299, mae: 0.537827, mean_q: 4.703928
 44071/100000: episode: 916, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 195.161, mean reward: 1.952 [1.436, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.396, 10.098], loss: 308.872192, mae: 1.794424, mean_q: 5.433713
 44171/100000: episode: 917, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 224.232, mean reward: 2.242 [1.483, 3.441], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.741, 10.098], loss: 1.046313, mae: 0.590415, mean_q: 4.790036
 44271/100000: episode: 918, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 183.048, mean reward: 1.830 [1.442, 2.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.626, 10.098], loss: 3.505864, mae: 0.571677, mean_q: 4.623631
 44371/100000: episode: 919, duration: 0.491s, episode steps: 100, steps per second: 203, episode reward: 186.202, mean reward: 1.862 [1.448, 2.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.492, 10.098], loss: 0.228307, mae: 0.426511, mean_q: 4.401663
 44471/100000: episode: 920, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 235.293, mean reward: 2.353 [1.459, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.291, 10.472], loss: 1.803213, mae: 0.493597, mean_q: 4.405440
 44571/100000: episode: 921, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 243.055, mean reward: 2.431 [1.573, 6.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.092, 10.098], loss: 0.251247, mae: 0.393120, mean_q: 4.300314
 44671/100000: episode: 922, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 188.276, mean reward: 1.883 [1.476, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.818, 10.194], loss: 1.668333, mae: 0.451878, mean_q: 4.209201
 44771/100000: episode: 923, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 191.620, mean reward: 1.916 [1.449, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.079, 10.201], loss: 1.774996, mae: 0.437855, mean_q: 4.143597
 44871/100000: episode: 924, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 195.076, mean reward: 1.951 [1.462, 3.470], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.768, 10.300], loss: 0.175335, mae: 0.370269, mean_q: 4.043218
 44971/100000: episode: 925, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 197.117, mean reward: 1.971 [1.442, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.978, 10.098], loss: 0.250423, mae: 0.364201, mean_q: 4.022968
 45071/100000: episode: 926, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 213.834, mean reward: 2.138 [1.543, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.739, 10.266], loss: 0.159827, mae: 0.343036, mean_q: 3.963758
 45171/100000: episode: 927, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 188.466, mean reward: 1.885 [1.456, 3.097], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.211, 10.098], loss: 0.238469, mae: 0.357369, mean_q: 4.004355
 45271/100000: episode: 928, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 187.903, mean reward: 1.879 [1.460, 3.150], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.130, 10.098], loss: 3.373273, mae: 0.508120, mean_q: 4.101419
 45371/100000: episode: 929, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 222.773, mean reward: 2.228 [1.484, 4.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.254, 10.261], loss: 0.150650, mae: 0.355802, mean_q: 3.947468
 45471/100000: episode: 930, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 193.893, mean reward: 1.939 [1.455, 4.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.024, 10.311], loss: 3.141793, mae: 0.480370, mean_q: 4.039094
 45571/100000: episode: 931, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 202.683, mean reward: 2.027 [1.468, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.689, 10.098], loss: 0.293206, mae: 0.365001, mean_q: 3.928449
 45671/100000: episode: 932, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 215.577, mean reward: 2.156 [1.485, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.460, 10.261], loss: 0.097693, mae: 0.317146, mean_q: 3.902110
 45771/100000: episode: 933, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 186.742, mean reward: 1.867 [1.455, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.627, 10.202], loss: 0.094059, mae: 0.311626, mean_q: 3.934978
 45871/100000: episode: 934, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 201.387, mean reward: 2.014 [1.510, 3.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.111, 10.182], loss: 0.085646, mae: 0.299845, mean_q: 3.903660
 45971/100000: episode: 935, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 201.944, mean reward: 2.019 [1.497, 4.313], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.384, 10.098], loss: 0.091736, mae: 0.312556, mean_q: 3.927956
 46071/100000: episode: 936, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 190.854, mean reward: 1.909 [1.466, 3.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.999, 10.303], loss: 0.087573, mae: 0.301388, mean_q: 3.920080
 46171/100000: episode: 937, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 203.449, mean reward: 2.034 [1.463, 4.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.794, 10.098], loss: 0.094757, mae: 0.312113, mean_q: 3.921921
 46271/100000: episode: 938, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 206.091, mean reward: 2.061 [1.477, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.043, 10.098], loss: 0.090193, mae: 0.312332, mean_q: 3.929249
 46371/100000: episode: 939, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 192.153, mean reward: 1.922 [1.438, 3.273], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.779, 10.098], loss: 0.084325, mae: 0.305038, mean_q: 3.924917
 46471/100000: episode: 940, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 184.836, mean reward: 1.848 [1.492, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.901, 10.099], loss: 0.086059, mae: 0.299456, mean_q: 3.929912
 46571/100000: episode: 941, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 180.230, mean reward: 1.802 [1.462, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.660, 10.192], loss: 0.088233, mae: 0.306285, mean_q: 3.934993
 46671/100000: episode: 942, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 198.814, mean reward: 1.988 [1.450, 4.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.299, 10.152], loss: 0.095399, mae: 0.302528, mean_q: 3.918052
 46771/100000: episode: 943, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 186.036, mean reward: 1.860 [1.479, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.824, 10.098], loss: 0.082191, mae: 0.299076, mean_q: 3.890243
 46871/100000: episode: 944, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 185.334, mean reward: 1.853 [1.440, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.277, 10.101], loss: 0.093687, mae: 0.315119, mean_q: 3.913441
 46971/100000: episode: 945, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 202.765, mean reward: 2.028 [1.457, 10.746], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.116, 10.149], loss: 0.082043, mae: 0.300946, mean_q: 3.913521
 47071/100000: episode: 946, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 198.596, mean reward: 1.986 [1.461, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.960, 10.098], loss: 0.094784, mae: 0.309405, mean_q: 3.908756
 47171/100000: episode: 947, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 219.337, mean reward: 2.193 [1.520, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.793, 10.200], loss: 0.087926, mae: 0.295740, mean_q: 3.928702
 47271/100000: episode: 948, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 196.655, mean reward: 1.967 [1.445, 4.628], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.565, 10.204], loss: 0.091763, mae: 0.307521, mean_q: 3.909569
 47371/100000: episode: 949, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 189.728, mean reward: 1.897 [1.442, 3.782], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.059, 10.098], loss: 0.088341, mae: 0.301884, mean_q: 3.908622
 47471/100000: episode: 950, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 207.899, mean reward: 2.079 [1.519, 4.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.976, 10.098], loss: 0.086453, mae: 0.301491, mean_q: 3.915092
 47571/100000: episode: 951, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 188.501, mean reward: 1.885 [1.534, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.327, 10.098], loss: 0.089895, mae: 0.304810, mean_q: 3.921603
 47671/100000: episode: 952, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 183.694, mean reward: 1.837 [1.475, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.454, 10.307], loss: 0.096858, mae: 0.306184, mean_q: 3.927680
 47771/100000: episode: 953, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 196.793, mean reward: 1.968 [1.464, 4.586], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.308, 10.098], loss: 0.095620, mae: 0.317566, mean_q: 3.933177
 47871/100000: episode: 954, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 229.435, mean reward: 2.294 [1.584, 5.603], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.408, 10.211], loss: 0.098551, mae: 0.302996, mean_q: 3.897328
 47971/100000: episode: 955, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 206.185, mean reward: 2.062 [1.519, 3.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.996, 10.098], loss: 0.095089, mae: 0.318940, mean_q: 3.935242
 48071/100000: episode: 956, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 192.116, mean reward: 1.921 [1.510, 3.048], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.743, 10.234], loss: 0.095867, mae: 0.313067, mean_q: 3.934974
 48171/100000: episode: 957, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 201.392, mean reward: 2.014 [1.470, 7.014], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.802, 10.164], loss: 0.088378, mae: 0.307192, mean_q: 3.926388
 48271/100000: episode: 958, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 192.841, mean reward: 1.928 [1.437, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.822, 10.185], loss: 0.097880, mae: 0.307626, mean_q: 3.922032
 48371/100000: episode: 959, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 184.886, mean reward: 1.849 [1.436, 2.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.208, 10.331], loss: 0.108636, mae: 0.306844, mean_q: 3.925115
 48471/100000: episode: 960, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 194.717, mean reward: 1.947 [1.497, 2.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.414, 10.098], loss: 0.110645, mae: 0.315107, mean_q: 3.958009
 48571/100000: episode: 961, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 237.862, mean reward: 2.379 [1.496, 4.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.404, 10.293], loss: 0.112924, mae: 0.313245, mean_q: 3.930332
 48671/100000: episode: 962, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 196.571, mean reward: 1.966 [1.460, 5.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.582, 10.098], loss: 0.108514, mae: 0.315620, mean_q: 3.934688
 48771/100000: episode: 963, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 190.375, mean reward: 1.904 [1.429, 3.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.424, 10.228], loss: 0.116872, mae: 0.310025, mean_q: 3.947777
 48871/100000: episode: 964, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 200.019, mean reward: 2.000 [1.457, 3.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.733, 10.098], loss: 0.104467, mae: 0.313771, mean_q: 3.963211
 48971/100000: episode: 965, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 215.831, mean reward: 2.158 [1.501, 4.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.557, 10.242], loss: 0.092851, mae: 0.311817, mean_q: 3.963079
 49071/100000: episode: 966, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 181.847, mean reward: 1.818 [1.437, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.505, 10.437], loss: 0.101495, mae: 0.321233, mean_q: 3.969633
 49171/100000: episode: 967, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 208.481, mean reward: 2.085 [1.483, 5.149], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.887, 10.098], loss: 0.103668, mae: 0.314878, mean_q: 3.967196
 49271/100000: episode: 968, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 221.876, mean reward: 2.219 [1.491, 4.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.305, 10.098], loss: 0.132116, mae: 0.331040, mean_q: 3.978823
 49371/100000: episode: 969, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 190.271, mean reward: 1.903 [1.439, 2.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.276, 10.290], loss: 0.098884, mae: 0.317926, mean_q: 3.980925
 49471/100000: episode: 970, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 230.599, mean reward: 2.306 [1.453, 6.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.242, 10.540], loss: 0.090105, mae: 0.306342, mean_q: 3.945302
 49571/100000: episode: 971, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 194.407, mean reward: 1.944 [1.448, 10.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.845, 10.098], loss: 0.121169, mae: 0.321073, mean_q: 3.955731
 49671/100000: episode: 972, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 188.095, mean reward: 1.881 [1.450, 2.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.724, 10.228], loss: 0.108570, mae: 0.316120, mean_q: 3.950957
 49771/100000: episode: 973, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 194.511, mean reward: 1.945 [1.462, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.584, 10.426], loss: 0.143893, mae: 0.327837, mean_q: 3.972163
 49871/100000: episode: 974, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 255.321, mean reward: 2.553 [1.601, 4.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.367, 10.282], loss: 0.115919, mae: 0.309084, mean_q: 3.946827
[Info] 1-TH LEVEL FOUND: 6.238405227661133, Considering 10/90 traces
 49971/100000: episode: 975, duration: 4.662s, episode steps: 100, steps per second: 21, episode reward: 185.377, mean reward: 1.854 [1.445, 3.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.758, 10.282], loss: 0.087687, mae: 0.302716, mean_q: 3.961890
 49995/100000: episode: 976, duration: 0.129s, episode steps: 24, steps per second: 187, episode reward: 69.795, mean reward: 2.908 [2.260, 3.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.577, 10.463], loss: 0.102010, mae: 0.313406, mean_q: 3.936173
 50022/100000: episode: 977, duration: 0.145s, episode steps: 27, steps per second: 187, episode reward: 83.139, mean reward: 3.079 [2.495, 4.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.821, 10.458], loss: 0.109411, mae: 0.323344, mean_q: 3.974027
 50043/100000: episode: 978, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 50.026, mean reward: 2.382 [1.981, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.298, 10.344], loss: 0.117839, mae: 0.336816, mean_q: 3.952130
 50061/100000: episode: 979, duration: 0.088s, episode steps: 18, steps per second: 204, episode reward: 43.325, mean reward: 2.407 [2.033, 2.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.591, 10.337], loss: 0.156115, mae: 0.332250, mean_q: 4.021112
 50068/100000: episode: 980, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 19.339, mean reward: 2.763 [2.257, 3.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.446], loss: 0.106348, mae: 0.325587, mean_q: 3.985170
 50106/100000: episode: 981, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 108.403, mean reward: 2.853 [1.864, 6.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.343, 10.100], loss: 0.109852, mae: 0.324485, mean_q: 3.966911
 50113/100000: episode: 982, duration: 0.043s, episode steps: 7, steps per second: 165, episode reward: 24.819, mean reward: 3.546 [3.230, 3.832], mean action: 0.000 [0.000, 0.000], mean observation: 2.321 [-0.035, 10.510], loss: 0.087231, mae: 0.303854, mean_q: 4.011184
 50139/100000: episode: 983, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 65.241, mean reward: 2.509 [2.079, 3.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-1.633, 10.339], loss: 0.146672, mae: 0.336400, mean_q: 3.999037
 50166/100000: episode: 984, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 99.383, mean reward: 3.681 [2.336, 6.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.772, 10.631], loss: 0.112292, mae: 0.333227, mean_q: 4.011060
 50193/100000: episode: 985, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 108.675, mean reward: 4.025 [2.670, 14.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.527, 10.486], loss: 0.118486, mae: 0.345701, mean_q: 4.051530
 50200/100000: episode: 986, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 21.294, mean reward: 3.042 [2.698, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.798, 10.430], loss: 0.079173, mae: 0.285741, mean_q: 4.076727
 50238/100000: episode: 987, duration: 0.198s, episode steps: 38, steps per second: 192, episode reward: 104.950, mean reward: 2.762 [2.128, 3.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.413, 10.100], loss: 0.102900, mae: 0.321081, mean_q: 3.999350
 50260/100000: episode: 988, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 62.409, mean reward: 2.837 [2.091, 3.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.372], loss: 0.274095, mae: 0.374689, mean_q: 4.064025
 50278/100000: episode: 989, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 41.906, mean reward: 2.328 [1.947, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.318], loss: 0.108355, mae: 0.338720, mean_q: 4.058885
 50304/100000: episode: 990, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 102.686, mean reward: 3.949 [2.619, 6.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.323, 10.628], loss: 0.111520, mae: 0.311711, mean_q: 4.014453
 50328/100000: episode: 991, duration: 0.140s, episode steps: 24, steps per second: 171, episode reward: 72.349, mean reward: 3.015 [2.482, 3.687], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.419], loss: 0.206055, mae: 0.360339, mean_q: 4.079888
 50355/100000: episode: 992, duration: 0.126s, episode steps: 27, steps per second: 214, episode reward: 77.760, mean reward: 2.880 [2.397, 4.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.484, 10.452], loss: 0.108611, mae: 0.328064, mean_q: 4.113457
[Info] FALSIFICATION!
 50380/100000: episode: 993, duration: 0.287s, episode steps: 25, steps per second: 87, episode reward: 1115.261, mean reward: 44.610 [2.739, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.022, 10.797], loss: 0.137847, mae: 0.310583, mean_q: 4.097456
 50398/100000: episode: 994, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 43.058, mean reward: 2.392 [1.909, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.448], loss: 0.130137, mae: 0.353208, mean_q: 4.233747
 50416/100000: episode: 995, duration: 0.121s, episode steps: 18, steps per second: 149, episode reward: 40.961, mean reward: 2.276 [1.970, 2.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.340], loss: 0.193805, mae: 0.367751, mean_q: 4.094202
 50440/100000: episode: 996, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 60.255, mean reward: 2.511 [1.979, 4.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.333, 10.316], loss: 0.113069, mae: 0.351987, mean_q: 4.094320
 50466/100000: episode: 997, duration: 0.135s, episode steps: 26, steps per second: 193, episode reward: 101.635, mean reward: 3.909 [2.469, 5.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.431, 10.495], loss: 0.257863, mae: 0.389386, mean_q: 4.169108
 50493/100000: episode: 998, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 87.627, mean reward: 3.245 [2.389, 5.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.957, 10.503], loss: 0.260496, mae: 0.412858, mean_q: 4.168882
 50515/100000: episode: 999, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 67.894, mean reward: 3.086 [2.341, 4.323], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.516], loss: 0.293493, mae: 0.456069, mean_q: 4.247855
 50536/100000: episode: 1000, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 52.152, mean reward: 2.483 [1.942, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.478], loss: 0.179770, mae: 0.391957, mean_q: 4.142087
 50543/100000: episode: 1001, duration: 0.049s, episode steps: 7, steps per second: 143, episode reward: 30.504, mean reward: 4.358 [3.399, 5.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.337, 10.552], loss: 0.106325, mae: 0.352490, mean_q: 4.147315
 50567/100000: episode: 1002, duration: 0.138s, episode steps: 24, steps per second: 174, episode reward: 60.072, mean reward: 2.503 [1.921, 4.020], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.357, 10.460], loss: 0.200036, mae: 0.384906, mean_q: 4.191600
 50574/100000: episode: 1003, duration: 0.038s, episode steps: 7, steps per second: 183, episode reward: 22.811, mean reward: 3.259 [2.711, 4.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.318 [-0.043, 10.551], loss: 0.159043, mae: 0.373454, mean_q: 4.055030
 50595/100000: episode: 1004, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 51.344, mean reward: 2.445 [1.903, 5.052], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.503, 10.356], loss: 0.119655, mae: 0.344873, mean_q: 4.211445
 50616/100000: episode: 1005, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 72.689, mean reward: 3.461 [2.345, 4.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.048, 10.446], loss: 0.242453, mae: 0.394197, mean_q: 4.220577
 50634/100000: episode: 1006, duration: 0.096s, episode steps: 18, steps per second: 187, episode reward: 59.686, mean reward: 3.316 [2.676, 5.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.478], loss: 0.311544, mae: 0.421093, mean_q: 4.201512
 50652/100000: episode: 1007, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 50.878, mean reward: 2.827 [2.136, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.189, 10.431], loss: 0.196172, mae: 0.381774, mean_q: 4.245013
 50673/100000: episode: 1008, duration: 0.129s, episode steps: 21, steps per second: 163, episode reward: 49.133, mean reward: 2.340 [2.048, 2.752], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.351], loss: 0.157062, mae: 0.386613, mean_q: 4.236618
 50711/100000: episode: 1009, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 101.187, mean reward: 2.663 [1.551, 5.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-0.674, 10.100], loss: 0.140936, mae: 0.363191, mean_q: 4.135632
 50732/100000: episode: 1010, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 59.272, mean reward: 2.822 [2.218, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.161, 10.406], loss: 0.167261, mae: 0.380724, mean_q: 4.307919
 50758/100000: episode: 1011, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 97.551, mean reward: 3.752 [2.521, 6.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.517], loss: 0.177576, mae: 0.388992, mean_q: 4.161045
 50776/100000: episode: 1012, duration: 0.097s, episode steps: 18, steps per second: 185, episode reward: 39.686, mean reward: 2.205 [1.781, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.367, 10.277], loss: 0.114467, mae: 0.355613, mean_q: 4.266167
 50814/100000: episode: 1013, duration: 0.240s, episode steps: 38, steps per second: 159, episode reward: 100.710, mean reward: 2.650 [1.918, 4.715], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-1.436, 10.100], loss: 0.157481, mae: 0.384924, mean_q: 4.219273
 50832/100000: episode: 1014, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 44.167, mean reward: 2.454 [2.162, 2.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.410], loss: 852.047913, mae: 2.942706, mean_q: 5.234244
 50859/100000: episode: 1015, duration: 0.134s, episode steps: 27, steps per second: 201, episode reward: 79.539, mean reward: 2.946 [1.880, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.319, 10.314], loss: 1.312472, mae: 1.053352, mean_q: 4.395070
 50877/100000: episode: 1016, duration: 0.095s, episode steps: 18, steps per second: 189, episode reward: 41.612, mean reward: 2.312 [1.614, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.891, 10.332], loss: 0.617766, mae: 0.657667, mean_q: 4.313303
 50901/100000: episode: 1017, duration: 0.134s, episode steps: 24, steps per second: 180, episode reward: 59.690, mean reward: 2.487 [1.839, 3.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.832, 10.271], loss: 0.483010, mae: 0.591738, mean_q: 4.403018
 50928/100000: episode: 1018, duration: 0.154s, episode steps: 27, steps per second: 176, episode reward: 81.384, mean reward: 3.014 [2.058, 4.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.823, 10.304], loss: 0.291250, mae: 0.461832, mean_q: 4.361401
 50954/100000: episode: 1019, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 77.819, mean reward: 2.993 [2.387, 4.061], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.501, 10.374], loss: 0.209354, mae: 0.403723, mean_q: 4.356901
 50981/100000: episode: 1020, duration: 0.189s, episode steps: 27, steps per second: 143, episode reward: 125.957, mean reward: 4.665 [2.511, 9.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.292, 10.451], loss: 0.370709, mae: 0.476832, mean_q: 4.381499
 50988/100000: episode: 1021, duration: 0.053s, episode steps: 7, steps per second: 131, episode reward: 25.392, mean reward: 3.627 [2.706, 5.928], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.610, 10.622], loss: 0.243698, mae: 0.430185, mean_q: 4.295609
 51015/100000: episode: 1022, duration: 0.171s, episode steps: 27, steps per second: 158, episode reward: 97.179, mean reward: 3.599 [2.144, 5.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.318, 10.329], loss: 0.211086, mae: 0.431443, mean_q: 4.300830
 51022/100000: episode: 1023, duration: 0.069s, episode steps: 7, steps per second: 101, episode reward: 20.480, mean reward: 2.926 [2.525, 3.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.470], loss: 0.216290, mae: 0.442179, mean_q: 4.500318
 51046/100000: episode: 1024, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 50.271, mean reward: 2.095 [1.583, 2.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.334, 10.307], loss: 0.370370, mae: 0.464140, mean_q: 4.418354
 51068/100000: episode: 1025, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 55.550, mean reward: 2.525 [1.679, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.606, 10.231], loss: 703.767334, mae: 2.651791, mean_q: 5.466015
 51089/100000: episode: 1026, duration: 0.109s, episode steps: 21, steps per second: 192, episode reward: 57.774, mean reward: 2.751 [2.141, 4.004], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-1.004, 10.367], loss: 1.471467, mae: 1.252881, mean_q: 4.740097
 51107/100000: episode: 1027, duration: 0.100s, episode steps: 18, steps per second: 181, episode reward: 69.975, mean reward: 3.888 [2.663, 5.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.454], loss: 0.409627, mae: 0.640789, mean_q: 4.554357
 51128/100000: episode: 1028, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 60.099, mean reward: 2.862 [2.120, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-1.581, 10.511], loss: 0.258169, mae: 0.437528, mean_q: 4.498755
 51135/100000: episode: 1029, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 20.849, mean reward: 2.978 [2.463, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.480], loss: 0.423585, mae: 0.468529, mean_q: 4.540173
 51173/100000: episode: 1030, duration: 0.219s, episode steps: 38, steps per second: 173, episode reward: 137.854, mean reward: 3.628 [2.618, 6.354], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.646, 10.100], loss: 0.306790, mae: 0.454660, mean_q: 4.558294
 51200/100000: episode: 1031, duration: 0.154s, episode steps: 27, steps per second: 175, episode reward: 91.115, mean reward: 3.375 [1.653, 5.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.419, 10.280], loss: 0.304679, mae: 0.450205, mean_q: 4.536833
 51221/100000: episode: 1032, duration: 0.109s, episode steps: 21, steps per second: 192, episode reward: 53.593, mean reward: 2.552 [1.911, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.071, 10.393], loss: 0.342432, mae: 0.467676, mean_q: 4.606143
 51245/100000: episode: 1033, duration: 0.149s, episode steps: 24, steps per second: 161, episode reward: 57.377, mean reward: 2.391 [1.861, 3.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.612, 10.230], loss: 0.308088, mae: 0.465395, mean_q: 4.501286
 51267/100000: episode: 1034, duration: 0.138s, episode steps: 22, steps per second: 159, episode reward: 57.160, mean reward: 2.598 [1.539, 4.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.214], loss: 0.256730, mae: 0.445229, mean_q: 4.479396
 51289/100000: episode: 1035, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 46.143, mean reward: 2.097 [1.495, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.036, 10.245], loss: 0.210929, mae: 0.421630, mean_q: 4.441276
 51296/100000: episode: 1036, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 21.586, mean reward: 3.084 [2.353, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.504, 10.342], loss: 0.711666, mae: 0.603367, mean_q: 4.535617
 51314/100000: episode: 1037, duration: 0.103s, episode steps: 18, steps per second: 175, episode reward: 38.601, mean reward: 2.144 [1.762, 3.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-1.362, 10.305], loss: 0.236293, mae: 0.445880, mean_q: 4.533983
 51341/100000: episode: 1038, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 86.501, mean reward: 3.204 [2.345, 4.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.448, 10.410], loss: 0.290546, mae: 0.452659, mean_q: 4.424841
 51362/100000: episode: 1039, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 49.560, mean reward: 2.360 [1.969, 2.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.641, 10.390], loss: 734.981445, mae: 2.034813, mean_q: 4.581956
 51383/100000: episode: 1040, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 47.283, mean reward: 2.252 [1.959, 2.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.360], loss: 2.289392, mae: 1.657930, mean_q: 5.900585
 51404/100000: episode: 1041, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 53.586, mean reward: 2.552 [2.057, 3.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.231, 10.346], loss: 0.485251, mae: 0.727636, mean_q: 4.810972
 51411/100000: episode: 1042, duration: 0.045s, episode steps: 7, steps per second: 154, episode reward: 24.359, mean reward: 3.480 [3.163, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.035, 10.493], loss: 0.309078, mae: 0.571841, mean_q: 4.494568
 51418/100000: episode: 1043, duration: 0.036s, episode steps: 7, steps per second: 196, episode reward: 22.674, mean reward: 3.239 [2.671, 3.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.425], loss: 0.636232, mae: 0.637425, mean_q: 4.631658
 51444/100000: episode: 1044, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 70.606, mean reward: 2.716 [2.172, 4.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.274, 10.328], loss: 0.297683, mae: 0.514409, mean_q: 4.788148
 51465/100000: episode: 1045, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 51.817, mean reward: 2.467 [1.890, 3.410], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.454, 10.360], loss: 0.285158, mae: 0.489940, mean_q: 4.858998
 51489/100000: episode: 1046, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 69.341, mean reward: 2.889 [2.274, 3.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.483, 10.481], loss: 0.387471, mae: 0.508304, mean_q: 4.800020
 51516/100000: episode: 1047, duration: 0.132s, episode steps: 27, steps per second: 204, episode reward: 68.271, mean reward: 2.529 [1.691, 5.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.904, 10.190], loss: 0.299827, mae: 0.494629, mean_q: 4.715402
 51537/100000: episode: 1048, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 50.490, mean reward: 2.404 [1.825, 4.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.171, 10.384], loss: 0.306595, mae: 0.498767, mean_q: 4.729270
 51563/100000: episode: 1049, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 71.503, mean reward: 2.750 [2.240, 6.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.913, 10.400], loss: 0.237195, mae: 0.465298, mean_q: 4.668096
 51590/100000: episode: 1050, duration: 0.156s, episode steps: 27, steps per second: 174, episode reward: 192.308, mean reward: 7.123 [2.730, 31.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.656, 10.623], loss: 572.997925, mae: 2.355387, mean_q: 5.708875
 51612/100000: episode: 1051, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 62.192, mean reward: 2.827 [2.002, 5.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.035, 10.369], loss: 0.688715, mae: 0.857798, mean_q: 4.997122
 51630/100000: episode: 1052, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 50.156, mean reward: 2.786 [2.152, 4.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.265, 10.426], loss: 0.316047, mae: 0.547546, mean_q: 4.759185
 51648/100000: episode: 1053, duration: 0.093s, episode steps: 18, steps per second: 194, episode reward: 44.844, mean reward: 2.491 [1.809, 3.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.364], loss: 856.869263, mae: 2.392134, mean_q: 4.951094
 51674/100000: episode: 1054, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 65.300, mean reward: 2.512 [1.833, 3.801], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.201, 10.354], loss: 591.469727, mae: 2.760936, mean_q: 6.208786
 51681/100000: episode: 1055, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 21.363, mean reward: 3.052 [2.695, 3.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.445], loss: 1.096454, mae: 0.968355, mean_q: 4.296647
 51705/100000: episode: 1056, duration: 0.131s, episode steps: 24, steps per second: 184, episode reward: 69.229, mean reward: 2.885 [2.057, 5.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.136, 10.390], loss: 0.568472, mae: 0.676997, mean_q: 5.066077
 51712/100000: episode: 1057, duration: 0.051s, episode steps: 7, steps per second: 138, episode reward: 26.386, mean reward: 3.769 [3.165, 5.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.316 [-0.035, 10.586], loss: 0.813522, mae: 0.681221, mean_q: 5.213279
 51750/100000: episode: 1058, duration: 0.217s, episode steps: 38, steps per second: 175, episode reward: 116.080, mean reward: 3.055 [2.417, 8.641], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-1.582, 10.100], loss: 0.467573, mae: 0.544616, mean_q: 5.090712
 51774/100000: episode: 1059, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 65.322, mean reward: 2.722 [1.964, 4.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.361], loss: 0.419543, mae: 0.539685, mean_q: 5.034237
 51792/100000: episode: 1060, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 41.683, mean reward: 2.316 [1.920, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.103, 10.327], loss: 0.430737, mae: 0.513377, mean_q: 5.049963
 51813/100000: episode: 1061, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 44.488, mean reward: 2.118 [1.703, 3.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.289], loss: 0.468947, mae: 0.509266, mean_q: 5.030084
 51839/100000: episode: 1062, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 84.647, mean reward: 3.256 [2.043, 5.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.402, 10.311], loss: 0.570794, mae: 0.543849, mean_q: 4.971650
 51860/100000: episode: 1063, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 48.322, mean reward: 2.301 [1.537, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.443, 10.179], loss: 0.491550, mae: 0.550980, mean_q: 5.077282
 51886/100000: episode: 1064, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 67.369, mean reward: 2.591 [2.120, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.078, 10.450], loss: 0.397693, mae: 0.517666, mean_q: 4.918568
[Info] Complete ISplit Iteration
[Info] Levels: [6.238405, 7.2005043]
[Info] Cond. Prob: [0.1, 0.03]
[Info] Error Prob: 0.003

 51912/100000: episode: 1065, duration: 4.365s, episode steps: 26, steps per second: 6, episode reward: 57.596, mean reward: 2.215 [1.803, 2.619], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.515, 10.251], loss: 0.904239, mae: 0.584474, mean_q: 5.006669
 52012/100000: episode: 1066, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 191.511, mean reward: 1.915 [1.478, 5.090], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.766, 10.226], loss: 155.212402, mae: 1.272945, mean_q: 5.174706
 52112/100000: episode: 1067, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 204.680, mean reward: 2.047 [1.500, 4.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.386, 10.098], loss: 0.522369, mae: 0.565912, mean_q: 5.033830
 52212/100000: episode: 1068, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 190.276, mean reward: 1.903 [1.451, 2.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.670, 10.193], loss: 154.553085, mae: 1.088705, mean_q: 5.196685
 52312/100000: episode: 1069, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 195.204, mean reward: 1.952 [1.477, 3.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.578, 10.098], loss: 154.345276, mae: 0.934706, mean_q: 4.877987
 52412/100000: episode: 1070, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 188.782, mean reward: 1.888 [1.466, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.885, 10.098], loss: 0.631958, mae: 0.729722, mean_q: 5.056672
 52512/100000: episode: 1071, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 182.647, mean reward: 1.826 [1.447, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.712, 10.098], loss: 308.325500, mae: 1.650766, mean_q: 5.447846
 52612/100000: episode: 1072, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 189.664, mean reward: 1.897 [1.474, 2.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.473, 10.098], loss: 0.526154, mae: 0.592494, mean_q: 4.916330
 52712/100000: episode: 1073, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 191.147, mean reward: 1.911 [1.455, 2.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.172, 10.098], loss: 153.939972, mae: 1.167056, mean_q: 5.059335
 52812/100000: episode: 1074, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 225.871, mean reward: 2.259 [1.450, 5.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.254, 10.580], loss: 308.734741, mae: 1.626822, mean_q: 5.513315
 52912/100000: episode: 1075, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 189.924, mean reward: 1.899 [1.436, 4.915], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.937, 10.225], loss: 154.419769, mae: 1.119931, mean_q: 5.282150
 53012/100000: episode: 1076, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 193.572, mean reward: 1.936 [1.480, 3.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.739, 10.098], loss: 154.300110, mae: 1.019452, mean_q: 5.181826
 53112/100000: episode: 1077, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 193.588, mean reward: 1.936 [1.488, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.766, 10.212], loss: 307.467163, mae: 1.431082, mean_q: 5.253410
 53212/100000: episode: 1078, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 188.014, mean reward: 1.880 [1.478, 3.163], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.782, 10.098], loss: 0.829419, mae: 0.723251, mean_q: 5.140817
 53312/100000: episode: 1079, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 189.220, mean reward: 1.892 [1.479, 2.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.574, 10.224], loss: 0.495948, mae: 0.553152, mean_q: 4.905255
 53412/100000: episode: 1080, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 203.021, mean reward: 2.030 [1.443, 4.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.947, 10.098], loss: 0.566693, mae: 0.525320, mean_q: 4.791346
 53512/100000: episode: 1081, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 191.878, mean reward: 1.919 [1.478, 3.436], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.691, 10.108], loss: 307.497101, mae: 1.621388, mean_q: 5.361080
 53612/100000: episode: 1082, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 230.157, mean reward: 2.302 [1.452, 18.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.447, 10.304], loss: 0.531216, mae: 0.635287, mean_q: 4.766103
 53712/100000: episode: 1083, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 189.110, mean reward: 1.891 [1.463, 3.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.584, 10.098], loss: 0.636269, mae: 0.557633, mean_q: 4.810116
 53812/100000: episode: 1084, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 190.969, mean reward: 1.910 [1.442, 4.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.793, 10.272], loss: 0.492068, mae: 0.554406, mean_q: 4.714141
 53912/100000: episode: 1085, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 198.263, mean reward: 1.983 [1.467, 5.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.018, 10.319], loss: 307.771027, mae: 1.652130, mean_q: 5.394307
 54012/100000: episode: 1086, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 223.718, mean reward: 2.237 [1.464, 3.427], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.647, 10.225], loss: 307.338715, mae: 1.635330, mean_q: 5.406926
 54112/100000: episode: 1087, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 192.447, mean reward: 1.924 [1.477, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.678, 10.200], loss: 154.348465, mae: 1.315306, mean_q: 5.098566
 54212/100000: episode: 1088, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 184.992, mean reward: 1.850 [1.434, 3.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.956, 10.098], loss: 0.604927, mae: 0.646780, mean_q: 4.817476
 54312/100000: episode: 1089, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 209.132, mean reward: 2.091 [1.496, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.467, 10.098], loss: 154.259995, mae: 1.167160, mean_q: 5.153392
 54412/100000: episode: 1090, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 187.061, mean reward: 1.871 [1.455, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.179, 10.149], loss: 154.082047, mae: 1.100954, mean_q: 4.939918
 54512/100000: episode: 1091, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 193.897, mean reward: 1.939 [1.532, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.262, 10.180], loss: 0.668292, mae: 0.659705, mean_q: 4.785045
 54612/100000: episode: 1092, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 196.152, mean reward: 1.962 [1.454, 5.499], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.757, 10.257], loss: 0.438063, mae: 0.586237, mean_q: 4.719172
 54712/100000: episode: 1093, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 182.507, mean reward: 1.825 [1.447, 3.093], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.829, 10.098], loss: 0.515900, mae: 0.541263, mean_q: 4.683324
 54812/100000: episode: 1094, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 200.531, mean reward: 2.005 [1.446, 3.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.299, 10.175], loss: 307.607452, mae: 1.685496, mean_q: 5.389258
 54912/100000: episode: 1095, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 200.510, mean reward: 2.005 [1.452, 3.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.491, 10.466], loss: 0.600177, mae: 0.615916, mean_q: 4.688118
 55012/100000: episode: 1096, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 188.556, mean reward: 1.886 [1.455, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.877, 10.289], loss: 0.526095, mae: 0.591227, mean_q: 4.620044
 55112/100000: episode: 1097, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 184.421, mean reward: 1.844 [1.470, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.030, 10.098], loss: 307.372101, mae: 1.439348, mean_q: 4.920727
 55212/100000: episode: 1098, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 175.765, mean reward: 1.758 [1.440, 2.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.328, 10.126], loss: 154.340637, mae: 1.374497, mean_q: 5.354418
 55312/100000: episode: 1099, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 195.112, mean reward: 1.951 [1.453, 4.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.405, 10.098], loss: 0.440373, mae: 0.560983, mean_q: 4.583857
 55412/100000: episode: 1100, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 223.255, mean reward: 2.233 [1.465, 3.345], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.395, 10.161], loss: 0.381226, mae: 0.521667, mean_q: 4.466841
 55512/100000: episode: 1101, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 214.412, mean reward: 2.144 [1.445, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.812, 10.098], loss: 0.585809, mae: 0.511828, mean_q: 4.412459
 55612/100000: episode: 1102, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 197.663, mean reward: 1.977 [1.548, 6.362], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.175, 10.282], loss: 0.272925, mae: 0.445420, mean_q: 4.311224
 55712/100000: episode: 1103, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 200.383, mean reward: 2.004 [1.488, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.006, 10.098], loss: 0.552113, mae: 0.478392, mean_q: 4.316739
 55812/100000: episode: 1104, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 206.931, mean reward: 2.069 [1.437, 3.435], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.139, 10.231], loss: 0.246794, mae: 0.434397, mean_q: 4.279223
 55912/100000: episode: 1105, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 200.628, mean reward: 2.006 [1.457, 3.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.779, 10.098], loss: 0.268795, mae: 0.425603, mean_q: 4.241109
 56012/100000: episode: 1106, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 189.256, mean reward: 1.893 [1.470, 2.937], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.935, 10.175], loss: 0.232975, mae: 0.394404, mean_q: 4.179277
 56112/100000: episode: 1107, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 186.071, mean reward: 1.861 [1.440, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.743, 10.098], loss: 0.216390, mae: 0.388539, mean_q: 4.144526
 56212/100000: episode: 1108, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 193.278, mean reward: 1.933 [1.459, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.102, 10.160], loss: 0.442100, mae: 0.393751, mean_q: 4.128409
 56312/100000: episode: 1109, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 201.737, mean reward: 2.017 [1.443, 5.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.686, 10.098], loss: 0.279625, mae: 0.398694, mean_q: 4.117815
 56412/100000: episode: 1110, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 206.853, mean reward: 2.069 [1.466, 4.797], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.752, 10.098], loss: 0.180663, mae: 0.363465, mean_q: 4.048242
 56512/100000: episode: 1111, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 192.975, mean reward: 1.930 [1.497, 2.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.724, 10.098], loss: 0.218433, mae: 0.361309, mean_q: 4.034679
 56612/100000: episode: 1112, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 197.722, mean reward: 1.977 [1.442, 3.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.069, 10.334], loss: 0.154234, mae: 0.331371, mean_q: 3.994761
 56712/100000: episode: 1113, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 194.126, mean reward: 1.941 [1.484, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.651, 10.098], loss: 0.202584, mae: 0.337182, mean_q: 3.965721
 56812/100000: episode: 1114, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 231.282, mean reward: 2.313 [1.442, 7.599], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.614, 10.383], loss: 0.103059, mae: 0.313620, mean_q: 3.931627
 56912/100000: episode: 1115, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 194.065, mean reward: 1.941 [1.474, 3.241], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.161, 10.408], loss: 0.143181, mae: 0.315424, mean_q: 3.926309
 57012/100000: episode: 1116, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 191.844, mean reward: 1.918 [1.468, 2.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.103, 10.098], loss: 0.110858, mae: 0.321152, mean_q: 3.911351
 57112/100000: episode: 1117, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 201.969, mean reward: 2.020 [1.454, 3.808], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.571, 10.140], loss: 0.099163, mae: 0.300068, mean_q: 3.903112
 57212/100000: episode: 1118, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 194.014, mean reward: 1.940 [1.456, 3.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.953, 10.098], loss: 0.184640, mae: 0.324755, mean_q: 3.903787
 57312/100000: episode: 1119, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 198.137, mean reward: 1.981 [1.433, 3.360], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.397, 10.098], loss: 0.097941, mae: 0.309010, mean_q: 3.902722
 57412/100000: episode: 1120, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 206.381, mean reward: 2.064 [1.461, 3.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.822, 10.098], loss: 0.111594, mae: 0.319908, mean_q: 3.904895
 57512/100000: episode: 1121, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 192.810, mean reward: 1.928 [1.460, 4.471], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.593, 10.120], loss: 0.147249, mae: 0.318452, mean_q: 3.914802
 57612/100000: episode: 1122, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 209.469, mean reward: 2.095 [1.447, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.142, 10.098], loss: 0.097659, mae: 0.312947, mean_q: 3.915865
 57712/100000: episode: 1123, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 202.799, mean reward: 2.028 [1.471, 3.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.937, 10.279], loss: 0.097170, mae: 0.313212, mean_q: 3.921190
 57812/100000: episode: 1124, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 186.873, mean reward: 1.869 [1.492, 2.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.030, 10.098], loss: 0.141929, mae: 0.317847, mean_q: 3.910322
 57912/100000: episode: 1125, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 200.667, mean reward: 2.007 [1.440, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.681, 10.098], loss: 0.099316, mae: 0.308811, mean_q: 3.899910
 58012/100000: episode: 1126, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 203.923, mean reward: 2.039 [1.486, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.117, 10.098], loss: 0.098688, mae: 0.315489, mean_q: 3.914723
 58112/100000: episode: 1127, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 188.191, mean reward: 1.882 [1.471, 2.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.039, 10.147], loss: 0.103561, mae: 0.313459, mean_q: 3.914319
 58212/100000: episode: 1128, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 231.353, mean reward: 2.314 [1.489, 4.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.300, 10.098], loss: 0.129787, mae: 0.301517, mean_q: 3.918738
 58312/100000: episode: 1129, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 188.082, mean reward: 1.881 [1.486, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.318, 10.125], loss: 0.141616, mae: 0.316276, mean_q: 3.926469
 58412/100000: episode: 1130, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 182.398, mean reward: 1.824 [1.467, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.300, 10.153], loss: 0.142133, mae: 0.314289, mean_q: 3.918551
 58512/100000: episode: 1131, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 185.554, mean reward: 1.856 [1.484, 2.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.680, 10.122], loss: 0.150534, mae: 0.328298, mean_q: 3.951265
 58612/100000: episode: 1132, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 188.457, mean reward: 1.885 [1.463, 3.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.949, 10.201], loss: 0.137732, mae: 0.311937, mean_q: 3.933782
 58712/100000: episode: 1133, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 183.057, mean reward: 1.831 [1.463, 2.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.493, 10.208], loss: 0.104025, mae: 0.305797, mean_q: 3.937337
 58812/100000: episode: 1134, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 246.203, mean reward: 2.462 [1.483, 4.751], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.698, 10.098], loss: 0.090286, mae: 0.300297, mean_q: 3.925909
 58912/100000: episode: 1135, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 187.832, mean reward: 1.878 [1.492, 2.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.553, 10.098], loss: 0.087153, mae: 0.292214, mean_q: 3.927140
 59012/100000: episode: 1136, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 192.528, mean reward: 1.925 [1.489, 2.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.696, 10.125], loss: 0.098399, mae: 0.302924, mean_q: 3.902618
 59112/100000: episode: 1137, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 214.418, mean reward: 2.144 [1.498, 7.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.121, 10.150], loss: 0.089405, mae: 0.292792, mean_q: 3.924298
 59212/100000: episode: 1138, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 199.003, mean reward: 1.990 [1.493, 3.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.610, 10.264], loss: 0.091777, mae: 0.302001, mean_q: 3.929952
 59312/100000: episode: 1139, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 180.404, mean reward: 1.804 [1.436, 3.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.073, 10.253], loss: 0.097006, mae: 0.304066, mean_q: 3.932305
 59412/100000: episode: 1140, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 181.199, mean reward: 1.812 [1.457, 2.780], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.296, 10.181], loss: 0.090234, mae: 0.297204, mean_q: 3.926583
 59512/100000: episode: 1141, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 182.504, mean reward: 1.825 [1.447, 3.462], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.380, 10.098], loss: 0.086910, mae: 0.291005, mean_q: 3.897916
 59612/100000: episode: 1142, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 202.549, mean reward: 2.025 [1.447, 3.919], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.494, 10.406], loss: 0.098664, mae: 0.300650, mean_q: 3.908082
 59712/100000: episode: 1143, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 199.671, mean reward: 1.997 [1.513, 4.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.880, 10.098], loss: 0.097930, mae: 0.301863, mean_q: 3.911868
 59812/100000: episode: 1144, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 183.601, mean reward: 1.836 [1.474, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.532, 10.148], loss: 0.097795, mae: 0.311175, mean_q: 3.930506
 59912/100000: episode: 1145, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 183.231, mean reward: 1.832 [1.478, 2.944], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.726, 10.200], loss: 0.099202, mae: 0.302455, mean_q: 3.929729
 60012/100000: episode: 1146, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 207.601, mean reward: 2.076 [1.457, 4.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.898, 10.195], loss: 0.084203, mae: 0.290741, mean_q: 3.916556
 60112/100000: episode: 1147, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 218.908, mean reward: 2.189 [1.540, 4.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.798, 10.098], loss: 0.088156, mae: 0.300844, mean_q: 3.925298
 60212/100000: episode: 1148, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 200.635, mean reward: 2.006 [1.465, 3.406], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.492, 10.227], loss: 0.099724, mae: 0.303578, mean_q: 3.920898
 60312/100000: episode: 1149, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 197.480, mean reward: 1.975 [1.471, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.980, 10.371], loss: 0.097039, mae: 0.314988, mean_q: 3.947405
 60412/100000: episode: 1150, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 185.281, mean reward: 1.853 [1.471, 3.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.520, 10.263], loss: 0.081580, mae: 0.284241, mean_q: 3.917777
 60512/100000: episode: 1151, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 187.291, mean reward: 1.873 [1.471, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.673, 10.098], loss: 0.087572, mae: 0.295171, mean_q: 3.910773
 60612/100000: episode: 1152, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 220.310, mean reward: 2.203 [1.476, 4.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.832, 10.106], loss: 0.082253, mae: 0.288579, mean_q: 3.920604
 60712/100000: episode: 1153, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 197.139, mean reward: 1.971 [1.454, 3.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.723, 10.098], loss: 0.088047, mae: 0.298052, mean_q: 3.915394
 60812/100000: episode: 1154, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 190.212, mean reward: 1.902 [1.446, 3.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.849, 10.258], loss: 0.089719, mae: 0.296603, mean_q: 3.924701
 60912/100000: episode: 1155, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 189.094, mean reward: 1.891 [1.438, 3.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.398, 10.098], loss: 0.086493, mae: 0.296184, mean_q: 3.916511
 61012/100000: episode: 1156, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 185.374, mean reward: 1.854 [1.465, 3.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.922, 10.175], loss: 0.085856, mae: 0.297369, mean_q: 3.897004
 61112/100000: episode: 1157, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 208.623, mean reward: 2.086 [1.472, 3.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.942, 10.346], loss: 0.100545, mae: 0.303287, mean_q: 3.906732
 61212/100000: episode: 1158, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 178.594, mean reward: 1.786 [1.437, 2.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.323, 10.098], loss: 0.098277, mae: 0.300104, mean_q: 3.937210
 61312/100000: episode: 1159, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 182.023, mean reward: 1.820 [1.436, 3.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.404, 10.265], loss: 0.078724, mae: 0.286070, mean_q: 3.900913
 61412/100000: episode: 1160, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 186.452, mean reward: 1.865 [1.447, 2.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.358, 10.111], loss: 0.087247, mae: 0.294957, mean_q: 3.900189
 61512/100000: episode: 1161, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 203.742, mean reward: 2.037 [1.441, 4.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.249, 10.098], loss: 0.102861, mae: 0.308354, mean_q: 3.910822
 61612/100000: episode: 1162, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 206.015, mean reward: 2.060 [1.480, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.579, 10.247], loss: 0.087161, mae: 0.300944, mean_q: 3.909383
 61712/100000: episode: 1163, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 190.850, mean reward: 1.909 [1.516, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.574, 10.098], loss: 0.090658, mae: 0.298697, mean_q: 3.898061
 61812/100000: episode: 1164, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 202.846, mean reward: 2.028 [1.449, 3.747], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.152, 10.098], loss: 0.077189, mae: 0.285282, mean_q: 3.877952
[Info] 1-TH LEVEL FOUND: 5.200054168701172, Considering 10/90 traces
 61912/100000: episode: 1165, duration: 4.585s, episode steps: 100, steps per second: 22, episode reward: 186.976, mean reward: 1.870 [1.450, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.852, 10.112], loss: 0.080372, mae: 0.288223, mean_q: 3.905277
 61927/100000: episode: 1166, duration: 0.095s, episode steps: 15, steps per second: 157, episode reward: 44.007, mean reward: 2.934 [2.101, 3.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.339], loss: 0.089563, mae: 0.301565, mean_q: 3.887005
 61940/100000: episode: 1167, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 27.612, mean reward: 2.124 [1.855, 2.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.352, 10.100], loss: 0.066888, mae: 0.265847, mean_q: 3.875700
 61953/100000: episode: 1168, duration: 0.069s, episode steps: 13, steps per second: 189, episode reward: 33.244, mean reward: 2.557 [1.958, 3.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.557, 10.100], loss: 0.067766, mae: 0.270036, mean_q: 3.918111
 61961/100000: episode: 1169, duration: 0.047s, episode steps: 8, steps per second: 171, episode reward: 18.109, mean reward: 2.264 [1.827, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.342, 10.100], loss: 0.063781, mae: 0.279983, mean_q: 3.884375
 61976/100000: episode: 1170, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 52.236, mean reward: 3.482 [3.164, 4.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.312, 10.589], loss: 0.072823, mae: 0.277443, mean_q: 3.884818
 62003/100000: episode: 1171, duration: 0.141s, episode steps: 27, steps per second: 192, episode reward: 102.265, mean reward: 3.788 [2.464, 5.513], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-1.690, 10.100], loss: 0.106907, mae: 0.313327, mean_q: 3.912268
 62010/100000: episode: 1172, duration: 0.037s, episode steps: 7, steps per second: 192, episode reward: 20.478, mean reward: 2.925 [2.088, 4.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.304, 10.100], loss: 0.094589, mae: 0.314192, mean_q: 3.876833
 62037/100000: episode: 1173, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 61.465, mean reward: 2.276 [1.871, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.397, 10.100], loss: 0.089339, mae: 0.303161, mean_q: 3.921038
 62128/100000: episode: 1174, duration: 0.492s, episode steps: 91, steps per second: 185, episode reward: 217.524, mean reward: 2.390 [1.600, 3.429], mean action: 0.000 [0.000, 0.000], mean observation: 1.531 [-0.998, 10.100], loss: 0.086436, mae: 0.294331, mean_q: 3.917545
 62141/100000: episode: 1175, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 34.245, mean reward: 2.634 [2.111, 4.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.291, 10.100], loss: 0.102238, mae: 0.337657, mean_q: 4.010328
 62187/100000: episode: 1176, duration: 0.267s, episode steps: 46, steps per second: 173, episode reward: 140.242, mean reward: 3.049 [1.757, 11.792], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-1.532, 10.100], loss: 0.093961, mae: 0.311421, mean_q: 3.954876
 62206/100000: episode: 1177, duration: 0.098s, episode steps: 19, steps per second: 194, episode reward: 47.655, mean reward: 2.508 [2.218, 2.948], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.325], loss: 0.093240, mae: 0.290477, mean_q: 3.945282
 62225/100000: episode: 1178, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 40.897, mean reward: 2.152 [1.697, 3.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.235, 10.232], loss: 0.074791, mae: 0.283425, mean_q: 3.955266
 62233/100000: episode: 1179, duration: 0.053s, episode steps: 8, steps per second: 152, episode reward: 21.673, mean reward: 2.709 [2.282, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.320, 10.100], loss: 0.277078, mae: 0.363593, mean_q: 3.958767
 62248/100000: episode: 1180, duration: 0.077s, episode steps: 15, steps per second: 195, episode reward: 43.703, mean reward: 2.914 [2.398, 4.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.589, 10.360], loss: 0.136095, mae: 0.338426, mean_q: 3.967962
 62275/100000: episode: 1181, duration: 0.139s, episode steps: 27, steps per second: 195, episode reward: 102.193, mean reward: 3.785 [2.147, 5.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.436, 10.100], loss: 0.080008, mae: 0.297075, mean_q: 3.975277
 62356/100000: episode: 1182, duration: 0.418s, episode steps: 81, steps per second: 194, episode reward: 263.926, mean reward: 3.258 [1.587, 10.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.640 [-0.316, 10.672], loss: 0.108215, mae: 0.320600, mean_q: 3.986000
 62363/100000: episode: 1183, duration: 0.048s, episode steps: 7, steps per second: 144, episode reward: 18.769, mean reward: 2.681 [2.342, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.399, 10.100], loss: 0.124416, mae: 0.333642, mean_q: 4.100453
 62444/100000: episode: 1184, duration: 0.433s, episode steps: 81, steps per second: 187, episode reward: 170.755, mean reward: 2.108 [1.470, 6.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.641 [-0.707, 10.385], loss: 0.135680, mae: 0.336395, mean_q: 4.036724
 62490/100000: episode: 1185, duration: 0.256s, episode steps: 46, steps per second: 180, episode reward: 112.172, mean reward: 2.439 [1.681, 3.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.402, 10.100], loss: 0.169527, mae: 0.371096, mean_q: 4.031956
 62536/100000: episode: 1186, duration: 0.257s, episode steps: 46, steps per second: 179, episode reward: 232.104, mean reward: 5.046 [2.464, 12.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-1.295, 10.100], loss: 0.136102, mae: 0.341941, mean_q: 4.049413
 62563/100000: episode: 1187, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 85.309, mean reward: 3.160 [2.087, 5.304], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.311, 10.100], loss: 0.110548, mae: 0.331820, mean_q: 4.073262
 62578/100000: episode: 1188, duration: 0.078s, episode steps: 15, steps per second: 191, episode reward: 43.877, mean reward: 2.925 [2.442, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.519], loss: 0.374915, mae: 0.438103, mean_q: 4.191164
 62669/100000: episode: 1189, duration: 0.491s, episode steps: 91, steps per second: 185, episode reward: 197.587, mean reward: 2.171 [1.470, 4.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.538 [-1.747, 10.382], loss: 0.154603, mae: 0.353580, mean_q: 4.091769
 62678/100000: episode: 1190, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 25.677, mean reward: 2.853 [2.576, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.231, 10.432], loss: 0.105331, mae: 0.315613, mean_q: 4.009708
 62705/100000: episode: 1191, duration: 0.153s, episode steps: 27, steps per second: 176, episode reward: 79.810, mean reward: 2.956 [1.951, 5.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.283, 10.100], loss: 0.185211, mae: 0.362899, mean_q: 4.085031
 62724/100000: episode: 1192, duration: 0.115s, episode steps: 19, steps per second: 166, episode reward: 51.135, mean reward: 2.691 [2.065, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.035, 10.494], loss: 0.157514, mae: 0.379267, mean_q: 4.211248
 62732/100000: episode: 1193, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 20.436, mean reward: 2.554 [1.852, 3.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.303, 10.100], loss: 0.165641, mae: 0.384700, mean_q: 4.082518
 62759/100000: episode: 1194, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 92.701, mean reward: 3.433 [2.308, 5.316], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.351, 10.100], loss: 0.182500, mae: 0.385002, mean_q: 4.178821
 62850/100000: episode: 1195, duration: 0.466s, episode steps: 91, steps per second: 195, episode reward: 177.391, mean reward: 1.949 [1.455, 4.817], mean action: 0.000 [0.000, 0.000], mean observation: 1.548 [-0.250, 10.336], loss: 0.168468, mae: 0.351789, mean_q: 4.137284
 62896/100000: episode: 1196, duration: 0.230s, episode steps: 46, steps per second: 200, episode reward: 114.926, mean reward: 2.498 [1.517, 4.043], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.488, 10.100], loss: 0.175269, mae: 0.359122, mean_q: 4.100337
 62903/100000: episode: 1197, duration: 0.039s, episode steps: 7, steps per second: 179, episode reward: 19.217, mean reward: 2.745 [2.341, 4.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.256, 10.100], loss: 0.107464, mae: 0.304148, mean_q: 4.115696
 62930/100000: episode: 1198, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 83.304, mean reward: 3.085 [2.158, 4.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.617, 10.100], loss: 0.193694, mae: 0.369217, mean_q: 4.167403
 62949/100000: episode: 1199, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 49.462, mean reward: 2.603 [2.213, 3.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.055, 10.476], loss: 0.166624, mae: 0.369595, mean_q: 4.214081
 62957/100000: episode: 1200, duration: 0.046s, episode steps: 8, steps per second: 174, episode reward: 17.862, mean reward: 2.233 [1.866, 2.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.800, 10.100], loss: 0.142088, mae: 0.354153, mean_q: 4.177088
 62972/100000: episode: 1201, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 43.789, mean reward: 2.919 [2.373, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.200, 10.492], loss: 0.152267, mae: 0.378153, mean_q: 4.163848
 62979/100000: episode: 1202, duration: 0.048s, episode steps: 7, steps per second: 146, episode reward: 18.658, mean reward: 2.665 [2.162, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.297, 10.100], loss: 0.126629, mae: 0.311038, mean_q: 3.984050
 63070/100000: episode: 1203, duration: 0.493s, episode steps: 91, steps per second: 184, episode reward: 192.152, mean reward: 2.112 [1.500, 3.608], mean action: 0.000 [0.000, 0.000], mean observation: 1.538 [-1.114, 10.357], loss: 0.191836, mae: 0.372009, mean_q: 4.222691
 63097/100000: episode: 1204, duration: 0.167s, episode steps: 27, steps per second: 162, episode reward: 72.394, mean reward: 2.681 [1.871, 4.087], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.586, 10.100], loss: 0.186280, mae: 0.371308, mean_q: 4.208712
 63104/100000: episode: 1205, duration: 0.045s, episode steps: 7, steps per second: 157, episode reward: 19.691, mean reward: 2.813 [2.437, 3.275], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.755, 10.100], loss: 0.204032, mae: 0.396132, mean_q: 4.232620
 63131/100000: episode: 1206, duration: 0.160s, episode steps: 27, steps per second: 169, episode reward: 95.210, mean reward: 3.526 [2.273, 5.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-0.806, 10.100], loss: 0.235271, mae: 0.396435, mean_q: 4.151969
 63146/100000: episode: 1207, duration: 0.084s, episode steps: 15, steps per second: 180, episode reward: 37.135, mean reward: 2.476 [1.880, 3.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.354, 10.339], loss: 0.127058, mae: 0.349391, mean_q: 4.158034
 63155/100000: episode: 1208, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 27.693, mean reward: 3.077 [2.587, 3.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.467], loss: 0.182524, mae: 0.363153, mean_q: 4.140585
 63168/100000: episode: 1209, duration: 0.069s, episode steps: 13, steps per second: 187, episode reward: 30.645, mean reward: 2.357 [1.971, 2.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.104, 10.100], loss: 0.145444, mae: 0.357055, mean_q: 4.203891
 63249/100000: episode: 1210, duration: 0.426s, episode steps: 81, steps per second: 190, episode reward: 153.871, mean reward: 1.900 [1.458, 3.876], mean action: 0.000 [0.000, 0.000], mean observation: 1.630 [-1.065, 10.100], loss: 0.154763, mae: 0.358811, mean_q: 4.207446
 63330/100000: episode: 1211, duration: 0.428s, episode steps: 81, steps per second: 189, episode reward: 226.440, mean reward: 2.796 [1.985, 5.264], mean action: 0.000 [0.000, 0.000], mean observation: 1.617 [-0.449, 10.100], loss: 0.177768, mae: 0.375174, mean_q: 4.225093
 63411/100000: episode: 1212, duration: 0.448s, episode steps: 81, steps per second: 181, episode reward: 159.215, mean reward: 1.966 [1.524, 3.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.635 [-0.340, 10.216], loss: 0.159957, mae: 0.362491, mean_q: 4.214453
 63430/100000: episode: 1213, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 49.538, mean reward: 2.607 [2.050, 3.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.754, 10.323], loss: 0.164040, mae: 0.384363, mean_q: 4.298484
 63476/100000: episode: 1214, duration: 0.241s, episode steps: 46, steps per second: 191, episode reward: 173.024, mean reward: 3.761 [2.423, 6.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.911 [-1.436, 10.100], loss: 0.183616, mae: 0.399175, mean_q: 4.288370
 63484/100000: episode: 1215, duration: 0.044s, episode steps: 8, steps per second: 183, episode reward: 18.481, mean reward: 2.310 [1.905, 3.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.199, 10.100], loss: 0.199469, mae: 0.438131, mean_q: 4.421694
 63492/100000: episode: 1216, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 19.357, mean reward: 2.420 [2.109, 3.645], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.300, 10.100], loss: 0.287962, mae: 0.393358, mean_q: 4.250729
 63583/100000: episode: 1217, duration: 0.443s, episode steps: 91, steps per second: 206, episode reward: 181.920, mean reward: 1.999 [1.435, 3.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.537 [-0.455, 10.262], loss: 0.191036, mae: 0.395644, mean_q: 4.257586
 63602/100000: episode: 1218, duration: 0.121s, episode steps: 19, steps per second: 157, episode reward: 49.113, mean reward: 2.585 [1.718, 5.789], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.046, 10.261], loss: 0.194291, mae: 0.402984, mean_q: 4.352281
 63621/100000: episode: 1219, duration: 0.099s, episode steps: 19, steps per second: 193, episode reward: 50.308, mean reward: 2.648 [2.271, 3.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.368], loss: 0.187448, mae: 0.426869, mean_q: 4.286729
 63636/100000: episode: 1220, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 40.422, mean reward: 2.695 [1.923, 4.396], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.645, 10.341], loss: 0.249230, mae: 0.458046, mean_q: 4.351284
 63645/100000: episode: 1221, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 22.065, mean reward: 2.452 [2.103, 2.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.350], loss: 0.165687, mae: 0.417734, mean_q: 4.459179
 63652/100000: episode: 1222, duration: 0.039s, episode steps: 7, steps per second: 181, episode reward: 20.489, mean reward: 2.927 [2.542, 3.551], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.343, 10.100], loss: 0.226847, mae: 0.460803, mean_q: 4.491868
 63667/100000: episode: 1223, duration: 0.075s, episode steps: 15, steps per second: 199, episode reward: 44.275, mean reward: 2.952 [2.357, 4.310], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.468], loss: 0.161160, mae: 0.395077, mean_q: 4.295037
 63675/100000: episode: 1224, duration: 0.044s, episode steps: 8, steps per second: 184, episode reward: 20.777, mean reward: 2.597 [2.070, 3.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.325, 10.100], loss: 0.134349, mae: 0.343390, mean_q: 4.205148
 63690/100000: episode: 1225, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 44.152, mean reward: 2.943 [2.227, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.424], loss: 0.341999, mae: 0.460736, mean_q: 4.398986
 63736/100000: episode: 1226, duration: 0.231s, episode steps: 46, steps per second: 199, episode reward: 103.424, mean reward: 2.248 [1.569, 3.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.389, 10.100], loss: 0.173028, mae: 0.386680, mean_q: 4.323624
 63743/100000: episode: 1227, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 15.347, mean reward: 2.192 [1.654, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.296, 10.100], loss: 0.241119, mae: 0.424639, mean_q: 4.335104
 63789/100000: episode: 1228, duration: 0.248s, episode steps: 46, steps per second: 185, episode reward: 150.759, mean reward: 3.277 [2.156, 6.075], mean action: 0.000 [0.000, 0.000], mean observation: 1.925 [-0.380, 10.100], loss: 0.235106, mae: 0.415850, mean_q: 4.338032
 63808/100000: episode: 1229, duration: 0.111s, episode steps: 19, steps per second: 171, episode reward: 65.482, mean reward: 3.446 [2.512, 4.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.561], loss: 0.180010, mae: 0.405166, mean_q: 4.302939
 63827/100000: episode: 1230, duration: 0.108s, episode steps: 19, steps per second: 175, episode reward: 47.627, mean reward: 2.507 [1.978, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-1.674, 10.402], loss: 0.180845, mae: 0.417670, mean_q: 4.384738
 63873/100000: episode: 1231, duration: 0.247s, episode steps: 46, steps per second: 186, episode reward: 100.042, mean reward: 2.175 [1.464, 3.035], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.070, 10.353], loss: 0.168046, mae: 0.388752, mean_q: 4.264540
 63954/100000: episode: 1232, duration: 0.460s, episode steps: 81, steps per second: 176, episode reward: 166.073, mean reward: 2.050 [1.476, 4.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.634 [-0.498, 10.100], loss: 0.169194, mae: 0.382685, mean_q: 4.342204
 63961/100000: episode: 1233, duration: 0.039s, episode steps: 7, steps per second: 178, episode reward: 15.684, mean reward: 2.241 [2.076, 2.638], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.766, 10.100], loss: 0.205901, mae: 0.411568, mean_q: 4.428234
 63969/100000: episode: 1234, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 19.137, mean reward: 2.392 [1.849, 2.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.409, 10.100], loss: 0.191341, mae: 0.426628, mean_q: 4.229558
 63978/100000: episode: 1235, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 25.016, mean reward: 2.780 [2.409, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.483], loss: 0.119822, mae: 0.359838, mean_q: 4.399914
 63997/100000: episode: 1236, duration: 0.117s, episode steps: 19, steps per second: 162, episode reward: 41.368, mean reward: 2.177 [1.885, 2.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.341], loss: 0.208796, mae: 0.375546, mean_q: 4.332202
 64012/100000: episode: 1237, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 41.654, mean reward: 2.777 [2.157, 5.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.237, 10.339], loss: 0.165192, mae: 0.359527, mean_q: 4.343819
 64021/100000: episode: 1238, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 28.753, mean reward: 3.195 [2.893, 3.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.537], loss: 0.266926, mae: 0.416190, mean_q: 4.363353
 64040/100000: episode: 1239, duration: 0.121s, episode steps: 19, steps per second: 157, episode reward: 50.901, mean reward: 2.679 [2.337, 3.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.274, 10.416], loss: 0.138043, mae: 0.365365, mean_q: 4.305058
 64086/100000: episode: 1240, duration: 0.239s, episode steps: 46, steps per second: 193, episode reward: 114.322, mean reward: 2.485 [1.709, 3.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.237, 10.100], loss: 0.173996, mae: 0.384075, mean_q: 4.330223
 64101/100000: episode: 1241, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 43.667, mean reward: 2.911 [2.468, 3.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.454], loss: 0.190103, mae: 0.422135, mean_q: 4.392829
 64120/100000: episode: 1242, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 54.480, mean reward: 2.867 [2.089, 4.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.872, 10.353], loss: 0.156411, mae: 0.377217, mean_q: 4.369868
 64133/100000: episode: 1243, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 37.685, mean reward: 2.899 [2.178, 4.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.561, 10.100], loss: 0.139793, mae: 0.377392, mean_q: 4.288906
 64140/100000: episode: 1244, duration: 0.036s, episode steps: 7, steps per second: 195, episode reward: 18.816, mean reward: 2.688 [2.127, 3.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.398, 10.100], loss: 0.235001, mae: 0.435573, mean_q: 4.289734
 64221/100000: episode: 1245, duration: 0.431s, episode steps: 81, steps per second: 188, episode reward: 215.262, mean reward: 2.658 [1.670, 4.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.641 [-1.477, 10.100], loss: 0.138518, mae: 0.369588, mean_q: 4.369343
 64230/100000: episode: 1246, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 27.184, mean reward: 3.020 [2.552, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.486], loss: 0.106660, mae: 0.352864, mean_q: 4.339960
 64237/100000: episode: 1247, duration: 0.036s, episode steps: 7, steps per second: 193, episode reward: 19.304, mean reward: 2.758 [1.993, 4.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.246, 10.100], loss: 0.228354, mae: 0.369204, mean_q: 4.492637
 64318/100000: episode: 1248, duration: 0.429s, episode steps: 81, steps per second: 189, episode reward: 160.445, mean reward: 1.981 [1.468, 3.693], mean action: 0.000 [0.000, 0.000], mean observation: 1.635 [-0.474, 10.176], loss: 0.177903, mae: 0.401408, mean_q: 4.419168
 64331/100000: episode: 1249, duration: 0.076s, episode steps: 13, steps per second: 170, episode reward: 38.534, mean reward: 2.964 [1.939, 4.586], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.435, 10.100], loss: 0.140611, mae: 0.373849, mean_q: 4.404756
 64412/100000: episode: 1250, duration: 0.427s, episode steps: 81, steps per second: 190, episode reward: 169.485, mean reward: 2.092 [1.435, 3.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.637 [-0.364, 10.250], loss: 0.200311, mae: 0.400264, mean_q: 4.437901
 64439/100000: episode: 1251, duration: 0.150s, episode steps: 27, steps per second: 180, episode reward: 90.493, mean reward: 3.352 [2.458, 5.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-1.711, 10.100], loss: 0.158487, mae: 0.388678, mean_q: 4.505278
 64452/100000: episode: 1252, duration: 0.075s, episode steps: 13, steps per second: 172, episode reward: 28.898, mean reward: 2.223 [1.739, 2.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.645, 10.100], loss: 0.182678, mae: 0.397125, mean_q: 4.485891
 64533/100000: episode: 1253, duration: 0.420s, episode steps: 81, steps per second: 193, episode reward: 162.843, mean reward: 2.010 [1.442, 3.270], mean action: 0.000 [0.000, 0.000], mean observation: 1.634 [-0.676, 10.100], loss: 0.194178, mae: 0.400198, mean_q: 4.407302
 64546/100000: episode: 1254, duration: 0.078s, episode steps: 13, steps per second: 168, episode reward: 33.519, mean reward: 2.578 [2.023, 3.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.263, 10.100], loss: 0.236294, mae: 0.434688, mean_q: 4.475882
[Info] 2-TH LEVEL FOUND: 7.587808132171631, Considering 10/90 traces
 64559/100000: episode: 1255, duration: 4.068s, episode steps: 13, steps per second: 3, episode reward: 29.657, mean reward: 2.281 [2.023, 2.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.287, 10.100], loss: 0.154359, mae: 0.394183, mean_q: 4.378651
 64568/100000: episode: 1256, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 37.457, mean reward: 4.162 [2.816, 4.806], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.224, 10.507], loss: 0.209927, mae: 0.388035, mean_q: 4.429079
 64582/100000: episode: 1257, duration: 0.076s, episode steps: 14, steps per second: 183, episode reward: 55.200, mean reward: 3.943 [3.074, 6.167], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-1.963, 10.446], loss: 0.238967, mae: 0.408937, mean_q: 4.482335
 64604/100000: episode: 1258, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 77.001, mean reward: 3.500 [2.170, 6.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.163, 10.100], loss: 0.197579, mae: 0.411942, mean_q: 4.522251
 64626/100000: episode: 1259, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 80.605, mean reward: 3.664 [2.487, 6.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.333, 10.100], loss: 0.186915, mae: 0.378937, mean_q: 4.480370
 64638/100000: episode: 1260, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 59.748, mean reward: 4.979 [2.831, 14.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.431, 10.100], loss: 0.149224, mae: 0.371668, mean_q: 4.344389
 64679/100000: episode: 1261, duration: 0.221s, episode steps: 41, steps per second: 185, episode reward: 171.317, mean reward: 4.178 [2.018, 18.152], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.629, 10.100], loss: 0.213776, mae: 0.423011, mean_q: 4.519041
 64702/100000: episode: 1262, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 92.544, mean reward: 4.024 [2.753, 5.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.428, 10.100], loss: 0.148107, mae: 0.385874, mean_q: 4.400141
 64726/100000: episode: 1263, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 93.637, mean reward: 3.902 [3.012, 5.506], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.353, 10.100], loss: 0.377242, mae: 0.490289, mean_q: 4.686700
 64748/100000: episode: 1264, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 59.408, mean reward: 2.700 [2.175, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.285, 10.100], loss: 0.250168, mae: 0.472882, mean_q: 4.575830
 64760/100000: episode: 1265, duration: 0.059s, episode steps: 12, steps per second: 204, episode reward: 66.451, mean reward: 5.538 [3.003, 17.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.789, 10.100], loss: 0.195745, mae: 0.440893, mean_q: 4.630860
 64772/100000: episode: 1266, duration: 0.070s, episode steps: 12, steps per second: 170, episode reward: 53.171, mean reward: 4.431 [3.273, 5.535], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.299, 10.100], loss: 0.383272, mae: 0.526193, mean_q: 4.624012
 64813/100000: episode: 1267, duration: 0.227s, episode steps: 41, steps per second: 181, episode reward: 173.971, mean reward: 4.243 [2.422, 7.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-1.385, 10.100], loss: 0.184359, mae: 0.406364, mean_q: 4.670475
 64822/100000: episode: 1268, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 25.242, mean reward: 2.805 [1.963, 3.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.168, 10.295], loss: 0.200573, mae: 0.400757, mean_q: 4.574208
 64846/100000: episode: 1269, duration: 0.127s, episode steps: 24, steps per second: 188, episode reward: 88.885, mean reward: 3.704 [2.441, 6.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.154, 10.100], loss: 0.220923, mae: 0.451269, mean_q: 4.600389
 64887/100000: episode: 1270, duration: 0.214s, episode steps: 41, steps per second: 192, episode reward: 137.533, mean reward: 3.354 [2.171, 5.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.711, 10.100], loss: 0.291203, mae: 0.449203, mean_q: 4.630911
 64911/100000: episode: 1271, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 76.811, mean reward: 3.200 [2.490, 5.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.563, 10.100], loss: 0.223651, mae: 0.438357, mean_q: 4.664233
 64920/100000: episode: 1272, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 27.414, mean reward: 3.046 [2.400, 3.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.535], loss: 0.203896, mae: 0.423715, mean_q: 4.728665
 64940/100000: episode: 1273, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 72.150, mean reward: 3.608 [2.586, 4.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.246, 10.100], loss: 0.346832, mae: 0.475736, mean_q: 4.679376
 64960/100000: episode: 1274, duration: 0.119s, episode steps: 20, steps per second: 168, episode reward: 49.256, mean reward: 2.463 [2.114, 2.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.627, 10.100], loss: 0.253419, mae: 0.480674, mean_q: 4.762359
 64974/100000: episode: 1275, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 55.193, mean reward: 3.942 [2.714, 6.903], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.531], loss: 0.292213, mae: 0.486381, mean_q: 4.639667
 64998/100000: episode: 1276, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 107.970, mean reward: 4.499 [3.300, 7.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.363, 10.100], loss: 0.280027, mae: 0.490345, mean_q: 4.751733
 65021/100000: episode: 1277, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 93.570, mean reward: 4.068 [2.767, 6.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.798, 10.100], loss: 0.293096, mae: 0.465289, mean_q: 4.692502
 65044/100000: episode: 1278, duration: 0.128s, episode steps: 23, steps per second: 179, episode reward: 66.166, mean reward: 2.877 [2.179, 4.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-1.360, 10.100], loss: 0.313438, mae: 0.481548, mean_q: 4.796752
 65058/100000: episode: 1279, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 101.040, mean reward: 7.217 [4.603, 10.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.278, 10.623], loss: 0.276963, mae: 0.533615, mean_q: 4.744506
 65099/100000: episode: 1280, duration: 0.226s, episode steps: 41, steps per second: 181, episode reward: 124.814, mean reward: 3.044 [1.871, 5.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-1.119, 10.100], loss: 0.383311, mae: 0.493231, mean_q: 4.771635
 65140/100000: episode: 1281, duration: 0.211s, episode steps: 41, steps per second: 194, episode reward: 195.080, mean reward: 4.758 [2.088, 14.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.205, 10.100], loss: 0.273438, mae: 0.457323, mean_q: 4.814462
 65162/100000: episode: 1282, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 65.841, mean reward: 2.993 [2.266, 4.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.752, 10.100], loss: 0.199918, mae: 0.424519, mean_q: 4.722933
 65184/100000: episode: 1283, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 81.790, mean reward: 3.718 [2.535, 6.807], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.385, 10.100], loss: 0.395742, mae: 0.499105, mean_q: 4.850294
 65206/100000: episode: 1284, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 81.338, mean reward: 3.697 [2.352, 5.766], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.359, 10.100], loss: 0.525867, mae: 0.568019, mean_q: 4.946369
 65229/100000: episode: 1285, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 80.491, mean reward: 3.500 [2.637, 5.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.666, 10.100], loss: 0.201253, mae: 0.443125, mean_q: 4.812552
 65270/100000: episode: 1286, duration: 0.211s, episode steps: 41, steps per second: 194, episode reward: 132.990, mean reward: 3.244 [1.556, 7.069], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.769, 10.100], loss: 0.410238, mae: 0.529803, mean_q: 4.853417
 65282/100000: episode: 1287, duration: 0.076s, episode steps: 12, steps per second: 158, episode reward: 49.353, mean reward: 4.113 [2.951, 5.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.465, 10.100], loss: 0.227813, mae: 0.479064, mean_q: 4.953676
 65323/100000: episode: 1288, duration: 0.223s, episode steps: 41, steps per second: 184, episode reward: 116.210, mean reward: 2.834 [1.833, 4.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.378, 10.100], loss: 0.378914, mae: 0.546771, mean_q: 4.898675
 65345/100000: episode: 1289, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 64.531, mean reward: 2.933 [2.360, 4.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.578, 10.100], loss: 0.405650, mae: 0.567855, mean_q: 5.051226
 65365/100000: episode: 1290, duration: 0.118s, episode steps: 20, steps per second: 170, episode reward: 54.411, mean reward: 2.721 [2.171, 3.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.222, 10.100], loss: 0.372030, mae: 0.519628, mean_q: 4.979502
 65387/100000: episode: 1291, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 74.447, mean reward: 3.384 [2.383, 6.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.283, 10.100], loss: 0.239137, mae: 0.452943, mean_q: 4.796389
 65428/100000: episode: 1292, duration: 0.227s, episode steps: 41, steps per second: 181, episode reward: 143.395, mean reward: 3.497 [1.882, 6.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.959 [-0.537, 10.100], loss: 0.315345, mae: 0.518550, mean_q: 5.030467
 65451/100000: episode: 1293, duration: 0.147s, episode steps: 23, steps per second: 157, episode reward: 71.531, mean reward: 3.110 [2.183, 4.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.679, 10.100], loss: 0.256522, mae: 0.495177, mean_q: 4.835742
 65463/100000: episode: 1294, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 42.508, mean reward: 3.542 [2.532, 4.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.337, 10.100], loss: 0.378500, mae: 0.515888, mean_q: 5.001411
 65487/100000: episode: 1295, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 81.335, mean reward: 3.389 [2.076, 5.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.418, 10.100], loss: 0.319523, mae: 0.528494, mean_q: 5.051573
 65496/100000: episode: 1296, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 86.037, mean reward: 9.560 [3.861, 22.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.575, 10.670], loss: 0.482483, mae: 0.522967, mean_q: 4.882277
 65537/100000: episode: 1297, duration: 0.233s, episode steps: 41, steps per second: 176, episode reward: 182.770, mean reward: 4.458 [2.926, 7.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.339, 10.100], loss: 0.366824, mae: 0.540446, mean_q: 5.087608
 65561/100000: episode: 1298, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 64.425, mean reward: 2.684 [1.921, 4.267], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.283, 10.100], loss: 0.444533, mae: 0.533417, mean_q: 5.097014
 65602/100000: episode: 1299, duration: 0.199s, episode steps: 41, steps per second: 206, episode reward: 128.609, mean reward: 3.137 [2.235, 4.536], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.124, 10.100], loss: 0.332904, mae: 0.526936, mean_q: 5.133726
 65624/100000: episode: 1300, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 66.499, mean reward: 3.023 [2.382, 4.161], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.363, 10.100], loss: 0.412344, mae: 0.551787, mean_q: 5.145288
 65638/100000: episode: 1301, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 83.463, mean reward: 5.962 [4.372, 9.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.125, 10.606], loss: 0.331198, mae: 0.566646, mean_q: 5.210882
 65660/100000: episode: 1302, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 56.135, mean reward: 2.552 [2.006, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.281, 10.100], loss: 0.556282, mae: 0.623541, mean_q: 5.263196
 65682/100000: episode: 1303, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 68.800, mean reward: 3.127 [2.360, 5.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.199, 10.100], loss: 0.470640, mae: 0.536060, mean_q: 5.212523
 65694/100000: episode: 1304, duration: 0.059s, episode steps: 12, steps per second: 203, episode reward: 52.964, mean reward: 4.414 [2.915, 8.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.535, 10.100], loss: 0.512083, mae: 0.604714, mean_q: 5.290392
 65714/100000: episode: 1305, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 67.485, mean reward: 3.374 [2.372, 5.312], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.350, 10.100], loss: 0.354868, mae: 0.591852, mean_q: 5.173411
 65736/100000: episode: 1306, duration: 0.110s, episode steps: 22, steps per second: 200, episode reward: 93.552, mean reward: 4.252 [2.678, 7.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.405, 10.100], loss: 0.472763, mae: 0.572625, mean_q: 5.336640
 65777/100000: episode: 1307, duration: 0.208s, episode steps: 41, steps per second: 197, episode reward: 121.599, mean reward: 2.966 [1.775, 4.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.921, 10.100], loss: 0.350967, mae: 0.534646, mean_q: 5.106194
 65797/100000: episode: 1308, duration: 0.111s, episode steps: 20, steps per second: 179, episode reward: 72.996, mean reward: 3.650 [2.508, 9.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.277, 10.100], loss: 0.434342, mae: 0.586565, mean_q: 5.340862
 65838/100000: episode: 1309, duration: 0.212s, episode steps: 41, steps per second: 193, episode reward: 181.660, mean reward: 4.431 [1.649, 22.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.434, 10.100], loss: 0.394839, mae: 0.556852, mean_q: 5.313154
 65850/100000: episode: 1310, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 45.523, mean reward: 3.794 [2.588, 5.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.481, 10.100], loss: 0.829579, mae: 0.647548, mean_q: 5.349564
 65872/100000: episode: 1311, duration: 0.103s, episode steps: 22, steps per second: 213, episode reward: 68.764, mean reward: 3.126 [2.087, 6.171], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.414, 10.100], loss: 0.485466, mae: 0.614575, mean_q: 5.390975
 65894/100000: episode: 1312, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 69.448, mean reward: 3.157 [1.794, 7.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-1.072, 10.100], loss: 0.361147, mae: 0.562629, mean_q: 5.258841
 65917/100000: episode: 1313, duration: 0.115s, episode steps: 23, steps per second: 201, episode reward: 98.671, mean reward: 4.290 [2.569, 7.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.482, 10.100], loss: 0.528290, mae: 0.565714, mean_q: 5.311998
 65926/100000: episode: 1314, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 30.521, mean reward: 3.391 [2.874, 5.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.407], loss: 0.400073, mae: 0.612048, mean_q: 5.298588
 65950/100000: episode: 1315, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 82.970, mean reward: 3.457 [1.994, 5.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.196, 10.100], loss: 0.845753, mae: 0.665768, mean_q: 5.466295
 65974/100000: episode: 1316, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 73.843, mean reward: 3.077 [2.434, 3.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.403, 10.100], loss: 0.660639, mae: 0.609852, mean_q: 5.447711
 65994/100000: episode: 1317, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 63.705, mean reward: 3.185 [2.461, 5.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.384, 10.100], loss: 0.552473, mae: 0.592357, mean_q: 5.170553
 66008/100000: episode: 1318, duration: 0.087s, episode steps: 14, steps per second: 161, episode reward: 71.088, mean reward: 5.078 [3.299, 6.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.547, 10.500], loss: 0.269517, mae: 0.510700, mean_q: 5.321381
 66049/100000: episode: 1319, duration: 0.228s, episode steps: 41, steps per second: 179, episode reward: 119.331, mean reward: 2.911 [1.596, 10.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.972 [-0.368, 10.100], loss: 0.541190, mae: 0.586746, mean_q: 5.363506
 66073/100000: episode: 1320, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 66.557, mean reward: 2.773 [1.978, 4.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.329, 10.100], loss: 0.615443, mae: 0.642003, mean_q: 5.446513
 66097/100000: episode: 1321, duration: 0.129s, episode steps: 24, steps per second: 186, episode reward: 84.983, mean reward: 3.541 [2.653, 5.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.591, 10.100], loss: 0.371972, mae: 0.541866, mean_q: 5.346300
 66121/100000: episode: 1322, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 62.323, mean reward: 2.597 [2.132, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.284, 10.100], loss: 0.762641, mae: 0.660586, mean_q: 5.403101
 66141/100000: episode: 1323, duration: 0.103s, episode steps: 20, steps per second: 195, episode reward: 53.051, mean reward: 2.653 [2.070, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.311, 10.100], loss: 0.519011, mae: 0.626403, mean_q: 5.447814
 66165/100000: episode: 1324, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 108.909, mean reward: 4.538 [2.803, 10.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-0.419, 10.100], loss: 0.624014, mae: 0.673841, mean_q: 5.440754
 66189/100000: episode: 1325, duration: 0.135s, episode steps: 24, steps per second: 178, episode reward: 80.300, mean reward: 3.346 [2.418, 5.947], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.326, 10.100], loss: 0.358180, mae: 0.564731, mean_q: 5.312332
 66211/100000: episode: 1326, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 70.610, mean reward: 3.210 [2.386, 4.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.006, 10.100], loss: 0.931311, mae: 0.670082, mean_q: 5.561699
 66235/100000: episode: 1327, duration: 0.122s, episode steps: 24, steps per second: 196, episode reward: 93.696, mean reward: 3.904 [2.232, 6.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.552, 10.100], loss: 0.474420, mae: 0.593535, mean_q: 5.538245
 66247/100000: episode: 1328, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 45.498, mean reward: 3.792 [2.538, 7.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.415, 10.100], loss: 0.741131, mae: 0.579641, mean_q: 5.304851
 66267/100000: episode: 1329, duration: 0.104s, episode steps: 20, steps per second: 193, episode reward: 50.813, mean reward: 2.541 [2.024, 3.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.440, 10.100], loss: 0.436824, mae: 0.591332, mean_q: 5.620770
 66289/100000: episode: 1330, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 78.400, mean reward: 3.564 [2.548, 7.178], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.286, 10.100], loss: 0.589243, mae: 0.633436, mean_q: 5.642072
 66298/100000: episode: 1331, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 37.674, mean reward: 4.186 [2.908, 6.026], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.074, 10.616], loss: 0.324925, mae: 0.547248, mean_q: 5.322412
 66339/100000: episode: 1332, duration: 0.226s, episode steps: 41, steps per second: 182, episode reward: 182.582, mean reward: 4.453 [2.804, 14.220], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.491, 10.100], loss: 0.611370, mae: 0.600006, mean_q: 5.501298
[Info] FALSIFICATION!
 66356/100000: episode: 1333, duration: 0.347s, episode steps: 17, steps per second: 49, episode reward: 1069.482, mean reward: 62.911 [3.107, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.137, 10.020], loss: 0.483378, mae: 0.615719, mean_q: 5.478603
 66370/100000: episode: 1334, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 87.838, mean reward: 6.274 [3.489, 10.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.613], loss: 0.543056, mae: 0.667883, mean_q: 5.720412
 66392/100000: episode: 1335, duration: 0.110s, episode steps: 22, steps per second: 201, episode reward: 82.195, mean reward: 3.736 [2.398, 6.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.459, 10.100], loss: 0.516404, mae: 0.646631, mean_q: 5.563352
 66415/100000: episode: 1336, duration: 0.144s, episode steps: 23, steps per second: 159, episode reward: 64.158, mean reward: 2.789 [1.922, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.200, 10.100], loss: 673.959900, mae: 3.089947, mean_q: 6.598289
 66437/100000: episode: 1337, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 66.218, mean reward: 3.010 [1.939, 5.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.087, 10.100], loss: 0.719577, mae: 0.859690, mean_q: 5.512872
 66449/100000: episode: 1338, duration: 0.058s, episode steps: 12, steps per second: 205, episode reward: 34.706, mean reward: 2.892 [2.293, 3.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.505, 10.100], loss: 1.057665, mae: 0.752587, mean_q: 5.109009
 66458/100000: episode: 1339, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 62.638, mean reward: 6.960 [2.657, 12.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.155, 10.527], loss: 1719.342529, mae: 4.114557, mean_q: 4.930413
 66481/100000: episode: 1340, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 77.395, mean reward: 3.365 [2.283, 8.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.708, 10.100], loss: 6.458140, mae: 2.933530, mean_q: 7.751297
 66505/100000: episode: 1341, duration: 0.138s, episode steps: 24, steps per second: 175, episode reward: 85.071, mean reward: 3.545 [2.804, 5.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.320, 10.100], loss: 1.575325, mae: 1.320093, mean_q: 5.091136
 66529/100000: episode: 1342, duration: 0.119s, episode steps: 24, steps per second: 201, episode reward: 71.870, mean reward: 2.995 [2.336, 4.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.332, 10.100], loss: 1.349573, mae: 1.071339, mean_q: 5.271548
 66538/100000: episode: 1343, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 35.022, mean reward: 3.891 [3.117, 4.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.038, 10.496], loss: 1.028431, mae: 0.946530, mean_q: 5.555179
 66562/100000: episode: 1344, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 135.196, mean reward: 5.633 [2.709, 15.601], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.536, 10.100], loss: 1.002431, mae: 0.879888, mean_q: 5.695180
[Info] Complete ISplit Iteration
[Info] Levels: [5.200054, 7.587808, 7.5854206]
[Info] Cond. Prob: [0.1, 0.1, 0.35]
[Info] Error Prob: 0.0035000000000000005

 66585/100000: episode: 1345, duration: 4.317s, episode steps: 23, steps per second: 5, episode reward: 80.626, mean reward: 3.505 [2.909, 5.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.333, 10.100], loss: 0.989978, mae: 0.828731, mean_q: 5.792188
 66685/100000: episode: 1346, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 216.459, mean reward: 2.165 [1.532, 4.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.911, 10.327], loss: 309.124695, mae: 1.708209, mean_q: 6.318878
 66785/100000: episode: 1347, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 192.429, mean reward: 1.924 [1.502, 2.913], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.443, 10.208], loss: 0.916615, mae: 0.835185, mean_q: 6.007211
 66885/100000: episode: 1348, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 189.464, mean reward: 1.895 [1.467, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.317, 10.401], loss: 155.158325, mae: 1.310586, mean_q: 6.315401
 66985/100000: episode: 1349, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 210.263, mean reward: 2.103 [1.502, 3.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.715, 10.235], loss: 1.128985, mae: 0.822915, mean_q: 6.055479
 67085/100000: episode: 1350, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 196.460, mean reward: 1.965 [1.521, 4.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.593, 10.402], loss: 0.818945, mae: 0.740869, mean_q: 5.916731
 67185/100000: episode: 1351, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 184.186, mean reward: 1.842 [1.499, 2.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.422, 10.243], loss: 155.176926, mae: 1.241318, mean_q: 6.147160
 67285/100000: episode: 1352, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 194.902, mean reward: 1.949 [1.463, 3.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.844, 10.098], loss: 0.686402, mae: 0.702681, mean_q: 5.654161
 67385/100000: episode: 1353, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 194.930, mean reward: 1.949 [1.453, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.816, 10.179], loss: 0.613901, mae: 0.672836, mean_q: 5.593392
 67485/100000: episode: 1354, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 194.737, mean reward: 1.947 [1.450, 3.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.588, 10.098], loss: 0.758863, mae: 0.705432, mean_q: 5.622173
 67585/100000: episode: 1355, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 193.912, mean reward: 1.939 [1.473, 3.279], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.927, 10.108], loss: 309.509064, mae: 1.616254, mean_q: 6.014055
 67685/100000: episode: 1356, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 201.714, mean reward: 2.017 [1.442, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-2.099, 10.098], loss: 0.820514, mae: 0.817830, mean_q: 5.823334
 67785/100000: episode: 1357, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 183.932, mean reward: 1.839 [1.443, 2.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.151, 10.429], loss: 155.045303, mae: 1.185202, mean_q: 5.915149
 67885/100000: episode: 1358, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 182.238, mean reward: 1.822 [1.441, 3.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.705, 10.188], loss: 154.934189, mae: 1.164037, mean_q: 5.870341
 67985/100000: episode: 1359, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 184.335, mean reward: 1.843 [1.483, 2.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.356, 10.098], loss: 0.631589, mae: 0.640808, mean_q: 5.574646
 68085/100000: episode: 1360, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 188.671, mean reward: 1.887 [1.469, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.163, 10.182], loss: 154.921539, mae: 1.146572, mean_q: 5.864571
 68185/100000: episode: 1361, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 231.691, mean reward: 2.317 [1.464, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.320, 10.098], loss: 154.968811, mae: 1.169452, mean_q: 5.816068
 68285/100000: episode: 1362, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 198.579, mean reward: 1.986 [1.476, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.297, 10.098], loss: 155.146606, mae: 1.193772, mean_q: 5.876756
 68385/100000: episode: 1363, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 188.343, mean reward: 1.883 [1.456, 3.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.853, 10.335], loss: 308.830444, mae: 1.640349, mean_q: 6.161361
 68485/100000: episode: 1364, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 185.054, mean reward: 1.851 [1.442, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.228, 10.266], loss: 0.747565, mae: 0.692536, mean_q: 5.700407
 68585/100000: episode: 1365, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 180.546, mean reward: 1.805 [1.452, 2.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.790, 10.170], loss: 0.649806, mae: 0.644363, mean_q: 5.535571
 68685/100000: episode: 1366, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 187.649, mean reward: 1.876 [1.468, 3.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.391, 10.358], loss: 309.015350, mae: 1.610512, mean_q: 6.028243
 68785/100000: episode: 1367, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 213.028, mean reward: 2.130 [1.486, 4.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.130, 10.309], loss: 154.828140, mae: 1.152406, mean_q: 5.786169
 68885/100000: episode: 1368, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 204.489, mean reward: 2.045 [1.499, 3.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.206, 10.098], loss: 0.751663, mae: 0.644703, mean_q: 5.462923
 68985/100000: episode: 1369, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 228.604, mean reward: 2.286 [1.466, 5.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.979, 10.098], loss: 308.777710, mae: 1.585452, mean_q: 5.998619
 69085/100000: episode: 1370, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 197.789, mean reward: 1.978 [1.443, 3.146], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.783, 10.098], loss: 0.588111, mae: 0.637577, mean_q: 5.454500
 69185/100000: episode: 1371, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 211.466, mean reward: 2.115 [1.460, 3.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.973, 10.317], loss: 308.487488, mae: 1.476584, mean_q: 5.759753
 69285/100000: episode: 1372, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 190.055, mean reward: 1.901 [1.458, 3.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.079, 10.247], loss: 0.919936, mae: 0.795681, mean_q: 5.619153
 69385/100000: episode: 1373, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 183.327, mean reward: 1.833 [1.443, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.514, 10.137], loss: 154.624466, mae: 1.051846, mean_q: 5.562644
 69485/100000: episode: 1374, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 195.998, mean reward: 1.960 [1.477, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.658, 10.128], loss: 154.518738, mae: 1.153547, mean_q: 5.775308
 69585/100000: episode: 1375, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 183.238, mean reward: 1.832 [1.443, 2.924], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.935, 10.145], loss: 0.606552, mae: 0.614219, mean_q: 5.327153
 69685/100000: episode: 1376, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 187.547, mean reward: 1.875 [1.469, 3.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.687, 10.131], loss: 0.555025, mae: 0.575243, mean_q: 5.129345
 69785/100000: episode: 1377, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 182.126, mean reward: 1.821 [1.444, 2.643], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.152, 10.319], loss: 154.942703, mae: 1.076215, mean_q: 5.368318
 69885/100000: episode: 1378, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 199.520, mean reward: 1.995 [1.526, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.269, 10.339], loss: 154.615158, mae: 1.078475, mean_q: 5.347663
 69985/100000: episode: 1379, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 185.917, mean reward: 1.859 [1.446, 2.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.324, 10.346], loss: 154.670807, mae: 1.035298, mean_q: 5.315211
 70085/100000: episode: 1380, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 186.491, mean reward: 1.865 [1.448, 3.222], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.976, 10.107], loss: 154.483521, mae: 0.952690, mean_q: 5.109567
 70185/100000: episode: 1381, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 194.562, mean reward: 1.946 [1.459, 4.106], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.232, 10.098], loss: 154.646393, mae: 1.124501, mean_q: 5.377192
 70285/100000: episode: 1382, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 187.252, mean reward: 1.873 [1.508, 2.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.146, 10.098], loss: 0.478735, mae: 0.548801, mean_q: 4.967461
 70385/100000: episode: 1383, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 194.023, mean reward: 1.940 [1.525, 3.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.532, 10.202], loss: 307.620361, mae: 1.272608, mean_q: 4.893766
 70485/100000: episode: 1384, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 194.778, mean reward: 1.948 [1.461, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.914, 10.121], loss: 0.970843, mae: 0.804470, mean_q: 5.120134
 70585/100000: episode: 1385, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 202.911, mean reward: 2.029 [1.456, 3.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.278, 10.150], loss: 154.113586, mae: 0.784815, mean_q: 4.623481
 70685/100000: episode: 1386, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 219.451, mean reward: 2.195 [1.469, 4.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.742, 10.098], loss: 154.278549, mae: 1.119277, mean_q: 5.124352
 70785/100000: episode: 1387, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 179.622, mean reward: 1.796 [1.473, 2.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.406, 10.116], loss: 154.191910, mae: 1.019284, mean_q: 4.929879
 70885/100000: episode: 1388, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 196.683, mean reward: 1.967 [1.476, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.498, 10.359], loss: 307.378021, mae: 1.330895, mean_q: 4.925735
 70985/100000: episode: 1389, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 201.371, mean reward: 2.014 [1.471, 5.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.512, 10.274], loss: 0.525919, mae: 0.564564, mean_q: 4.675934
 71085/100000: episode: 1390, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 183.620, mean reward: 1.836 [1.445, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.680, 10.105], loss: 153.862778, mae: 0.893706, mean_q: 4.664426
 71185/100000: episode: 1391, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 194.070, mean reward: 1.941 [1.444, 4.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.218, 10.116], loss: 0.275083, mae: 0.423657, mean_q: 4.268733
 71285/100000: episode: 1392, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 201.009, mean reward: 2.010 [1.470, 3.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.922, 10.136], loss: 153.925507, mae: 0.836905, mean_q: 4.394445
 71385/100000: episode: 1393, duration: 0.536s, episode steps: 100, steps per second: 186, episode reward: 204.987, mean reward: 2.050 [1.458, 3.296], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.361, 10.098], loss: 0.242506, mae: 0.409586, mean_q: 4.239908
 71485/100000: episode: 1394, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 178.931, mean reward: 1.789 [1.474, 2.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.742, 10.119], loss: 0.179234, mae: 0.356592, mean_q: 4.044636
 71585/100000: episode: 1395, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 189.895, mean reward: 1.899 [1.437, 2.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.290, 10.213], loss: 0.139049, mae: 0.327953, mean_q: 3.933953
 71685/100000: episode: 1396, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 189.203, mean reward: 1.892 [1.493, 2.614], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.154, 10.098], loss: 0.112043, mae: 0.307696, mean_q: 3.892677
 71785/100000: episode: 1397, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 209.873, mean reward: 2.099 [1.447, 6.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.639, 10.098], loss: 0.127546, mae: 0.314511, mean_q: 3.889071
 71885/100000: episode: 1398, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 193.613, mean reward: 1.936 [1.448, 2.883], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.562, 10.098], loss: 0.112487, mae: 0.310775, mean_q: 3.880335
 71985/100000: episode: 1399, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 184.583, mean reward: 1.846 [1.464, 2.504], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.171, 10.143], loss: 0.109506, mae: 0.306409, mean_q: 3.884392
 72085/100000: episode: 1400, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 180.046, mean reward: 1.800 [1.476, 2.640], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.674, 10.263], loss: 0.101315, mae: 0.296524, mean_q: 3.853725
 72185/100000: episode: 1401, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 208.437, mean reward: 2.084 [1.460, 3.346], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.309, 10.098], loss: 0.102490, mae: 0.300909, mean_q: 3.862787
 72285/100000: episode: 1402, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 198.151, mean reward: 1.982 [1.519, 2.835], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.826, 10.098], loss: 0.093423, mae: 0.299742, mean_q: 3.862464
 72385/100000: episode: 1403, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 184.802, mean reward: 1.848 [1.433, 3.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.099, 10.098], loss: 0.100016, mae: 0.298914, mean_q: 3.867923
 72485/100000: episode: 1404, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 194.645, mean reward: 1.946 [1.463, 10.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.240, 10.098], loss: 0.088703, mae: 0.286102, mean_q: 3.844594
 72585/100000: episode: 1405, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 196.569, mean reward: 1.966 [1.482, 3.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.318, 10.236], loss: 0.093200, mae: 0.290128, mean_q: 3.858829
 72685/100000: episode: 1406, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 191.571, mean reward: 1.916 [1.482, 2.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.768, 10.333], loss: 0.105495, mae: 0.298435, mean_q: 3.846613
 72785/100000: episode: 1407, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 207.875, mean reward: 2.079 [1.471, 4.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.221, 10.098], loss: 0.088814, mae: 0.287963, mean_q: 3.851595
 72885/100000: episode: 1408, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 178.369, mean reward: 1.784 [1.444, 2.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.906, 10.098], loss: 0.111326, mae: 0.299733, mean_q: 3.869419
 72985/100000: episode: 1409, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 176.729, mean reward: 1.767 [1.444, 3.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.626, 10.098], loss: 0.085720, mae: 0.280265, mean_q: 3.853056
 73085/100000: episode: 1410, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 197.461, mean reward: 1.975 [1.440, 3.295], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.147, 10.309], loss: 0.086432, mae: 0.286159, mean_q: 3.851115
 73185/100000: episode: 1411, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: 214.301, mean reward: 2.143 [1.441, 5.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.446, 10.098], loss: 0.076309, mae: 0.269397, mean_q: 3.827063
 73285/100000: episode: 1412, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 189.462, mean reward: 1.895 [1.454, 2.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.121, 10.098], loss: 0.129333, mae: 0.286642, mean_q: 3.852009
 73385/100000: episode: 1413, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 190.379, mean reward: 1.904 [1.505, 3.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.487, 10.296], loss: 0.082363, mae: 0.280392, mean_q: 3.834769
 73485/100000: episode: 1414, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 210.524, mean reward: 2.105 [1.450, 7.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.089, 10.248], loss: 0.105155, mae: 0.292020, mean_q: 3.847959
 73585/100000: episode: 1415, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 192.887, mean reward: 1.929 [1.501, 3.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.576, 10.185], loss: 0.086789, mae: 0.281848, mean_q: 3.842403
 73685/100000: episode: 1416, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 197.586, mean reward: 1.976 [1.517, 4.251], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.986, 10.363], loss: 0.123802, mae: 0.302631, mean_q: 3.859224
 73785/100000: episode: 1417, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 198.849, mean reward: 1.988 [1.523, 3.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.897, 10.098], loss: 0.115021, mae: 0.297695, mean_q: 3.842198
 73885/100000: episode: 1418, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 196.542, mean reward: 1.965 [1.446, 5.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.102, 10.098], loss: 0.105081, mae: 0.297411, mean_q: 3.858019
 73985/100000: episode: 1419, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 182.759, mean reward: 1.828 [1.505, 2.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.100, 10.209], loss: 0.101744, mae: 0.290502, mean_q: 3.851835
 74085/100000: episode: 1420, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 185.339, mean reward: 1.853 [1.445, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.220, 10.098], loss: 0.093882, mae: 0.286259, mean_q: 3.849501
 74185/100000: episode: 1421, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 191.139, mean reward: 1.911 [1.467, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.253, 10.124], loss: 0.082206, mae: 0.275416, mean_q: 3.838843
 74285/100000: episode: 1422, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 189.733, mean reward: 1.897 [1.463, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.732, 10.098], loss: 0.105834, mae: 0.288052, mean_q: 3.820062
 74385/100000: episode: 1423, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 232.079, mean reward: 2.321 [1.560, 5.972], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.420, 10.478], loss: 0.091642, mae: 0.279684, mean_q: 3.836925
 74485/100000: episode: 1424, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.045, mean reward: 1.870 [1.500, 3.246], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.328, 10.098], loss: 0.092459, mae: 0.280199, mean_q: 3.838155
 74585/100000: episode: 1425, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 199.554, mean reward: 1.996 [1.506, 4.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.422, 10.170], loss: 0.093283, mae: 0.283676, mean_q: 3.853268
 74685/100000: episode: 1426, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 208.713, mean reward: 2.087 [1.511, 4.226], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.796, 10.098], loss: 0.101530, mae: 0.290009, mean_q: 3.846354
 74785/100000: episode: 1427, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 189.827, mean reward: 1.898 [1.459, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.713, 10.132], loss: 0.100193, mae: 0.294595, mean_q: 3.877010
 74885/100000: episode: 1428, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 187.187, mean reward: 1.872 [1.469, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.112, 10.311], loss: 0.116344, mae: 0.298187, mean_q: 3.861742
 74985/100000: episode: 1429, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 201.779, mean reward: 2.018 [1.472, 4.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.615, 10.099], loss: 0.132581, mae: 0.307844, mean_q: 3.862019
 75085/100000: episode: 1430, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 204.991, mean reward: 2.050 [1.444, 4.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.684, 10.098], loss: 0.096148, mae: 0.297522, mean_q: 3.860508
 75185/100000: episode: 1431, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 194.553, mean reward: 1.946 [1.442, 3.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.784, 10.098], loss: 0.100477, mae: 0.293131, mean_q: 3.843202
 75285/100000: episode: 1432, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 187.298, mean reward: 1.873 [1.443, 2.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.393, 10.270], loss: 0.104116, mae: 0.290487, mean_q: 3.833942
 75385/100000: episode: 1433, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 193.126, mean reward: 1.931 [1.449, 4.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.869, 10.098], loss: 0.089912, mae: 0.290279, mean_q: 3.867996
 75485/100000: episode: 1434, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 192.099, mean reward: 1.921 [1.470, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.451, 10.243], loss: 0.104662, mae: 0.304524, mean_q: 3.859579
 75585/100000: episode: 1435, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 181.325, mean reward: 1.813 [1.441, 2.969], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.052, 10.158], loss: 0.088542, mae: 0.285673, mean_q: 3.847681
 75685/100000: episode: 1436, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 205.575, mean reward: 2.056 [1.501, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.931, 10.098], loss: 0.091539, mae: 0.299709, mean_q: 3.858827
 75785/100000: episode: 1437, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 197.857, mean reward: 1.979 [1.469, 4.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.857, 10.224], loss: 0.106386, mae: 0.297543, mean_q: 3.865331
 75885/100000: episode: 1438, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 193.425, mean reward: 1.934 [1.450, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.882, 10.146], loss: 0.098589, mae: 0.294474, mean_q: 3.853388
 75985/100000: episode: 1439, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 181.253, mean reward: 1.813 [1.477, 2.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.373, 10.098], loss: 0.105895, mae: 0.294275, mean_q: 3.848855
 76085/100000: episode: 1440, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 206.990, mean reward: 2.070 [1.539, 7.099], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.058, 10.098], loss: 0.104181, mae: 0.295233, mean_q: 3.857130
 76185/100000: episode: 1441, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 187.429, mean reward: 1.874 [1.444, 3.794], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.048, 10.098], loss: 0.106535, mae: 0.298619, mean_q: 3.854276
 76285/100000: episode: 1442, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 206.651, mean reward: 2.067 [1.490, 4.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.448, 10.263], loss: 0.086142, mae: 0.289094, mean_q: 3.844742
 76385/100000: episode: 1443, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 208.614, mean reward: 2.086 [1.476, 3.282], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.531, 10.098], loss: 0.096619, mae: 0.283580, mean_q: 3.839325
 76485/100000: episode: 1444, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 216.621, mean reward: 2.166 [1.508, 3.759], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.586, 10.449], loss: 0.102931, mae: 0.295526, mean_q: 3.857525
[Info] 1-TH LEVEL FOUND: 5.377434253692627, Considering 10/90 traces
 76585/100000: episode: 1445, duration: 4.610s, episode steps: 100, steps per second: 22, episode reward: 198.455, mean reward: 1.985 [1.494, 5.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.425, 10.279], loss: 0.099889, mae: 0.301548, mean_q: 3.884132
 76636/100000: episode: 1446, duration: 0.289s, episode steps: 51, steps per second: 176, episode reward: 150.068, mean reward: 2.943 [1.991, 5.150], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-1.185, 10.406], loss: 0.102434, mae: 0.311232, mean_q: 3.920576
 76655/100000: episode: 1447, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 50.084, mean reward: 2.636 [2.030, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.849, 10.364], loss: 0.116389, mae: 0.303883, mean_q: 3.888377
 76674/100000: episode: 1448, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 57.906, mean reward: 3.048 [2.499, 3.837], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.515], loss: 0.191585, mae: 0.333982, mean_q: 3.881587
 76693/100000: episode: 1449, duration: 0.107s, episode steps: 19, steps per second: 178, episode reward: 48.937, mean reward: 2.576 [2.252, 3.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.419], loss: 0.078102, mae: 0.282006, mean_q: 3.895486
 76703/100000: episode: 1450, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 31.125, mean reward: 3.113 [2.638, 3.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.041, 10.385], loss: 0.070232, mae: 0.265934, mean_q: 3.922239
 76708/100000: episode: 1451, duration: 0.033s, episode steps: 5, steps per second: 152, episode reward: 11.126, mean reward: 2.225 [1.955, 2.737], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.057, 10.345], loss: 0.162863, mae: 0.292810, mean_q: 3.883057
 76759/100000: episode: 1452, duration: 0.268s, episode steps: 51, steps per second: 190, episode reward: 111.260, mean reward: 2.182 [1.523, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.428, 10.132], loss: 0.095587, mae: 0.297503, mean_q: 3.908778
 76773/100000: episode: 1453, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 37.663, mean reward: 2.690 [2.268, 3.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.665, 10.431], loss: 0.068736, mae: 0.278598, mean_q: 3.903022
 76827/100000: episode: 1454, duration: 0.285s, episode steps: 54, steps per second: 189, episode reward: 144.425, mean reward: 2.675 [1.659, 4.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.366, 10.205], loss: 0.103850, mae: 0.295756, mean_q: 3.924574
 76852/100000: episode: 1455, duration: 0.136s, episode steps: 25, steps per second: 183, episode reward: 83.030, mean reward: 3.321 [2.004, 4.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.322, 10.348], loss: 0.124574, mae: 0.324764, mean_q: 3.922421
 76866/100000: episode: 1456, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 37.150, mean reward: 2.654 [2.040, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.375], loss: 0.162066, mae: 0.345989, mean_q: 3.958698
 76885/100000: episode: 1457, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 69.924, mean reward: 3.680 [2.621, 5.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.577], loss: 0.104440, mae: 0.329591, mean_q: 3.986322
 76904/100000: episode: 1458, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 57.170, mean reward: 3.009 [2.308, 5.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-1.198, 10.315], loss: 0.099393, mae: 0.302949, mean_q: 3.942690
 76914/100000: episode: 1459, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 31.743, mean reward: 3.174 [2.373, 4.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.554], loss: 0.151693, mae: 0.356831, mean_q: 4.007995
 76928/100000: episode: 1460, duration: 0.068s, episode steps: 14, steps per second: 207, episode reward: 32.955, mean reward: 2.354 [1.930, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.582, 10.358], loss: 0.097613, mae: 0.320037, mean_q: 3.997145
 76938/100000: episode: 1461, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 23.002, mean reward: 2.300 [1.987, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.550, 10.387], loss: 0.129178, mae: 0.337302, mean_q: 4.011999
 76963/100000: episode: 1462, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 115.170, mean reward: 4.607 [2.396, 7.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.333, 10.410], loss: 0.146737, mae: 0.346019, mean_q: 4.033549
 76978/100000: episode: 1463, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 36.349, mean reward: 2.423 [2.020, 4.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.489], loss: 0.151432, mae: 0.383542, mean_q: 4.037144
 76993/100000: episode: 1464, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 42.174, mean reward: 2.812 [2.048, 3.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.193, 10.399], loss: 0.113969, mae: 0.338915, mean_q: 3.979721
 77007/100000: episode: 1465, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 31.931, mean reward: 2.281 [1.825, 2.823], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.336], loss: 0.080725, mae: 0.290326, mean_q: 3.971307
 77026/100000: episode: 1466, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 68.437, mean reward: 3.602 [2.503, 4.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.496], loss: 0.163723, mae: 0.335640, mean_q: 4.005808
 77031/100000: episode: 1467, duration: 0.036s, episode steps: 5, steps per second: 140, episode reward: 10.986, mean reward: 2.197 [1.987, 2.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.350], loss: 0.103993, mae: 0.339859, mean_q: 4.098072
 77085/100000: episode: 1468, duration: 0.266s, episode steps: 54, steps per second: 203, episode reward: 124.097, mean reward: 2.298 [1.624, 3.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.857 [-0.406, 10.203], loss: 0.118565, mae: 0.322766, mean_q: 3.987514
 77136/100000: episode: 1469, duration: 0.276s, episode steps: 51, steps per second: 185, episode reward: 124.958, mean reward: 2.450 [1.724, 3.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.351, 10.291], loss: 0.132810, mae: 0.322092, mean_q: 4.029343
 77141/100000: episode: 1470, duration: 0.031s, episode steps: 5, steps per second: 161, episode reward: 13.523, mean reward: 2.705 [2.449, 3.076], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.403], loss: 0.093502, mae: 0.304717, mean_q: 3.948730
 77157/100000: episode: 1471, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 57.671, mean reward: 3.604 [2.541, 7.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.462, 10.100], loss: 0.124529, mae: 0.333671, mean_q: 4.056293
 77176/100000: episode: 1472, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 65.017, mean reward: 3.422 [2.375, 5.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.035, 10.502], loss: 0.121747, mae: 0.324523, mean_q: 4.016444
 77201/100000: episode: 1473, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 87.424, mean reward: 3.497 [2.642, 4.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.506], loss: 0.113339, mae: 0.326174, mean_q: 4.003720
 77226/100000: episode: 1474, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 60.064, mean reward: 2.403 [1.676, 4.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.913, 10.264], loss: 0.133899, mae: 0.343670, mean_q: 4.029744
 77241/100000: episode: 1475, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 65.338, mean reward: 4.356 [2.233, 6.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.614], loss: 0.142957, mae: 0.352434, mean_q: 4.049376
 77266/100000: episode: 1476, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 57.273, mean reward: 2.291 [1.436, 4.810], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.216], loss: 0.157319, mae: 0.351124, mean_q: 4.082752
 77271/100000: episode: 1477, duration: 0.027s, episode steps: 5, steps per second: 184, episode reward: 13.471, mean reward: 2.694 [2.286, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.467], loss: 0.197781, mae: 0.396965, mean_q: 4.246762
 77296/100000: episode: 1478, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 77.865, mean reward: 3.115 [1.634, 12.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-1.191, 10.214], loss: 0.143540, mae: 0.354541, mean_q: 4.051973
 77347/100000: episode: 1479, duration: 0.271s, episode steps: 51, steps per second: 188, episode reward: 115.765, mean reward: 2.270 [1.466, 4.134], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.341, 10.108], loss: 0.233282, mae: 0.398780, mean_q: 4.101362
[Info] FALSIFICATION!
 77362/100000: episode: 1480, duration: 0.247s, episode steps: 15, steps per second: 61, episode reward: 1106.732, mean reward: 73.782 [2.319, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.048, 10.753], loss: 0.229065, mae: 0.395181, mean_q: 4.206768
 77387/100000: episode: 1481, duration: 0.132s, episode steps: 25, steps per second: 189, episode reward: 78.135, mean reward: 3.125 [1.848, 4.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-1.233, 10.300], loss: 614.312012, mae: 1.927900, mean_q: 4.543404
 77401/100000: episode: 1482, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 57.849, mean reward: 4.132 [2.979, 5.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.541], loss: 0.523315, mae: 0.615817, mean_q: 4.194904
 77452/100000: episode: 1483, duration: 0.247s, episode steps: 51, steps per second: 207, episode reward: 159.885, mean reward: 3.135 [2.345, 4.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-0.340, 10.498], loss: 301.620972, mae: 1.320672, mean_q: 4.429864
 77506/100000: episode: 1484, duration: 0.275s, episode steps: 54, steps per second: 196, episode reward: 157.656, mean reward: 2.920 [1.509, 7.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.856 [-0.824, 10.195], loss: 566.737732, mae: 1.948793, mean_q: 4.786557
 77557/100000: episode: 1485, duration: 0.262s, episode steps: 51, steps per second: 194, episode reward: 134.149, mean reward: 2.630 [1.526, 15.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.565, 10.273], loss: 1.436816, mae: 0.998698, mean_q: 4.949210
 77571/100000: episode: 1486, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 40.351, mean reward: 2.882 [2.145, 3.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.035, 10.382], loss: 0.342593, mae: 0.505396, mean_q: 4.893869
 77581/100000: episode: 1487, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 27.007, mean reward: 2.701 [2.132, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.440], loss: 0.519559, mae: 0.470906, mean_q: 4.760686
 77591/100000: episode: 1488, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 31.617, mean reward: 3.162 [2.697, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.473], loss: 0.183329, mae: 0.427429, mean_q: 4.566886
 77642/100000: episode: 1489, duration: 0.268s, episode steps: 51, steps per second: 191, episode reward: 127.796, mean reward: 2.506 [1.758, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.346, 10.290], loss: 0.289286, mae: 0.453182, mean_q: 4.674606
 77693/100000: episode: 1490, duration: 0.252s, episode steps: 51, steps per second: 202, episode reward: 127.613, mean reward: 2.502 [1.472, 3.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.338, 10.162], loss: 0.591801, mae: 0.452920, mean_q: 4.596989
 77712/100000: episode: 1491, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 44.924, mean reward: 2.364 [1.836, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.329], loss: 802.899048, mae: 3.125535, mean_q: 5.782302
 77766/100000: episode: 1492, duration: 0.304s, episode steps: 54, steps per second: 178, episode reward: 124.089, mean reward: 2.298 [1.514, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.640, 10.216], loss: 279.977173, mae: 2.055021, mean_q: 5.252716
 77780/100000: episode: 1493, duration: 0.084s, episode steps: 14, steps per second: 166, episode reward: 38.263, mean reward: 2.733 [2.045, 3.431], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.233, 10.392], loss: 0.304173, mae: 0.584185, mean_q: 4.569842
 77799/100000: episode: 1494, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 47.137, mean reward: 2.481 [2.142, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.790, 10.413], loss: 0.244966, mae: 0.494248, mean_q: 4.651674
 77814/100000: episode: 1495, duration: 0.091s, episode steps: 15, steps per second: 164, episode reward: 54.457, mean reward: 3.630 [2.221, 8.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.360], loss: 1013.019897, mae: 2.783045, mean_q: 4.896953
 77868/100000: episode: 1496, duration: 0.278s, episode steps: 54, steps per second: 194, episode reward: 239.184, mean reward: 4.429 [2.003, 14.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.871 [-0.378, 10.529], loss: 1.569641, mae: 0.984239, mean_q: 5.158777
 77873/100000: episode: 1497, duration: 0.027s, episode steps: 5, steps per second: 187, episode reward: 13.999, mean reward: 2.800 [2.380, 3.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.483], loss: 0.288210, mae: 0.537957, mean_q: 4.544547
 77924/100000: episode: 1498, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 201.964, mean reward: 3.960 [1.802, 7.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.885 [-0.339, 10.261], loss: 298.838470, mae: 1.682859, mean_q: 5.407617
 77943/100000: episode: 1499, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 45.483, mean reward: 2.394 [1.962, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-1.538, 10.355], loss: 0.599472, mae: 0.635582, mean_q: 4.576971
 77994/100000: episode: 1500, duration: 0.276s, episode steps: 51, steps per second: 185, episode reward: 99.400, mean reward: 1.949 [1.442, 3.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.387, 10.126], loss: 0.252019, mae: 0.480240, mean_q: 4.559621
 78013/100000: episode: 1501, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 43.615, mean reward: 2.296 [1.853, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.060, 10.322], loss: 0.344882, mae: 0.490264, mean_q: 4.597611
 78023/100000: episode: 1502, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 28.382, mean reward: 2.838 [2.471, 3.088], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.466], loss: 0.227970, mae: 0.470855, mean_q: 4.579829
 78048/100000: episode: 1503, duration: 0.150s, episode steps: 25, steps per second: 167, episode reward: 73.327, mean reward: 2.933 [2.092, 4.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.434, 10.294], loss: 0.379916, mae: 0.512574, mean_q: 4.628616
 78063/100000: episode: 1504, duration: 0.075s, episode steps: 15, steps per second: 200, episode reward: 38.214, mean reward: 2.548 [1.851, 3.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.472, 10.511], loss: 0.242976, mae: 0.467299, mean_q: 4.577107
 78117/100000: episode: 1505, duration: 0.300s, episode steps: 54, steps per second: 180, episode reward: 134.681, mean reward: 2.494 [1.571, 4.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.867 [-0.367, 10.227], loss: 0.246536, mae: 0.460807, mean_q: 4.493331
 78142/100000: episode: 1506, duration: 0.138s, episode steps: 25, steps per second: 182, episode reward: 65.920, mean reward: 2.637 [1.973, 4.849], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.105, 10.360], loss: 0.203708, mae: 0.429796, mean_q: 4.500800
 78157/100000: episode: 1507, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 32.881, mean reward: 2.192 [1.902, 2.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.530, 10.353], loss: 0.216685, mae: 0.450527, mean_q: 4.481934
 78162/100000: episode: 1508, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 12.769, mean reward: 2.554 [2.155, 3.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.247], loss: 0.234630, mae: 0.430284, mean_q: 4.509685
 78178/100000: episode: 1509, duration: 0.089s, episode steps: 16, steps per second: 179, episode reward: 48.297, mean reward: 3.019 [2.264, 5.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.472, 10.100], loss: 952.746155, mae: 3.604039, mean_q: 6.037481
 78194/100000: episode: 1510, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 70.019, mean reward: 4.376 [2.569, 14.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.871, 10.100], loss: 3.104289, mae: 1.432233, mean_q: 5.659937
 78209/100000: episode: 1511, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 35.271, mean reward: 2.351 [1.952, 3.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-1.078, 10.384], loss: 0.630496, mae: 0.584783, mean_q: 4.431667
 78223/100000: episode: 1512, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 30.776, mean reward: 2.198 [1.599, 2.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.180, 10.271], loss: 0.306939, mae: 0.520363, mean_q: 4.469345
 78248/100000: episode: 1513, duration: 0.117s, episode steps: 25, steps per second: 213, episode reward: 77.512, mean reward: 3.100 [2.198, 5.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.974, 10.402], loss: 0.230349, mae: 0.456226, mean_q: 4.411758
 78302/100000: episode: 1514, duration: 0.278s, episode steps: 54, steps per second: 194, episode reward: 143.758, mean reward: 2.662 [1.496, 4.589], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.742, 10.181], loss: 559.740356, mae: 2.252319, mean_q: 5.341535
 78356/100000: episode: 1515, duration: 0.297s, episode steps: 54, steps per second: 182, episode reward: 109.127, mean reward: 2.021 [1.475, 4.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.865 [-0.359, 10.197], loss: 281.388245, mae: 1.901664, mean_q: 5.737127
 78370/100000: episode: 1516, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 46.109, mean reward: 3.293 [2.343, 3.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.428, 10.398], loss: 1.925519, mae: 0.683455, mean_q: 4.591893
 78375/100000: episode: 1517, duration: 0.036s, episode steps: 5, steps per second: 139, episode reward: 11.639, mean reward: 2.328 [2.165, 2.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.350], loss: 0.262283, mae: 0.514348, mean_q: 4.725358
 78389/100000: episode: 1518, duration: 0.067s, episode steps: 14, steps per second: 208, episode reward: 38.808, mean reward: 2.772 [2.236, 3.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.228, 10.475], loss: 0.265710, mae: 0.516636, mean_q: 4.932611
 78408/100000: episode: 1519, duration: 0.099s, episode steps: 19, steps per second: 191, episode reward: 64.195, mean reward: 3.379 [1.541, 9.085], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-1.024, 10.450], loss: 0.476071, mae: 0.520253, mean_q: 4.724682
 78423/100000: episode: 1520, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 41.944, mean reward: 2.796 [2.412, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.407, 10.371], loss: 0.344900, mae: 0.556797, mean_q: 4.730200
 78439/100000: episode: 1521, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 38.684, mean reward: 2.418 [2.085, 2.942], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.371, 10.100], loss: 0.279416, mae: 0.489375, mean_q: 4.736973
 78490/100000: episode: 1522, duration: 0.289s, episode steps: 51, steps per second: 176, episode reward: 120.487, mean reward: 2.362 [1.581, 4.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.338, 10.224], loss: 0.333814, mae: 0.485079, mean_q: 4.695702
 78509/100000: episode: 1523, duration: 0.101s, episode steps: 19, steps per second: 189, episode reward: 48.439, mean reward: 2.549 [2.194, 3.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.480], loss: 0.291777, mae: 0.477380, mean_q: 4.634291
 78524/100000: episode: 1524, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 34.846, mean reward: 2.323 [2.028, 2.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.343], loss: 0.601496, mae: 0.527389, mean_q: 4.534943
 78540/100000: episode: 1525, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 49.298, mean reward: 3.081 [2.634, 3.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.240, 10.100], loss: 0.229101, mae: 0.485025, mean_q: 4.792399
 78559/100000: episode: 1526, duration: 0.100s, episode steps: 19, steps per second: 189, episode reward: 65.744, mean reward: 3.460 [2.265, 5.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.086, 10.548], loss: 0.204691, mae: 0.451921, mean_q: 4.597579
 78574/100000: episode: 1527, duration: 0.073s, episode steps: 15, steps per second: 207, episode reward: 35.616, mean reward: 2.374 [1.920, 3.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.335], loss: 2.758917, mae: 0.620149, mean_q: 4.751338
 78579/100000: episode: 1528, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: 14.296, mean reward: 2.859 [2.143, 3.808], mean action: 0.000 [0.000, 0.000], mean observation: 2.312 [-0.180, 10.449], loss: 0.199991, mae: 0.436192, mean_q: 4.443722
 78584/100000: episode: 1529, duration: 0.028s, episode steps: 5, steps per second: 176, episode reward: 13.566, mean reward: 2.713 [2.417, 2.991], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.429], loss: 4.035436, mae: 0.731544, mean_q: 4.875513
 78638/100000: episode: 1530, duration: 0.296s, episode steps: 54, steps per second: 182, episode reward: 129.799, mean reward: 2.404 [1.659, 3.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.870 [-0.347, 10.303], loss: 0.640321, mae: 0.506289, mean_q: 4.638147
 78689/100000: episode: 1531, duration: 0.270s, episode steps: 51, steps per second: 189, episode reward: 167.980, mean reward: 3.294 [1.829, 25.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-1.004, 10.259], loss: 298.837921, mae: 1.812041, mean_q: 5.333716
 78708/100000: episode: 1532, duration: 0.090s, episode steps: 19, steps per second: 211, episode reward: 73.227, mean reward: 3.854 [2.561, 5.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.552], loss: 0.517411, mae: 0.536288, mean_q: 4.585074
 78762/100000: episode: 1533, duration: 0.283s, episode steps: 54, steps per second: 191, episode reward: 124.905, mean reward: 2.313 [1.713, 4.322], mean action: 0.000 [0.000, 0.000], mean observation: 1.869 [-0.964, 10.392], loss: 0.414246, mae: 0.511847, mean_q: 4.640340
 78787/100000: episode: 1534, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 74.717, mean reward: 2.989 [2.142, 4.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.035, 10.343], loss: 0.966084, mae: 0.519123, mean_q: 4.627052
[Info] Complete ISplit Iteration
[Info] Levels: [5.3774343, 17.28005]
[Info] Cond. Prob: [0.1, 0.01]
[Info] Error Prob: 0.001

 78806/100000: episode: 1535, duration: 4.175s, episode steps: 19, steps per second: 5, episode reward: 48.132, mean reward: 2.533 [1.984, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.425], loss: 0.356128, mae: 0.531621, mean_q: 4.711435
 78906/100000: episode: 1536, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 189.225, mean reward: 1.892 [1.457, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.105, 10.200], loss: 0.393161, mae: 0.489721, mean_q: 4.608334
 79006/100000: episode: 1537, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 208.965, mean reward: 2.090 [1.514, 3.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.043, 10.247], loss: 0.711124, mae: 0.522176, mean_q: 4.668063
 79106/100000: episode: 1538, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 182.430, mean reward: 1.824 [1.465, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.372, 10.098], loss: 0.520631, mae: 0.481689, mean_q: 4.586468
 79206/100000: episode: 1539, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 209.726, mean reward: 2.097 [1.459, 3.895], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.531, 10.098], loss: 0.310333, mae: 0.481027, mean_q: 4.588056
 79306/100000: episode: 1540, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 186.923, mean reward: 1.869 [1.436, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.075, 10.098], loss: 304.607391, mae: 1.711082, mean_q: 5.303920
 79406/100000: episode: 1541, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 185.236, mean reward: 1.852 [1.464, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.610, 10.394], loss: 0.526524, mae: 0.528444, mean_q: 4.676336
 79506/100000: episode: 1542, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 189.624, mean reward: 1.896 [1.454, 4.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.553, 10.098], loss: 152.107880, mae: 1.184713, mean_q: 4.992916
 79606/100000: episode: 1543, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 188.793, mean reward: 1.888 [1.467, 3.974], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.920, 10.098], loss: 152.477142, mae: 1.210442, mean_q: 5.078542
 79706/100000: episode: 1544, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 202.162, mean reward: 2.022 [1.468, 5.637], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.031, 10.098], loss: 0.560670, mae: 0.531003, mean_q: 4.634598
 79806/100000: episode: 1545, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 193.109, mean reward: 1.931 [1.453, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.987, 10.126], loss: 301.974854, mae: 1.675325, mean_q: 5.345881
 79906/100000: episode: 1546, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 200.852, mean reward: 2.009 [1.466, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.798, 10.109], loss: 0.445302, mae: 0.518837, mean_q: 4.713799
 80006/100000: episode: 1547, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 198.089, mean reward: 1.981 [1.457, 4.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.763, 10.098], loss: 0.410013, mae: 0.491236, mean_q: 4.665726
 80106/100000: episode: 1548, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 191.915, mean reward: 1.919 [1.463, 3.799], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.503, 10.098], loss: 152.300873, mae: 1.189008, mean_q: 4.958869
 80206/100000: episode: 1549, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 182.705, mean reward: 1.827 [1.457, 2.992], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.545, 10.297], loss: 0.349716, mae: 0.505461, mean_q: 4.670972
 80306/100000: episode: 1550, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 194.765, mean reward: 1.948 [1.440, 3.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.985, 10.098], loss: 152.357300, mae: 1.176810, mean_q: 5.023052
 80406/100000: episode: 1551, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 188.407, mean reward: 1.884 [1.446, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.292, 10.169], loss: 152.356750, mae: 1.197353, mean_q: 4.940987
 80506/100000: episode: 1552, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 243.395, mean reward: 2.434 [1.496, 5.087], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.049, 10.312], loss: 598.586853, mae: 2.755864, mean_q: 6.092205
 80606/100000: episode: 1553, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 221.777, mean reward: 2.218 [1.455, 4.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.978, 10.258], loss: 297.288940, mae: 1.752948, mean_q: 5.577149
 80706/100000: episode: 1554, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 204.076, mean reward: 2.041 [1.457, 5.302], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.665, 10.254], loss: 0.907216, mae: 0.670431, mean_q: 4.911175
 80806/100000: episode: 1555, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 189.755, mean reward: 1.898 [1.461, 3.307], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.530, 10.220], loss: 300.604492, mae: 1.706483, mean_q: 5.441240
 80906/100000: episode: 1556, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 182.240, mean reward: 1.822 [1.465, 3.682], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.068, 10.213], loss: 0.783541, mae: 0.629652, mean_q: 4.752237
 81006/100000: episode: 1557, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 195.643, mean reward: 1.956 [1.453, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.284, 10.135], loss: 0.733070, mae: 0.558704, mean_q: 4.713740
 81106/100000: episode: 1558, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 187.362, mean reward: 1.874 [1.472, 5.802], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.135, 10.142], loss: 0.431369, mae: 0.523198, mean_q: 4.643319
 81206/100000: episode: 1559, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 180.730, mean reward: 1.807 [1.445, 2.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.722, 10.098], loss: 0.566545, mae: 0.510785, mean_q: 4.625103
 81306/100000: episode: 1560, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 194.185, mean reward: 1.942 [1.468, 2.957], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.947, 10.285], loss: 151.352325, mae: 1.140298, mean_q: 5.006566
 81406/100000: episode: 1561, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 198.226, mean reward: 1.982 [1.487, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.761, 10.225], loss: 151.034576, mae: 1.131170, mean_q: 4.972449
 81506/100000: episode: 1562, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 186.771, mean reward: 1.868 [1.449, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.179, 10.098], loss: 150.308334, mae: 1.140525, mean_q: 4.975627
 81606/100000: episode: 1563, duration: 0.533s, episode steps: 100, steps per second: 187, episode reward: 185.964, mean reward: 1.860 [1.461, 3.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.152, 10.098], loss: 296.754700, mae: 1.865499, mean_q: 5.303088
 81706/100000: episode: 1564, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 200.313, mean reward: 2.003 [1.448, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.200, 10.276], loss: 0.656478, mae: 0.551094, mean_q: 4.600749
 81806/100000: episode: 1565, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 198.878, mean reward: 1.989 [1.489, 3.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.876, 10.098], loss: 150.523712, mae: 1.110018, mean_q: 4.928567
 81906/100000: episode: 1566, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 191.180, mean reward: 1.912 [1.448, 2.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.545, 10.198], loss: 297.993134, mae: 1.683990, mean_q: 5.256517
 82006/100000: episode: 1567, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 206.023, mean reward: 2.060 [1.497, 3.149], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.394, 10.319], loss: 0.665477, mae: 0.571927, mean_q: 4.443980
 82106/100000: episode: 1568, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 178.740, mean reward: 1.787 [1.456, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.741, 10.098], loss: 295.184845, mae: 1.754884, mean_q: 5.211377
 82206/100000: episode: 1569, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 188.767, mean reward: 1.888 [1.535, 2.998], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.926, 10.098], loss: 298.098053, mae: 1.718984, mean_q: 5.164658
 82306/100000: episode: 1570, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 195.912, mean reward: 1.959 [1.482, 4.713], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.665, 10.240], loss: 149.444702, mae: 1.223709, mean_q: 4.764678
 82406/100000: episode: 1571, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 185.481, mean reward: 1.855 [1.458, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.003, 10.098], loss: 0.867341, mae: 0.556670, mean_q: 4.424234
 82506/100000: episode: 1572, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 195.835, mean reward: 1.958 [1.462, 4.306], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.822, 10.098], loss: 0.414122, mae: 0.483621, mean_q: 4.378313
 82606/100000: episode: 1573, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 202.903, mean reward: 2.029 [1.452, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.730, 10.098], loss: 0.339884, mae: 0.455733, mean_q: 4.325913
 82706/100000: episode: 1574, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 185.857, mean reward: 1.859 [1.462, 3.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.533, 10.098], loss: 0.333929, mae: 0.428450, mean_q: 4.210766
 82806/100000: episode: 1575, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 200.056, mean reward: 2.001 [1.470, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.502, 10.129], loss: 0.319574, mae: 0.433065, mean_q: 4.225447
 82906/100000: episode: 1576, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 198.061, mean reward: 1.981 [1.448, 4.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.308, 10.307], loss: 0.414341, mae: 0.420304, mean_q: 4.126809
 83006/100000: episode: 1577, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 197.069, mean reward: 1.971 [1.451, 3.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.972, 10.098], loss: 0.329164, mae: 0.410670, mean_q: 4.118743
 83106/100000: episode: 1578, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 177.656, mean reward: 1.777 [1.450, 2.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.753, 10.200], loss: 0.134062, mae: 0.367553, mean_q: 4.030820
 83206/100000: episode: 1579, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 212.277, mean reward: 2.123 [1.447, 3.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.307, 10.284], loss: 0.114151, mae: 0.344576, mean_q: 3.998800
 83306/100000: episode: 1580, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 221.783, mean reward: 2.218 [1.439, 5.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.136, 10.465], loss: 0.206695, mae: 0.365507, mean_q: 4.000058
 83406/100000: episode: 1581, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 202.758, mean reward: 2.028 [1.494, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.799, 10.098], loss: 0.196543, mae: 0.354475, mean_q: 3.973029
 83506/100000: episode: 1582, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 198.473, mean reward: 1.985 [1.482, 4.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.024, 10.165], loss: 0.212341, mae: 0.361718, mean_q: 3.992321
 83606/100000: episode: 1583, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 238.056, mean reward: 2.381 [1.474, 5.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.325, 10.098], loss: 0.109052, mae: 0.342896, mean_q: 3.964380
 83706/100000: episode: 1584, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 195.672, mean reward: 1.957 [1.462, 2.978], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.587, 10.109], loss: 0.101554, mae: 0.320943, mean_q: 3.944216
 83806/100000: episode: 1585, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 181.173, mean reward: 1.812 [1.474, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.409, 10.098], loss: 0.096723, mae: 0.315813, mean_q: 3.916383
 83906/100000: episode: 1586, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 179.126, mean reward: 1.791 [1.446, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.530, 10.098], loss: 0.100261, mae: 0.317693, mean_q: 3.898157
 84006/100000: episode: 1587, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 200.790, mean reward: 2.008 [1.466, 3.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.336, 10.098], loss: 0.100949, mae: 0.314173, mean_q: 3.886836
 84106/100000: episode: 1588, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 184.874, mean reward: 1.849 [1.487, 3.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.480, 10.252], loss: 0.095061, mae: 0.311919, mean_q: 3.890132
 84206/100000: episode: 1589, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 202.055, mean reward: 2.021 [1.479, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.166, 10.302], loss: 0.094579, mae: 0.314768, mean_q: 3.866868
 84306/100000: episode: 1590, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 195.887, mean reward: 1.959 [1.465, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.742, 10.349], loss: 0.107715, mae: 0.320673, mean_q: 3.870391
 84406/100000: episode: 1591, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 206.959, mean reward: 2.070 [1.453, 5.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.545, 10.098], loss: 0.109354, mae: 0.323962, mean_q: 3.908988
 84506/100000: episode: 1592, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 190.305, mean reward: 1.903 [1.464, 4.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.629, 10.151], loss: 0.092025, mae: 0.305292, mean_q: 3.906749
 84606/100000: episode: 1593, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 184.249, mean reward: 1.842 [1.467, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.588, 10.176], loss: 0.097661, mae: 0.312868, mean_q: 3.926305
 84706/100000: episode: 1594, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 188.242, mean reward: 1.882 [1.437, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.547, 10.116], loss: 0.102201, mae: 0.317598, mean_q: 3.915478
 84806/100000: episode: 1595, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 188.865, mean reward: 1.889 [1.458, 3.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.715, 10.176], loss: 0.092520, mae: 0.302325, mean_q: 3.886380
 84906/100000: episode: 1596, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 182.346, mean reward: 1.823 [1.468, 2.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.469, 10.361], loss: 0.093358, mae: 0.301304, mean_q: 3.872422
 85006/100000: episode: 1597, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 194.238, mean reward: 1.942 [1.449, 3.065], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.773, 10.098], loss: 0.083922, mae: 0.298449, mean_q: 3.865969
 85106/100000: episode: 1598, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.295, mean reward: 1.903 [1.466, 3.136], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.813, 10.117], loss: 0.094959, mae: 0.301489, mean_q: 3.851963
 85206/100000: episode: 1599, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 185.415, mean reward: 1.854 [1.448, 3.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.609, 10.183], loss: 0.085038, mae: 0.292964, mean_q: 3.873171
 85306/100000: episode: 1600, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 187.889, mean reward: 1.879 [1.451, 2.896], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.416, 10.305], loss: 0.092144, mae: 0.302934, mean_q: 3.878865
 85406/100000: episode: 1601, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 195.383, mean reward: 1.954 [1.472, 3.591], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.384, 10.098], loss: 0.084251, mae: 0.294747, mean_q: 3.865678
 85506/100000: episode: 1602, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 245.110, mean reward: 2.451 [1.444, 9.739], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.457, 10.098], loss: 0.099216, mae: 0.302147, mean_q: 3.857997
 85606/100000: episode: 1603, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 183.785, mean reward: 1.838 [1.455, 3.321], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.167, 10.350], loss: 0.087289, mae: 0.297366, mean_q: 3.854016
 85706/100000: episode: 1604, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 227.731, mean reward: 2.277 [1.461, 4.595], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.689, 10.415], loss: 0.078993, mae: 0.283100, mean_q: 3.848209
 85806/100000: episode: 1605, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 200.350, mean reward: 2.003 [1.494, 6.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.659, 10.098], loss: 0.087493, mae: 0.288717, mean_q: 3.857278
 85906/100000: episode: 1606, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 190.793, mean reward: 1.908 [1.445, 3.192], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.550, 10.253], loss: 0.089486, mae: 0.293039, mean_q: 3.856635
 86006/100000: episode: 1607, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 186.687, mean reward: 1.867 [1.444, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.133, 10.301], loss: 0.088131, mae: 0.289980, mean_q: 3.868632
 86106/100000: episode: 1608, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 195.975, mean reward: 1.960 [1.476, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.680, 10.334], loss: 0.122004, mae: 0.319143, mean_q: 3.899930
 86206/100000: episode: 1609, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 186.416, mean reward: 1.864 [1.465, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.012, 10.233], loss: 0.091096, mae: 0.289361, mean_q: 3.868175
 86306/100000: episode: 1610, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 185.261, mean reward: 1.853 [1.436, 4.223], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.334, 10.136], loss: 0.082756, mae: 0.282899, mean_q: 3.865205
 86406/100000: episode: 1611, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 183.997, mean reward: 1.840 [1.463, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.079, 10.152], loss: 0.095170, mae: 0.299302, mean_q: 3.863157
 86506/100000: episode: 1612, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 193.847, mean reward: 1.938 [1.458, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.876, 10.098], loss: 0.088429, mae: 0.286293, mean_q: 3.851850
 86606/100000: episode: 1613, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 217.536, mean reward: 2.175 [1.459, 4.154], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.987, 10.455], loss: 0.089949, mae: 0.293601, mean_q: 3.883639
 86706/100000: episode: 1614, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 207.177, mean reward: 2.072 [1.501, 2.833], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.252, 10.194], loss: 0.090208, mae: 0.291361, mean_q: 3.879693
 86806/100000: episode: 1615, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 203.341, mean reward: 2.033 [1.471, 5.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.161, 10.155], loss: 0.102922, mae: 0.298634, mean_q: 3.872000
 86906/100000: episode: 1616, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 188.287, mean reward: 1.883 [1.435, 3.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.409, 10.098], loss: 0.107181, mae: 0.304126, mean_q: 3.893184
 87006/100000: episode: 1617, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 178.503, mean reward: 1.785 [1.446, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.881, 10.141], loss: 0.106219, mae: 0.294150, mean_q: 3.877063
 87106/100000: episode: 1618, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 201.148, mean reward: 2.011 [1.483, 3.908], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.146, 10.231], loss: 0.093602, mae: 0.295563, mean_q: 3.880994
 87206/100000: episode: 1619, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 204.766, mean reward: 2.048 [1.464, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.855, 10.098], loss: 0.088008, mae: 0.297909, mean_q: 3.892293
 87306/100000: episode: 1620, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 199.504, mean reward: 1.995 [1.478, 3.016], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.864, 10.271], loss: 0.103297, mae: 0.306193, mean_q: 3.892467
 87406/100000: episode: 1621, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 195.579, mean reward: 1.956 [1.482, 4.419], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.105, 10.098], loss: 0.090491, mae: 0.294371, mean_q: 3.896605
 87506/100000: episode: 1622, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 196.830, mean reward: 1.968 [1.438, 3.832], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.415, 10.219], loss: 0.087615, mae: 0.283220, mean_q: 3.873827
 87606/100000: episode: 1623, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 191.553, mean reward: 1.916 [1.463, 4.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.458, 10.098], loss: 0.100917, mae: 0.292722, mean_q: 3.889715
 87706/100000: episode: 1624, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 186.761, mean reward: 1.868 [1.438, 2.921], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.408, 10.099], loss: 0.105009, mae: 0.299935, mean_q: 3.889042
 87806/100000: episode: 1625, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 201.677, mean reward: 2.017 [1.458, 4.171], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.367, 10.415], loss: 0.093584, mae: 0.301788, mean_q: 3.870331
 87906/100000: episode: 1626, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 173.365, mean reward: 1.734 [1.440, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.310, 10.119], loss: 0.093291, mae: 0.296957, mean_q: 3.893746
 88006/100000: episode: 1627, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 187.478, mean reward: 1.875 [1.457, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.452, 10.098], loss: 0.094277, mae: 0.294346, mean_q: 3.889935
 88106/100000: episode: 1628, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 181.117, mean reward: 1.811 [1.446, 3.283], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.946, 10.098], loss: 0.099570, mae: 0.303002, mean_q: 3.896945
 88206/100000: episode: 1629, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 184.927, mean reward: 1.849 [1.458, 2.696], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.668, 10.098], loss: 0.102458, mae: 0.297704, mean_q: 3.892311
 88306/100000: episode: 1630, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 180.360, mean reward: 1.804 [1.482, 3.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.663, 10.232], loss: 0.083324, mae: 0.288756, mean_q: 3.861888
 88406/100000: episode: 1631, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 197.219, mean reward: 1.972 [1.508, 3.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.859, 10.185], loss: 0.100260, mae: 0.294288, mean_q: 3.867407
 88506/100000: episode: 1632, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 197.266, mean reward: 1.973 [1.440, 3.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.981, 10.176], loss: 0.091430, mae: 0.292261, mean_q: 3.853210
 88606/100000: episode: 1633, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 189.655, mean reward: 1.897 [1.462, 3.620], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.822, 10.098], loss: 0.097216, mae: 0.285074, mean_q: 3.853850
 88706/100000: episode: 1634, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 196.975, mean reward: 1.970 [1.472, 3.186], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.171, 10.194], loss: 0.071066, mae: 0.269113, mean_q: 3.811968
[Info] 1-TH LEVEL FOUND: 5.093883037567139, Considering 10/90 traces
 88806/100000: episode: 1635, duration: 4.580s, episode steps: 100, steps per second: 22, episode reward: 195.249, mean reward: 1.952 [1.506, 6.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.448, 10.098], loss: 0.102853, mae: 0.293515, mean_q: 3.835062
 88856/100000: episode: 1636, duration: 0.274s, episode steps: 50, steps per second: 182, episode reward: 103.584, mean reward: 2.072 [1.515, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.619, 10.247], loss: 0.080606, mae: 0.281064, mean_q: 3.856755
 88879/100000: episode: 1637, duration: 0.137s, episode steps: 23, steps per second: 168, episode reward: 65.583, mean reward: 2.851 [1.780, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.702, 10.309], loss: 0.064863, mae: 0.263768, mean_q: 3.836692
 88903/100000: episode: 1638, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 57.930, mean reward: 2.414 [1.891, 3.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.481, 10.100], loss: 0.158501, mae: 0.310913, mean_q: 3.866426
 88931/100000: episode: 1639, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 92.736, mean reward: 3.312 [2.366, 6.610], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.806, 10.458], loss: 0.087307, mae: 0.294732, mean_q: 3.862095
 88952/100000: episode: 1640, duration: 0.122s, episode steps: 21, steps per second: 173, episode reward: 75.322, mean reward: 3.587 [2.393, 5.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.924, 10.566], loss: 0.116546, mae: 0.315178, mean_q: 3.842524
 89002/100000: episode: 1641, duration: 0.245s, episode steps: 50, steps per second: 204, episode reward: 99.234, mean reward: 1.985 [1.544, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.341, 10.100], loss: 0.082341, mae: 0.283917, mean_q: 3.862744
 89026/100000: episode: 1642, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 58.344, mean reward: 2.431 [1.969, 3.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.136, 10.100], loss: 0.113146, mae: 0.320193, mean_q: 3.912738
 89044/100000: episode: 1643, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 34.605, mean reward: 1.923 [1.613, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.169, 10.100], loss: 0.083407, mae: 0.289918, mean_q: 3.886586
 89132/100000: episode: 1644, duration: 0.468s, episode steps: 88, steps per second: 188, episode reward: 279.018, mean reward: 3.171 [1.493, 11.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.576 [-0.918, 10.633], loss: 0.116580, mae: 0.314441, mean_q: 3.883488
 89155/100000: episode: 1645, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 62.661, mean reward: 2.724 [2.148, 3.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.510, 10.351], loss: 0.160190, mae: 0.335652, mean_q: 3.943841
 89189/100000: episode: 1646, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 73.166, mean reward: 2.152 [1.532, 2.742], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.301, 10.100], loss: 0.122848, mae: 0.305677, mean_q: 3.915637
 89213/100000: episode: 1647, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 60.104, mean reward: 2.504 [2.027, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.216, 10.100], loss: 0.140060, mae: 0.318461, mean_q: 3.924255
 89247/100000: episode: 1648, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 69.999, mean reward: 2.059 [1.565, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-1.594, 10.100], loss: 0.133880, mae: 0.306065, mean_q: 3.956163
 89281/100000: episode: 1649, duration: 0.177s, episode steps: 34, steps per second: 192, episode reward: 121.113, mean reward: 3.562 [2.554, 6.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.278, 10.100], loss: 0.155190, mae: 0.319331, mean_q: 3.901229
 89315/100000: episode: 1650, duration: 0.174s, episode steps: 34, steps per second: 195, episode reward: 83.260, mean reward: 2.449 [1.826, 4.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.807, 10.100], loss: 0.176680, mae: 0.335751, mean_q: 3.965930
 89365/100000: episode: 1651, duration: 0.282s, episode steps: 50, steps per second: 177, episode reward: 117.273, mean reward: 2.345 [1.813, 4.165], mean action: 0.000 [0.000, 0.000], mean observation: 1.906 [-0.334, 10.385], loss: 0.133651, mae: 0.327098, mean_q: 3.956336
 89453/100000: episode: 1652, duration: 0.483s, episode steps: 88, steps per second: 182, episode reward: 173.185, mean reward: 1.968 [1.491, 5.166], mean action: 0.000 [0.000, 0.000], mean observation: 1.572 [-1.431, 10.327], loss: 0.117736, mae: 0.312029, mean_q: 3.945123
 89474/100000: episode: 1653, duration: 0.107s, episode steps: 21, steps per second: 197, episode reward: 50.754, mean reward: 2.417 [1.860, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.284], loss: 0.129357, mae: 0.333993, mean_q: 3.975123
 89495/100000: episode: 1654, duration: 0.104s, episode steps: 21, steps per second: 202, episode reward: 61.913, mean reward: 2.948 [1.954, 5.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.336], loss: 0.132874, mae: 0.346783, mean_q: 3.978966
 89516/100000: episode: 1655, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 69.915, mean reward: 3.329 [2.686, 4.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.220, 10.411], loss: 0.094749, mae: 0.296540, mean_q: 4.042692
 89539/100000: episode: 1656, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 64.852, mean reward: 2.820 [2.286, 3.547], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.412], loss: 0.087811, mae: 0.298980, mean_q: 4.005515
 89578/100000: episode: 1657, duration: 0.209s, episode steps: 39, steps per second: 187, episode reward: 89.060, mean reward: 2.284 [1.766, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-1.120, 10.100], loss: 0.117661, mae: 0.317620, mean_q: 3.991041
 89612/100000: episode: 1658, duration: 0.200s, episode steps: 34, steps per second: 170, episode reward: 87.054, mean reward: 2.560 [1.964, 3.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.269, 10.100], loss: 0.150807, mae: 0.348944, mean_q: 4.071145
 89640/100000: episode: 1659, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 85.645, mean reward: 3.059 [2.122, 4.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.244, 10.409], loss: 0.113582, mae: 0.317057, mean_q: 4.050244
 89728/100000: episode: 1660, duration: 0.458s, episode steps: 88, steps per second: 192, episode reward: 180.274, mean reward: 2.049 [1.486, 5.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.570 [-0.914, 10.348], loss: 0.145544, mae: 0.331191, mean_q: 4.050272
 89756/100000: episode: 1661, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 106.095, mean reward: 3.789 [2.276, 7.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.203, 10.467], loss: 0.165673, mae: 0.349510, mean_q: 4.095648
 89780/100000: episode: 1662, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 61.813, mean reward: 2.576 [2.079, 4.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.424, 10.100], loss: 0.165447, mae: 0.344085, mean_q: 4.012894
 89814/100000: episode: 1663, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 72.791, mean reward: 2.141 [1.474, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.453, 10.145], loss: 0.150076, mae: 0.347711, mean_q: 4.029195
 89842/100000: episode: 1664, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 73.556, mean reward: 2.627 [2.034, 3.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-1.216, 10.311], loss: 0.151883, mae: 0.350322, mean_q: 4.066623
 89866/100000: episode: 1665, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 58.360, mean reward: 2.432 [1.925, 5.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.349, 10.100], loss: 0.099163, mae: 0.314719, mean_q: 4.072653
 89890/100000: episode: 1666, duration: 0.117s, episode steps: 24, steps per second: 206, episode reward: 54.472, mean reward: 2.270 [1.876, 2.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.686, 10.100], loss: 0.160108, mae: 0.357267, mean_q: 4.127805
 89978/100000: episode: 1667, duration: 0.466s, episode steps: 88, steps per second: 189, episode reward: 167.097, mean reward: 1.899 [1.457, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.574 [-1.221, 10.162], loss: 0.123192, mae: 0.330042, mean_q: 4.060079
 90028/100000: episode: 1668, duration: 0.259s, episode steps: 50, steps per second: 193, episode reward: 125.083, mean reward: 2.502 [1.767, 3.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.336, 10.266], loss: 0.138152, mae: 0.336315, mean_q: 4.043737
 90078/100000: episode: 1669, duration: 0.259s, episode steps: 50, steps per second: 193, episode reward: 123.600, mean reward: 2.472 [1.940, 3.375], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.345, 10.481], loss: 0.128505, mae: 0.338617, mean_q: 4.098615
 90117/100000: episode: 1670, duration: 0.215s, episode steps: 39, steps per second: 182, episode reward: 130.783, mean reward: 3.353 [1.999, 4.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.956 [-0.986, 10.100], loss: 0.115457, mae: 0.327171, mean_q: 4.123247
 90157/100000: episode: 1671, duration: 0.222s, episode steps: 40, steps per second: 180, episode reward: 133.122, mean reward: 3.328 [2.363, 5.847], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.307, 10.100], loss: 0.144113, mae: 0.339971, mean_q: 4.125836
 90197/100000: episode: 1672, duration: 0.210s, episode steps: 40, steps per second: 190, episode reward: 109.768, mean reward: 2.744 [1.503, 5.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.999 [-0.135, 10.116], loss: 0.186783, mae: 0.375181, mean_q: 4.168927
 90237/100000: episode: 1673, duration: 0.207s, episode steps: 40, steps per second: 193, episode reward: 87.446, mean reward: 2.186 [1.536, 2.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.000 [-0.103, 10.100], loss: 0.172881, mae: 0.378067, mean_q: 4.222486
 90287/100000: episode: 1674, duration: 0.300s, episode steps: 50, steps per second: 167, episode reward: 144.886, mean reward: 2.898 [1.909, 5.231], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.538, 10.346], loss: 0.173850, mae: 0.384094, mean_q: 4.207850
 90321/100000: episode: 1675, duration: 0.180s, episode steps: 34, steps per second: 189, episode reward: 90.430, mean reward: 2.660 [1.990, 3.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.775, 10.100], loss: 0.133831, mae: 0.355985, mean_q: 4.211776
 90342/100000: episode: 1676, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 63.435, mean reward: 3.021 [1.965, 4.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.342], loss: 0.112682, mae: 0.327217, mean_q: 4.140478
 90392/100000: episode: 1677, duration: 0.263s, episode steps: 50, steps per second: 190, episode reward: 109.021, mean reward: 2.180 [1.616, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-0.703, 10.401], loss: 0.185605, mae: 0.382530, mean_q: 4.225585
 90432/100000: episode: 1678, duration: 0.207s, episode steps: 40, steps per second: 193, episode reward: 344.488, mean reward: 8.612 [2.788, 33.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.950 [-0.302, 10.100], loss: 0.131446, mae: 0.340095, mean_q: 4.207468
 90482/100000: episode: 1679, duration: 0.266s, episode steps: 50, steps per second: 188, episode reward: 116.336, mean reward: 2.327 [1.536, 3.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.635, 10.153], loss: 0.523486, mae: 0.439657, mean_q: 4.218515
 90506/100000: episode: 1680, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 50.547, mean reward: 2.106 [1.603, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.360, 10.100], loss: 0.190388, mae: 0.395644, mean_q: 4.250072
 90534/100000: episode: 1681, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 129.425, mean reward: 4.622 [3.497, 6.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.478], loss: 0.187994, mae: 0.387556, mean_q: 4.269788
 90552/100000: episode: 1682, duration: 0.103s, episode steps: 18, steps per second: 174, episode reward: 41.341, mean reward: 2.297 [1.785, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.214, 10.100], loss: 0.652765, mae: 0.481627, mean_q: 4.351661
 90573/100000: episode: 1683, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 52.251, mean reward: 2.488 [1.910, 2.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.262, 10.255], loss: 0.223673, mae: 0.431594, mean_q: 4.265611
 90601/100000: episode: 1684, duration: 0.155s, episode steps: 28, steps per second: 180, episode reward: 80.294, mean reward: 2.868 [1.826, 4.354], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-1.155, 10.287], loss: 0.668090, mae: 0.483517, mean_q: 4.274949
 90622/100000: episode: 1685, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 69.448, mean reward: 3.307 [2.163, 4.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.416], loss: 0.346494, mae: 0.447877, mean_q: 4.384833
 90640/100000: episode: 1686, duration: 0.101s, episode steps: 18, steps per second: 179, episode reward: 36.026, mean reward: 2.001 [1.659, 2.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.165 [-0.121, 10.100], loss: 0.179524, mae: 0.371433, mean_q: 4.215652
 90668/100000: episode: 1687, duration: 0.160s, episode steps: 28, steps per second: 175, episode reward: 70.551, mean reward: 2.520 [1.970, 3.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.035, 10.238], loss: 0.217518, mae: 0.393206, mean_q: 4.264414
 90718/100000: episode: 1688, duration: 0.261s, episode steps: 50, steps per second: 192, episode reward: 123.466, mean reward: 2.469 [1.976, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.900 [-0.794, 10.305], loss: 0.805206, mae: 0.522867, mean_q: 4.380515
 90806/100000: episode: 1689, duration: 0.464s, episode steps: 88, steps per second: 190, episode reward: 173.971, mean reward: 1.977 [1.483, 4.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.565 [-0.758, 10.100], loss: 0.271025, mae: 0.414465, mean_q: 4.287853
 90840/100000: episode: 1690, duration: 0.189s, episode steps: 34, steps per second: 180, episode reward: 79.991, mean reward: 2.353 [1.516, 4.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.057, 10.100], loss: 0.362323, mae: 0.437193, mean_q: 4.345396
 90863/100000: episode: 1691, duration: 0.133s, episode steps: 23, steps per second: 172, episode reward: 74.840, mean reward: 3.254 [2.382, 4.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.102, 10.402], loss: 0.169525, mae: 0.388743, mean_q: 4.353976
 90902/100000: episode: 1692, duration: 0.208s, episode steps: 39, steps per second: 188, episode reward: 121.439, mean reward: 3.114 [2.172, 5.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.578, 10.100], loss: 0.353754, mae: 0.430906, mean_q: 4.381248
 90952/100000: episode: 1693, duration: 0.270s, episode steps: 50, steps per second: 185, episode reward: 133.149, mean reward: 2.663 [1.504, 5.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.879 [-1.326, 10.100], loss: 0.432180, mae: 0.475501, mean_q: 4.399129
 90976/100000: episode: 1694, duration: 0.136s, episode steps: 24, steps per second: 176, episode reward: 67.897, mean reward: 2.829 [2.014, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.433, 10.100], loss: 0.303244, mae: 0.449409, mean_q: 4.417061
 91004/100000: episode: 1695, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 80.545, mean reward: 2.877 [2.273, 4.565], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.491, 10.408], loss: 0.332998, mae: 0.459256, mean_q: 4.409279
 91032/100000: episode: 1696, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 60.595, mean reward: 2.164 [1.621, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.035, 10.208], loss: 0.961874, mae: 0.534151, mean_q: 4.414600
 91056/100000: episode: 1697, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 58.682, mean reward: 2.445 [2.026, 3.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-1.633, 10.100], loss: 0.219645, mae: 0.423585, mean_q: 4.409805
 91144/100000: episode: 1698, duration: 0.464s, episode steps: 88, steps per second: 190, episode reward: 172.149, mean reward: 1.956 [1.477, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.573 [-0.423, 10.267], loss: 0.239051, mae: 0.413243, mean_q: 4.412477
 91165/100000: episode: 1699, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 50.633, mean reward: 2.411 [1.929, 2.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.370, 10.310], loss: 0.291833, mae: 0.462956, mean_q: 4.503385
 91215/100000: episode: 1700, duration: 0.277s, episode steps: 50, steps per second: 180, episode reward: 124.485, mean reward: 2.490 [1.715, 5.404], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-1.467, 10.409], loss: 0.318884, mae: 0.416255, mean_q: 4.391915
 91265/100000: episode: 1701, duration: 0.267s, episode steps: 50, steps per second: 187, episode reward: 187.972, mean reward: 3.759 [2.421, 6.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.739, 10.531], loss: 0.244333, mae: 0.446634, mean_q: 4.444471
 91286/100000: episode: 1702, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 58.570, mean reward: 2.789 [2.082, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.647, 10.489], loss: 0.339555, mae: 0.482306, mean_q: 4.408718
 91309/100000: episode: 1703, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 62.780, mean reward: 2.730 [2.020, 4.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.035, 10.410], loss: 0.268655, mae: 0.437624, mean_q: 4.536483
 91330/100000: episode: 1704, duration: 0.130s, episode steps: 21, steps per second: 161, episode reward: 84.519, mean reward: 4.025 [2.627, 6.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.109, 10.609], loss: 0.224437, mae: 0.445481, mean_q: 4.442850
 91380/100000: episode: 1705, duration: 0.257s, episode steps: 50, steps per second: 195, episode reward: 105.930, mean reward: 2.119 [1.451, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.338, 10.232], loss: 0.240786, mae: 0.433781, mean_q: 4.451299
 91468/100000: episode: 1706, duration: 0.449s, episode steps: 88, steps per second: 196, episode reward: 169.227, mean reward: 1.923 [1.483, 3.511], mean action: 0.000 [0.000, 0.000], mean observation: 1.565 [-1.093, 10.263], loss: 0.561278, mae: 0.463878, mean_q: 4.443313
 91491/100000: episode: 1707, duration: 0.135s, episode steps: 23, steps per second: 171, episode reward: 72.379, mean reward: 3.147 [2.365, 4.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.863, 10.521], loss: 0.269326, mae: 0.470049, mean_q: 4.554019
 91509/100000: episode: 1708, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 37.848, mean reward: 2.103 [1.873, 2.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.610, 10.100], loss: 0.372052, mae: 0.499378, mean_q: 4.440498
 91543/100000: episode: 1709, duration: 0.199s, episode steps: 34, steps per second: 171, episode reward: 70.114, mean reward: 2.062 [1.637, 2.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.032 [-0.380, 10.100], loss: 0.308894, mae: 0.468324, mean_q: 4.529904
 91582/100000: episode: 1710, duration: 0.198s, episode steps: 39, steps per second: 197, episode reward: 111.882, mean reward: 2.869 [1.894, 5.012], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.481, 10.100], loss: 0.272909, mae: 0.439502, mean_q: 4.485870
 91616/100000: episode: 1711, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 84.163, mean reward: 2.475 [1.919, 3.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.225, 10.100], loss: 0.254325, mae: 0.440129, mean_q: 4.478213
 91650/100000: episode: 1712, duration: 0.188s, episode steps: 34, steps per second: 181, episode reward: 90.147, mean reward: 2.651 [2.122, 3.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.673, 10.100], loss: 0.300185, mae: 0.468414, mean_q: 4.493029
 91700/100000: episode: 1713, duration: 0.258s, episode steps: 50, steps per second: 194, episode reward: 105.164, mean reward: 2.103 [1.502, 5.110], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.409, 10.179], loss: 0.306563, mae: 0.431086, mean_q: 4.574379
 91723/100000: episode: 1714, duration: 0.132s, episode steps: 23, steps per second: 174, episode reward: 64.563, mean reward: 2.807 [1.648, 4.861], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.246, 10.269], loss: 0.304811, mae: 0.430657, mean_q: 4.432456
 91773/100000: episode: 1715, duration: 0.251s, episode steps: 50, steps per second: 199, episode reward: 136.447, mean reward: 2.729 [1.501, 6.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.902 [-0.356, 10.126], loss: 0.261526, mae: 0.467217, mean_q: 4.554669
 91813/100000: episode: 1716, duration: 0.213s, episode steps: 40, steps per second: 187, episode reward: 79.953, mean reward: 1.999 [1.604, 3.057], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.915, 10.100], loss: 0.297513, mae: 0.458055, mean_q: 4.536748
 91853/100000: episode: 1717, duration: 0.214s, episode steps: 40, steps per second: 187, episode reward: 141.242, mean reward: 3.531 [2.075, 5.425], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-1.619, 10.100], loss: 0.401867, mae: 0.454246, mean_q: 4.517600
 91941/100000: episode: 1718, duration: 0.457s, episode steps: 88, steps per second: 193, episode reward: 184.513, mean reward: 2.097 [1.448, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.559 [-0.992, 10.100], loss: 0.357913, mae: 0.475411, mean_q: 4.571844
 91965/100000: episode: 1719, duration: 0.139s, episode steps: 24, steps per second: 173, episode reward: 56.515, mean reward: 2.355 [1.762, 3.220], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.727, 10.100], loss: 0.661937, mae: 0.555353, mean_q: 4.700805
 91993/100000: episode: 1720, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 87.364, mean reward: 3.120 [2.410, 4.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.089, 10.443], loss: 0.437796, mae: 0.471248, mean_q: 4.643256
 92016/100000: episode: 1721, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 103.223, mean reward: 4.488 [2.864, 7.730], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.337, 10.648], loss: 0.327293, mae: 0.494033, mean_q: 4.514533
 92037/100000: episode: 1722, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 140.183, mean reward: 6.675 [2.844, 25.108], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.641], loss: 0.490131, mae: 0.525075, mean_q: 4.652671
 92060/100000: episode: 1723, duration: 0.118s, episode steps: 23, steps per second: 195, episode reward: 73.370, mean reward: 3.190 [2.312, 4.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.750, 10.490], loss: 0.204572, mae: 0.454065, mean_q: 4.678244
 92088/100000: episode: 1724, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 84.963, mean reward: 3.034 [1.992, 7.473], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.616, 10.386], loss: 0.393638, mae: 0.491929, mean_q: 4.646300
[Info] 2-TH LEVEL FOUND: 7.876584053039551, Considering 10/90 traces
 92127/100000: episode: 1725, duration: 4.343s, episode steps: 39, steps per second: 9, episode reward: 92.278, mean reward: 2.366 [1.653, 3.455], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.484, 10.100], loss: 0.513576, mae: 0.536472, mean_q: 4.778310
 92150/100000: episode: 1726, duration: 0.109s, episode steps: 23, steps per second: 211, episode reward: 94.859, mean reward: 4.124 [2.721, 10.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-1.597, 10.100], loss: 0.237092, mae: 0.465915, mean_q: 4.651213
 92184/100000: episode: 1727, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 90.022, mean reward: 2.648 [1.677, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.029 [-0.712, 10.100], loss: 0.503322, mae: 0.504702, mean_q: 4.677107
 92218/100000: episode: 1728, duration: 0.175s, episode steps: 34, steps per second: 194, episode reward: 95.343, mean reward: 2.804 [1.825, 4.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-2.064, 10.100], loss: 0.781921, mae: 0.544261, mean_q: 4.707421
 92238/100000: episode: 1729, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 55.863, mean reward: 2.793 [2.178, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.300, 10.100], loss: 0.332552, mae: 0.499183, mean_q: 4.732120
 92272/100000: episode: 1730, duration: 0.191s, episode steps: 34, steps per second: 178, episode reward: 117.481, mean reward: 3.455 [2.131, 5.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.026 [-0.551, 10.100], loss: 0.512003, mae: 0.531365, mean_q: 4.833418
 92292/100000: episode: 1731, duration: 0.096s, episode steps: 20, steps per second: 209, episode reward: 53.888, mean reward: 2.694 [2.252, 3.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.308, 10.100], loss: 0.618754, mae: 0.539185, mean_q: 4.787596
 92308/100000: episode: 1732, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 46.216, mean reward: 2.889 [2.289, 3.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-1.189, 10.100], loss: 0.305933, mae: 0.520619, mean_q: 4.763024
 92342/100000: episode: 1733, duration: 0.185s, episode steps: 34, steps per second: 184, episode reward: 83.663, mean reward: 2.461 [1.731, 3.286], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.354, 10.100], loss: 0.387170, mae: 0.523787, mean_q: 4.839393
 92368/100000: episode: 1734, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 99.261, mean reward: 3.818 [1.786, 7.280], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-1.054, 10.100], loss: 0.274600, mae: 0.474637, mean_q: 4.800698
 92401/100000: episode: 1735, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 79.327, mean reward: 2.404 [1.531, 4.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.117, 10.123], loss: 0.959876, mae: 0.574171, mean_q: 4.866877
 92424/100000: episode: 1736, duration: 0.126s, episode steps: 23, steps per second: 182, episode reward: 55.779, mean reward: 2.425 [1.716, 3.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.269, 10.100], loss: 0.302265, mae: 0.499764, mean_q: 4.769310
 92452/100000: episode: 1737, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 140.005, mean reward: 5.000 [2.795, 12.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.264, 10.100], loss: 0.982552, mae: 0.595286, mean_q: 4.871767
 92461/100000: episode: 1738, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 72.518, mean reward: 8.058 [6.216, 10.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.477, 10.621], loss: 0.344477, mae: 0.527499, mean_q: 4.794445
 92489/100000: episode: 1739, duration: 0.146s, episode steps: 28, steps per second: 191, episode reward: 133.380, mean reward: 4.764 [2.248, 10.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.245, 10.100], loss: 0.743116, mae: 0.578877, mean_q: 4.912211
[Info] FALSIFICATION!
 92517/100000: episode: 1740, duration: 0.421s, episode steps: 28, steps per second: 66, episode reward: 1085.106, mean reward: 38.754 [2.345, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.975 [-0.758, 10.046], loss: 0.427112, mae: 0.557261, mean_q: 4.912827
 92542/100000: episode: 1741, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 127.998, mean reward: 5.120 [3.475, 8.771], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.930, 10.100], loss: 1.052767, mae: 0.629258, mean_q: 4.886330
 92567/100000: episode: 1742, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 75.751, mean reward: 3.030 [2.134, 4.298], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.604, 10.100], loss: 620.475525, mae: 3.707777, mean_q: 7.045550
 92592/100000: episode: 1743, duration: 0.144s, episode steps: 25, steps per second: 174, episode reward: 117.241, mean reward: 4.690 [2.520, 9.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.279, 10.100], loss: 2.698045, mae: 1.660758, mean_q: 3.671454
 92608/100000: episode: 1744, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 44.077, mean reward: 2.755 [2.466, 3.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.365, 10.100], loss: 0.727451, mae: 0.806220, mean_q: 5.497116
 92628/100000: episode: 1745, duration: 0.107s, episode steps: 20, steps per second: 186, episode reward: 67.441, mean reward: 3.372 [2.646, 5.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.277, 10.100], loss: 1.351521, mae: 0.708743, mean_q: 5.045959
 92653/100000: episode: 1746, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 94.242, mean reward: 3.770 [2.672, 6.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.579, 10.100], loss: 0.710440, mae: 0.630750, mean_q: 4.974479
 92662/100000: episode: 1747, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 55.326, mean reward: 6.147 [5.000, 7.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.469], loss: 0.426006, mae: 0.563049, mean_q: 4.960245
 92674/100000: episode: 1748, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 62.728, mean reward: 5.227 [4.095, 6.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.485], loss: 1280.465942, mae: 4.982632, mean_q: 7.233271
 92702/100000: episode: 1749, duration: 0.141s, episode steps: 28, steps per second: 199, episode reward: 139.217, mean reward: 4.972 [3.317, 7.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.402, 10.100], loss: 2.027278, mae: 1.513630, mean_q: 5.015216
 92735/100000: episode: 1750, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 123.975, mean reward: 3.757 [1.870, 7.120], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.397, 10.224], loss: 465.706451, mae: 2.968918, mean_q: 6.569217
 92761/100000: episode: 1751, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 85.764, mean reward: 3.299 [2.200, 5.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.787, 10.100], loss: 1.054998, mae: 1.036922, mean_q: 4.880149
 92773/100000: episode: 1752, duration: 0.079s, episode steps: 12, steps per second: 151, episode reward: 72.186, mean reward: 6.016 [4.259, 9.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.554], loss: 0.530605, mae: 0.670775, mean_q: 5.397270
 92798/100000: episode: 1753, duration: 0.153s, episode steps: 25, steps per second: 164, episode reward: 128.790, mean reward: 5.152 [3.118, 20.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.489, 10.100], loss: 0.673601, mae: 0.703175, mean_q: 5.260678
 92826/100000: episode: 1754, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 75.978, mean reward: 2.714 [1.746, 4.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.649, 10.100], loss: 0.439460, mae: 0.598301, mean_q: 5.304114
 92851/100000: episode: 1755, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 94.530, mean reward: 3.781 [2.773, 6.509], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.353, 10.100], loss: 0.824246, mae: 0.608004, mean_q: 5.145757
 92863/100000: episode: 1756, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 97.728, mean reward: 8.144 [4.503, 11.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.617], loss: 0.944341, mae: 0.701970, mean_q: 5.410416
 92886/100000: episode: 1757, duration: 0.112s, episode steps: 23, steps per second: 205, episode reward: 57.540, mean reward: 2.502 [1.791, 3.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.274, 10.100], loss: 0.974167, mae: 0.660910, mean_q: 5.200631
 92911/100000: episode: 1758, duration: 0.142s, episode steps: 25, steps per second: 176, episode reward: 93.444, mean reward: 3.738 [2.657, 5.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.395, 10.100], loss: 0.606318, mae: 0.658886, mean_q: 5.396952
 92936/100000: episode: 1759, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 70.439, mean reward: 2.818 [1.830, 6.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.325, 10.100], loss: 0.546804, mae: 0.610336, mean_q: 5.292520
 92962/100000: episode: 1760, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 76.047, mean reward: 2.925 [1.728, 4.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-1.134, 10.100], loss: 0.649791, mae: 0.651138, mean_q: 5.353864
 92974/100000: episode: 1761, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 55.714, mean reward: 4.643 [3.262, 8.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.088, 10.476], loss: 0.467279, mae: 0.601034, mean_q: 5.188420
 92990/100000: episode: 1762, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 41.433, mean reward: 2.590 [2.141, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.189, 10.100], loss: 0.392533, mae: 0.597310, mean_q: 5.315956
 93015/100000: episode: 1763, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 77.749, mean reward: 3.110 [2.469, 4.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.547, 10.100], loss: 0.555873, mae: 0.625475, mean_q: 5.381582
 93027/100000: episode: 1764, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 59.647, mean reward: 4.971 [4.269, 6.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.465], loss: 0.362329, mae: 0.577657, mean_q: 5.286422
 93047/100000: episode: 1765, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 63.874, mean reward: 3.194 [2.277, 5.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.480, 10.100], loss: 0.541089, mae: 0.615790, mean_q: 5.347559
 93067/100000: episode: 1766, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 60.866, mean reward: 3.043 [2.506, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.334, 10.100], loss: 0.481952, mae: 0.629434, mean_q: 5.367747
 93083/100000: episode: 1767, duration: 0.092s, episode steps: 16, steps per second: 175, episode reward: 34.903, mean reward: 2.181 [1.765, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.529, 10.100], loss: 1.020960, mae: 0.678615, mean_q: 5.439740
[Info] FALSIFICATION!
 93100/100000: episode: 1768, duration: 0.246s, episode steps: 17, steps per second: 69, episode reward: 1134.813, mean reward: 66.754 [3.467, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.873 [-0.692, 9.484], loss: 0.694725, mae: 0.622627, mean_q: 5.385445
 93120/100000: episode: 1769, duration: 0.101s, episode steps: 20, steps per second: 197, episode reward: 54.237, mean reward: 2.712 [1.945, 3.367], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.175, 10.100], loss: 0.954640, mae: 0.696193, mean_q: 5.588107
 93129/100000: episode: 1770, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 58.918, mean reward: 6.546 [4.525, 10.101], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.539], loss: 0.704075, mae: 0.648235, mean_q: 5.367954
 93154/100000: episode: 1771, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 84.255, mean reward: 3.370 [2.257, 4.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.453, 10.100], loss: 614.746704, mae: 2.762324, mean_q: 6.503601
 93188/100000: episode: 1772, duration: 0.183s, episode steps: 34, steps per second: 185, episode reward: 122.025, mean reward: 3.589 [2.310, 5.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.007 [-0.445, 10.100], loss: 452.901123, mae: 2.863960, mean_q: 6.854079
 93213/100000: episode: 1773, duration: 0.138s, episode steps: 25, steps per second: 182, episode reward: 71.274, mean reward: 2.851 [2.235, 4.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.939, 10.100], loss: 1.386878, mae: 1.199967, mean_q: 5.151001
 93247/100000: episode: 1774, duration: 0.174s, episode steps: 34, steps per second: 196, episode reward: 90.932, mean reward: 2.674 [1.989, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.733, 10.100], loss: 888.433838, mae: 2.845121, mean_q: 6.096809
 93267/100000: episode: 1775, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 50.423, mean reward: 2.521 [2.079, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.128, 10.100], loss: 11.445118, mae: 3.539960, mean_q: 9.016497
 93279/100000: episode: 1776, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 78.916, mean reward: 6.576 [4.465, 9.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.588], loss: 1.844650, mae: 1.650098, mean_q: 4.441964
 93291/100000: episode: 1777, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 64.557, mean reward: 5.380 [4.106, 8.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.318, 10.598], loss: 1.121085, mae: 1.024922, mean_q: 5.200770
 93317/100000: episode: 1778, duration: 0.136s, episode steps: 26, steps per second: 192, episode reward: 142.591, mean reward: 5.484 [4.055, 10.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-1.319, 10.100], loss: 0.962608, mae: 0.822805, mean_q: 5.961712
 93333/100000: episode: 1779, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 50.935, mean reward: 3.183 [2.415, 5.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.225, 10.100], loss: 946.581421, mae: 3.176869, mean_q: 6.454690
 93361/100000: episode: 1780, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 73.104, mean reward: 2.611 [1.641, 4.038], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.692, 10.100], loss: 542.043518, mae: 2.946143, mean_q: 7.371743
 93389/100000: episode: 1781, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 76.617, mean reward: 2.736 [1.787, 4.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.609, 10.100], loss: 2.091425, mae: 1.244240, mean_q: 6.047679
 93423/100000: episode: 1782, duration: 0.176s, episode steps: 34, steps per second: 193, episode reward: 91.719, mean reward: 2.698 [1.672, 4.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.030 [-1.009, 10.100], loss: 449.225555, mae: 2.398606, mean_q: 6.915124
 93435/100000: episode: 1783, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 134.717, mean reward: 11.226 [7.003, 30.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.702], loss: 1255.517212, mae: 3.875917, mean_q: 6.655756
 93455/100000: episode: 1784, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 51.719, mean reward: 2.586 [2.123, 3.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-1.070, 10.100], loss: 752.752930, mae: 3.461117, mean_q: 7.873200
 93464/100000: episode: 1785, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 48.805, mean reward: 5.423 [4.314, 6.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.618], loss: 5.136532, mae: 2.366160, mean_q: 8.380531
 93490/100000: episode: 1786, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 86.330, mean reward: 3.320 [1.928, 4.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.998, 10.100], loss: 1.779974, mae: 1.258901, mean_q: 6.320305
 93513/100000: episode: 1787, duration: 0.124s, episode steps: 23, steps per second: 185, episode reward: 86.439, mean reward: 3.758 [2.102, 11.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.667, 10.100], loss: 1.429631, mae: 1.012043, mean_q: 6.354089
 93525/100000: episode: 1788, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 48.170, mean reward: 4.014 [2.728, 5.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.498], loss: 0.694391, mae: 0.828929, mean_q: 6.239627
 93548/100000: episode: 1789, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 79.007, mean reward: 3.435 [2.246, 4.540], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.035, 10.100], loss: 0.871727, mae: 0.825152, mean_q: 6.100169
 93581/100000: episode: 1790, duration: 0.161s, episode steps: 33, steps per second: 205, episode reward: 110.111, mean reward: 3.337 [2.360, 6.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.005 [-1.227, 10.100], loss: 1.040384, mae: 0.831271, mean_q: 6.163673
 93614/100000: episode: 1791, duration: 0.168s, episode steps: 33, steps per second: 197, episode reward: 97.648, mean reward: 2.959 [2.452, 3.884], mean action: 0.000 [0.000, 0.000], mean observation: 2.014 [-1.078, 10.100], loss: 0.642891, mae: 0.772592, mean_q: 6.084773
 93640/100000: episode: 1792, duration: 0.150s, episode steps: 26, steps per second: 173, episode reward: 86.796, mean reward: 3.338 [2.350, 4.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.421, 10.100], loss: 579.906982, mae: 1.993321, mean_q: 6.016973
 93649/100000: episode: 1793, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 95.758, mean reward: 10.640 [5.865, 24.491], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.278, 10.571], loss: 7.711519, mae: 3.343848, mean_q: 9.190622
 93672/100000: episode: 1794, duration: 0.126s, episode steps: 23, steps per second: 183, episode reward: 82.898, mean reward: 3.604 [2.218, 7.532], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.217, 10.100], loss: 664.670532, mae: 3.043784, mean_q: 7.205842
 93695/100000: episode: 1795, duration: 0.136s, episode steps: 23, steps per second: 170, episode reward: 73.116, mean reward: 3.179 [2.360, 6.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.286, 10.100], loss: 2.885924, mae: 1.629147, mean_q: 6.909134
 93723/100000: episode: 1796, duration: 0.169s, episode steps: 28, steps per second: 166, episode reward: 65.276, mean reward: 2.331 [1.560, 4.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-1.171, 10.206], loss: 1083.455200, mae: 4.566564, mean_q: 8.131925
 93746/100000: episode: 1797, duration: 0.124s, episode steps: 23, steps per second: 186, episode reward: 80.542, mean reward: 3.502 [2.351, 5.978], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.266, 10.100], loss: 1313.083618, mae: 6.094019, mean_q: 9.807584
 93780/100000: episode: 1798, duration: 0.174s, episode steps: 34, steps per second: 196, episode reward: 158.087, mean reward: 4.650 [2.361, 25.974], mean action: 0.000 [0.000, 0.000], mean observation: 2.010 [-0.326, 10.100], loss: 441.463135, mae: 3.522824, mean_q: 7.527035
 93800/100000: episode: 1799, duration: 0.110s, episode steps: 20, steps per second: 181, episode reward: 54.511, mean reward: 2.726 [2.032, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.136, 10.100], loss: 2.415972, mae: 1.508569, mean_q: 6.248437
 93816/100000: episode: 1800, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 52.904, mean reward: 3.307 [2.668, 6.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.353, 10.100], loss: 1.468497, mae: 1.065464, mean_q: 6.540382
 93841/100000: episode: 1801, duration: 0.145s, episode steps: 25, steps per second: 173, episode reward: 62.270, mean reward: 2.491 [1.828, 3.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.597, 10.100], loss: 607.293579, mae: 3.141668, mean_q: 7.769120
 93875/100000: episode: 1802, duration: 0.190s, episode steps: 34, steps per second: 179, episode reward: 106.682, mean reward: 3.138 [2.293, 5.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-1.170, 10.100], loss: 443.382599, mae: 2.617404, mean_q: 7.746202
 93887/100000: episode: 1803, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 71.208, mean reward: 5.934 [4.265, 8.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.641], loss: 1.174285, mae: 1.069063, mean_q: 5.945876
 93899/100000: episode: 1804, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 57.658, mean reward: 4.805 [3.330, 7.215], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.463, 10.605], loss: 1.667058, mae: 1.112992, mean_q: 6.456582
 93911/100000: episode: 1805, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 53.273, mean reward: 4.439 [3.548, 5.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.520], loss: 1247.977295, mae: 3.583010, mean_q: 6.468725
 93944/100000: episode: 1806, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 104.126, mean reward: 3.155 [2.165, 4.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.751, 10.100], loss: 3.227886, mae: 1.752169, mean_q: 7.656197
 93969/100000: episode: 1807, duration: 0.147s, episode steps: 25, steps per second: 170, episode reward: 85.928, mean reward: 3.437 [2.216, 5.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.427, 10.100], loss: 1.753223, mae: 1.140384, mean_q: 6.150378
 93997/100000: episode: 1808, duration: 0.135s, episode steps: 28, steps per second: 208, episode reward: 80.925, mean reward: 2.890 [2.077, 4.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.245, 10.100], loss: 1601.034546, mae: 6.512389, mean_q: 9.541392
 94031/100000: episode: 1809, duration: 0.168s, episode steps: 34, steps per second: 202, episode reward: 115.355, mean reward: 3.393 [2.572, 4.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.008 [-0.731, 10.100], loss: 447.869690, mae: 2.874927, mean_q: 7.779819
 94057/100000: episode: 1810, duration: 0.134s, episode steps: 26, steps per second: 194, episode reward: 85.597, mean reward: 3.292 [2.369, 4.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.513, 10.100], loss: 582.598877, mae: 2.744243, mean_q: 6.976903
 94090/100000: episode: 1811, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 127.236, mean reward: 3.856 [2.845, 6.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.020 [-1.547, 10.100], loss: 4.106558, mae: 1.818250, mean_q: 7.726251
 94118/100000: episode: 1812, duration: 0.160s, episode steps: 28, steps per second: 175, episode reward: 97.961, mean reward: 3.499 [2.406, 9.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.226, 10.100], loss: 2.159372, mae: 1.203097, mean_q: 6.444085
 94134/100000: episode: 1813, duration: 0.081s, episode steps: 16, steps per second: 199, episode reward: 51.849, mean reward: 3.241 [2.979, 3.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.617, 10.100], loss: 1.508013, mae: 1.080817, mean_q: 6.793275
 94157/100000: episode: 1814, duration: 0.120s, episode steps: 23, steps per second: 192, episode reward: 96.895, mean reward: 4.213 [3.414, 6.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.877, 10.100], loss: 1.733027, mae: 1.042962, mean_q: 6.579662
[Info] Complete ISplit Iteration
[Info] Levels: [5.093883, 7.876584, 16.703959]
[Info] Cond. Prob: [0.1, 0.1, 0.06]
[Info] Error Prob: 0.0006000000000000001

 94173/100000: episode: 1815, duration: 4.312s, episode steps: 16, steps per second: 4, episode reward: 49.049, mean reward: 3.066 [2.427, 3.960], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.865, 10.100], loss: 1.917990, mae: 1.117957, mean_q: 6.612954
 94273/100000: episode: 1816, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 186.334, mean reward: 1.863 [1.459, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.715, 10.209], loss: 153.285248, mae: 1.544878, mean_q: 6.785812
 94373/100000: episode: 1817, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 203.718, mean reward: 2.037 [1.532, 3.027], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.300, 10.212], loss: 753.539062, mae: 4.674156, mean_q: 8.049181
 94473/100000: episode: 1818, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 192.168, mean reward: 1.922 [1.459, 2.764], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.875, 10.098], loss: 153.347885, mae: 1.441693, mean_q: 6.606322
 94573/100000: episode: 1819, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 201.467, mean reward: 2.015 [1.442, 4.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.453, 10.155], loss: 1.679616, mae: 1.099791, mean_q: 6.299103
 94673/100000: episode: 1820, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 185.948, mean reward: 1.859 [1.467, 4.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.594, 10.135], loss: 1.264493, mae: 0.930073, mean_q: 6.054992
 94773/100000: episode: 1821, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 196.411, mean reward: 1.964 [1.449, 3.411], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.125, 10.098], loss: 153.797943, mae: 1.479871, mean_q: 6.430281
 94873/100000: episode: 1822, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 182.014, mean reward: 1.820 [1.466, 3.821], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.306, 10.242], loss: 152.078430, mae: 1.507329, mean_q: 6.365295
 94973/100000: episode: 1823, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 209.637, mean reward: 2.096 [1.481, 11.206], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.815, 10.140], loss: 153.303619, mae: 1.501637, mean_q: 6.330066
 95073/100000: episode: 1824, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 191.243, mean reward: 1.912 [1.455, 3.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.273, 10.439], loss: 301.694946, mae: 2.120281, mean_q: 6.688441
 95173/100000: episode: 1825, duration: 0.539s, episode steps: 100, steps per second: 186, episode reward: 218.774, mean reward: 2.188 [1.463, 4.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.653, 10.315], loss: 153.484848, mae: 1.495651, mean_q: 6.404520
 95273/100000: episode: 1826, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 190.741, mean reward: 1.907 [1.441, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.368, 10.272], loss: 302.860596, mae: 2.152253, mean_q: 6.717985
 95373/100000: episode: 1827, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 200.171, mean reward: 2.002 [1.488, 3.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.136, 10.098], loss: 301.621979, mae: 2.299239, mean_q: 6.723351
 95473/100000: episode: 1828, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 196.175, mean reward: 1.962 [1.485, 4.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.181, 10.139], loss: 152.917191, mae: 1.489096, mean_q: 6.322501
 95573/100000: episode: 1829, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 183.224, mean reward: 1.832 [1.461, 4.135], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.922, 10.142], loss: 151.095322, mae: 1.510016, mean_q: 6.247351
 95673/100000: episode: 1830, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 190.661, mean reward: 1.907 [1.451, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.447, 10.211], loss: 153.067490, mae: 1.458571, mean_q: 6.224098
 95773/100000: episode: 1831, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 192.747, mean reward: 1.927 [1.437, 3.571], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.063, 10.291], loss: 1.294312, mae: 0.887666, mean_q: 5.654036
 95873/100000: episode: 1832, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 197.994, mean reward: 1.980 [1.444, 2.874], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.691, 10.260], loss: 604.827271, mae: 3.416845, mean_q: 7.353192
 95973/100000: episode: 1833, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 174.670, mean reward: 1.747 [1.461, 2.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.272, 10.098], loss: 1.422522, mae: 0.997699, mean_q: 5.692025
 96073/100000: episode: 1834, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 211.691, mean reward: 2.117 [1.461, 5.168], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.306, 10.152], loss: 151.101578, mae: 1.436984, mean_q: 6.078164
 96173/100000: episode: 1835, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 197.187, mean reward: 1.972 [1.485, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.928, 10.259], loss: 150.751877, mae: 1.437673, mean_q: 5.950623
 96273/100000: episode: 1836, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 195.984, mean reward: 1.960 [1.484, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.725, 10.098], loss: 302.438995, mae: 2.017162, mean_q: 6.340771
 96373/100000: episode: 1837, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 178.747, mean reward: 1.787 [1.451, 3.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.562, 10.115], loss: 152.578476, mae: 1.501464, mean_q: 6.060637
 96473/100000: episode: 1838, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 201.208, mean reward: 2.012 [1.439, 3.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.548, 10.098], loss: 150.877838, mae: 1.344542, mean_q: 5.797149
 96573/100000: episode: 1839, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 193.983, mean reward: 1.940 [1.452, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.572, 10.155], loss: 301.571320, mae: 1.867463, mean_q: 5.961982
 96673/100000: episode: 1840, duration: 0.585s, episode steps: 100, steps per second: 171, episode reward: 188.485, mean reward: 1.885 [1.469, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.858, 10.098], loss: 1.908710, mae: 1.167683, mean_q: 5.845546
 96773/100000: episode: 1841, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 221.317, mean reward: 2.213 [1.465, 3.884], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.882, 10.098], loss: 153.012146, mae: 1.498735, mean_q: 5.842658
 96873/100000: episode: 1842, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 189.975, mean reward: 1.900 [1.481, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.024, 10.113], loss: 451.452118, mae: 2.571285, mean_q: 6.458743
 96973/100000: episode: 1843, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: 189.359, mean reward: 1.894 [1.449, 3.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.869, 10.177], loss: 153.092163, mae: 1.816975, mean_q: 6.012189
 97073/100000: episode: 1844, duration: 0.563s, episode steps: 100, steps per second: 178, episode reward: 218.643, mean reward: 2.186 [1.476, 4.476], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.721, 10.301], loss: 302.408325, mae: 1.764280, mean_q: 5.890191
 97173/100000: episode: 1845, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 184.485, mean reward: 1.845 [1.477, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.603, 10.111], loss: 300.809906, mae: 2.719085, mean_q: 6.584689
 97273/100000: episode: 1846, duration: 0.610s, episode steps: 100, steps per second: 164, episode reward: 191.930, mean reward: 1.919 [1.480, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.461, 10.174], loss: 152.096069, mae: 1.470481, mean_q: 5.818619
 97373/100000: episode: 1847, duration: 0.655s, episode steps: 100, steps per second: 153, episode reward: 190.032, mean reward: 1.900 [1.469, 3.426], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.567, 10.261], loss: 0.957286, mae: 0.816896, mean_q: 5.326179
 97473/100000: episode: 1848, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 181.752, mean reward: 1.818 [1.433, 2.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.362, 10.187], loss: 1.070920, mae: 0.780828, mean_q: 5.101166
 97573/100000: episode: 1849, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 210.936, mean reward: 2.109 [1.492, 7.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-2.400, 10.098], loss: 0.788323, mae: 0.725675, mean_q: 4.905712
 97673/100000: episode: 1850, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 195.647, mean reward: 1.956 [1.445, 4.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.526, 10.212], loss: 0.728835, mae: 0.680027, mean_q: 4.808225
 97773/100000: episode: 1851, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 195.512, mean reward: 1.955 [1.465, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.860, 10.156], loss: 0.819631, mae: 0.659926, mean_q: 4.785743
 97873/100000: episode: 1852, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 208.464, mean reward: 2.085 [1.485, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.465, 10.098], loss: 299.404724, mae: 1.938887, mean_q: 5.409926
 97973/100000: episode: 1853, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 181.027, mean reward: 1.810 [1.459, 5.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.746, 10.233], loss: 150.115875, mae: 1.278125, mean_q: 5.118339
 98073/100000: episode: 1854, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 184.960, mean reward: 1.850 [1.463, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.402, 10.098], loss: 0.881061, mae: 0.719606, mean_q: 4.581755
 98173/100000: episode: 1855, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: 189.408, mean reward: 1.894 [1.496, 4.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.846, 10.098], loss: 0.724857, mae: 0.607170, mean_q: 4.519222
 98273/100000: episode: 1856, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 184.197, mean reward: 1.842 [1.443, 3.155], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.823, 10.098], loss: 0.649396, mae: 0.569545, mean_q: 4.434074
 98373/100000: episode: 1857, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 199.711, mean reward: 1.997 [1.490, 4.820], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.764, 10.098], loss: 0.598562, mae: 0.563345, mean_q: 4.356108
 98473/100000: episode: 1858, duration: 0.608s, episode steps: 100, steps per second: 164, episode reward: 186.863, mean reward: 1.869 [1.479, 5.097], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.669, 10.098], loss: 0.436151, mae: 0.513623, mean_q: 4.285376
 98573/100000: episode: 1859, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 211.100, mean reward: 2.111 [1.438, 5.738], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.736, 10.098], loss: 0.312464, mae: 0.494924, mean_q: 4.202736
 98673/100000: episode: 1860, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 203.903, mean reward: 2.039 [1.467, 3.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.155, 10.098], loss: 0.254736, mae: 0.479370, mean_q: 4.172599
 98773/100000: episode: 1861, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 191.428, mean reward: 1.914 [1.451, 4.358], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.325, 10.098], loss: 0.252208, mae: 0.461333, mean_q: 4.086841
 98873/100000: episode: 1862, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 189.563, mean reward: 1.896 [1.465, 3.466], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.499, 10.098], loss: 0.196297, mae: 0.435305, mean_q: 4.026676
 98973/100000: episode: 1863, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 185.916, mean reward: 1.859 [1.441, 2.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.411, 10.267], loss: 0.161448, mae: 0.405674, mean_q: 3.975028
 99073/100000: episode: 1864, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 201.282, mean reward: 2.013 [1.431, 4.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.389, 10.198], loss: 0.186704, mae: 0.422115, mean_q: 3.947595
 99173/100000: episode: 1865, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 207.482, mean reward: 2.075 [1.482, 4.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.832, 10.367], loss: 0.149031, mae: 0.392519, mean_q: 3.873798
 99273/100000: episode: 1866, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: 179.272, mean reward: 1.793 [1.457, 2.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.839, 10.098], loss: 0.131248, mae: 0.376117, mean_q: 3.848969
 99373/100000: episode: 1867, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 178.762, mean reward: 1.788 [1.449, 2.870], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.479, 10.112], loss: 0.158821, mae: 0.394147, mean_q: 3.864043
 99473/100000: episode: 1868, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 203.150, mean reward: 2.032 [1.498, 3.879], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.383, 10.098], loss: 0.157869, mae: 0.384093, mean_q: 3.858446
 99573/100000: episode: 1869, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 185.170, mean reward: 1.852 [1.436, 3.958], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.026, 10.159], loss: 0.137423, mae: 0.377278, mean_q: 3.867255
 99673/100000: episode: 1870, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 186.909, mean reward: 1.869 [1.450, 3.244], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.773, 10.191], loss: 0.145180, mae: 0.372594, mean_q: 3.857093
 99773/100000: episode: 1871, duration: 0.792s, episode steps: 100, steps per second: 126, episode reward: 187.615, mean reward: 1.876 [1.442, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.521, 10.234], loss: 0.136724, mae: 0.373211, mean_q: 3.837879
 99873/100000: episode: 1872, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 193.558, mean reward: 1.936 [1.459, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.491, 10.098], loss: 0.116997, mae: 0.350354, mean_q: 3.828769
 99973/100000: episode: 1873, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 186.753, mean reward: 1.868 [1.440, 2.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-1.007, 10.368], loss: 0.133072, mae: 0.368998, mean_q: 3.853565
done, took 599.187 seconds
[Info] End Importance Splitting. Falsification occurred 7 times.
