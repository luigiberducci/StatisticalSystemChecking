Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 25)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                832       
_________________________________________________________________
dense_2 (Dense)              (None, 16)                528       
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 9         
=================================================================
Total params: 1,505
Trainable params: 1,505
Non-trainable params: 0
_________________________________________________________________
None
[Info] Start Importance Splitting.
Training for 100000 steps ...
   100/100000: episode: 1, duration: 0.171s, episode steps: 100, steps per second: 584, episode reward: 191.107, mean reward: 1.911 [1.466, 2.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.244, 10.098], loss: --, mae: --, mean_q: --
   200/100000: episode: 2, duration: 0.065s, episode steps: 100, steps per second: 1546, episode reward: 200.234, mean reward: 2.002 [1.499, 4.195], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-2.152, 10.098], loss: --, mae: --, mean_q: --
   300/100000: episode: 3, duration: 0.065s, episode steps: 100, steps per second: 1542, episode reward: 189.579, mean reward: 1.896 [1.461, 4.242], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.415, 10.263], loss: --, mae: --, mean_q: --
   400/100000: episode: 4, duration: 0.072s, episode steps: 100, steps per second: 1380, episode reward: 351.876, mean reward: 3.519 [1.484, 18.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.724, 10.098], loss: --, mae: --, mean_q: --
   500/100000: episode: 5, duration: 0.081s, episode steps: 100, steps per second: 1233, episode reward: 191.295, mean reward: 1.913 [1.455, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.871, 10.115], loss: --, mae: --, mean_q: --
   600/100000: episode: 6, duration: 0.069s, episode steps: 100, steps per second: 1451, episode reward: 187.950, mean reward: 1.879 [1.438, 3.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.562, 10.098], loss: --, mae: --, mean_q: --
   700/100000: episode: 7, duration: 0.064s, episode steps: 100, steps per second: 1552, episode reward: 188.424, mean reward: 1.884 [1.468, 3.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.189, 10.221], loss: --, mae: --, mean_q: --
   800/100000: episode: 8, duration: 0.064s, episode steps: 100, steps per second: 1556, episode reward: 194.461, mean reward: 1.945 [1.536, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.650, 10.098], loss: --, mae: --, mean_q: --
   900/100000: episode: 9, duration: 0.064s, episode steps: 100, steps per second: 1553, episode reward: 178.690, mean reward: 1.787 [1.484, 3.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.367, 10.098], loss: --, mae: --, mean_q: --
  1000/100000: episode: 10, duration: 0.065s, episode steps: 100, steps per second: 1548, episode reward: 193.900, mean reward: 1.939 [1.465, 3.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.942, 10.108], loss: --, mae: --, mean_q: --
  1100/100000: episode: 11, duration: 0.065s, episode steps: 100, steps per second: 1546, episode reward: 194.804, mean reward: 1.948 [1.448, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.325, 10.251], loss: --, mae: --, mean_q: --
  1200/100000: episode: 12, duration: 0.064s, episode steps: 100, steps per second: 1557, episode reward: 186.560, mean reward: 1.866 [1.461, 3.403], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.729, 10.098], loss: --, mae: --, mean_q: --
  1300/100000: episode: 13, duration: 0.065s, episode steps: 100, steps per second: 1545, episode reward: 204.500, mean reward: 2.045 [1.483, 3.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.252, 10.123], loss: --, mae: --, mean_q: --
  1400/100000: episode: 14, duration: 0.070s, episode steps: 100, steps per second: 1428, episode reward: 209.528, mean reward: 2.095 [1.472, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.450, 10.098], loss: --, mae: --, mean_q: --
  1500/100000: episode: 15, duration: 0.070s, episode steps: 100, steps per second: 1433, episode reward: 193.929, mean reward: 1.939 [1.456, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.491, 10.098], loss: --, mae: --, mean_q: --
  1600/100000: episode: 16, duration: 0.073s, episode steps: 100, steps per second: 1374, episode reward: 235.072, mean reward: 2.351 [1.466, 4.724], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.863, 10.257], loss: --, mae: --, mean_q: --
  1700/100000: episode: 17, duration: 0.078s, episode steps: 100, steps per second: 1289, episode reward: 187.136, mean reward: 1.871 [1.437, 2.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.069, 10.138], loss: --, mae: --, mean_q: --
  1800/100000: episode: 18, duration: 0.065s, episode steps: 100, steps per second: 1546, episode reward: 185.334, mean reward: 1.853 [1.468, 2.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.440, 10.209], loss: --, mae: --, mean_q: --
  1900/100000: episode: 19, duration: 0.075s, episode steps: 100, steps per second: 1339, episode reward: 187.633, mean reward: 1.876 [1.485, 2.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.671, 10.195], loss: --, mae: --, mean_q: --
  2000/100000: episode: 20, duration: 0.066s, episode steps: 100, steps per second: 1505, episode reward: 188.708, mean reward: 1.887 [1.450, 2.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.478, 10.149], loss: --, mae: --, mean_q: --
  2100/100000: episode: 21, duration: 0.065s, episode steps: 100, steps per second: 1546, episode reward: 211.060, mean reward: 2.111 [1.483, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.576, 10.253], loss: --, mae: --, mean_q: --
  2200/100000: episode: 22, duration: 0.065s, episode steps: 100, steps per second: 1544, episode reward: 196.988, mean reward: 1.970 [1.434, 3.343], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.319, 10.098], loss: --, mae: --, mean_q: --
  2300/100000: episode: 23, duration: 0.065s, episode steps: 100, steps per second: 1543, episode reward: 186.962, mean reward: 1.870 [1.502, 3.078], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.254, 10.098], loss: --, mae: --, mean_q: --
  2400/100000: episode: 24, duration: 0.064s, episode steps: 100, steps per second: 1553, episode reward: 183.906, mean reward: 1.839 [1.460, 3.790], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.187, 10.100], loss: --, mae: --, mean_q: --
  2500/100000: episode: 25, duration: 0.067s, episode steps: 100, steps per second: 1501, episode reward: 200.892, mean reward: 2.009 [1.462, 3.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.293, 10.098], loss: --, mae: --, mean_q: --
  2600/100000: episode: 26, duration: 0.079s, episode steps: 100, steps per second: 1268, episode reward: 193.175, mean reward: 1.932 [1.467, 2.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.538, 10.153], loss: --, mae: --, mean_q: --
  2700/100000: episode: 27, duration: 0.070s, episode steps: 100, steps per second: 1436, episode reward: 184.606, mean reward: 1.846 [1.460, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.801, 10.098], loss: --, mae: --, mean_q: --
  2800/100000: episode: 28, duration: 0.069s, episode steps: 100, steps per second: 1454, episode reward: 217.641, mean reward: 2.176 [1.504, 3.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.584, 10.098], loss: --, mae: --, mean_q: --
  2900/100000: episode: 29, duration: 0.065s, episode steps: 100, steps per second: 1529, episode reward: 197.635, mean reward: 1.976 [1.465, 3.456], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.234, 10.154], loss: --, mae: --, mean_q: --
  3000/100000: episode: 30, duration: 0.065s, episode steps: 100, steps per second: 1544, episode reward: 183.853, mean reward: 1.839 [1.485, 2.988], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.609, 10.158], loss: --, mae: --, mean_q: --
  3100/100000: episode: 31, duration: 0.069s, episode steps: 100, steps per second: 1455, episode reward: 200.181, mean reward: 2.002 [1.441, 3.396], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.895, 10.122], loss: --, mae: --, mean_q: --
  3200/100000: episode: 32, duration: 0.065s, episode steps: 100, steps per second: 1543, episode reward: 181.122, mean reward: 1.811 [1.438, 2.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.437, 10.332], loss: --, mae: --, mean_q: --
  3300/100000: episode: 33, duration: 0.065s, episode steps: 100, steps per second: 1545, episode reward: 193.527, mean reward: 1.935 [1.474, 4.630], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.634, 10.236], loss: --, mae: --, mean_q: --
  3400/100000: episode: 34, duration: 0.070s, episode steps: 100, steps per second: 1426, episode reward: 245.206, mean reward: 2.452 [1.496, 5.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.429, 10.098], loss: --, mae: --, mean_q: --
  3500/100000: episode: 35, duration: 0.065s, episode steps: 100, steps per second: 1542, episode reward: 189.522, mean reward: 1.895 [1.478, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.459, 10.171], loss: --, mae: --, mean_q: --
  3600/100000: episode: 36, duration: 0.065s, episode steps: 100, steps per second: 1542, episode reward: 204.338, mean reward: 2.043 [1.490, 4.492], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.833, 10.098], loss: --, mae: --, mean_q: --
  3700/100000: episode: 37, duration: 0.067s, episode steps: 100, steps per second: 1487, episode reward: 190.499, mean reward: 1.905 [1.450, 3.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.699, 10.204], loss: --, mae: --, mean_q: --
  3800/100000: episode: 38, duration: 0.069s, episode steps: 100, steps per second: 1452, episode reward: 188.761, mean reward: 1.888 [1.438, 3.502], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.857, 10.192], loss: --, mae: --, mean_q: --
  3900/100000: episode: 39, duration: 0.065s, episode steps: 100, steps per second: 1533, episode reward: 220.387, mean reward: 2.204 [1.464, 4.494], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.929, 10.351], loss: --, mae: --, mean_q: --
  4000/100000: episode: 40, duration: 0.064s, episode steps: 100, steps per second: 1553, episode reward: 188.979, mean reward: 1.890 [1.458, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.903, 10.148], loss: --, mae: --, mean_q: --
  4100/100000: episode: 41, duration: 0.064s, episode steps: 100, steps per second: 1559, episode reward: 182.427, mean reward: 1.824 [1.445, 3.506], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.513, 10.098], loss: --, mae: --, mean_q: --
  4200/100000: episode: 42, duration: 0.064s, episode steps: 100, steps per second: 1554, episode reward: 184.033, mean reward: 1.840 [1.471, 3.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.705, 10.124], loss: --, mae: --, mean_q: --
  4300/100000: episode: 43, duration: 0.065s, episode steps: 100, steps per second: 1550, episode reward: 218.038, mean reward: 2.180 [1.484, 3.798], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.482, 10.214], loss: --, mae: --, mean_q: --
  4400/100000: episode: 44, duration: 0.064s, episode steps: 100, steps per second: 1558, episode reward: 193.467, mean reward: 1.935 [1.436, 3.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.100, 10.160], loss: --, mae: --, mean_q: --
  4500/100000: episode: 45, duration: 0.064s, episode steps: 100, steps per second: 1551, episode reward: 186.336, mean reward: 1.863 [1.453, 3.527], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.752, 10.098], loss: --, mae: --, mean_q: --
  4600/100000: episode: 46, duration: 0.065s, episode steps: 100, steps per second: 1539, episode reward: 199.751, mean reward: 1.998 [1.500, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.372, 10.098], loss: --, mae: --, mean_q: --
  4700/100000: episode: 47, duration: 0.065s, episode steps: 100, steps per second: 1543, episode reward: 179.649, mean reward: 1.796 [1.457, 2.950], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.677, 10.098], loss: --, mae: --, mean_q: --
  4800/100000: episode: 48, duration: 0.065s, episode steps: 100, steps per second: 1548, episode reward: 210.672, mean reward: 2.107 [1.482, 3.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.491, 10.181], loss: --, mae: --, mean_q: --
  4866/100000: episode: 49, duration: 0.043s, episode steps: 66, steps per second: 1534, episode reward: 1178.754, mean reward: 17.860 [1.455, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.106 [-1.064, 7.336], loss: --, mae: --, mean_q: --
  4966/100000: episode: 50, duration: 0.065s, episode steps: 100, steps per second: 1539, episode reward: 202.505, mean reward: 2.025 [1.468, 4.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.271, 10.098], loss: --, mae: --, mean_q: --
  5066/100000: episode: 51, duration: 1.111s, episode steps: 100, steps per second: 90, episode reward: 195.744, mean reward: 1.957 [1.464, 4.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.584, 10.098], loss: 239.243853, mae: 1.042792, mean_q: 2.970108
  5166/100000: episode: 52, duration: 0.593s, episode steps: 100, steps per second: 169, episode reward: 199.243, mean reward: 1.992 [1.533, 3.297], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.420, 10.364], loss: 155.580612, mae: 0.831602, mean_q: 3.454211
  5266/100000: episode: 53, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 199.325, mean reward: 1.993 [1.471, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.387, 10.254], loss: 0.197172, mae: 0.357844, mean_q: 3.569571
  5366/100000: episode: 54, duration: 0.605s, episode steps: 100, steps per second: 165, episode reward: 196.299, mean reward: 1.963 [1.469, 3.305], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.485, 10.204], loss: 310.092743, mae: 1.133405, mean_q: 3.921798
  5466/100000: episode: 55, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 182.109, mean reward: 1.821 [1.468, 2.922], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.741, 10.098], loss: 155.155670, mae: 1.036320, mean_q: 4.419109
  5566/100000: episode: 56, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 193.232, mean reward: 1.932 [1.439, 3.148], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.434, 10.375], loss: 0.133492, mae: 0.340696, mean_q: 3.984497
  5666/100000: episode: 57, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 184.871, mean reward: 1.849 [1.447, 3.860], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.856, 10.119], loss: 154.975815, mae: 0.835633, mean_q: 4.229306
  5766/100000: episode: 58, duration: 0.573s, episode steps: 100, steps per second: 175, episode reward: 179.929, mean reward: 1.799 [1.493, 3.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.905, 10.219], loss: 0.133854, mae: 0.337254, mean_q: 4.005251
  5866/100000: episode: 59, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 185.004, mean reward: 1.850 [1.480, 2.627], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.935, 10.098], loss: 154.993637, mae: 0.853259, mean_q: 4.225926
  5966/100000: episode: 60, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 191.965, mean reward: 1.920 [1.439, 5.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.904, 10.110], loss: 154.901688, mae: 0.890369, mean_q: 4.298649
  6066/100000: episode: 61, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 181.477, mean reward: 1.815 [1.461, 2.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.820, 10.175], loss: 0.122097, mae: 0.345874, mean_q: 4.040147
  6166/100000: episode: 62, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 183.540, mean reward: 1.835 [1.450, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.314, 10.135], loss: 463.565521, mae: 1.751606, mean_q: 4.547077
  6266/100000: episode: 63, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 184.262, mean reward: 1.843 [1.458, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.310, 10.183], loss: 154.561798, mae: 1.125121, mean_q: 4.741043
  6366/100000: episode: 64, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 198.488, mean reward: 1.985 [1.455, 3.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.473, 10.098], loss: 154.577133, mae: 0.921153, mean_q: 4.481130
  6466/100000: episode: 65, duration: 0.594s, episode steps: 100, steps per second: 168, episode reward: 190.091, mean reward: 1.901 [1.446, 4.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.710, 10.119], loss: 0.172739, mae: 0.400538, mean_q: 4.180577
  6566/100000: episode: 66, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 183.618, mean reward: 1.836 [1.451, 3.503], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.986, 10.243], loss: 0.130246, mae: 0.352845, mean_q: 4.037386
  6666/100000: episode: 67, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 196.938, mean reward: 1.969 [1.472, 2.994], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.826, 10.220], loss: 154.714554, mae: 0.871308, mean_q: 4.312499
  6766/100000: episode: 68, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: 201.519, mean reward: 2.015 [1.464, 6.076], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.860, 10.325], loss: 0.131784, mae: 0.359393, mean_q: 4.025167
  6866/100000: episode: 69, duration: 0.569s, episode steps: 100, steps per second: 176, episode reward: 188.093, mean reward: 1.881 [1.514, 2.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.811, 10.098], loss: 0.151567, mae: 0.353766, mean_q: 3.996458
  6966/100000: episode: 70, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 186.614, mean reward: 1.866 [1.477, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.299, 10.098], loss: 0.148769, mae: 0.341035, mean_q: 3.935554
  7066/100000: episode: 71, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 190.695, mean reward: 1.907 [1.454, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.884, 10.279], loss: 308.972443, mae: 1.208539, mean_q: 4.247476
  7166/100000: episode: 72, duration: 0.568s, episode steps: 100, steps per second: 176, episode reward: 188.096, mean reward: 1.881 [1.496, 4.280], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.687, 10.200], loss: 0.427586, mae: 0.603844, mean_q: 4.208344
  7266/100000: episode: 73, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 217.174, mean reward: 2.172 [1.491, 3.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.777, 10.098], loss: 154.731354, mae: 0.888818, mean_q: 4.333006
  7366/100000: episode: 74, duration: 0.547s, episode steps: 100, steps per second: 183, episode reward: 194.428, mean reward: 1.944 [1.453, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.658, 10.132], loss: 0.134351, mae: 0.354587, mean_q: 4.033084
  7466/100000: episode: 75, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 188.015, mean reward: 1.880 [1.473, 3.575], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.478, 10.148], loss: 0.140552, mae: 0.342682, mean_q: 3.987543
  7566/100000: episode: 76, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 181.631, mean reward: 1.816 [1.431, 3.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.663, 10.191], loss: 154.772736, mae: 0.880073, mean_q: 4.259318
  7666/100000: episode: 77, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 213.222, mean reward: 2.132 [1.455, 3.473], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.744, 10.098], loss: 308.675323, mae: 1.398272, mean_q: 4.583368
  7766/100000: episode: 78, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 195.141, mean reward: 1.951 [1.465, 3.763], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.657, 10.431], loss: 308.015808, mae: 1.438812, mean_q: 4.641852
  7866/100000: episode: 79, duration: 0.558s, episode steps: 100, steps per second: 179, episode reward: 191.295, mean reward: 1.913 [1.452, 4.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.820, 10.098], loss: 0.339066, mae: 0.560594, mean_q: 4.279136
  7966/100000: episode: 80, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 187.490, mean reward: 1.875 [1.448, 3.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.736, 10.277], loss: 308.176117, mae: 1.366072, mean_q: 4.601650
  8066/100000: episode: 81, duration: 0.566s, episode steps: 100, steps per second: 177, episode reward: 188.980, mean reward: 1.890 [1.503, 2.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.947, 10.138], loss: 0.317064, mae: 0.540200, mean_q: 4.307577
  8166/100000: episode: 82, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 180.800, mean reward: 1.808 [1.442, 3.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.235, 10.098], loss: 154.329575, mae: 0.932125, mean_q: 4.426874
  8266/100000: episode: 83, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 187.997, mean reward: 1.880 [1.463, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.422, 10.147], loss: 0.209974, mae: 0.445172, mean_q: 4.111115
  8366/100000: episode: 84, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 185.053, mean reward: 1.851 [1.508, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.874, 10.152], loss: 154.364746, mae: 0.941487, mean_q: 4.341331
  8466/100000: episode: 85, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 181.286, mean reward: 1.813 [1.448, 3.652], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.305, 10.146], loss: 154.238510, mae: 0.943857, mean_q: 4.368171
  8566/100000: episode: 86, duration: 0.572s, episode steps: 100, steps per second: 175, episode reward: 180.242, mean reward: 1.802 [1.461, 3.050], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.188, 10.121], loss: 154.064789, mae: 0.724377, mean_q: 4.074092
  8666/100000: episode: 87, duration: 0.556s, episode steps: 100, steps per second: 180, episode reward: 179.258, mean reward: 1.793 [1.465, 2.843], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.729, 10.148], loss: 154.153793, mae: 1.159869, mean_q: 4.621671
  8766/100000: episode: 88, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 195.980, mean reward: 1.960 [1.434, 3.202], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.536, 10.098], loss: 612.759094, mae: 2.143331, mean_q: 4.730221
  8866/100000: episode: 89, duration: 0.580s, episode steps: 100, steps per second: 172, episode reward: 183.736, mean reward: 1.837 [1.464, 2.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.746, 10.098], loss: 1.225142, mae: 0.924364, mean_q: 4.619161
  8966/100000: episode: 90, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 188.238, mean reward: 1.882 [1.473, 3.374], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.556, 10.147], loss: 154.434296, mae: 1.011683, mean_q: 4.566435
  9066/100000: episode: 91, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 193.618, mean reward: 1.936 [1.467, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.616, 10.098], loss: 153.967010, mae: 0.755727, mean_q: 4.205654
  9166/100000: episode: 92, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 190.140, mean reward: 1.901 [1.474, 3.024], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.867, 10.223], loss: 0.505554, mae: 0.647778, mean_q: 4.349575
  9266/100000: episode: 93, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 200.656, mean reward: 2.007 [1.485, 4.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.576, 10.098], loss: 0.166775, mae: 0.381886, mean_q: 4.010676
  9366/100000: episode: 94, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 196.223, mean reward: 1.962 [1.464, 2.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.236, 10.098], loss: 154.052994, mae: 0.678092, mean_q: 3.943875
  9466/100000: episode: 95, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 201.981, mean reward: 2.020 [1.489, 3.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.436, 10.098], loss: 0.409169, mae: 0.554249, mean_q: 4.183441
  9566/100000: episode: 96, duration: 0.554s, episode steps: 100, steps per second: 180, episode reward: 186.460, mean reward: 1.865 [1.477, 3.995], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.218, 10.098], loss: 307.402222, mae: 1.213442, mean_q: 4.299274
  9666/100000: episode: 97, duration: 0.578s, episode steps: 100, steps per second: 173, episode reward: 194.000, mean reward: 1.940 [1.474, 4.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.130, 10.170], loss: 153.333267, mae: 1.150312, mean_q: 4.606599
  9766/100000: episode: 98, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: 184.565, mean reward: 1.846 [1.440, 3.116], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.307, 10.098], loss: 154.055817, mae: 0.893232, mean_q: 4.352193
  9866/100000: episode: 99, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 190.075, mean reward: 1.901 [1.431, 3.625], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.885, 10.124], loss: 153.497467, mae: 0.942458, mean_q: 4.448566
  9966/100000: episode: 100, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 185.136, mean reward: 1.851 [1.456, 2.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.268, 10.240], loss: 0.175816, mae: 0.379935, mean_q: 4.076534
 10066/100000: episode: 101, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 183.934, mean reward: 1.839 [1.455, 3.220], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.541, 10.098], loss: 0.141556, mae: 0.335643, mean_q: 3.945270
 10166/100000: episode: 102, duration: 0.548s, episode steps: 100, steps per second: 182, episode reward: 208.707, mean reward: 2.087 [1.507, 5.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.313, 10.259], loss: 0.109637, mae: 0.309100, mean_q: 3.888893
 10266/100000: episode: 103, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: 197.013, mean reward: 1.970 [1.482, 3.158], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.441, 10.335], loss: 0.108049, mae: 0.304108, mean_q: 3.840781
 10366/100000: episode: 104, duration: 0.543s, episode steps: 100, steps per second: 184, episode reward: 218.344, mean reward: 2.183 [1.517, 5.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.699, 10.098], loss: 0.108969, mae: 0.307576, mean_q: 3.815759
 10466/100000: episode: 105, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 248.183, mean reward: 2.482 [1.469, 4.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.215, 10.098], loss: 0.107006, mae: 0.309457, mean_q: 3.813432
 10566/100000: episode: 106, duration: 0.559s, episode steps: 100, steps per second: 179, episode reward: 197.950, mean reward: 1.980 [1.461, 4.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.455, 10.098], loss: 0.094565, mae: 0.296636, mean_q: 3.802484
 10666/100000: episode: 107, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 182.461, mean reward: 1.825 [1.450, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.305, 10.098], loss: 0.100114, mae: 0.298905, mean_q: 3.799911
 10766/100000: episode: 108, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 174.909, mean reward: 1.749 [1.437, 2.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.445, 10.098], loss: 0.101666, mae: 0.303877, mean_q: 3.804905
 10866/100000: episode: 109, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 186.175, mean reward: 1.862 [1.452, 3.688], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.575, 10.274], loss: 0.108432, mae: 0.304379, mean_q: 3.801195
 10966/100000: episode: 110, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 187.725, mean reward: 1.877 [1.472, 3.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.371, 10.098], loss: 0.097785, mae: 0.305887, mean_q: 3.821799
 11066/100000: episode: 111, duration: 0.551s, episode steps: 100, steps per second: 182, episode reward: 203.648, mean reward: 2.036 [1.478, 3.596], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-1.113, 10.314], loss: 0.106522, mae: 0.310035, mean_q: 3.818246
 11166/100000: episode: 112, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 189.953, mean reward: 1.900 [1.457, 3.578], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.364, 10.277], loss: 0.092282, mae: 0.298019, mean_q: 3.822280
 11266/100000: episode: 113, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 217.443, mean reward: 2.174 [1.479, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.251, 10.321], loss: 0.100364, mae: 0.308695, mean_q: 3.825675
 11366/100000: episode: 114, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 179.699, mean reward: 1.797 [1.458, 3.547], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.457, 10.113], loss: 0.096563, mae: 0.302706, mean_q: 3.829251
 11466/100000: episode: 115, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 191.145, mean reward: 1.911 [1.447, 3.766], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.858, 10.098], loss: 0.097340, mae: 0.299738, mean_q: 3.816706
 11566/100000: episode: 116, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: 193.118, mean reward: 1.931 [1.453, 4.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.096, 10.098], loss: 0.090814, mae: 0.298760, mean_q: 3.826869
 11666/100000: episode: 117, duration: 0.570s, episode steps: 100, steps per second: 176, episode reward: 183.971, mean reward: 1.840 [1.449, 2.828], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.624, 10.098], loss: 0.095680, mae: 0.299547, mean_q: 3.813422
 11766/100000: episode: 118, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 207.399, mean reward: 2.074 [1.455, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.076, 10.263], loss: 0.086070, mae: 0.290101, mean_q: 3.801849
 11866/100000: episode: 119, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 193.934, mean reward: 1.939 [1.441, 3.914], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.202, 10.241], loss: 0.099734, mae: 0.305501, mean_q: 3.810556
 11966/100000: episode: 120, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 186.089, mean reward: 1.861 [1.442, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.314, 10.098], loss: 0.087143, mae: 0.296857, mean_q: 3.826515
 12066/100000: episode: 121, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 209.803, mean reward: 2.098 [1.481, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.523, 10.339], loss: 0.102269, mae: 0.313252, mean_q: 3.840991
 12166/100000: episode: 122, duration: 0.571s, episode steps: 100, steps per second: 175, episode reward: 185.792, mean reward: 1.858 [1.483, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.878, 10.098], loss: 0.092860, mae: 0.302816, mean_q: 3.812568
 12266/100000: episode: 123, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 183.017, mean reward: 1.830 [1.447, 3.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.100, 10.098], loss: 0.102211, mae: 0.302435, mean_q: 3.825529
 12366/100000: episode: 124, duration: 0.554s, episode steps: 100, steps per second: 181, episode reward: 214.648, mean reward: 2.146 [1.468, 4.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.822, 10.304], loss: 0.089575, mae: 0.299306, mean_q: 3.804269
 12466/100000: episode: 125, duration: 0.550s, episode steps: 100, steps per second: 182, episode reward: 184.074, mean reward: 1.841 [1.457, 4.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.750, 10.098], loss: 0.090404, mae: 0.296924, mean_q: 3.827159
 12566/100000: episode: 126, duration: 0.557s, episode steps: 100, steps per second: 179, episode reward: 195.810, mean reward: 1.958 [1.456, 4.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.414, 10.098], loss: 0.100245, mae: 0.310150, mean_q: 3.824621
 12666/100000: episode: 127, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 209.973, mean reward: 2.100 [1.479, 4.490], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.556, 10.427], loss: 0.094490, mae: 0.303185, mean_q: 3.815526
 12766/100000: episode: 128, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 194.697, mean reward: 1.947 [1.477, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.780, 10.098], loss: 0.091444, mae: 0.297148, mean_q: 3.813285
 12866/100000: episode: 129, duration: 0.565s, episode steps: 100, steps per second: 177, episode reward: 191.948, mean reward: 1.919 [1.457, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.387, 10.098], loss: 0.092901, mae: 0.305719, mean_q: 3.823816
 12966/100000: episode: 130, duration: 0.562s, episode steps: 100, steps per second: 178, episode reward: 200.636, mean reward: 2.006 [1.496, 3.546], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-2.174, 10.099], loss: 0.098954, mae: 0.313142, mean_q: 3.840122
 13066/100000: episode: 131, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 194.564, mean reward: 1.946 [1.453, 5.036], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.482, 10.159], loss: 0.097295, mae: 0.302970, mean_q: 3.831038
 13166/100000: episode: 132, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 186.460, mean reward: 1.865 [1.443, 2.907], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.407, 10.098], loss: 0.094794, mae: 0.302917, mean_q: 3.831143
 13266/100000: episode: 133, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 199.514, mean reward: 1.995 [1.442, 9.081], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.867, 10.098], loss: 0.103620, mae: 0.320817, mean_q: 3.834589
 13366/100000: episode: 134, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 195.458, mean reward: 1.955 [1.540, 2.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.043, 10.098], loss: 0.099527, mae: 0.301693, mean_q: 3.836482
 13466/100000: episode: 135, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 199.835, mean reward: 1.998 [1.440, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.514, 10.111], loss: 0.106535, mae: 0.320518, mean_q: 3.855326
 13566/100000: episode: 136, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 191.139, mean reward: 1.911 [1.487, 3.103], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.083, 10.099], loss: 0.102893, mae: 0.313715, mean_q: 3.855243
 13666/100000: episode: 137, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 213.631, mean reward: 2.136 [1.517, 5.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.398, 10.324], loss: 0.103985, mae: 0.317124, mean_q: 3.862469
 13766/100000: episode: 138, duration: 0.567s, episode steps: 100, steps per second: 176, episode reward: 191.337, mean reward: 1.913 [1.439, 3.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.022, 10.098], loss: 0.100624, mae: 0.308160, mean_q: 3.863077
 13866/100000: episode: 139, duration: 0.545s, episode steps: 100, steps per second: 184, episode reward: 181.406, mean reward: 1.814 [1.458, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.856, 10.121], loss: 0.101219, mae: 0.316978, mean_q: 3.865973
 13966/100000: episode: 140, duration: 0.553s, episode steps: 100, steps per second: 181, episode reward: 181.185, mean reward: 1.812 [1.477, 3.439], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.400, 10.138], loss: 0.119079, mae: 0.333679, mean_q: 3.883417
 14066/100000: episode: 141, duration: 0.552s, episode steps: 100, steps per second: 181, episode reward: 197.035, mean reward: 1.970 [1.532, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.244, 10.098], loss: 0.109289, mae: 0.318006, mean_q: 3.871472
 14166/100000: episode: 142, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 206.410, mean reward: 2.064 [1.446, 4.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.924, 10.198], loss: 0.104458, mae: 0.314447, mean_q: 3.855346
 14266/100000: episode: 143, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 196.374, mean reward: 1.964 [1.505, 3.440], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.544, 10.172], loss: 0.119348, mae: 0.319947, mean_q: 3.868581
 14366/100000: episode: 144, duration: 0.557s, episode steps: 100, steps per second: 180, episode reward: 191.149, mean reward: 1.911 [1.476, 4.662], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.894, 10.286], loss: 0.120050, mae: 0.330198, mean_q: 3.885919
 14466/100000: episode: 145, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 198.476, mean reward: 1.985 [1.451, 3.143], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.813, 10.128], loss: 0.117270, mae: 0.326365, mean_q: 3.875882
 14566/100000: episode: 146, duration: 0.539s, episode steps: 100, steps per second: 185, episode reward: 194.818, mean reward: 1.948 [1.511, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.047, 10.098], loss: 0.097818, mae: 0.310292, mean_q: 3.878527
 14666/100000: episode: 147, duration: 0.549s, episode steps: 100, steps per second: 182, episode reward: 183.756, mean reward: 1.838 [1.451, 3.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.676, 10.150], loss: 0.095248, mae: 0.307873, mean_q: 3.857616
 14766/100000: episode: 148, duration: 0.567s, episode steps: 100, steps per second: 177, episode reward: 198.958, mean reward: 1.990 [1.452, 3.916], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.922, 10.098], loss: 0.092147, mae: 0.303353, mean_q: 3.859848
 14866/100000: episode: 149, duration: 0.564s, episode steps: 100, steps per second: 177, episode reward: 183.545, mean reward: 1.835 [1.445, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.527, 10.183], loss: 0.118531, mae: 0.321779, mean_q: 3.876359
[Info] 1-TH LEVEL FOUND: 4.691474437713623, Considering 10/90 traces
 14966/100000: episode: 150, duration: 5.123s, episode steps: 100, steps per second: 20, episode reward: 209.658, mean reward: 2.097 [1.454, 4.156], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.805, 10.098], loss: 0.101587, mae: 0.314439, mean_q: 3.875498
 15017/100000: episode: 151, duration: 0.340s, episode steps: 51, steps per second: 150, episode reward: 113.021, mean reward: 2.216 [1.536, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.282, 10.386], loss: 0.096473, mae: 0.314825, mean_q: 3.883137
 15068/100000: episode: 152, duration: 0.244s, episode steps: 51, steps per second: 209, episode reward: 111.976, mean reward: 2.196 [1.508, 5.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-1.635, 10.100], loss: 0.103277, mae: 0.315498, mean_q: 3.893519
 15119/100000: episode: 153, duration: 0.280s, episode steps: 51, steps per second: 182, episode reward: 99.463, mean reward: 1.950 [1.444, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-1.258, 10.159], loss: 0.101244, mae: 0.318551, mean_q: 3.886461
 15167/100000: episode: 154, duration: 0.247s, episode steps: 48, steps per second: 195, episode reward: 100.017, mean reward: 2.084 [1.550, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.922 [-1.695, 10.310], loss: 0.097959, mae: 0.321305, mean_q: 3.908089
 15218/100000: episode: 155, duration: 0.260s, episode steps: 51, steps per second: 196, episode reward: 106.078, mean reward: 2.080 [1.440, 4.199], mean action: 0.000 [0.000, 0.000], mean observation: 1.888 [-0.394, 10.220], loss: 0.104977, mae: 0.322072, mean_q: 3.876959
 15264/100000: episode: 156, duration: 0.223s, episode steps: 46, steps per second: 206, episode reward: 92.192, mean reward: 2.004 [1.490, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.497, 10.163], loss: 0.125580, mae: 0.332329, mean_q: 3.902714
 15311/100000: episode: 157, duration: 0.255s, episode steps: 47, steps per second: 184, episode reward: 98.731, mean reward: 2.101 [1.639, 3.038], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-1.085, 10.264], loss: 0.093787, mae: 0.308847, mean_q: 3.894864
 15362/100000: episode: 158, duration: 0.251s, episode steps: 51, steps per second: 203, episode reward: 90.840, mean reward: 1.781 [1.470, 2.938], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.274, 10.100], loss: 0.158879, mae: 0.345192, mean_q: 3.871981
 15412/100000: episode: 159, duration: 0.265s, episode steps: 50, steps per second: 189, episode reward: 113.651, mean reward: 2.273 [1.544, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.876 [-0.444, 10.100], loss: 0.084313, mae: 0.297023, mean_q: 3.891164
 15463/100000: episode: 160, duration: 0.265s, episode steps: 51, steps per second: 192, episode reward: 113.758, mean reward: 2.231 [1.549, 3.174], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-1.557, 10.219], loss: 0.104170, mae: 0.297175, mean_q: 3.853180
 15514/100000: episode: 161, duration: 0.256s, episode steps: 51, steps per second: 199, episode reward: 126.074, mean reward: 2.472 [1.756, 5.304], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.667, 10.406], loss: 0.101111, mae: 0.315812, mean_q: 3.887505
 15564/100000: episode: 162, duration: 0.245s, episode steps: 50, steps per second: 204, episode reward: 105.069, mean reward: 2.101 [1.542, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.913 [-1.512, 10.342], loss: 0.127633, mae: 0.320929, mean_q: 3.850369
 15615/100000: episode: 163, duration: 0.262s, episode steps: 51, steps per second: 195, episode reward: 109.092, mean reward: 2.139 [1.655, 3.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.903 [-1.128, 10.299], loss: 0.128933, mae: 0.331782, mean_q: 3.914583
 15665/100000: episode: 164, duration: 0.247s, episode steps: 50, steps per second: 202, episode reward: 153.406, mean reward: 3.068 [2.003, 4.654], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.316, 10.431], loss: 0.121789, mae: 0.335529, mean_q: 3.910787
 15711/100000: episode: 165, duration: 0.231s, episode steps: 46, steps per second: 199, episode reward: 87.587, mean reward: 1.904 [1.450, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.165, 10.190], loss: 0.109337, mae: 0.325836, mean_q: 3.906617
 15761/100000: episode: 166, duration: 0.266s, episode steps: 50, steps per second: 188, episode reward: 101.750, mean reward: 2.035 [1.499, 2.898], mean action: 0.000 [0.000, 0.000], mean observation: 1.910 [-0.550, 10.218], loss: 0.128287, mae: 0.332516, mean_q: 3.945040
 15812/100000: episode: 167, duration: 0.251s, episode steps: 51, steps per second: 203, episode reward: 98.696, mean reward: 1.935 [1.502, 2.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.892 [-0.707, 10.223], loss: 0.093165, mae: 0.310533, mean_q: 3.907381
 15858/100000: episode: 168, duration: 0.241s, episode steps: 46, steps per second: 191, episode reward: 89.153, mean reward: 1.938 [1.575, 2.762], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.733, 10.168], loss: 0.143624, mae: 0.342785, mean_q: 3.955332
 15909/100000: episode: 169, duration: 0.252s, episode steps: 51, steps per second: 202, episode reward: 108.059, mean reward: 2.119 [1.589, 4.172], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.834, 10.165], loss: 0.114913, mae: 0.342264, mean_q: 3.946728
 15955/100000: episode: 170, duration: 0.226s, episode steps: 46, steps per second: 204, episode reward: 97.285, mean reward: 2.115 [1.534, 4.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.170, 10.100], loss: 0.112916, mae: 0.329541, mean_q: 3.949974
 16006/100000: episode: 171, duration: 0.250s, episode steps: 51, steps per second: 204, episode reward: 92.011, mean reward: 1.804 [1.487, 3.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.783, 10.100], loss: 0.113843, mae: 0.320771, mean_q: 3.937459
 16103/100000: episode: 172, duration: 0.475s, episode steps: 97, steps per second: 204, episode reward: 194.537, mean reward: 2.006 [1.446, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.478 [-1.022, 10.100], loss: 0.122049, mae: 0.331276, mean_q: 3.940027
 16149/100000: episode: 173, duration: 0.236s, episode steps: 46, steps per second: 195, episode reward: 119.665, mean reward: 2.601 [1.825, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-1.289, 10.304], loss: 0.098241, mae: 0.321142, mean_q: 3.915166
 16196/100000: episode: 174, duration: 0.250s, episode steps: 47, steps per second: 188, episode reward: 87.784, mean reward: 1.868 [1.568, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-0.239, 10.111], loss: 0.115748, mae: 0.340375, mean_q: 3.966011
 16247/100000: episode: 175, duration: 0.246s, episode steps: 51, steps per second: 207, episode reward: 102.088, mean reward: 2.002 [1.484, 3.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.880 [-0.789, 10.100], loss: 0.127140, mae: 0.338232, mean_q: 3.968938
 16294/100000: episode: 176, duration: 0.231s, episode steps: 47, steps per second: 204, episode reward: 93.327, mean reward: 1.986 [1.478, 3.230], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.752, 10.233], loss: 0.090451, mae: 0.311622, mean_q: 3.912887
 16345/100000: episode: 177, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 136.216, mean reward: 2.671 [1.825, 4.047], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.460, 10.306], loss: 0.100836, mae: 0.325111, mean_q: 3.956111
 16442/100000: episode: 178, duration: 0.480s, episode steps: 97, steps per second: 202, episode reward: 176.076, mean reward: 1.815 [1.443, 3.376], mean action: 0.000 [0.000, 0.000], mean observation: 1.482 [-1.540, 10.100], loss: 0.106052, mae: 0.324168, mean_q: 3.926477
 16493/100000: episode: 179, duration: 0.258s, episode steps: 51, steps per second: 198, episode reward: 130.933, mean reward: 2.567 [1.938, 4.183], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-0.783, 10.391], loss: 0.115392, mae: 0.319339, mean_q: 3.954160
 16544/100000: episode: 180, duration: 0.258s, episode steps: 51, steps per second: 198, episode reward: 125.033, mean reward: 2.452 [1.498, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-1.015, 10.427], loss: 0.091470, mae: 0.314741, mean_q: 3.947006
 16592/100000: episode: 181, duration: 0.255s, episode steps: 48, steps per second: 188, episode reward: 97.233, mean reward: 2.026 [1.486, 3.314], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.217, 10.100], loss: 0.110186, mae: 0.339471, mean_q: 3.959302
 16643/100000: episode: 182, duration: 0.266s, episode steps: 51, steps per second: 192, episode reward: 106.435, mean reward: 2.087 [1.440, 4.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.877 [-1.337, 10.108], loss: 0.099222, mae: 0.327334, mean_q: 3.996855
 16694/100000: episode: 183, duration: 0.256s, episode steps: 51, steps per second: 199, episode reward: 100.811, mean reward: 1.977 [1.646, 2.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.755, 10.320], loss: 0.097762, mae: 0.321518, mean_q: 3.962973
 16745/100000: episode: 184, duration: 0.263s, episode steps: 51, steps per second: 194, episode reward: 98.283, mean reward: 1.927 [1.494, 3.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.289, 10.100], loss: 0.113158, mae: 0.335666, mean_q: 3.992164
 16792/100000: episode: 185, duration: 0.240s, episode steps: 47, steps per second: 196, episode reward: 117.586, mean reward: 2.502 [1.721, 7.656], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.213, 10.336], loss: 0.128825, mae: 0.338450, mean_q: 3.979000
 16843/100000: episode: 186, duration: 0.252s, episode steps: 51, steps per second: 202, episode reward: 112.359, mean reward: 2.203 [1.443, 4.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.884 [-0.585, 10.100], loss: 0.121422, mae: 0.350682, mean_q: 3.993424
 16890/100000: episode: 187, duration: 0.240s, episode steps: 47, steps per second: 196, episode reward: 100.108, mean reward: 2.130 [1.582, 3.416], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.860, 10.336], loss: 0.120604, mae: 0.346571, mean_q: 3.976739
 16987/100000: episode: 188, duration: 0.489s, episode steps: 97, steps per second: 198, episode reward: 201.052, mean reward: 2.073 [1.467, 5.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.488 [-0.201, 10.357], loss: 0.128920, mae: 0.338693, mean_q: 3.981965
 17035/100000: episode: 189, duration: 0.236s, episode steps: 48, steps per second: 204, episode reward: 108.341, mean reward: 2.257 [1.687, 3.098], mean action: 0.000 [0.000, 0.000], mean observation: 1.920 [-0.535, 10.400], loss: 0.113779, mae: 0.341506, mean_q: 3.968554
 17085/100000: episode: 190, duration: 0.248s, episode steps: 50, steps per second: 201, episode reward: 95.320, mean reward: 1.906 [1.491, 2.293], mean action: 0.000 [0.000, 0.000], mean observation: 1.904 [-0.249, 10.100], loss: 0.124945, mae: 0.352502, mean_q: 3.972146
 17133/100000: episode: 191, duration: 0.254s, episode steps: 48, steps per second: 189, episode reward: 85.083, mean reward: 1.773 [1.453, 2.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.423, 10.161], loss: 0.098251, mae: 0.321670, mean_q: 3.992533
 17181/100000: episode: 192, duration: 0.237s, episode steps: 48, steps per second: 203, episode reward: 93.928, mean reward: 1.957 [1.639, 2.785], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-1.057, 10.223], loss: 0.124448, mae: 0.349493, mean_q: 4.016217
 17232/100000: episode: 193, duration: 0.265s, episode steps: 51, steps per second: 193, episode reward: 119.855, mean reward: 2.350 [1.809, 3.309], mean action: 0.000 [0.000, 0.000], mean observation: 1.905 [-0.295, 10.353], loss: 0.106358, mae: 0.337448, mean_q: 4.011683
 17278/100000: episode: 194, duration: 0.228s, episode steps: 46, steps per second: 202, episode reward: 92.743, mean reward: 2.016 [1.496, 3.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.940 [-1.583, 10.217], loss: 0.124674, mae: 0.326675, mean_q: 4.032433
 17328/100000: episode: 195, duration: 0.261s, episode steps: 50, steps per second: 191, episode reward: 125.350, mean reward: 2.507 [1.811, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-1.005, 10.371], loss: 0.103108, mae: 0.322622, mean_q: 3.989841
 17379/100000: episode: 196, duration: 0.269s, episode steps: 51, steps per second: 190, episode reward: 104.681, mean reward: 2.053 [1.493, 3.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.949, 10.409], loss: 0.111822, mae: 0.332575, mean_q: 4.005660
 17430/100000: episode: 197, duration: 0.265s, episode steps: 51, steps per second: 193, episode reward: 116.459, mean reward: 2.284 [1.752, 3.079], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.343, 10.210], loss: 0.109475, mae: 0.342954, mean_q: 4.021574
 17476/100000: episode: 198, duration: 0.236s, episode steps: 46, steps per second: 195, episode reward: 107.308, mean reward: 2.333 [1.596, 5.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.163, 10.174], loss: 0.115047, mae: 0.343628, mean_q: 4.040008
 17526/100000: episode: 199, duration: 0.255s, episode steps: 50, steps per second: 196, episode reward: 93.772, mean reward: 1.875 [1.474, 3.044], mean action: 0.000 [0.000, 0.000], mean observation: 1.894 [-0.799, 10.100], loss: 0.118925, mae: 0.335508, mean_q: 4.003334
 17577/100000: episode: 200, duration: 0.260s, episode steps: 51, steps per second: 196, episode reward: 97.602, mean reward: 1.914 [1.560, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.512, 10.243], loss: 0.117243, mae: 0.344258, mean_q: 4.041513
 17625/100000: episode: 201, duration: 0.255s, episode steps: 48, steps per second: 188, episode reward: 94.365, mean reward: 1.966 [1.514, 3.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.915 [-0.628, 10.100], loss: 0.119846, mae: 0.343960, mean_q: 4.031781
 17676/100000: episode: 202, duration: 0.261s, episode steps: 51, steps per second: 195, episode reward: 98.891, mean reward: 1.939 [1.475, 3.172], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-1.014, 10.100], loss: 0.106198, mae: 0.339874, mean_q: 4.013991
 17722/100000: episode: 203, duration: 0.227s, episode steps: 46, steps per second: 203, episode reward: 112.774, mean reward: 2.452 [1.850, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.156, 10.396], loss: 0.104320, mae: 0.336805, mean_q: 4.008938
 17773/100000: episode: 204, duration: 0.269s, episode steps: 51, steps per second: 189, episode reward: 93.516, mean reward: 1.834 [1.450, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 1.893 [-0.264, 10.100], loss: 0.116941, mae: 0.338405, mean_q: 4.044486
 17824/100000: episode: 205, duration: 0.256s, episode steps: 51, steps per second: 199, episode reward: 99.462, mean reward: 1.950 [1.606, 2.378], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-1.060, 10.327], loss: 0.150075, mae: 0.366325, mean_q: 4.042624
 17875/100000: episode: 206, duration: 0.247s, episode steps: 51, steps per second: 207, episode reward: 109.784, mean reward: 2.153 [1.453, 2.869], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-1.036, 10.109], loss: 0.107427, mae: 0.339058, mean_q: 4.024923
 17925/100000: episode: 207, duration: 0.256s, episode steps: 50, steps per second: 196, episode reward: 91.439, mean reward: 1.829 [1.494, 2.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.238, 10.186], loss: 0.096447, mae: 0.321526, mean_q: 4.006134
 17973/100000: episode: 208, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 116.353, mean reward: 2.424 [1.447, 9.948], mean action: 0.000 [0.000, 0.000], mean observation: 1.916 [-0.577, 10.100], loss: 0.122627, mae: 0.345630, mean_q: 4.035346
 18024/100000: episode: 209, duration: 0.250s, episode steps: 51, steps per second: 204, episode reward: 104.806, mean reward: 2.055 [1.562, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 1.890 [-1.270, 10.302], loss: 0.133473, mae: 0.347532, mean_q: 4.060583
 18071/100000: episode: 210, duration: 0.238s, episode steps: 47, steps per second: 197, episode reward: 94.438, mean reward: 2.009 [1.645, 3.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-0.208, 10.190], loss: 0.105034, mae: 0.329340, mean_q: 4.009932
 18119/100000: episode: 211, duration: 0.240s, episode steps: 48, steps per second: 200, episode reward: 95.381, mean reward: 1.987 [1.500, 2.936], mean action: 0.000 [0.000, 0.000], mean observation: 1.921 [-0.917, 10.100], loss: 0.117178, mae: 0.346239, mean_q: 4.014572
 18166/100000: episode: 212, duration: 0.239s, episode steps: 47, steps per second: 197, episode reward: 87.991, mean reward: 1.872 [1.459, 3.449], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.478, 10.155], loss: 0.148414, mae: 0.359992, mean_q: 4.023434
 18214/100000: episode: 213, duration: 0.242s, episode steps: 48, steps per second: 199, episode reward: 91.966, mean reward: 1.916 [1.514, 2.981], mean action: 0.000 [0.000, 0.000], mean observation: 1.917 [-0.627, 10.166], loss: 0.110616, mae: 0.349302, mean_q: 4.067666
 18260/100000: episode: 214, duration: 0.235s, episode steps: 46, steps per second: 196, episode reward: 92.800, mean reward: 2.017 [1.456, 3.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.149, 10.175], loss: 0.129762, mae: 0.341137, mean_q: 4.060424
 18311/100000: episode: 215, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 159.351, mean reward: 3.125 [1.996, 4.880], mean action: 0.000 [0.000, 0.000], mean observation: 1.883 [-1.366, 10.426], loss: 0.111267, mae: 0.341320, mean_q: 4.024548
 18362/100000: episode: 216, duration: 0.243s, episode steps: 51, steps per second: 210, episode reward: 100.658, mean reward: 1.974 [1.508, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.306, 10.353], loss: 0.132679, mae: 0.346771, mean_q: 4.068713
 18413/100000: episode: 217, duration: 0.249s, episode steps: 51, steps per second: 205, episode reward: 102.063, mean reward: 2.001 [1.521, 2.519], mean action: 0.000 [0.000, 0.000], mean observation: 1.897 [-0.630, 10.123], loss: 0.131861, mae: 0.352514, mean_q: 4.081539
 18463/100000: episode: 218, duration: 0.257s, episode steps: 50, steps per second: 194, episode reward: 93.466, mean reward: 1.869 [1.519, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 1.895 [-0.660, 10.157], loss: 0.161315, mae: 0.359728, mean_q: 4.094696
 18509/100000: episode: 219, duration: 0.222s, episode steps: 46, steps per second: 207, episode reward: 102.204, mean reward: 2.222 [1.554, 3.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.307, 10.259], loss: 0.154658, mae: 0.360281, mean_q: 4.073322
 18560/100000: episode: 220, duration: 0.252s, episode steps: 51, steps per second: 202, episode reward: 109.392, mean reward: 2.145 [1.472, 4.006], mean action: 0.000 [0.000, 0.000], mean observation: 1.878 [-0.493, 10.111], loss: 0.123156, mae: 0.352131, mean_q: 4.094656
 18611/100000: episode: 221, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 109.473, mean reward: 2.147 [1.558, 4.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.907 [-0.627, 10.487], loss: 0.131509, mae: 0.350778, mean_q: 4.066395
 18662/100000: episode: 222, duration: 0.259s, episode steps: 51, steps per second: 197, episode reward: 97.941, mean reward: 1.920 [1.442, 4.059], mean action: 0.000 [0.000, 0.000], mean observation: 1.889 [-1.016, 10.106], loss: 0.154652, mae: 0.352296, mean_q: 4.099518
 18708/100000: episode: 223, duration: 0.233s, episode steps: 46, steps per second: 197, episode reward: 112.881, mean reward: 2.454 [1.800, 4.142], mean action: 0.000 [0.000, 0.000], mean observation: 1.943 [-0.399, 10.555], loss: 0.113308, mae: 0.343733, mean_q: 4.098012
 18754/100000: episode: 224, duration: 0.261s, episode steps: 46, steps per second: 177, episode reward: 97.038, mean reward: 2.110 [1.569, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.875, 10.203], loss: 0.107050, mae: 0.322306, mean_q: 4.064005
 18805/100000: episode: 225, duration: 0.246s, episode steps: 51, steps per second: 207, episode reward: 102.055, mean reward: 2.001 [1.501, 5.474], mean action: 0.000 [0.000, 0.000], mean observation: 1.881 [-1.046, 10.100], loss: 0.133783, mae: 0.354687, mean_q: 4.110743
 18856/100000: episode: 226, duration: 0.260s, episode steps: 51, steps per second: 196, episode reward: 109.977, mean reward: 2.156 [1.553, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.901 [-0.291, 10.240], loss: 0.118997, mae: 0.346152, mean_q: 4.100303
 18904/100000: episode: 227, duration: 0.244s, episode steps: 48, steps per second: 197, episode reward: 97.379, mean reward: 2.029 [1.577, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-0.697, 10.345], loss: 0.116747, mae: 0.343029, mean_q: 4.072181
 18955/100000: episode: 228, duration: 0.309s, episode steps: 51, steps per second: 165, episode reward: 120.077, mean reward: 2.354 [1.753, 3.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.899 [-0.591, 10.453], loss: 0.136239, mae: 0.339886, mean_q: 4.062875
 19006/100000: episode: 229, duration: 0.253s, episode steps: 51, steps per second: 201, episode reward: 96.837, mean reward: 1.899 [1.439, 2.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.941, 10.109], loss: 0.096782, mae: 0.320616, mean_q: 4.059243
 19053/100000: episode: 230, duration: 0.248s, episode steps: 47, steps per second: 190, episode reward: 101.783, mean reward: 2.166 [1.611, 3.804], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-0.830, 10.192], loss: 0.154176, mae: 0.341062, mean_q: 4.079538
 19103/100000: episode: 231, duration: 0.233s, episode steps: 50, steps per second: 214, episode reward: 107.755, mean reward: 2.155 [1.754, 3.330], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-1.143, 10.398], loss: 0.133163, mae: 0.343255, mean_q: 4.102145
 19154/100000: episode: 232, duration: 0.249s, episode steps: 51, steps per second: 205, episode reward: 98.080, mean reward: 1.923 [1.544, 3.897], mean action: 0.000 [0.000, 0.000], mean observation: 1.887 [-0.291, 10.100], loss: 0.137751, mae: 0.345635, mean_q: 4.121265
 19200/100000: episode: 233, duration: 0.236s, episode steps: 46, steps per second: 195, episode reward: 98.959, mean reward: 2.151 [1.682, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.963, 10.287], loss: 0.138471, mae: 0.360264, mean_q: 4.096340
 19251/100000: episode: 234, duration: 0.269s, episode steps: 51, steps per second: 190, episode reward: 100.392, mean reward: 1.968 [1.454, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.898 [-0.293, 10.246], loss: 0.103389, mae: 0.326454, mean_q: 4.073797
 19302/100000: episode: 235, duration: 0.256s, episode steps: 51, steps per second: 199, episode reward: 95.012, mean reward: 1.863 [1.534, 2.517], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-0.366, 10.209], loss: 0.126407, mae: 0.368408, mean_q: 4.116298
 19353/100000: episode: 236, duration: 0.268s, episode steps: 51, steps per second: 190, episode reward: 104.265, mean reward: 2.044 [1.660, 3.170], mean action: 0.000 [0.000, 0.000], mean observation: 1.896 [-0.509, 10.310], loss: 0.139784, mae: 0.370871, mean_q: 4.063655
 19400/100000: episode: 237, duration: 0.239s, episode steps: 47, steps per second: 196, episode reward: 104.487, mean reward: 2.223 [1.750, 3.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.527, 10.311], loss: 0.139610, mae: 0.357133, mean_q: 4.093064
 19451/100000: episode: 238, duration: 0.260s, episode steps: 51, steps per second: 196, episode reward: 120.193, mean reward: 2.357 [1.547, 11.829], mean action: 0.000 [0.000, 0.000], mean observation: 1.891 [-1.096, 10.243], loss: 0.133248, mae: 0.344859, mean_q: 4.107071
 19498/100000: episode: 239, duration: 0.234s, episode steps: 47, steps per second: 201, episode reward: 81.762, mean reward: 1.740 [1.479, 3.529], mean action: 0.000 [0.000, 0.000], mean observation: 1.919 [-1.361, 10.100], loss: 0.166252, mae: 0.347422, mean_q: 4.079149
[Info] 2-TH LEVEL FOUND: 5.200882911682129, Considering 10/90 traces
 19544/100000: episode: 240, duration: 4.425s, episode steps: 46, steps per second: 10, episode reward: 110.950, mean reward: 2.412 [1.761, 5.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-1.536, 10.271], loss: 0.189745, mae: 0.387163, mean_q: 4.145757
 19576/100000: episode: 241, duration: 0.169s, episode steps: 32, steps per second: 190, episode reward: 61.370, mean reward: 1.918 [1.600, 3.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-1.284, 10.110], loss: 0.116101, mae: 0.336288, mean_q: 4.078076
 19609/100000: episode: 242, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 127.621, mean reward: 3.867 [1.947, 14.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.837, 10.538], loss: 0.176416, mae: 0.392040, mean_q: 4.164198
 19642/100000: episode: 243, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 73.714, mean reward: 2.234 [1.664, 2.885], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.823, 10.247], loss: 0.130182, mae: 0.349602, mean_q: 4.113590
 19686/100000: episode: 244, duration: 0.241s, episode steps: 44, steps per second: 182, episode reward: 127.691, mean reward: 2.902 [2.275, 3.670], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.192, 10.453], loss: 0.152320, mae: 0.341627, mean_q: 4.137183
 19730/100000: episode: 245, duration: 0.214s, episode steps: 44, steps per second: 205, episode reward: 86.489, mean reward: 1.966 [1.444, 3.518], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.666, 10.109], loss: 0.169942, mae: 0.384712, mean_q: 4.162243
 19775/100000: episode: 246, duration: 0.237s, episode steps: 45, steps per second: 190, episode reward: 110.662, mean reward: 2.459 [1.622, 3.248], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.230, 10.192], loss: 0.271363, mae: 0.400351, mean_q: 4.209831
 19816/100000: episode: 247, duration: 0.207s, episode steps: 41, steps per second: 198, episode reward: 247.573, mean reward: 6.038 [2.331, 14.030], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.771, 10.542], loss: 0.145837, mae: 0.358477, mean_q: 4.141195
 19848/100000: episode: 248, duration: 0.155s, episode steps: 32, steps per second: 207, episode reward: 64.813, mean reward: 2.025 [1.472, 2.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.502, 10.100], loss: 0.191030, mae: 0.381494, mean_q: 4.223324
 19889/100000: episode: 249, duration: 0.212s, episode steps: 41, steps per second: 194, episode reward: 82.658, mean reward: 2.016 [1.453, 3.053], mean action: 0.000 [0.000, 0.000], mean observation: 1.970 [-0.247, 10.100], loss: 0.287465, mae: 0.407806, mean_q: 4.246267
 19922/100000: episode: 250, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 66.310, mean reward: 2.009 [1.539, 2.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.090, 10.184], loss: 0.155642, mae: 0.360975, mean_q: 4.172644
 19968/100000: episode: 251, duration: 0.225s, episode steps: 46, steps per second: 205, episode reward: 118.009, mean reward: 2.565 [1.533, 4.845], mean action: 0.000 [0.000, 0.000], mean observation: 1.933 [-0.614, 10.140], loss: 0.258886, mae: 0.425319, mean_q: 4.200726
 20013/100000: episode: 252, duration: 0.229s, episode steps: 45, steps per second: 196, episode reward: 90.555, mean reward: 2.012 [1.464, 3.290], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.305, 10.117], loss: 0.147716, mae: 0.379445, mean_q: 4.157442
 20057/100000: episode: 253, duration: 0.243s, episode steps: 44, steps per second: 181, episode reward: 105.090, mean reward: 2.388 [1.684, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-1.698, 10.366], loss: 0.364236, mae: 0.428827, mean_q: 4.196256
 20088/100000: episode: 254, duration: 0.157s, episode steps: 31, steps per second: 198, episode reward: 81.532, mean reward: 2.630 [1.824, 5.534], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.600, 10.245], loss: 0.221394, mae: 0.395928, mean_q: 4.198620
 20121/100000: episode: 255, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 88.351, mean reward: 2.677 [1.882, 4.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.057, 10.336], loss: 0.361095, mae: 0.445319, mean_q: 4.258018
 20154/100000: episode: 256, duration: 0.167s, episode steps: 33, steps per second: 197, episode reward: 90.284, mean reward: 2.736 [1.899, 3.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.035, 10.425], loss: 0.171704, mae: 0.371549, mean_q: 4.200906
 20195/100000: episode: 257, duration: 0.205s, episode steps: 41, steps per second: 200, episode reward: 148.994, mean reward: 3.634 [2.441, 10.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.986 [-0.797, 10.429], loss: 0.177438, mae: 0.406010, mean_q: 4.154398
 20236/100000: episode: 258, duration: 0.212s, episode steps: 41, steps per second: 193, episode reward: 85.120, mean reward: 2.076 [1.482, 3.573], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.665, 10.171], loss: 0.149300, mae: 0.377359, mean_q: 4.192719
 20268/100000: episode: 259, duration: 0.166s, episode steps: 32, steps per second: 192, episode reward: 83.415, mean reward: 2.607 [2.058, 3.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-1.380, 10.422], loss: 0.337129, mae: 0.455473, mean_q: 4.305308
 20313/100000: episode: 260, duration: 0.220s, episode steps: 45, steps per second: 205, episode reward: 120.724, mean reward: 2.683 [1.889, 3.450], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-1.556, 10.398], loss: 0.242731, mae: 0.396168, mean_q: 4.230721
 20358/100000: episode: 261, duration: 0.241s, episode steps: 45, steps per second: 187, episode reward: 99.449, mean reward: 2.210 [1.806, 3.749], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.214, 10.290], loss: 0.285035, mae: 0.429620, mean_q: 4.266981
 20399/100000: episode: 262, duration: 0.199s, episode steps: 41, steps per second: 206, episode reward: 105.679, mean reward: 2.578 [1.462, 4.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.973 [-0.115, 10.109], loss: 0.153103, mae: 0.366388, mean_q: 4.278647
 20440/100000: episode: 263, duration: 0.204s, episode steps: 41, steps per second: 201, episode reward: 101.673, mean reward: 2.480 [1.860, 4.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.789, 10.243], loss: 0.258076, mae: 0.410548, mean_q: 4.309130
 20481/100000: episode: 264, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 94.399, mean reward: 2.302 [1.686, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-0.600, 10.208], loss: 0.157989, mae: 0.365798, mean_q: 4.306791
 20505/100000: episode: 265, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 68.769, mean reward: 2.865 [2.286, 4.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.399], loss: 0.160740, mae: 0.353502, mean_q: 4.256569
 20549/100000: episode: 266, duration: 0.221s, episode steps: 44, steps per second: 199, episode reward: 109.511, mean reward: 2.489 [1.692, 4.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-0.519, 10.192], loss: 0.246206, mae: 0.397062, mean_q: 4.335643
 20593/100000: episode: 267, duration: 0.224s, episode steps: 44, steps per second: 197, episode reward: 111.922, mean reward: 2.544 [1.717, 4.120], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.405, 10.415], loss: 0.231959, mae: 0.407580, mean_q: 4.333552
 20634/100000: episode: 268, duration: 0.198s, episode steps: 41, steps per second: 207, episode reward: 79.127, mean reward: 1.930 [1.556, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-0.822, 10.100], loss: 0.218887, mae: 0.373569, mean_q: 4.279752
 20678/100000: episode: 269, duration: 0.239s, episode steps: 44, steps per second: 184, episode reward: 101.414, mean reward: 2.305 [1.439, 8.770], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.191, 10.100], loss: 0.258461, mae: 0.413522, mean_q: 4.320615
 20722/100000: episode: 270, duration: 0.227s, episode steps: 44, steps per second: 194, episode reward: 117.147, mean reward: 2.662 [2.114, 3.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-1.215, 10.545], loss: 0.206755, mae: 0.367835, mean_q: 4.305156
 20755/100000: episode: 271, duration: 0.158s, episode steps: 33, steps per second: 208, episode reward: 75.109, mean reward: 2.276 [1.810, 2.994], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.295, 10.307], loss: 0.171425, mae: 0.358037, mean_q: 4.258513
 20796/100000: episode: 272, duration: 0.216s, episode steps: 41, steps per second: 190, episode reward: 110.998, mean reward: 2.707 [2.109, 4.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-1.313, 10.371], loss: 0.203348, mae: 0.372571, mean_q: 4.337391
 20828/100000: episode: 273, duration: 0.155s, episode steps: 32, steps per second: 207, episode reward: 112.814, mean reward: 3.525 [2.026, 6.733], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-1.479, 10.351], loss: 0.335696, mae: 0.430135, mean_q: 4.360750
 20872/100000: episode: 274, duration: 0.237s, episode steps: 44, steps per second: 186, episode reward: 103.954, mean reward: 2.363 [1.560, 5.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.838, 10.288], loss: 0.263603, mae: 0.432595, mean_q: 4.363489
 20913/100000: episode: 275, duration: 0.210s, episode steps: 41, steps per second: 195, episode reward: 94.566, mean reward: 2.306 [1.506, 3.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-1.495, 10.142], loss: 0.191947, mae: 0.394258, mean_q: 4.310824
 20959/100000: episode: 276, duration: 0.225s, episode steps: 46, steps per second: 205, episode reward: 115.387, mean reward: 2.508 [1.944, 3.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-1.436, 10.404], loss: 0.190054, mae: 0.392465, mean_q: 4.379599
 20990/100000: episode: 277, duration: 0.156s, episode steps: 31, steps per second: 198, episode reward: 86.497, mean reward: 2.790 [1.688, 5.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.510, 10.393], loss: 0.412419, mae: 0.491298, mean_q: 4.365856
 21034/100000: episode: 278, duration: 0.223s, episode steps: 44, steps per second: 198, episode reward: 127.775, mean reward: 2.904 [1.905, 6.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.532, 10.340], loss: 0.209848, mae: 0.403009, mean_q: 4.382222
 21078/100000: episode: 279, duration: 0.233s, episode steps: 44, steps per second: 189, episode reward: 121.429, mean reward: 2.760 [1.929, 6.431], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.653, 10.359], loss: 0.190609, mae: 0.386057, mean_q: 4.361798
 21122/100000: episode: 280, duration: 0.214s, episode steps: 44, steps per second: 206, episode reward: 105.023, mean reward: 2.387 [1.476, 3.730], mean action: 0.000 [0.000, 0.000], mean observation: 1.953 [-0.718, 10.162], loss: 0.245039, mae: 0.420426, mean_q: 4.424214
 21163/100000: episode: 281, duration: 0.201s, episode steps: 41, steps per second: 204, episode reward: 83.884, mean reward: 2.046 [1.447, 3.768], mean action: 0.000 [0.000, 0.000], mean observation: 1.982 [-0.228, 10.154], loss: 0.297169, mae: 0.426287, mean_q: 4.438094
 21187/100000: episode: 282, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 69.402, mean reward: 2.892 [2.403, 3.489], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.497, 10.411], loss: 0.320858, mae: 0.412960, mean_q: 4.419641
 21232/100000: episode: 283, duration: 0.224s, episode steps: 45, steps per second: 201, episode reward: 92.958, mean reward: 2.066 [1.555, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-0.900, 10.323], loss: 0.146198, mae: 0.376081, mean_q: 4.418599
 21256/100000: episode: 284, duration: 0.113s, episode steps: 24, steps per second: 212, episode reward: 67.126, mean reward: 2.797 [2.372, 3.399], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.566, 10.486], loss: 0.232542, mae: 0.423670, mean_q: 4.478722
 21297/100000: episode: 285, duration: 0.209s, episode steps: 41, steps per second: 197, episode reward: 138.982, mean reward: 3.390 [1.980, 5.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.980 [-0.821, 10.529], loss: 0.224267, mae: 0.402984, mean_q: 4.409265
 21341/100000: episode: 286, duration: 0.220s, episode steps: 44, steps per second: 200, episode reward: 127.921, mean reward: 2.907 [2.183, 4.105], mean action: 0.000 [0.000, 0.000], mean observation: 1.958 [-0.557, 10.406], loss: 0.267246, mae: 0.428347, mean_q: 4.470358
 21373/100000: episode: 287, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 106.440, mean reward: 3.326 [2.017, 9.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-1.275, 10.507], loss: 0.239467, mae: 0.423829, mean_q: 4.437120
 21406/100000: episode: 288, duration: 0.169s, episode steps: 33, steps per second: 196, episode reward: 106.906, mean reward: 3.240 [2.126, 8.218], mean action: 0.000 [0.000, 0.000], mean observation: 2.040 [-0.231, 10.338], loss: 0.313125, mae: 0.424443, mean_q: 4.509499
 21452/100000: episode: 289, duration: 0.253s, episode steps: 46, steps per second: 182, episode reward: 116.012, mean reward: 2.522 [1.622, 4.364], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.272, 10.254], loss: 0.313904, mae: 0.469685, mean_q: 4.458419
 21476/100000: episode: 290, duration: 0.121s, episode steps: 24, steps per second: 198, episode reward: 54.417, mean reward: 2.267 [1.507, 2.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.387, 10.183], loss: 0.284106, mae: 0.451695, mean_q: 4.483593
 21508/100000: episode: 291, duration: 0.158s, episode steps: 32, steps per second: 203, episode reward: 66.896, mean reward: 2.091 [1.654, 2.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.138, 10.298], loss: 0.211001, mae: 0.415746, mean_q: 4.490816
 21541/100000: episode: 292, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 117.151, mean reward: 3.550 [2.080, 5.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.446, 10.548], loss: 0.283205, mae: 0.425961, mean_q: 4.485149
 21586/100000: episode: 293, duration: 0.227s, episode steps: 45, steps per second: 198, episode reward: 137.974, mean reward: 3.066 [1.773, 5.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.944 [-1.053, 10.342], loss: 0.318745, mae: 0.456348, mean_q: 4.525548
 21610/100000: episode: 294, duration: 0.117s, episode steps: 24, steps per second: 205, episode reward: 75.767, mean reward: 3.157 [2.045, 4.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.173, 10.404], loss: 0.242394, mae: 0.439018, mean_q: 4.557369
 21656/100000: episode: 295, duration: 0.240s, episode steps: 46, steps per second: 191, episode reward: 96.023, mean reward: 2.087 [1.616, 2.806], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.254, 10.244], loss: 0.249204, mae: 0.420847, mean_q: 4.557366
 21680/100000: episode: 296, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 55.243, mean reward: 2.302 [1.600, 3.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.748, 10.179], loss: 0.135624, mae: 0.363491, mean_q: 4.393062
 21704/100000: episode: 297, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 60.680, mean reward: 2.528 [1.849, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.069, 10.274], loss: 0.198013, mae: 0.413736, mean_q: 4.507916
 21749/100000: episode: 298, duration: 0.226s, episode steps: 45, steps per second: 199, episode reward: 94.044, mean reward: 2.090 [1.535, 3.397], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.229, 10.366], loss: 0.208837, mae: 0.409371, mean_q: 4.556345
 21795/100000: episode: 299, duration: 0.233s, episode steps: 46, steps per second: 197, episode reward: 115.732, mean reward: 2.516 [1.915, 4.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.932 [-0.681, 10.375], loss: 0.250461, mae: 0.451315, mean_q: 4.564456
 21826/100000: episode: 300, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 83.376, mean reward: 2.690 [1.874, 5.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.438, 10.293], loss: 0.192083, mae: 0.394553, mean_q: 4.506033
 21859/100000: episode: 301, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 90.644, mean reward: 2.747 [2.174, 3.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.292, 10.424], loss: 0.336620, mae: 0.452896, mean_q: 4.505958
 21904/100000: episode: 302, duration: 0.244s, episode steps: 45, steps per second: 184, episode reward: 147.264, mean reward: 3.273 [1.707, 6.438], mean action: 0.000 [0.000, 0.000], mean observation: 1.937 [-0.587, 10.248], loss: 0.242381, mae: 0.423472, mean_q: 4.539205
 21945/100000: episode: 303, duration: 0.225s, episode steps: 41, steps per second: 182, episode reward: 89.190, mean reward: 2.175 [1.545, 3.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.977 [-0.692, 10.178], loss: 0.286970, mae: 0.438599, mean_q: 4.571425
 21977/100000: episode: 304, duration: 0.165s, episode steps: 32, steps per second: 193, episode reward: 80.900, mean reward: 2.528 [1.781, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.569, 10.251], loss: 0.235143, mae: 0.437590, mean_q: 4.589149
 22021/100000: episode: 305, duration: 0.236s, episode steps: 44, steps per second: 186, episode reward: 99.787, mean reward: 2.268 [1.560, 4.125], mean action: 0.000 [0.000, 0.000], mean observation: 1.954 [-0.377, 10.100], loss: 0.216491, mae: 0.424813, mean_q: 4.611738
 22045/100000: episode: 306, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 70.279, mean reward: 2.928 [2.128, 4.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.234, 10.396], loss: 0.235998, mae: 0.417208, mean_q: 4.562336
 22086/100000: episode: 307, duration: 0.206s, episode steps: 41, steps per second: 199, episode reward: 79.093, mean reward: 1.929 [1.482, 3.329], mean action: 0.000 [0.000, 0.000], mean observation: 1.976 [-0.168, 10.138], loss: 0.233230, mae: 0.435437, mean_q: 4.597614
 22110/100000: episode: 308, duration: 0.120s, episode steps: 24, steps per second: 201, episode reward: 63.688, mean reward: 2.654 [1.665, 4.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.155, 10.246], loss: 0.255825, mae: 0.493175, mean_q: 4.653290
 22142/100000: episode: 309, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 62.886, mean reward: 1.965 [1.493, 2.791], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.682, 10.100], loss: 0.331775, mae: 0.518509, mean_q: 4.550668
 22173/100000: episode: 310, duration: 0.151s, episode steps: 31, steps per second: 206, episode reward: 70.597, mean reward: 2.277 [1.594, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.548, 10.130], loss: 0.229743, mae: 0.447468, mean_q: 4.504935
 22214/100000: episode: 311, duration: 0.229s, episode steps: 41, steps per second: 179, episode reward: 97.181, mean reward: 2.370 [1.899, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.491, 10.374], loss: 0.291887, mae: 0.446652, mean_q: 4.634812
 22255/100000: episode: 312, duration: 0.221s, episode steps: 41, steps per second: 185, episode reward: 101.200, mean reward: 2.468 [1.777, 3.993], mean action: 0.000 [0.000, 0.000], mean observation: 1.987 [-0.358, 10.293], loss: 0.389769, mae: 0.491529, mean_q: 4.600613
 22279/100000: episode: 313, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 57.208, mean reward: 2.384 [1.781, 3.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.390], loss: 0.311602, mae: 0.414905, mean_q: 4.529021
 22320/100000: episode: 314, duration: 0.220s, episode steps: 41, steps per second: 186, episode reward: 89.224, mean reward: 2.176 [1.515, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.633, 10.147], loss: 0.212064, mae: 0.420254, mean_q: 4.605490
 22365/100000: episode: 315, duration: 0.227s, episode steps: 45, steps per second: 198, episode reward: 120.467, mean reward: 2.677 [1.516, 5.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.724, 10.203], loss: 0.274672, mae: 0.435397, mean_q: 4.579270
 22411/100000: episode: 316, duration: 0.236s, episode steps: 46, steps per second: 195, episode reward: 100.958, mean reward: 2.195 [1.492, 3.418], mean action: 0.000 [0.000, 0.000], mean observation: 1.927 [-0.264, 10.168], loss: 0.245144, mae: 0.417438, mean_q: 4.558362
 22452/100000: episode: 317, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 84.742, mean reward: 2.067 [1.498, 3.224], mean action: 0.000 [0.000, 0.000], mean observation: 1.978 [-1.106, 10.125], loss: 0.298639, mae: 0.450157, mean_q: 4.574375
 22497/100000: episode: 318, duration: 0.216s, episode steps: 45, steps per second: 208, episode reward: 100.392, mean reward: 2.231 [1.513, 4.447], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.292, 10.125], loss: 0.221290, mae: 0.426986, mean_q: 4.572185
 22538/100000: episode: 319, duration: 0.211s, episode steps: 41, steps per second: 194, episode reward: 80.783, mean reward: 1.970 [1.479, 2.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-1.384, 10.229], loss: 0.271983, mae: 0.489471, mean_q: 4.672576
 22571/100000: episode: 320, duration: 0.175s, episode steps: 33, steps per second: 189, episode reward: 84.033, mean reward: 2.546 [1.955, 3.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.314, 10.335], loss: 0.172459, mae: 0.396462, mean_q: 4.539201
 22595/100000: episode: 321, duration: 0.125s, episode steps: 24, steps per second: 193, episode reward: 65.621, mean reward: 2.734 [2.107, 3.404], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.035, 10.375], loss: 0.202372, mae: 0.418855, mean_q: 4.613389
 22627/100000: episode: 322, duration: 0.161s, episode steps: 32, steps per second: 198, episode reward: 67.678, mean reward: 2.115 [1.525, 3.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.224, 10.155], loss: 0.207070, mae: 0.410106, mean_q: 4.595034
 22668/100000: episode: 323, duration: 0.203s, episode steps: 41, steps per second: 202, episode reward: 87.282, mean reward: 2.129 [1.499, 4.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.981 [-0.224, 10.102], loss: 0.231457, mae: 0.411791, mean_q: 4.608186
 22701/100000: episode: 324, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 76.146, mean reward: 2.307 [1.598, 3.618], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.077, 10.241], loss: 0.208115, mae: 0.411089, mean_q: 4.591772
 22742/100000: episode: 325, duration: 0.211s, episode steps: 41, steps per second: 194, episode reward: 108.534, mean reward: 2.647 [1.807, 4.720], mean action: 0.000 [0.000, 0.000], mean observation: 1.979 [-0.851, 10.391], loss: 0.225247, mae: 0.425275, mean_q: 4.650223
 22788/100000: episode: 326, duration: 0.243s, episode steps: 46, steps per second: 189, episode reward: 85.959, mean reward: 1.869 [1.500, 2.548], mean action: 0.000 [0.000, 0.000], mean observation: 1.939 [-0.724, 10.215], loss: 0.261815, mae: 0.436914, mean_q: 4.664412
 22834/100000: episode: 327, duration: 0.244s, episode steps: 46, steps per second: 189, episode reward: 101.703, mean reward: 2.211 [1.531, 3.953], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.893, 10.378], loss: 0.247679, mae: 0.433933, mean_q: 4.665598
 22867/100000: episode: 328, duration: 0.163s, episode steps: 33, steps per second: 202, episode reward: 98.768, mean reward: 2.993 [1.961, 4.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.605, 10.488], loss: 0.313534, mae: 0.484348, mean_q: 4.638186
 22908/100000: episode: 329, duration: 0.209s, episode steps: 41, steps per second: 196, episode reward: 99.066, mean reward: 2.416 [1.569, 4.289], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.084, 10.192], loss: 0.175173, mae: 0.415708, mean_q: 4.677273
[Info] 3-TH LEVEL FOUND: 7.552455902099609, Considering 10/90 traces
 22940/100000: episode: 330, duration: 4.311s, episode steps: 32, steps per second: 7, episode reward: 72.438, mean reward: 2.264 [1.689, 3.415], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.170, 10.287], loss: 0.213107, mae: 0.397975, mean_q: 4.580746
 22979/100000: episode: 331, duration: 0.210s, episode steps: 39, steps per second: 186, episode reward: 110.905, mean reward: 2.844 [2.218, 3.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.907, 10.513], loss: 0.246802, mae: 0.424207, mean_q: 4.642210
 23007/100000: episode: 332, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 130.361, mean reward: 4.656 [3.619, 6.193], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.804, 10.480], loss: 0.265653, mae: 0.438591, mean_q: 4.736080
 23045/100000: episode: 333, duration: 0.191s, episode steps: 38, steps per second: 199, episode reward: 92.544, mean reward: 2.435 [1.489, 3.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.004 [-1.157, 10.100], loss: 0.242614, mae: 0.419242, mean_q: 4.675264
[Info] FALSIFICATION!
 23056/100000: episode: 334, duration: 0.429s, episode steps: 11, steps per second: 26, episode reward: 1065.873, mean reward: 96.898 [3.386, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.016, 10.203], loss: 0.433426, mae: 0.498452, mean_q: 4.855555
 23074/100000: episode: 335, duration: 0.132s, episode steps: 18, steps per second: 136, episode reward: 59.962, mean reward: 3.331 [2.052, 6.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-1.003, 10.299], loss: 860.735962, mae: 5.352167, mean_q: 7.030457
 23112/100000: episode: 336, duration: 0.189s, episode steps: 38, steps per second: 201, episode reward: 137.790, mean reward: 3.626 [2.051, 6.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.209, 10.420], loss: 1.438677, mae: 1.193961, mean_q: 4.013818
 23150/100000: episode: 337, duration: 0.197s, episode steps: 38, steps per second: 193, episode reward: 109.793, mean reward: 2.889 [2.264, 3.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.013 [-0.094, 10.410], loss: 0.417731, mae: 0.602561, mean_q: 4.570657
 23189/100000: episode: 338, duration: 0.200s, episode steps: 39, steps per second: 195, episode reward: 130.952, mean reward: 3.358 [1.492, 6.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.992 [-0.694, 10.280], loss: 0.347846, mae: 0.505209, mean_q: 4.509655
 23224/100000: episode: 339, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 94.323, mean reward: 2.695 [1.620, 5.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.035, 10.189], loss: 440.027161, mae: 1.494016, mean_q: 4.735931
 23250/100000: episode: 340, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 81.475, mean reward: 3.134 [2.269, 4.240], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-1.161, 10.426], loss: 2.248430, mae: 1.548487, mean_q: 5.429972
 23270/100000: episode: 341, duration: 0.096s, episode steps: 20, steps per second: 208, episode reward: 140.194, mean reward: 7.010 [3.474, 14.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.518], loss: 767.761658, mae: 2.226746, mean_q: 4.630183
 23291/100000: episode: 342, duration: 0.101s, episode steps: 21, steps per second: 209, episode reward: 56.875, mean reward: 2.708 [2.210, 3.887], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.035, 10.400], loss: 2.188290, mae: 1.519413, mean_q: 5.865386
 23317/100000: episode: 343, duration: 0.127s, episode steps: 26, steps per second: 205, episode reward: 84.514, mean reward: 3.251 [2.304, 4.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.451], loss: 0.575103, mae: 0.696655, mean_q: 4.428681
 23356/100000: episode: 344, duration: 0.194s, episode steps: 39, steps per second: 201, episode reward: 118.236, mean reward: 3.032 [1.528, 5.731], mean action: 0.000 [0.000, 0.000], mean observation: 1.998 [-1.456, 10.263], loss: 789.725403, mae: 3.313011, mean_q: 6.177457
 23376/100000: episode: 345, duration: 0.099s, episode steps: 20, steps per second: 201, episode reward: 83.119, mean reward: 4.156 [2.550, 8.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.424, 10.474], loss: 2.670044, mae: 1.711221, mean_q: 6.575742
 23398/100000: episode: 346, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 71.503, mean reward: 3.250 [2.426, 4.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.558, 10.460], loss: 0.957007, mae: 0.958833, mean_q: 4.797082
 23431/100000: episode: 347, duration: 0.173s, episode steps: 33, steps per second: 191, episode reward: 112.417, mean reward: 3.407 [2.286, 5.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.786, 10.472], loss: 0.635140, mae: 0.771388, mean_q: 5.030391
 23466/100000: episode: 348, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 106.767, mean reward: 3.050 [1.853, 5.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.088, 10.294], loss: 0.680898, mae: 0.755403, mean_q: 5.107322
 23494/100000: episode: 349, duration: 0.140s, episode steps: 28, steps per second: 200, episode reward: 78.261, mean reward: 2.795 [1.644, 5.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.035, 10.475], loss: 547.156860, mae: 1.751802, mean_q: 5.007976
 23533/100000: episode: 350, duration: 0.201s, episode steps: 39, steps per second: 194, episode reward: 100.552, mean reward: 2.578 [1.721, 3.928], mean action: 0.000 [0.000, 0.000], mean observation: 1.996 [-0.095, 10.257], loss: 393.540436, mae: 2.301800, mean_q: 6.248364
 23566/100000: episode: 351, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 118.227, mean reward: 3.583 [2.282, 7.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.912, 10.442], loss: 1.673378, mae: 1.178820, mean_q: 5.562479
 23586/100000: episode: 352, duration: 0.097s, episode steps: 20, steps per second: 207, episode reward: 164.491, mean reward: 8.225 [3.881, 27.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-1.081, 10.576], loss: 0.949347, mae: 0.750186, mean_q: 5.044267
 23625/100000: episode: 353, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 122.115, mean reward: 3.131 [1.818, 4.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.112, 10.385], loss: 1.042521, mae: 0.773271, mean_q: 5.285620
 23660/100000: episode: 354, duration: 0.173s, episode steps: 35, steps per second: 203, episode reward: 123.035, mean reward: 3.515 [2.235, 7.042], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.473, 10.616], loss: 0.933806, mae: 0.717778, mean_q: 5.211981
 23699/100000: episode: 355, duration: 0.183s, episode steps: 39, steps per second: 213, episode reward: 99.048, mean reward: 2.540 [1.796, 6.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.103, 10.273], loss: 0.542317, mae: 0.679748, mean_q: 5.164944
 23721/100000: episode: 356, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 88.350, mean reward: 4.016 [2.786, 7.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.451], loss: 0.419857, mae: 0.613970, mean_q: 5.115820
 23741/100000: episode: 357, duration: 0.103s, episode steps: 20, steps per second: 193, episode reward: 66.445, mean reward: 3.322 [2.763, 4.112], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.891, 10.521], loss: 0.446728, mae: 0.622529, mean_q: 5.196729
 23759/100000: episode: 358, duration: 0.087s, episode steps: 18, steps per second: 206, episode reward: 68.510, mean reward: 3.806 [2.718, 4.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.841, 10.392], loss: 0.565968, mae: 0.679576, mean_q: 5.158458
 23792/100000: episode: 359, duration: 0.170s, episode steps: 33, steps per second: 194, episode reward: 118.146, mean reward: 3.580 [2.081, 6.169], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.035, 10.317], loss: 0.417888, mae: 0.608778, mean_q: 5.152765
 23813/100000: episode: 360, duration: 0.116s, episode steps: 21, steps per second: 180, episode reward: 74.989, mean reward: 3.571 [2.358, 5.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.524], loss: 0.533800, mae: 0.650716, mean_q: 5.163806
 23841/100000: episode: 361, duration: 0.142s, episode steps: 28, steps per second: 198, episode reward: 78.493, mean reward: 2.803 [2.089, 3.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.585, 10.319], loss: 0.533404, mae: 0.623668, mean_q: 5.168210
 23880/100000: episode: 362, duration: 0.200s, episode steps: 39, steps per second: 195, episode reward: 120.817, mean reward: 3.098 [2.096, 5.063], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.478, 10.394], loss: 0.652889, mae: 0.601525, mean_q: 5.175791
 23915/100000: episode: 363, duration: 0.173s, episode steps: 35, steps per second: 202, episode reward: 104.887, mean reward: 2.997 [1.704, 5.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.771, 10.239], loss: 0.339585, mae: 0.558011, mean_q: 5.184366
 23937/100000: episode: 364, duration: 0.135s, episode steps: 22, steps per second: 163, episode reward: 66.205, mean reward: 3.009 [2.230, 4.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.526, 10.523], loss: 0.369757, mae: 0.572117, mean_q: 5.179389
 23965/100000: episode: 365, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 74.565, mean reward: 2.663 [1.680, 5.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.120, 10.272], loss: 0.523110, mae: 0.637506, mean_q: 5.185584
 23991/100000: episode: 366, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 97.446, mean reward: 3.748 [2.381, 4.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.200, 10.536], loss: 0.793711, mae: 0.637230, mean_q: 5.261500
 24024/100000: episode: 367, duration: 0.174s, episode steps: 33, steps per second: 190, episode reward: 149.684, mean reward: 4.536 [2.283, 18.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.607, 10.358], loss: 465.811035, mae: 2.490178, mean_q: 6.388792
 24052/100000: episode: 368, duration: 0.163s, episode steps: 28, steps per second: 171, episode reward: 70.881, mean reward: 2.531 [1.873, 3.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.302, 10.264], loss: 2.170891, mae: 0.900685, mean_q: 4.819037
 24078/100000: episode: 369, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 88.274, mean reward: 3.395 [1.755, 8.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.035, 10.253], loss: 0.909366, mae: 0.715359, mean_q: 5.466996
 24113/100000: episode: 370, duration: 0.203s, episode steps: 35, steps per second: 172, episode reward: 130.554, mean reward: 3.730 [2.348, 6.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-0.504, 10.335], loss: 0.646909, mae: 0.681361, mean_q: 5.270756
 24134/100000: episode: 371, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 83.819, mean reward: 3.991 [2.965, 4.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.264, 10.482], loss: 0.426787, mae: 0.623938, mean_q: 5.423831
 24167/100000: episode: 372, duration: 0.182s, episode steps: 33, steps per second: 181, episode reward: 126.793, mean reward: 3.842 [2.367, 6.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.035, 10.305], loss: 0.570776, mae: 0.701880, mean_q: 5.458618
 24188/100000: episode: 373, duration: 0.104s, episode steps: 21, steps per second: 201, episode reward: 64.783, mean reward: 3.085 [2.326, 4.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.492], loss: 0.484379, mae: 0.621325, mean_q: 5.321412
 24221/100000: episode: 374, duration: 0.159s, episode steps: 33, steps per second: 208, episode reward: 81.183, mean reward: 2.460 [1.492, 4.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.049 [-0.107, 10.177], loss: 462.668304, mae: 1.616283, mean_q: 5.441767
 24254/100000: episode: 375, duration: 0.191s, episode steps: 33, steps per second: 173, episode reward: 98.286, mean reward: 2.978 [2.142, 4.628], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.773, 10.419], loss: 3.680365, mae: 2.000399, mean_q: 6.380939
 24275/100000: episode: 376, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 87.874, mean reward: 4.184 [2.734, 7.483], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-1.480, 10.587], loss: 1.056019, mae: 0.867810, mean_q: 4.955817
 24297/100000: episode: 377, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 65.176, mean reward: 2.963 [2.146, 3.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.996, 10.385], loss: 1.085510, mae: 0.786643, mean_q: 5.481722
 24335/100000: episode: 378, duration: 0.186s, episode steps: 38, steps per second: 204, episode reward: 122.835, mean reward: 3.232 [1.796, 10.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.018 [-0.311, 10.302], loss: 403.754303, mae: 1.642379, mean_q: 5.541156
 24353/100000: episode: 379, duration: 0.097s, episode steps: 18, steps per second: 186, episode reward: 53.090, mean reward: 2.949 [2.370, 3.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.260, 10.503], loss: 3.113992, mae: 1.921948, mean_q: 7.104950
 24373/100000: episode: 380, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 137.110, mean reward: 6.855 [3.819, 12.549], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-1.150, 10.608], loss: 0.968961, mae: 0.913233, mean_q: 5.041887
 24394/100000: episode: 381, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 86.121, mean reward: 4.101 [2.375, 18.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-1.408, 10.509], loss: 0.752553, mae: 0.748655, mean_q: 5.328507
 24420/100000: episode: 382, duration: 0.134s, episode steps: 26, steps per second: 193, episode reward: 79.191, mean reward: 3.046 [2.605, 4.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-1.304, 10.487], loss: 0.881813, mae: 0.773525, mean_q: 5.479131
 24448/100000: episode: 383, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 79.095, mean reward: 2.825 [1.709, 7.803], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.058, 10.257], loss: 1.428795, mae: 0.769811, mean_q: 5.552636
 24469/100000: episode: 384, duration: 0.104s, episode steps: 21, steps per second: 201, episode reward: 90.126, mean reward: 4.292 [2.498, 6.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.468, 10.579], loss: 0.579429, mae: 0.696096, mean_q: 5.482080
 24491/100000: episode: 385, duration: 0.124s, episode steps: 22, steps per second: 178, episode reward: 70.517, mean reward: 3.205 [2.263, 4.922], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.291, 10.470], loss: 0.616765, mae: 0.666945, mean_q: 5.468242
 24509/100000: episode: 386, duration: 0.089s, episode steps: 18, steps per second: 201, episode reward: 56.441, mean reward: 3.136 [2.061, 5.122], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.329, 10.400], loss: 0.391044, mae: 0.608975, mean_q: 5.394064
 24531/100000: episode: 387, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 122.461, mean reward: 5.566 [2.292, 34.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.035, 10.480], loss: 0.582632, mae: 0.692297, mean_q: 5.405107
 24551/100000: episode: 388, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 55.555, mean reward: 2.778 [1.915, 4.056], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.370, 10.336], loss: 0.733847, mae: 0.744517, mean_q: 5.634105
 24590/100000: episode: 389, duration: 0.207s, episode steps: 39, steps per second: 188, episode reward: 113.175, mean reward: 2.902 [1.877, 4.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.113, 10.269], loss: 0.603907, mae: 0.670811, mean_q: 5.464993
 24616/100000: episode: 390, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 89.774, mean reward: 3.453 [2.631, 4.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.035, 10.489], loss: 0.523484, mae: 0.673180, mean_q: 5.509919
 24644/100000: episode: 391, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 75.912, mean reward: 2.711 [1.889, 4.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.623, 10.448], loss: 0.602271, mae: 0.667970, mean_q: 5.561333
 24666/100000: episode: 392, duration: 0.117s, episode steps: 22, steps per second: 189, episode reward: 63.751, mean reward: 2.898 [2.125, 4.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.372], loss: 1.573190, mae: 0.679727, mean_q: 5.544239
 24694/100000: episode: 393, duration: 0.137s, episode steps: 28, steps per second: 205, episode reward: 69.292, mean reward: 2.475 [1.946, 3.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.502, 10.349], loss: 0.538527, mae: 0.649093, mean_q: 5.469016
 24727/100000: episode: 394, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 111.421, mean reward: 3.376 [1.939, 5.359], mean action: 0.000 [0.000, 0.000], mean observation: 2.038 [-0.852, 10.331], loss: 0.522426, mae: 0.642336, mean_q: 5.469383
 24766/100000: episode: 395, duration: 0.208s, episode steps: 39, steps per second: 187, episode reward: 170.123, mean reward: 4.362 [2.045, 11.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.001 [-0.319, 10.393], loss: 0.798795, mae: 0.640014, mean_q: 5.507433
 24786/100000: episode: 396, duration: 0.109s, episode steps: 20, steps per second: 184, episode reward: 90.617, mean reward: 4.531 [2.485, 10.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.625, 10.437], loss: 0.552702, mae: 0.662777, mean_q: 5.552230
 24814/100000: episode: 397, duration: 0.150s, episode steps: 28, steps per second: 186, episode reward: 84.517, mean reward: 3.018 [2.407, 3.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.571, 10.485], loss: 0.641258, mae: 0.672528, mean_q: 5.620466
 24849/100000: episode: 398, duration: 0.173s, episode steps: 35, steps per second: 202, episode reward: 99.610, mean reward: 2.846 [1.973, 4.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.045 [-0.117, 10.438], loss: 0.871172, mae: 0.687999, mean_q: 5.559143
 24884/100000: episode: 399, duration: 0.172s, episode steps: 35, steps per second: 203, episode reward: 129.744, mean reward: 3.707 [2.156, 4.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.437, 10.363], loss: 0.588701, mae: 0.663683, mean_q: 5.582354
 24910/100000: episode: 400, duration: 0.138s, episode steps: 26, steps per second: 189, episode reward: 94.182, mean reward: 3.622 [2.492, 6.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.196, 10.445], loss: 0.526841, mae: 0.659423, mean_q: 5.530358
 24928/100000: episode: 401, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 94.332, mean reward: 5.241 [2.742, 11.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.332, 10.561], loss: 0.318495, mae: 0.599557, mean_q: 5.443130
 24954/100000: episode: 402, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 140.349, mean reward: 5.398 [2.974, 13.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.613], loss: 0.524907, mae: 0.628048, mean_q: 5.633135
 24972/100000: episode: 403, duration: 0.100s, episode steps: 18, steps per second: 180, episode reward: 52.658, mean reward: 2.925 [1.757, 4.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.402], loss: 853.683350, mae: 4.109926, mean_q: 7.657057
 25000/100000: episode: 404, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 77.903, mean reward: 2.782 [1.896, 3.581], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.403, 10.395], loss: 1.832534, mae: 1.358938, mean_q: 5.418640
 25026/100000: episode: 405, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 111.689, mean reward: 4.296 [2.924, 6.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.854, 10.636], loss: 0.802781, mae: 0.821121, mean_q: 5.714315
 25054/100000: episode: 406, duration: 0.135s, episode steps: 28, steps per second: 208, episode reward: 86.466, mean reward: 3.088 [2.235, 4.317], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.718, 10.428], loss: 548.734619, mae: 2.720017, mean_q: 6.965481
 25076/100000: episode: 407, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 54.623, mean reward: 2.483 [1.886, 3.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.248, 10.352], loss: 1.638828, mae: 0.994732, mean_q: 5.455953
 25115/100000: episode: 408, duration: 0.209s, episode steps: 39, steps per second: 187, episode reward: 115.673, mean reward: 2.966 [2.066, 4.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.009 [-0.282, 10.305], loss: 0.763509, mae: 0.800091, mean_q: 5.732455
 25135/100000: episode: 409, duration: 0.106s, episode steps: 20, steps per second: 188, episode reward: 77.722, mean reward: 3.886 [2.368, 6.911], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.084, 10.439], loss: 1.418908, mae: 0.831277, mean_q: 5.763825
 25157/100000: episode: 410, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 84.850, mean reward: 3.857 [2.613, 5.529], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.035, 10.561], loss: 3.327300, mae: 1.854182, mean_q: 7.347973
 25190/100000: episode: 411, duration: 0.176s, episode steps: 33, steps per second: 187, episode reward: 116.767, mean reward: 3.538 [2.416, 4.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.054, 10.448], loss: 1.222740, mae: 0.826835, mean_q: 5.408044
 25225/100000: episode: 412, duration: 0.179s, episode steps: 35, steps per second: 196, episode reward: 93.430, mean reward: 2.669 [1.724, 4.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.591, 10.377], loss: 437.264526, mae: 2.056553, mean_q: 6.504878
 25258/100000: episode: 413, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 170.938, mean reward: 5.180 [2.336, 15.175], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.975, 10.529], loss: 1.587184, mae: 1.230267, mean_q: 6.011152
 25296/100000: episode: 414, duration: 0.185s, episode steps: 38, steps per second: 206, episode reward: 106.855, mean reward: 2.812 [2.027, 4.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.859, 10.372], loss: 0.987094, mae: 0.871721, mean_q: 5.958573
 25331/100000: episode: 415, duration: 0.177s, episode steps: 35, steps per second: 198, episode reward: 120.773, mean reward: 3.451 [2.411, 7.192], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-1.405, 10.441], loss: 0.495384, mae: 0.702907, mean_q: 5.849098
 25351/100000: episode: 416, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 86.667, mean reward: 4.333 [2.724, 7.629], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.881, 10.527], loss: 1.475124, mae: 0.854091, mean_q: 5.902103
 25389/100000: episode: 417, duration: 0.213s, episode steps: 38, steps per second: 179, episode reward: 147.745, mean reward: 3.888 [2.752, 5.713], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.690, 10.506], loss: 0.517993, mae: 0.720304, mean_q: 5.923603
 25427/100000: episode: 418, duration: 0.199s, episode steps: 38, steps per second: 191, episode reward: 123.739, mean reward: 3.256 [2.097, 6.631], mean action: 0.000 [0.000, 0.000], mean observation: 2.006 [-0.254, 10.406], loss: 0.519439, mae: 0.666437, mean_q: 5.810273
 25462/100000: episode: 419, duration: 0.172s, episode steps: 35, steps per second: 203, episode reward: 91.236, mean reward: 2.607 [1.951, 5.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-0.035, 10.322], loss: 0.576648, mae: 0.723295, mean_q: 5.905812
[Info] Complete ISplit Iteration
[Info] Levels: [4.6914744, 5.200883, 7.552456, 11.362081]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.04]
[Info] Error Prob: 4.000000000000001e-05

 25483/100000: episode: 420, duration: 4.347s, episode steps: 21, steps per second: 5, episode reward: 81.295, mean reward: 3.871 [2.578, 6.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.209, 10.585], loss: 1.242557, mae: 0.711677, mean_q: 5.826569
 25583/100000: episode: 421, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 177.252, mean reward: 1.773 [1.455, 2.721], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.375, 10.105], loss: 0.743225, mae: 0.694172, mean_q: 5.830991
 25683/100000: episode: 422, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 189.629, mean reward: 1.896 [1.456, 3.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.670, 10.134], loss: 0.450498, mae: 0.621107, mean_q: 5.757584
 25783/100000: episode: 423, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 195.470, mean reward: 1.955 [1.471, 4.505], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.765, 10.098], loss: 0.703730, mae: 0.666014, mean_q: 5.774227
 25883/100000: episode: 424, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 182.720, mean reward: 1.827 [1.471, 3.118], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.714, 10.100], loss: 0.845886, mae: 0.685454, mean_q: 5.811617
 25983/100000: episode: 425, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 196.907, mean reward: 1.969 [1.464, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.667, 10.265], loss: 0.652110, mae: 0.636613, mean_q: 5.683205
 26083/100000: episode: 426, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 179.528, mean reward: 1.795 [1.453, 2.910], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.353, 10.123], loss: 0.840007, mae: 0.629103, mean_q: 5.586434
 26183/100000: episode: 427, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 191.573, mean reward: 1.916 [1.430, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.387, 10.098], loss: 154.730133, mae: 1.652714, mean_q: 6.016100
 26283/100000: episode: 428, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 193.136, mean reward: 1.931 [1.450, 3.056], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.508, 10.237], loss: 153.658295, mae: 1.104217, mean_q: 5.891253
 26383/100000: episode: 429, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 226.119, mean reward: 2.261 [1.464, 4.834], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.606, 10.117], loss: 153.733109, mae: 1.513296, mean_q: 5.982380
 26483/100000: episode: 430, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 190.362, mean reward: 1.904 [1.459, 4.701], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.457, 10.098], loss: 154.051498, mae: 1.609327, mean_q: 6.220463
 26583/100000: episode: 431, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 189.083, mean reward: 1.891 [1.497, 2.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.804, 10.098], loss: 1.003645, mae: 0.773634, mean_q: 5.825137
 26683/100000: episode: 432, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 219.859, mean reward: 2.199 [1.448, 4.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.946, 10.098], loss: 153.547882, mae: 1.254210, mean_q: 5.881760
 26783/100000: episode: 433, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 178.155, mean reward: 1.782 [1.457, 2.545], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.455, 10.098], loss: 153.876648, mae: 1.495234, mean_q: 5.919247
 26883/100000: episode: 434, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 208.504, mean reward: 2.085 [1.483, 3.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.174, 10.098], loss: 153.387695, mae: 1.221329, mean_q: 5.803556
 26983/100000: episode: 435, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 199.671, mean reward: 1.997 [1.475, 3.458], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.523, 10.263], loss: 1.315894, mae: 0.802710, mean_q: 5.550198
 27083/100000: episode: 436, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 202.226, mean reward: 2.022 [1.439, 3.863], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.731, 10.309], loss: 153.982285, mae: 1.338803, mean_q: 5.998052
 27183/100000: episode: 437, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 196.059, mean reward: 1.961 [1.466, 3.269], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.394, 10.272], loss: 304.043488, mae: 1.960184, mean_q: 6.024134
 27283/100000: episode: 438, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 182.566, mean reward: 1.826 [1.449, 2.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.518, 10.245], loss: 303.223999, mae: 1.947676, mean_q: 6.147326
 27383/100000: episode: 439, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 195.791, mean reward: 1.958 [1.464, 3.454], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.952, 10.098], loss: 306.094543, mae: 2.247312, mean_q: 6.328637
 27483/100000: episode: 440, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 180.932, mean reward: 1.809 [1.442, 3.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.433, 10.101], loss: 304.144135, mae: 1.987660, mean_q: 6.242194
 27583/100000: episode: 441, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 200.747, mean reward: 2.007 [1.439, 4.039], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.316, 10.417], loss: 152.880844, mae: 1.526088, mean_q: 5.941385
 27683/100000: episode: 442, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 195.512, mean reward: 1.955 [1.475, 2.977], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.282, 10.225], loss: 0.974082, mae: 0.791452, mean_q: 5.598768
 27783/100000: episode: 443, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 189.125, mean reward: 1.891 [1.481, 3.735], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.640, 10.098], loss: 0.779999, mae: 0.647399, mean_q: 5.431833
 27883/100000: episode: 444, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 182.918, mean reward: 1.829 [1.443, 3.162], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.005, 10.099], loss: 305.994110, mae: 1.912573, mean_q: 6.092253
 27983/100000: episode: 445, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 180.325, mean reward: 1.803 [1.441, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.365, 10.098], loss: 0.962266, mae: 0.746159, mean_q: 5.301671
 28083/100000: episode: 446, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 190.588, mean reward: 1.906 [1.479, 3.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.639, 10.451], loss: 0.710226, mae: 0.643133, mean_q: 5.262266
 28183/100000: episode: 447, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 179.514, mean reward: 1.795 [1.455, 2.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.077, 10.131], loss: 0.474642, mae: 0.593074, mean_q: 5.183055
 28283/100000: episode: 448, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 190.484, mean reward: 1.905 [1.460, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.992, 10.273], loss: 0.559598, mae: 0.575353, mean_q: 5.082065
 28383/100000: episode: 449, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 191.546, mean reward: 1.915 [1.456, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.592, 10.281], loss: 0.563750, mae: 0.550858, mean_q: 5.034079
 28483/100000: episode: 450, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 192.880, mean reward: 1.929 [1.447, 3.278], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.479, 10.098], loss: 0.646592, mae: 0.524691, mean_q: 4.912385
 28583/100000: episode: 451, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 191.741, mean reward: 1.917 [1.458, 3.388], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.012, 10.098], loss: 0.390126, mae: 0.531368, mean_q: 4.932307
 28683/100000: episode: 452, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 193.833, mean reward: 1.938 [1.485, 4.198], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.054, 10.120], loss: 0.505694, mae: 0.529506, mean_q: 4.846773
 28783/100000: episode: 453, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 205.939, mean reward: 2.059 [1.493, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.004, 10.295], loss: 0.391258, mae: 0.496479, mean_q: 4.768035
 28883/100000: episode: 454, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 189.487, mean reward: 1.895 [1.471, 3.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.578, 10.098], loss: 0.527852, mae: 0.522683, mean_q: 4.735828
 28983/100000: episode: 455, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 206.194, mean reward: 2.062 [1.473, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.940, 10.098], loss: 0.353892, mae: 0.488417, mean_q: 4.670289
 29083/100000: episode: 456, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 179.451, mean reward: 1.795 [1.442, 3.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.963, 10.110], loss: 0.309919, mae: 0.464449, mean_q: 4.610557
 29183/100000: episode: 457, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 187.950, mean reward: 1.880 [1.467, 2.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.984, 10.171], loss: 0.317337, mae: 0.470483, mean_q: 4.569393
 29283/100000: episode: 458, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 184.294, mean reward: 1.843 [1.456, 3.214], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.765, 10.149], loss: 0.291917, mae: 0.456196, mean_q: 4.537960
 29383/100000: episode: 459, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 185.966, mean reward: 1.860 [1.450, 3.621], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.725, 10.098], loss: 0.346002, mae: 0.461511, mean_q: 4.488866
 29483/100000: episode: 460, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 190.673, mean reward: 1.907 [1.455, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.272, 10.150], loss: 0.160991, mae: 0.395714, mean_q: 4.329438
 29583/100000: episode: 461, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 189.858, mean reward: 1.899 [1.454, 2.815], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.905, 10.098], loss: 0.218882, mae: 0.411003, mean_q: 4.363337
 29683/100000: episode: 462, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 201.793, mean reward: 2.018 [1.450, 3.323], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.364, 10.098], loss: 0.289294, mae: 0.425411, mean_q: 4.329175
 29783/100000: episode: 463, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 183.645, mean reward: 1.836 [1.473, 2.838], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.719, 10.247], loss: 0.198840, mae: 0.389125, mean_q: 4.180691
 29883/100000: episode: 464, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 193.740, mean reward: 1.937 [1.476, 3.442], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.612, 10.193], loss: 0.195939, mae: 0.383630, mean_q: 4.156575
 29983/100000: episode: 465, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 209.953, mean reward: 2.100 [1.534, 6.115], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.095, 10.315], loss: 0.139493, mae: 0.353230, mean_q: 4.115482
 30083/100000: episode: 466, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 191.147, mean reward: 1.911 [1.475, 2.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.729, 10.098], loss: 0.132844, mae: 0.341473, mean_q: 4.048271
 30183/100000: episode: 467, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 201.382, mean reward: 2.014 [1.506, 3.209], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.122, 10.232], loss: 0.138808, mae: 0.337223, mean_q: 3.987023
 30283/100000: episode: 468, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 195.919, mean reward: 1.959 [1.467, 3.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.890, 10.294], loss: 0.110326, mae: 0.332735, mean_q: 3.948117
 30383/100000: episode: 469, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 190.530, mean reward: 1.905 [1.462, 3.973], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.599, 10.098], loss: 0.099418, mae: 0.318672, mean_q: 3.873282
 30483/100000: episode: 470, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 195.148, mean reward: 1.951 [1.511, 3.533], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.059, 10.098], loss: 0.089503, mae: 0.302584, mean_q: 3.825146
 30583/100000: episode: 471, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 197.204, mean reward: 1.972 [1.442, 5.793], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.671, 10.271], loss: 0.094068, mae: 0.306852, mean_q: 3.820815
 30683/100000: episode: 472, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: 194.802, mean reward: 1.948 [1.486, 9.033], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.863, 10.099], loss: 0.096428, mae: 0.300406, mean_q: 3.826491
 30783/100000: episode: 473, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 195.521, mean reward: 1.955 [1.458, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.135, 10.098], loss: 0.098098, mae: 0.306054, mean_q: 3.817414
 30883/100000: episode: 474, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 199.176, mean reward: 1.992 [1.461, 3.235], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.421, 10.098], loss: 0.096826, mae: 0.304201, mean_q: 3.822587
 30983/100000: episode: 475, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 193.931, mean reward: 1.939 [1.454, 4.645], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.770, 10.098], loss: 0.084252, mae: 0.298564, mean_q: 3.820830
 31083/100000: episode: 476, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 192.269, mean reward: 1.923 [1.489, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.633, 10.098], loss: 0.088468, mae: 0.302833, mean_q: 3.822900
 31183/100000: episode: 477, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 181.205, mean reward: 1.812 [1.470, 2.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.767, 10.165], loss: 0.088859, mae: 0.292095, mean_q: 3.817036
 31283/100000: episode: 478, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 200.454, mean reward: 2.005 [1.464, 4.091], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.572, 10.330], loss: 0.107103, mae: 0.304939, mean_q: 3.823043
 31383/100000: episode: 479, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 204.808, mean reward: 2.048 [1.510, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.850, 10.241], loss: 0.078422, mae: 0.284276, mean_q: 3.816750
 31483/100000: episode: 480, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 210.235, mean reward: 2.102 [1.554, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.760, 10.098], loss: 0.092067, mae: 0.293657, mean_q: 3.831623
 31583/100000: episode: 481, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 197.137, mean reward: 1.971 [1.487, 4.382], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.884, 10.296], loss: 0.097958, mae: 0.299711, mean_q: 3.819442
 31683/100000: episode: 482, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 252.958, mean reward: 2.530 [1.679, 4.037], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.937, 10.491], loss: 0.095810, mae: 0.301847, mean_q: 3.843387
 31783/100000: episode: 483, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 184.808, mean reward: 1.848 [1.443, 3.727], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.032, 10.189], loss: 0.098198, mae: 0.303535, mean_q: 3.858198
 31883/100000: episode: 484, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 193.671, mean reward: 1.937 [1.463, 7.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.847, 10.098], loss: 0.102006, mae: 0.309974, mean_q: 3.860198
 31983/100000: episode: 485, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 193.716, mean reward: 1.937 [1.467, 3.101], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.948, 10.311], loss: 0.078755, mae: 0.284625, mean_q: 3.832258
 32083/100000: episode: 486, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 189.892, mean reward: 1.899 [1.490, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.827, 10.189], loss: 0.081474, mae: 0.287435, mean_q: 3.822509
 32183/100000: episode: 487, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 182.571, mean reward: 1.826 [1.490, 2.649], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.747, 10.115], loss: 0.101185, mae: 0.295294, mean_q: 3.830586
 32283/100000: episode: 488, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 191.095, mean reward: 1.911 [1.460, 3.559], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.218, 10.098], loss: 0.084335, mae: 0.295133, mean_q: 3.846151
 32383/100000: episode: 489, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 186.889, mean reward: 1.869 [1.446, 3.554], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.575, 10.212], loss: 0.099923, mae: 0.298535, mean_q: 3.835881
 32483/100000: episode: 490, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 184.776, mean reward: 1.848 [1.449, 2.558], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.476, 10.318], loss: 0.088497, mae: 0.292861, mean_q: 3.847198
 32583/100000: episode: 491, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 178.052, mean reward: 1.781 [1.443, 3.190], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.161, 10.169], loss: 0.100020, mae: 0.290456, mean_q: 3.846482
 32683/100000: episode: 492, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 197.193, mean reward: 1.972 [1.478, 3.497], mean action: 0.000 [0.000, 0.000], mean observation: 1.443 [-0.736, 10.368], loss: 0.083988, mae: 0.293756, mean_q: 3.837109
 32783/100000: episode: 493, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 192.360, mean reward: 1.924 [1.499, 3.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.587, 10.098], loss: 0.089355, mae: 0.287074, mean_q: 3.839437
 32883/100000: episode: 494, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 225.472, mean reward: 2.255 [1.466, 4.344], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.044, 10.098], loss: 0.078952, mae: 0.278956, mean_q: 3.815007
 32983/100000: episode: 495, duration: 0.538s, episode steps: 100, steps per second: 186, episode reward: 193.415, mean reward: 1.934 [1.455, 3.066], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.548, 10.098], loss: 0.080135, mae: 0.283998, mean_q: 3.812024
 33083/100000: episode: 496, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 244.613, mean reward: 2.446 [1.450, 4.092], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.136, 10.098], loss: 0.088787, mae: 0.288148, mean_q: 3.826088
 33183/100000: episode: 497, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 189.454, mean reward: 1.895 [1.438, 3.191], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.901, 10.274], loss: 0.109205, mae: 0.306511, mean_q: 3.853263
 33283/100000: episode: 498, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 186.344, mean reward: 1.863 [1.456, 3.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.417, 10.098], loss: 0.079844, mae: 0.286813, mean_q: 3.833145
 33383/100000: episode: 499, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 205.221, mean reward: 2.052 [1.447, 3.569], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.706, 10.269], loss: 0.112604, mae: 0.311876, mean_q: 3.873686
 33483/100000: episode: 500, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 191.511, mean reward: 1.915 [1.466, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.750, 10.384], loss: 0.100939, mae: 0.302859, mean_q: 3.856217
 33583/100000: episode: 501, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 181.935, mean reward: 1.819 [1.441, 2.891], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.392, 10.098], loss: 0.091405, mae: 0.300567, mean_q: 3.855552
 33683/100000: episode: 502, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 196.810, mean reward: 1.968 [1.476, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.375, 10.098], loss: 0.091477, mae: 0.295967, mean_q: 3.859939
 33783/100000: episode: 503, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 193.365, mean reward: 1.934 [1.450, 3.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.791, 10.098], loss: 0.092084, mae: 0.294135, mean_q: 3.868851
 33883/100000: episode: 504, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 197.797, mean reward: 1.978 [1.464, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.698, 10.098], loss: 0.090574, mae: 0.298350, mean_q: 3.860577
 33983/100000: episode: 505, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 182.969, mean reward: 1.830 [1.455, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.776, 10.111], loss: 0.094763, mae: 0.311085, mean_q: 3.861749
 34083/100000: episode: 506, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 219.794, mean reward: 2.198 [1.470, 6.251], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.326, 10.348], loss: 0.099822, mae: 0.307896, mean_q: 3.882409
 34183/100000: episode: 507, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 209.939, mean reward: 2.099 [1.494, 3.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.798, 10.098], loss: 0.107615, mae: 0.304634, mean_q: 3.892968
 34283/100000: episode: 508, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 201.431, mean reward: 2.014 [1.471, 3.975], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.567, 10.098], loss: 0.104115, mae: 0.315280, mean_q: 3.889635
 34383/100000: episode: 509, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 199.197, mean reward: 1.992 [1.444, 4.326], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.143, 10.098], loss: 0.082512, mae: 0.291196, mean_q: 3.893030
 34483/100000: episode: 510, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 186.889, mean reward: 1.869 [1.448, 3.475], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.225, 10.098], loss: 0.101269, mae: 0.303745, mean_q: 3.900395
 34583/100000: episode: 511, duration: 0.475s, episode steps: 100, steps per second: 210, episode reward: 209.781, mean reward: 2.098 [1.459, 3.648], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.720, 10.098], loss: 0.082741, mae: 0.291270, mean_q: 3.886880
 34683/100000: episode: 512, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 191.683, mean reward: 1.917 [1.471, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-0.994, 10.347], loss: 0.117308, mae: 0.314861, mean_q: 3.906830
 34783/100000: episode: 513, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 224.522, mean reward: 2.245 [1.481, 4.902], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-1.510, 10.098], loss: 0.090952, mae: 0.300207, mean_q: 3.899858
 34883/100000: episode: 514, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 200.255, mean reward: 2.003 [1.498, 3.107], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.818, 10.098], loss: 0.087495, mae: 0.294108, mean_q: 3.898401
 34983/100000: episode: 515, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 199.244, mean reward: 1.992 [1.450, 3.276], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.681, 10.098], loss: 0.092104, mae: 0.302594, mean_q: 3.901708
 35083/100000: episode: 516, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 191.048, mean reward: 1.910 [1.435, 3.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.050, 10.335], loss: 0.112097, mae: 0.317114, mean_q: 3.906309
 35183/100000: episode: 517, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 178.428, mean reward: 1.784 [1.469, 2.562], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.744, 10.129], loss: 0.099011, mae: 0.308794, mean_q: 3.899893
 35283/100000: episode: 518, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 192.569, mean reward: 1.926 [1.508, 3.368], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.655, 10.098], loss: 0.101661, mae: 0.302741, mean_q: 3.899958
 35383/100000: episode: 519, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 191.963, mean reward: 1.920 [1.491, 3.328], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.647, 10.317], loss: 0.107114, mae: 0.305940, mean_q: 3.910613
[Info] 1-TH LEVEL FOUND: 4.840064525604248, Considering 10/90 traces
 35483/100000: episode: 520, duration: 4.726s, episode steps: 100, steps per second: 21, episode reward: 178.452, mean reward: 1.785 [1.438, 3.372], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.877, 10.171], loss: 0.103886, mae: 0.303114, mean_q: 3.899958
 35524/100000: episode: 521, duration: 0.217s, episode steps: 41, steps per second: 189, episode reward: 127.377, mean reward: 3.107 [2.144, 4.216], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.099, 10.481], loss: 0.094579, mae: 0.303218, mean_q: 3.927367
 35567/100000: episode: 522, duration: 0.228s, episode steps: 43, steps per second: 189, episode reward: 87.291, mean reward: 2.030 [1.595, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.290, 10.179], loss: 0.104234, mae: 0.306663, mean_q: 3.929178
 35602/100000: episode: 523, duration: 0.177s, episode steps: 35, steps per second: 198, episode reward: 72.596, mean reward: 2.074 [1.696, 3.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.526, 10.246], loss: 0.092487, mae: 0.294985, mean_q: 3.900226
 35646/100000: episode: 524, duration: 0.224s, episode steps: 44, steps per second: 197, episode reward: 87.362, mean reward: 1.985 [1.576, 2.723], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.249, 10.273], loss: 0.101266, mae: 0.315850, mean_q: 3.950254
 35677/100000: episode: 525, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 77.926, mean reward: 2.514 [1.863, 3.694], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.381, 10.417], loss: 0.081934, mae: 0.291896, mean_q: 3.923765
 35708/100000: episode: 526, duration: 0.162s, episode steps: 31, steps per second: 192, episode reward: 71.307, mean reward: 2.300 [1.918, 4.511], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.795, 10.306], loss: 0.090691, mae: 0.302982, mean_q: 3.918005
 35717/100000: episode: 527, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 22.547, mean reward: 2.505 [2.108, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.035, 10.431], loss: 0.084394, mae: 0.307069, mean_q: 3.941491
 35741/100000: episode: 528, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 46.721, mean reward: 1.947 [1.624, 2.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.769, 10.226], loss: 0.082294, mae: 0.301470, mean_q: 3.917167
 35776/100000: episode: 529, duration: 0.173s, episode steps: 35, steps per second: 203, episode reward: 71.603, mean reward: 2.046 [1.532, 3.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.465, 10.249], loss: 0.087802, mae: 0.293129, mean_q: 3.923768
 35807/100000: episode: 530, duration: 0.150s, episode steps: 31, steps per second: 206, episode reward: 65.214, mean reward: 2.104 [1.644, 3.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.213, 10.258], loss: 0.089373, mae: 0.316093, mean_q: 3.940353
 35838/100000: episode: 531, duration: 0.154s, episode steps: 31, steps per second: 201, episode reward: 73.773, mean reward: 2.380 [1.890, 3.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.289, 10.214], loss: 0.081037, mae: 0.294760, mean_q: 3.918219
 35881/100000: episode: 532, duration: 0.232s, episode steps: 43, steps per second: 185, episode reward: 181.308, mean reward: 4.216 [2.097, 8.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.968 [-0.512, 10.421], loss: 0.093484, mae: 0.311050, mean_q: 3.945900
 35905/100000: episode: 533, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 67.492, mean reward: 2.812 [1.870, 4.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.149 [-0.775, 10.520], loss: 0.095888, mae: 0.311414, mean_q: 4.010004
 35929/100000: episode: 534, duration: 0.119s, episode steps: 24, steps per second: 201, episode reward: 50.345, mean reward: 2.098 [1.661, 2.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.450, 10.390], loss: 0.131895, mae: 0.329889, mean_q: 3.979035
 35973/100000: episode: 535, duration: 0.216s, episode steps: 44, steps per second: 204, episode reward: 95.423, mean reward: 2.169 [1.630, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 1.962 [-0.134, 10.180], loss: 0.112465, mae: 0.330668, mean_q: 3.981471
 36019/100000: episode: 536, duration: 0.227s, episode steps: 46, steps per second: 203, episode reward: 101.719, mean reward: 2.211 [1.805, 3.484], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.528, 10.321], loss: 0.102362, mae: 0.311584, mean_q: 3.991711
 36047/100000: episode: 537, duration: 0.144s, episode steps: 28, steps per second: 195, episode reward: 55.032, mean reward: 1.965 [1.525, 3.249], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-1.018, 10.194], loss: 0.106926, mae: 0.313396, mean_q: 3.952994
 36078/100000: episode: 538, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 80.993, mean reward: 2.613 [1.754, 6.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-1.237, 10.451], loss: 0.108129, mae: 0.316963, mean_q: 3.990820
 36109/100000: episode: 539, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 66.473, mean reward: 2.144 [1.594, 3.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.102, 10.215], loss: 0.145808, mae: 0.340114, mean_q: 4.014487
 36152/100000: episode: 540, duration: 0.224s, episode steps: 43, steps per second: 192, episode reward: 99.113, mean reward: 2.305 [1.440, 8.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.951 [-2.328, 10.176], loss: 0.101775, mae: 0.314669, mean_q: 3.985262
 36183/100000: episode: 541, duration: 0.161s, episode steps: 31, steps per second: 193, episode reward: 71.574, mean reward: 2.309 [1.947, 3.719], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.633, 10.526], loss: 0.132068, mae: 0.328874, mean_q: 4.019711
 36218/100000: episode: 542, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 84.452, mean reward: 2.413 [1.831, 3.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.053 [-0.261, 10.304], loss: 0.128018, mae: 0.331073, mean_q: 4.043091
 36264/100000: episode: 543, duration: 0.248s, episode steps: 46, steps per second: 185, episode reward: 93.102, mean reward: 2.024 [1.454, 5.029], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-1.413, 10.100], loss: 0.131622, mae: 0.347462, mean_q: 4.038534
 36273/100000: episode: 544, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 23.746, mean reward: 2.638 [2.078, 3.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.452], loss: 0.137297, mae: 0.348425, mean_q: 4.106313
 36317/100000: episode: 545, duration: 0.219s, episode steps: 44, steps per second: 201, episode reward: 124.991, mean reward: 2.841 [1.779, 4.997], mean action: 0.000 [0.000, 0.000], mean observation: 1.942 [-0.635, 10.259], loss: 0.105411, mae: 0.317068, mean_q: 4.019585
 36348/100000: episode: 546, duration: 0.164s, episode steps: 31, steps per second: 188, episode reward: 74.518, mean reward: 2.404 [1.610, 4.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.614, 10.213], loss: 0.123037, mae: 0.323715, mean_q: 4.050948
 36379/100000: episode: 547, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 67.986, mean reward: 2.193 [1.540, 3.702], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.035, 10.160], loss: 0.092964, mae: 0.306923, mean_q: 4.023530
 36422/100000: episode: 548, duration: 0.226s, episode steps: 43, steps per second: 190, episode reward: 100.473, mean reward: 2.337 [1.869, 3.514], mean action: 0.000 [0.000, 0.000], mean observation: 1.971 [-1.058, 10.404], loss: 0.107536, mae: 0.317084, mean_q: 4.024195
 36466/100000: episode: 549, duration: 0.236s, episode steps: 44, steps per second: 187, episode reward: 103.126, mean reward: 2.344 [1.710, 4.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.663, 10.300], loss: 0.108354, mae: 0.314148, mean_q: 4.036101
 36510/100000: episode: 550, duration: 0.226s, episode steps: 44, steps per second: 195, episode reward: 99.375, mean reward: 2.259 [1.502, 5.565], mean action: 0.000 [0.000, 0.000], mean observation: 1.960 [-0.751, 10.336], loss: 0.108852, mae: 0.330199, mean_q: 4.015748
 36551/100000: episode: 551, duration: 0.194s, episode steps: 41, steps per second: 211, episode reward: 176.636, mean reward: 4.308 [2.064, 22.318], mean action: 0.000 [0.000, 0.000], mean observation: 1.984 [-0.764, 10.667], loss: 0.109889, mae: 0.324889, mean_q: 4.025386
 36575/100000: episode: 552, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 46.282, mean reward: 1.928 [1.516, 3.102], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.300, 10.249], loss: 0.132468, mae: 0.338463, mean_q: 4.059997
 36618/100000: episode: 553, duration: 0.231s, episode steps: 43, steps per second: 186, episode reward: 83.942, mean reward: 1.952 [1.492, 2.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.969 [-0.438, 10.152], loss: 0.100943, mae: 0.321564, mean_q: 4.071527
 36664/100000: episode: 554, duration: 0.225s, episode steps: 46, steps per second: 205, episode reward: 103.082, mean reward: 2.241 [1.546, 4.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.929 [-0.192, 10.215], loss: 0.140435, mae: 0.337168, mean_q: 4.068411
 36692/100000: episode: 555, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 68.963, mean reward: 2.463 [1.846, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.639, 10.260], loss: 0.339807, mae: 0.366082, mean_q: 4.041831
 36716/100000: episode: 556, duration: 0.121s, episode steps: 24, steps per second: 199, episode reward: 48.315, mean reward: 2.013 [1.739, 2.476], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.035, 10.245], loss: 0.426538, mae: 0.452028, mean_q: 4.154961
 36762/100000: episode: 557, duration: 0.227s, episode steps: 46, steps per second: 202, episode reward: 94.156, mean reward: 2.047 [1.556, 2.622], mean action: 0.000 [0.000, 0.000], mean observation: 1.935 [-1.176, 10.237], loss: 0.269642, mae: 0.386683, mean_q: 4.147674
 36771/100000: episode: 558, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 23.844, mean reward: 2.649 [2.121, 3.728], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.308, 10.398], loss: 0.161520, mae: 0.407228, mean_q: 4.029760
 36780/100000: episode: 559, duration: 0.054s, episode steps: 9, steps per second: 166, episode reward: 26.128, mean reward: 2.903 [2.320, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.477], loss: 0.235606, mae: 0.466214, mean_q: 4.090235
 36824/100000: episode: 560, duration: 0.234s, episode steps: 44, steps per second: 188, episode reward: 93.903, mean reward: 2.134 [1.613, 3.980], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.956, 10.304], loss: 0.297845, mae: 0.391236, mean_q: 4.094246
 36870/100000: episode: 561, duration: 0.240s, episode steps: 46, steps per second: 191, episode reward: 83.173, mean reward: 1.808 [1.481, 2.956], mean action: 0.000 [0.000, 0.000], mean observation: 1.941 [-0.157, 10.100], loss: 0.251333, mae: 0.361047, mean_q: 4.078162
 36901/100000: episode: 562, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 91.245, mean reward: 2.943 [1.868, 5.069], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.129, 10.591], loss: 0.153602, mae: 0.359687, mean_q: 4.126822
 36944/100000: episode: 563, duration: 0.220s, episode steps: 43, steps per second: 195, episode reward: 105.043, mean reward: 2.443 [1.525, 4.408], mean action: 0.000 [0.000, 0.000], mean observation: 1.961 [-0.832, 10.171], loss: 0.130578, mae: 0.338941, mean_q: 4.049382
 36968/100000: episode: 564, duration: 0.124s, episode steps: 24, steps per second: 194, episode reward: 60.317, mean reward: 2.513 [1.843, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.035, 10.452], loss: 0.376545, mae: 0.396116, mean_q: 4.095638
 36977/100000: episode: 565, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 21.864, mean reward: 2.429 [1.854, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.410], loss: 0.908581, mae: 0.608098, mean_q: 4.371954
 37008/100000: episode: 566, duration: 0.172s, episode steps: 31, steps per second: 180, episode reward: 95.835, mean reward: 3.091 [1.829, 6.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.082, 10.579], loss: 0.357151, mae: 0.424925, mean_q: 4.139920
 37036/100000: episode: 567, duration: 0.136s, episode steps: 28, steps per second: 206, episode reward: 71.901, mean reward: 2.568 [2.077, 3.313], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.156, 10.324], loss: 0.352080, mae: 0.396165, mean_q: 4.057490
 37077/100000: episode: 568, duration: 0.212s, episode steps: 41, steps per second: 193, episode reward: 93.584, mean reward: 2.283 [1.818, 3.576], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.079, 10.382], loss: 0.310642, mae: 0.401321, mean_q: 4.170512
 37118/100000: episode: 569, duration: 0.211s, episode steps: 41, steps per second: 194, episode reward: 85.428, mean reward: 2.084 [1.544, 2.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.990 [-0.080, 10.268], loss: 0.143260, mae: 0.353427, mean_q: 4.139410
 37161/100000: episode: 570, duration: 0.235s, episode steps: 43, steps per second: 183, episode reward: 92.487, mean reward: 2.151 [1.482, 3.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.967 [-0.393, 10.276], loss: 0.124096, mae: 0.332928, mean_q: 4.106418
 37170/100000: episode: 571, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 30.107, mean reward: 3.345 [2.494, 4.800], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.531], loss: 0.103064, mae: 0.327522, mean_q: 4.006630
 37205/100000: episode: 572, duration: 0.179s, episode steps: 35, steps per second: 196, episode reward: 76.857, mean reward: 2.196 [1.735, 2.927], mean action: 0.000 [0.000, 0.000], mean observation: 2.044 [-0.080, 10.277], loss: 0.124267, mae: 0.341814, mean_q: 4.121503
 37249/100000: episode: 573, duration: 0.225s, episode steps: 44, steps per second: 195, episode reward: 87.623, mean reward: 1.991 [1.454, 2.811], mean action: 0.000 [0.000, 0.000], mean observation: 1.946 [-0.279, 10.100], loss: 0.143883, mae: 0.364315, mean_q: 4.160985
 37280/100000: episode: 574, duration: 0.160s, episode steps: 31, steps per second: 194, episode reward: 90.796, mean reward: 2.929 [1.703, 8.151], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.035, 10.258], loss: 0.170088, mae: 0.371626, mean_q: 4.173110
 37323/100000: episode: 575, duration: 0.217s, episode steps: 43, steps per second: 198, episode reward: 111.806, mean reward: 2.600 [1.664, 5.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.974 [-1.700, 10.155], loss: 0.291449, mae: 0.388061, mean_q: 4.135945
 37366/100000: episode: 576, duration: 0.226s, episode steps: 43, steps per second: 190, episode reward: 96.906, mean reward: 2.254 [1.570, 3.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.955 [-0.234, 10.142], loss: 0.135396, mae: 0.340143, mean_q: 4.128242
 37401/100000: episode: 577, duration: 0.196s, episode steps: 35, steps per second: 178, episode reward: 73.234, mean reward: 2.092 [1.584, 4.373], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-2.037, 10.309], loss: 0.157814, mae: 0.375218, mean_q: 4.206286
 37429/100000: episode: 578, duration: 0.164s, episode steps: 28, steps per second: 171, episode reward: 75.765, mean reward: 2.706 [1.857, 4.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-2.450, 10.456], loss: 0.135131, mae: 0.349730, mean_q: 4.187543
 37464/100000: episode: 579, duration: 0.182s, episode steps: 35, steps per second: 192, episode reward: 69.718, mean reward: 1.992 [1.637, 2.503], mean action: 0.000 [0.000, 0.000], mean observation: 2.048 [-0.805, 10.279], loss: 0.141772, mae: 0.353684, mean_q: 4.183210
 37495/100000: episode: 580, duration: 0.155s, episode steps: 31, steps per second: 200, episode reward: 80.036, mean reward: 2.582 [1.994, 5.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.309, 10.395], loss: 0.357673, mae: 0.412975, mean_q: 4.249834
 37539/100000: episode: 581, duration: 0.218s, episode steps: 44, steps per second: 202, episode reward: 97.110, mean reward: 2.207 [1.490, 3.032], mean action: 0.000 [0.000, 0.000], mean observation: 1.948 [-0.721, 10.116], loss: 0.155863, mae: 0.372601, mean_q: 4.218563
 37585/100000: episode: 582, duration: 0.238s, episode steps: 46, steps per second: 193, episode reward: 81.910, mean reward: 1.781 [1.435, 2.294], mean action: 0.000 [0.000, 0.000], mean observation: 1.949 [-1.135, 10.318], loss: 0.290652, mae: 0.400143, mean_q: 4.212380
 37613/100000: episode: 583, duration: 0.159s, episode steps: 28, steps per second: 176, episode reward: 73.915, mean reward: 2.640 [1.942, 3.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.035, 10.454], loss: 0.161557, mae: 0.381836, mean_q: 4.216184
 37659/100000: episode: 584, duration: 0.238s, episode steps: 46, steps per second: 193, episode reward: 88.333, mean reward: 1.920 [1.463, 2.572], mean action: 0.000 [0.000, 0.000], mean observation: 1.924 [-1.048, 10.100], loss: 0.131274, mae: 0.349229, mean_q: 4.222892
 37690/100000: episode: 585, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 71.264, mean reward: 2.299 [1.563, 3.877], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.811, 10.200], loss: 0.316543, mae: 0.368617, mean_q: 4.185117
 37736/100000: episode: 586, duration: 0.223s, episode steps: 46, steps per second: 206, episode reward: 94.896, mean reward: 2.063 [1.456, 3.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.930 [-1.004, 10.151], loss: 0.188720, mae: 0.409280, mean_q: 4.232863
 37764/100000: episode: 587, duration: 0.137s, episode steps: 28, steps per second: 204, episode reward: 71.249, mean reward: 2.545 [1.979, 3.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.140, 10.332], loss: 0.115336, mae: 0.335642, mean_q: 4.230334
 37788/100000: episode: 588, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 44.410, mean reward: 1.850 [1.564, 2.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.862, 10.154], loss: 0.137438, mae: 0.352001, mean_q: 4.186122
 37819/100000: episode: 589, duration: 0.153s, episode steps: 31, steps per second: 203, episode reward: 83.168, mean reward: 2.683 [2.136, 3.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.675, 10.377], loss: 0.142074, mae: 0.349049, mean_q: 4.262066
 37828/100000: episode: 590, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 25.587, mean reward: 2.843 [2.039, 3.655], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.083, 10.524], loss: 0.200628, mae: 0.397491, mean_q: 4.230047
 37863/100000: episode: 591, duration: 0.176s, episode steps: 35, steps per second: 199, episode reward: 89.217, mean reward: 2.549 [1.504, 4.157], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.594, 10.127], loss: 0.137938, mae: 0.364029, mean_q: 4.243335
 37894/100000: episode: 592, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 66.055, mean reward: 2.131 [1.603, 2.704], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.261, 10.370], loss: 0.121351, mae: 0.331466, mean_q: 4.194170
 37918/100000: episode: 593, duration: 0.116s, episode steps: 24, steps per second: 206, episode reward: 54.980, mean reward: 2.291 [1.593, 3.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.572, 10.373], loss: 0.119696, mae: 0.341755, mean_q: 4.243175
 37953/100000: episode: 594, duration: 0.194s, episode steps: 35, steps per second: 181, episode reward: 67.466, mean reward: 1.928 [1.502, 2.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.674, 10.161], loss: 0.157708, mae: 0.373058, mean_q: 4.246334
 37994/100000: episode: 595, duration: 0.222s, episode steps: 41, steps per second: 185, episode reward: 93.719, mean reward: 2.286 [1.581, 3.004], mean action: 0.000 [0.000, 0.000], mean observation: 1.995 [-0.100, 10.388], loss: 0.137053, mae: 0.343837, mean_q: 4.209059
 38025/100000: episode: 596, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 100.049, mean reward: 3.227 [1.871, 4.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.035, 10.533], loss: 0.137726, mae: 0.351666, mean_q: 4.213694
 38034/100000: episode: 597, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 23.042, mean reward: 2.560 [2.239, 3.528], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.383], loss: 0.105116, mae: 0.321931, mean_q: 4.373263
 38058/100000: episode: 598, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 58.541, mean reward: 2.439 [1.717, 4.419], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.035, 10.376], loss: 0.180528, mae: 0.363974, mean_q: 4.289781
 38099/100000: episode: 599, duration: 0.207s, episode steps: 41, steps per second: 198, episode reward: 101.287, mean reward: 2.470 [1.926, 3.387], mean action: 0.000 [0.000, 0.000], mean observation: 1.989 [-0.586, 10.283], loss: 0.142954, mae: 0.367076, mean_q: 4.247169
 38134/100000: episode: 600, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 76.850, mean reward: 2.196 [1.776, 3.294], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.146, 10.348], loss: 0.113251, mae: 0.335131, mean_q: 4.158599
 38177/100000: episode: 601, duration: 0.208s, episode steps: 43, steps per second: 207, episode reward: 97.171, mean reward: 2.260 [1.465, 5.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.965 [-0.341, 10.216], loss: 0.186728, mae: 0.392963, mean_q: 4.296976
 38186/100000: episode: 602, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 29.249, mean reward: 3.250 [2.184, 4.153], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.485], loss: 0.109857, mae: 0.314950, mean_q: 4.289042
 38229/100000: episode: 603, duration: 0.209s, episode steps: 43, steps per second: 206, episode reward: 88.111, mean reward: 2.049 [1.497, 3.109], mean action: 0.000 [0.000, 0.000], mean observation: 1.945 [-1.027, 10.100], loss: 0.166684, mae: 0.388511, mean_q: 4.273640
 38257/100000: episode: 604, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 62.993, mean reward: 2.250 [1.954, 2.550], mean action: 0.000 [0.000, 0.000], mean observation: 2.108 [-0.229, 10.375], loss: 0.140141, mae: 0.359962, mean_q: 4.281204
 38266/100000: episode: 605, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 21.561, mean reward: 2.396 [1.959, 2.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.349, 10.394], loss: 0.154990, mae: 0.327956, mean_q: 4.314194
 38312/100000: episode: 606, duration: 0.237s, episode steps: 46, steps per second: 194, episode reward: 82.787, mean reward: 1.800 [1.442, 2.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.936 [-0.746, 10.141], loss: 0.259422, mae: 0.355125, mean_q: 4.275452
 38343/100000: episode: 607, duration: 0.152s, episode steps: 31, steps per second: 204, episode reward: 70.203, mean reward: 2.265 [1.582, 4.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.070, 10.181], loss: 0.115684, mae: 0.336748, mean_q: 4.255549
 38371/100000: episode: 608, duration: 0.162s, episode steps: 28, steps per second: 173, episode reward: 56.986, mean reward: 2.035 [1.464, 2.809], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.776, 10.102], loss: 0.151758, mae: 0.354411, mean_q: 4.259935
 38395/100000: episode: 609, duration: 0.115s, episode steps: 24, steps per second: 208, episode reward: 57.320, mean reward: 2.388 [1.963, 3.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.308], loss: 0.149898, mae: 0.344023, mean_q: 4.235508
[Info] 2-TH LEVEL FOUND: 6.526585578918457, Considering 10/90 traces
 38439/100000: episode: 610, duration: 4.475s, episode steps: 44, steps per second: 10, episode reward: 96.622, mean reward: 2.196 [1.625, 2.984], mean action: 0.000 [0.000, 0.000], mean observation: 1.947 [-0.299, 10.100], loss: 0.137505, mae: 0.361873, mean_q: 4.275963
 38461/100000: episode: 611, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 62.729, mean reward: 2.851 [1.739, 4.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.769, 10.312], loss: 0.178780, mae: 0.376363, mean_q: 4.249676
 38484/100000: episode: 612, duration: 0.120s, episode steps: 23, steps per second: 191, episode reward: 81.885, mean reward: 3.560 [2.607, 4.225], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.393, 10.452], loss: 0.172309, mae: 0.387160, mean_q: 4.334788
 38512/100000: episode: 613, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 91.859, mean reward: 3.281 [2.502, 4.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.844, 10.481], loss: 0.343616, mae: 0.387108, mean_q: 4.273005
 38524/100000: episode: 614, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 36.677, mean reward: 3.056 [2.566, 3.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.413], loss: 0.182420, mae: 0.384296, mean_q: 4.447942
 38550/100000: episode: 615, duration: 0.131s, episode steps: 26, steps per second: 198, episode reward: 57.587, mean reward: 2.215 [1.452, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.128, 10.167], loss: 0.119193, mae: 0.335952, mean_q: 4.323547
 38576/100000: episode: 616, duration: 0.125s, episode steps: 26, steps per second: 208, episode reward: 75.574, mean reward: 2.907 [1.996, 4.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.189, 10.270], loss: 0.131507, mae: 0.346803, mean_q: 4.290683
 38588/100000: episode: 617, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 45.781, mean reward: 3.815 [2.994, 4.561], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.523, 10.521], loss: 0.166293, mae: 0.356337, mean_q: 4.280613
 38621/100000: episode: 618, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 89.263, mean reward: 2.705 [1.591, 3.990], mean action: 0.000 [0.000, 0.000], mean observation: 2.061 [-0.425, 10.335], loss: 0.742522, mae: 0.545393, mean_q: 4.420675
 38633/100000: episode: 619, duration: 0.060s, episode steps: 12, steps per second: 202, episode reward: 54.928, mean reward: 4.577 [3.241, 9.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.440], loss: 0.205154, mae: 0.428254, mean_q: 4.290212
 38655/100000: episode: 620, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 62.052, mean reward: 2.821 [1.993, 4.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.711, 10.385], loss: 0.124852, mae: 0.358326, mean_q: 4.307601
 38685/100000: episode: 621, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 139.002, mean reward: 4.633 [2.594, 9.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.361, 10.453], loss: 0.170367, mae: 0.377043, mean_q: 4.414097
 38715/100000: episode: 622, duration: 0.152s, episode steps: 30, steps per second: 197, episode reward: 95.388, mean reward: 3.180 [1.788, 5.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.327, 10.232], loss: 0.164106, mae: 0.386261, mean_q: 4.393893
 38737/100000: episode: 623, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 74.459, mean reward: 3.385 [2.545, 4.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.526], loss: 0.500235, mae: 0.487858, mean_q: 4.406264
 38770/100000: episode: 624, duration: 0.180s, episode steps: 33, steps per second: 183, episode reward: 79.768, mean reward: 2.417 [1.762, 4.755], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.098, 10.254], loss: 0.377151, mae: 0.474147, mean_q: 4.378759
 38796/100000: episode: 625, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 54.331, mean reward: 2.090 [1.512, 3.137], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.035, 10.103], loss: 0.269807, mae: 0.444770, mean_q: 4.476439
 38819/100000: episode: 626, duration: 0.119s, episode steps: 23, steps per second: 193, episode reward: 78.386, mean reward: 3.408 [2.504, 4.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.208, 10.553], loss: 0.198352, mae: 0.402221, mean_q: 4.463178
 38849/100000: episode: 627, duration: 0.152s, episode steps: 30, steps per second: 197, episode reward: 106.123, mean reward: 3.537 [2.171, 5.943], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.568, 10.418], loss: 0.168159, mae: 0.378096, mean_q: 4.410693
 38861/100000: episode: 628, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 37.529, mean reward: 3.127 [2.544, 3.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.142, 10.487], loss: 0.119301, mae: 0.348676, mean_q: 4.364927
 38884/100000: episode: 629, duration: 0.114s, episode steps: 23, steps per second: 201, episode reward: 63.396, mean reward: 2.756 [1.909, 4.289], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.220, 10.523], loss: 0.141217, mae: 0.364495, mean_q: 4.396506
 38890/100000: episode: 630, duration: 0.044s, episode steps: 6, steps per second: 138, episode reward: 23.537, mean reward: 3.923 [3.262, 4.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.362 [-0.035, 10.552], loss: 0.180456, mae: 0.396881, mean_q: 4.350028
 38896/100000: episode: 631, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 16.239, mean reward: 2.706 [2.284, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-1.444, 10.310], loss: 0.086468, mae: 0.318742, mean_q: 4.315666
 38922/100000: episode: 632, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 89.995, mean reward: 3.461 [1.903, 11.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.156, 10.359], loss: 0.185690, mae: 0.391238, mean_q: 4.498383
 38955/100000: episode: 633, duration: 0.167s, episode steps: 33, steps per second: 198, episode reward: 153.352, mean reward: 4.647 [2.485, 9.539], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.627, 10.383], loss: 0.143858, mae: 0.375499, mean_q: 4.456150
 38967/100000: episode: 634, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 37.139, mean reward: 3.095 [2.473, 4.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.441], loss: 0.151773, mae: 0.378355, mean_q: 4.497059
 38997/100000: episode: 635, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 92.598, mean reward: 3.087 [2.350, 5.029], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-1.313, 10.497], loss: 0.168716, mae: 0.385532, mean_q: 4.460025
 39025/100000: episode: 636, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 66.927, mean reward: 2.390 [1.754, 2.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.244, 10.371], loss: 0.157588, mae: 0.387854, mean_q: 4.467350
 39058/100000: episode: 637, duration: 0.178s, episode steps: 33, steps per second: 186, episode reward: 99.587, mean reward: 3.018 [2.322, 3.684], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.773, 10.349], loss: 0.143194, mae: 0.375187, mean_q: 4.470829
 39084/100000: episode: 638, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 79.269, mean reward: 3.049 [2.206, 4.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.035, 10.330], loss: 0.457958, mae: 0.464095, mean_q: 4.593451
 39114/100000: episode: 639, duration: 0.157s, episode steps: 30, steps per second: 191, episode reward: 92.328, mean reward: 3.078 [1.902, 4.654], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.800, 10.298], loss: 0.214989, mae: 0.397337, mean_q: 4.546598
 39137/100000: episode: 640, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 112.701, mean reward: 4.900 [2.663, 8.658], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.644, 10.565], loss: 0.440046, mae: 0.436161, mean_q: 4.546405
 39163/100000: episode: 641, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 75.093, mean reward: 2.888 [2.096, 4.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.035, 10.552], loss: 0.213564, mae: 0.435993, mean_q: 4.605630
 39191/100000: episode: 642, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 73.155, mean reward: 2.613 [2.009, 3.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-1.317, 10.351], loss: 0.170559, mae: 0.402246, mean_q: 4.577643
 39217/100000: episode: 643, duration: 0.125s, episode steps: 26, steps per second: 207, episode reward: 55.162, mean reward: 2.122 [1.633, 2.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.367, 10.185], loss: 0.187551, mae: 0.403181, mean_q: 4.580389
 39223/100000: episode: 644, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 19.600, mean reward: 3.267 [2.792, 4.277], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.114, 10.418], loss: 0.249051, mae: 0.555468, mean_q: 4.859829
 39253/100000: episode: 645, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 71.311, mean reward: 2.377 [1.703, 3.827], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.191, 10.300], loss: 0.188996, mae: 0.405404, mean_q: 4.579580
 39281/100000: episode: 646, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 77.277, mean reward: 2.760 [1.982, 4.148], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.619, 10.374], loss: 0.197270, mae: 0.408060, mean_q: 4.694561
 39309/100000: episode: 647, duration: 0.158s, episode steps: 28, steps per second: 177, episode reward: 71.027, mean reward: 2.537 [1.757, 4.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-0.514, 10.315], loss: 0.208739, mae: 0.415634, mean_q: 4.597165
 39335/100000: episode: 648, duration: 0.138s, episode steps: 26, steps per second: 188, episode reward: 78.944, mean reward: 3.036 [2.201, 5.077], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.052, 10.508], loss: 0.194185, mae: 0.406673, mean_q: 4.595961
 39363/100000: episode: 649, duration: 0.144s, episode steps: 28, steps per second: 194, episode reward: 59.657, mean reward: 2.131 [1.568, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.967, 10.206], loss: 0.217956, mae: 0.439622, mean_q: 4.735229
 39393/100000: episode: 650, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 98.948, mean reward: 3.298 [2.303, 5.057], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.311, 10.419], loss: 0.180024, mae: 0.393150, mean_q: 4.636443
 39426/100000: episode: 651, duration: 0.166s, episode steps: 33, steps per second: 199, episode reward: 87.748, mean reward: 2.659 [1.813, 4.595], mean action: 0.000 [0.000, 0.000], mean observation: 2.067 [-0.098, 10.428], loss: 0.182709, mae: 0.383899, mean_q: 4.650497
 39452/100000: episode: 652, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 95.039, mean reward: 3.655 [2.418, 5.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.035, 10.475], loss: 0.185974, mae: 0.397066, mean_q: 4.715014
 39474/100000: episode: 653, duration: 0.115s, episode steps: 22, steps per second: 191, episode reward: 72.312, mean reward: 3.287 [2.369, 4.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.436], loss: 0.189282, mae: 0.415881, mean_q: 4.748347
 39496/100000: episode: 654, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 68.439, mean reward: 3.111 [2.395, 4.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.427], loss: 0.188804, mae: 0.426699, mean_q: 4.640992
 39524/100000: episode: 655, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 79.791, mean reward: 2.850 [2.235, 8.066], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-1.024, 10.449], loss: 0.236471, mae: 0.423790, mean_q: 4.606098
 39546/100000: episode: 656, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 52.084, mean reward: 2.367 [2.123, 3.047], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.581, 10.369], loss: 0.282999, mae: 0.447951, mean_q: 4.792845
 39574/100000: episode: 657, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 78.825, mean reward: 2.815 [2.149, 3.486], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-1.069, 10.422], loss: 0.215974, mae: 0.424212, mean_q: 4.750173
 39597/100000: episode: 658, duration: 0.112s, episode steps: 23, steps per second: 204, episode reward: 81.888, mean reward: 3.560 [2.408, 5.382], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.271, 10.473], loss: 0.179276, mae: 0.418912, mean_q: 4.626852
 39609/100000: episode: 659, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 40.439, mean reward: 3.370 [2.651, 4.086], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.538], loss: 0.216035, mae: 0.430293, mean_q: 4.702120
 39631/100000: episode: 660, duration: 0.137s, episode steps: 22, steps per second: 160, episode reward: 139.142, mean reward: 6.325 [2.774, 19.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.240, 10.595], loss: 0.206255, mae: 0.416517, mean_q: 4.740361
 39653/100000: episode: 661, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 49.645, mean reward: 2.257 [1.699, 3.355], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.213, 10.273], loss: 0.532143, mae: 0.496288, mean_q: 4.744915
 39675/100000: episode: 662, duration: 0.116s, episode steps: 22, steps per second: 189, episode reward: 55.293, mean reward: 2.513 [2.018, 3.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.210, 10.450], loss: 0.257247, mae: 0.474367, mean_q: 4.839895
 39698/100000: episode: 663, duration: 0.131s, episode steps: 23, steps per second: 176, episode reward: 114.600, mean reward: 4.983 [2.362, 13.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.434], loss: 0.367384, mae: 0.476438, mean_q: 4.906651
 39720/100000: episode: 664, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 66.494, mean reward: 3.022 [2.129, 6.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.349], loss: 0.253949, mae: 0.467111, mean_q: 4.819088
 39750/100000: episode: 665, duration: 0.153s, episode steps: 30, steps per second: 196, episode reward: 78.658, mean reward: 2.622 [1.642, 3.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.178, 10.262], loss: 0.260230, mae: 0.459113, mean_q: 4.933199
 39773/100000: episode: 666, duration: 0.129s, episode steps: 23, steps per second: 178, episode reward: 61.471, mean reward: 2.673 [2.212, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.406, 10.375], loss: 0.202423, mae: 0.418675, mean_q: 4.719251
 39801/100000: episode: 667, duration: 0.137s, episode steps: 28, steps per second: 204, episode reward: 114.475, mean reward: 4.088 [2.497, 6.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.259, 10.446], loss: 0.254174, mae: 0.434523, mean_q: 4.859643
 39827/100000: episode: 668, duration: 0.127s, episode steps: 26, steps per second: 204, episode reward: 78.405, mean reward: 3.016 [2.270, 4.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.797, 10.467], loss: 0.234109, mae: 0.448541, mean_q: 4.824563
 39839/100000: episode: 669, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 43.839, mean reward: 3.653 [2.688, 5.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.301, 10.522], loss: 0.226541, mae: 0.450523, mean_q: 4.750526
 39845/100000: episode: 670, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 19.061, mean reward: 3.177 [2.954, 3.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.424], loss: 0.255364, mae: 0.420439, mean_q: 4.761679
 39871/100000: episode: 671, duration: 0.132s, episode steps: 26, steps per second: 196, episode reward: 104.311, mean reward: 4.012 [2.693, 6.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.035, 10.599], loss: 0.425296, mae: 0.519267, mean_q: 4.915549
 39893/100000: episode: 672, duration: 0.106s, episode steps: 22, steps per second: 208, episode reward: 73.022, mean reward: 3.319 [2.198, 4.575], mean action: 0.000 [0.000, 0.000], mean observation: 2.152 [-0.863, 10.435], loss: 0.330625, mae: 0.546041, mean_q: 4.817275
 39919/100000: episode: 673, duration: 0.129s, episode steps: 26, steps per second: 202, episode reward: 83.915, mean reward: 3.227 [2.203, 6.015], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.370, 10.340], loss: 0.619588, mae: 0.536851, mean_q: 4.914137
 39949/100000: episode: 674, duration: 0.145s, episode steps: 30, steps per second: 207, episode reward: 132.717, mean reward: 4.424 [2.116, 7.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.580, 10.404], loss: 0.279088, mae: 0.470249, mean_q: 4.968526
 39972/100000: episode: 675, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 81.212, mean reward: 3.531 [2.595, 4.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.235, 10.555], loss: 0.288567, mae: 0.489395, mean_q: 4.953892
 40005/100000: episode: 676, duration: 0.159s, episode steps: 33, steps per second: 207, episode reward: 84.894, mean reward: 2.573 [2.120, 3.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.197, 10.369], loss: 0.445742, mae: 0.514281, mean_q: 5.028449
 40035/100000: episode: 677, duration: 0.159s, episode steps: 30, steps per second: 188, episode reward: 83.240, mean reward: 2.775 [1.875, 4.403], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.548, 10.312], loss: 0.257984, mae: 0.468571, mean_q: 4.971210
 40058/100000: episode: 678, duration: 0.115s, episode steps: 23, steps per second: 200, episode reward: 106.697, mean reward: 4.639 [2.235, 8.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.200, 10.408], loss: 0.169286, mae: 0.390241, mean_q: 4.930867
 40064/100000: episode: 679, duration: 0.041s, episode steps: 6, steps per second: 148, episode reward: 21.863, mean reward: 3.644 [2.736, 6.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.443], loss: 0.223522, mae: 0.463326, mean_q: 5.002636
 40092/100000: episode: 680, duration: 0.149s, episode steps: 28, steps per second: 188, episode reward: 78.692, mean reward: 2.810 [1.659, 3.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-0.707, 10.162], loss: 0.402106, mae: 0.500732, mean_q: 5.017021
 40120/100000: episode: 681, duration: 0.138s, episode steps: 28, steps per second: 202, episode reward: 94.598, mean reward: 3.378 [2.021, 4.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.079, 10.411], loss: 0.562441, mae: 0.536837, mean_q: 5.083095
 40150/100000: episode: 682, duration: 0.147s, episode steps: 30, steps per second: 204, episode reward: 101.232, mean reward: 3.374 [2.531, 7.344], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.035, 10.435], loss: 0.256348, mae: 0.478036, mean_q: 4.952604
 40162/100000: episode: 683, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 40.611, mean reward: 3.384 [2.444, 6.079], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.909, 10.398], loss: 0.244520, mae: 0.450449, mean_q: 5.047639
 40190/100000: episode: 684, duration: 0.143s, episode steps: 28, steps per second: 196, episode reward: 111.385, mean reward: 3.978 [2.719, 6.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.035, 10.470], loss: 0.302021, mae: 0.500457, mean_q: 5.077796
 40220/100000: episode: 685, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 82.353, mean reward: 2.745 [2.102, 3.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.035, 10.376], loss: 0.344372, mae: 0.466946, mean_q: 5.142398
 40246/100000: episode: 686, duration: 0.132s, episode steps: 26, steps per second: 196, episode reward: 68.020, mean reward: 2.616 [1.847, 4.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.128, 10.249], loss: 0.681399, mae: 0.534676, mean_q: 5.133903
 40272/100000: episode: 687, duration: 0.139s, episode steps: 26, steps per second: 187, episode reward: 76.425, mean reward: 2.939 [2.259, 3.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.716, 10.453], loss: 0.353329, mae: 0.481090, mean_q: 5.069099
 40294/100000: episode: 688, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 55.355, mean reward: 2.516 [1.974, 3.548], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.035, 10.319], loss: 0.245143, mae: 0.486010, mean_q: 5.086613
 40306/100000: episode: 689, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 51.213, mean reward: 4.268 [2.891, 7.065], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.615], loss: 0.244250, mae: 0.484065, mean_q: 5.179163
 40336/100000: episode: 690, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 101.806, mean reward: 3.394 [1.985, 4.836], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.035, 10.472], loss: 0.580033, mae: 0.562088, mean_q: 5.219491
[Info] FALSIFICATION!
 40365/100000: episode: 691, duration: 0.396s, episode steps: 29, steps per second: 73, episode reward: 1195.632, mean reward: 41.229 [2.647, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.300, 10.435], loss: 0.288482, mae: 0.493624, mean_q: 5.168196
 40377/100000: episode: 692, duration: 0.067s, episode steps: 12, steps per second: 180, episode reward: 50.484, mean reward: 4.207 [3.079, 5.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.405], loss: 0.620340, mae: 0.636661, mean_q: 5.216198
 40403/100000: episode: 693, duration: 0.131s, episode steps: 26, steps per second: 199, episode reward: 63.763, mean reward: 2.452 [2.169, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.294, 10.413], loss: 0.684317, mae: 0.608906, mean_q: 5.218078
 40425/100000: episode: 694, duration: 0.107s, episode steps: 22, steps per second: 206, episode reward: 67.193, mean reward: 3.054 [1.799, 4.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.230, 10.322], loss: 0.302503, mae: 0.489445, mean_q: 5.158183
 40448/100000: episode: 695, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 83.860, mean reward: 3.646 [2.758, 5.172], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.511, 10.563], loss: 0.335861, mae: 0.535353, mean_q: 5.216904
 40470/100000: episode: 696, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 59.803, mean reward: 2.718 [2.156, 4.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.040, 10.368], loss: 696.502136, mae: 3.929720, mean_q: 7.310286
 40503/100000: episode: 697, duration: 0.172s, episode steps: 33, steps per second: 192, episode reward: 130.360, mean reward: 3.950 [2.649, 8.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.050 [-0.554, 10.466], loss: 1.476588, mae: 1.206024, mean_q: 4.808505
 40525/100000: episode: 698, duration: 0.108s, episode steps: 22, steps per second: 203, episode reward: 54.082, mean reward: 2.458 [2.143, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.472, 10.404], loss: 701.181335, mae: 2.747612, mean_q: 6.071551
 40548/100000: episode: 699, duration: 0.117s, episode steps: 23, steps per second: 197, episode reward: 81.824, mean reward: 3.558 [2.605, 6.357], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.688, 10.639], loss: 0.610425, mae: 0.756905, mean_q: 4.732310
[Info] Complete ISplit Iteration
[Info] Levels: [4.8400645, 6.5265856, 8.333282]
[Info] Cond. Prob: [0.1, 0.1, 0.14]
[Info] Error Prob: 0.0014000000000000004

 40574/100000: episode: 700, duration: 4.516s, episode steps: 26, steps per second: 6, episode reward: 72.240, mean reward: 2.778 [2.000, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.143 [-0.131, 10.377], loss: 0.718472, mae: 0.618759, mean_q: 5.231880
 40674/100000: episode: 701, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 228.135, mean reward: 2.281 [1.474, 4.409], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.828, 10.098], loss: 154.787094, mae: 1.222356, mean_q: 5.472513
 40774/100000: episode: 702, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 186.619, mean reward: 1.866 [1.460, 3.317], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.551, 10.158], loss: 0.405338, mae: 0.570285, mean_q: 5.233871
 40874/100000: episode: 703, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 203.893, mean reward: 2.039 [1.485, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.882, 10.098], loss: 0.419216, mae: 0.552383, mean_q: 5.272915
 40974/100000: episode: 704, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 188.936, mean reward: 1.889 [1.466, 2.737], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.567, 10.098], loss: 0.540578, mae: 0.561789, mean_q: 5.206464
 41074/100000: episode: 705, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 190.897, mean reward: 1.909 [1.481, 3.020], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.296, 10.233], loss: 154.544174, mae: 1.190265, mean_q: 5.425705
 41174/100000: episode: 706, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 179.826, mean reward: 1.798 [1.450, 3.181], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.498, 10.183], loss: 0.528033, mae: 0.576294, mean_q: 5.316085
 41274/100000: episode: 707, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 190.064, mean reward: 1.901 [1.490, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.661, 10.255], loss: 0.826365, mae: 0.603692, mean_q: 5.251112
 41374/100000: episode: 708, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 207.442, mean reward: 2.074 [1.453, 4.108], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.830, 10.098], loss: 0.520673, mae: 0.551047, mean_q: 5.238465
 41474/100000: episode: 709, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 188.860, mean reward: 1.889 [1.462, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.382, 10.209], loss: 0.526563, mae: 0.540941, mean_q: 5.174314
 41574/100000: episode: 710, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 206.629, mean reward: 2.066 [1.436, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.865, 10.098], loss: 0.490104, mae: 0.536962, mean_q: 5.165945
 41674/100000: episode: 711, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 190.710, mean reward: 1.907 [1.451, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.708, 10.098], loss: 154.309586, mae: 1.193157, mean_q: 5.290602
 41774/100000: episode: 712, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 182.566, mean reward: 1.826 [1.460, 3.386], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.798, 10.201], loss: 0.489850, mae: 0.556774, mean_q: 5.191825
 41874/100000: episode: 713, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 191.087, mean reward: 1.911 [1.444, 3.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.816, 10.098], loss: 0.454475, mae: 0.546364, mean_q: 5.158062
 41974/100000: episode: 714, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 208.294, mean reward: 2.083 [1.467, 4.128], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.672, 10.098], loss: 308.351440, mae: 2.044080, mean_q: 5.758113
 42074/100000: episode: 715, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 180.685, mean reward: 1.807 [1.474, 2.831], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.212, 10.173], loss: 153.602493, mae: 1.248126, mean_q: 5.364910
 42174/100000: episode: 716, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 196.236, mean reward: 1.962 [1.478, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.703, 10.272], loss: 154.374878, mae: 1.414428, mean_q: 5.453585
 42274/100000: episode: 717, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 193.158, mean reward: 1.932 [1.472, 2.842], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.781, 10.195], loss: 0.498783, mae: 0.621481, mean_q: 5.173135
 42374/100000: episode: 718, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 235.958, mean reward: 2.360 [1.447, 4.666], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.924, 10.098], loss: 154.174789, mae: 1.339941, mean_q: 5.412068
 42474/100000: episode: 719, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 195.624, mean reward: 1.956 [1.502, 3.117], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.631, 10.152], loss: 0.544277, mae: 0.568256, mean_q: 5.077805
 42574/100000: episode: 720, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 209.895, mean reward: 2.099 [1.479, 4.063], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.462, 10.098], loss: 153.903000, mae: 1.303560, mean_q: 5.431636
 42674/100000: episode: 721, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 176.397, mean reward: 1.764 [1.444, 2.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.725, 10.116], loss: 0.616845, mae: 0.601006, mean_q: 5.177787
 42774/100000: episode: 722, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 208.436, mean reward: 2.084 [1.463, 3.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-1.088, 10.355], loss: 307.068756, mae: 1.812962, mean_q: 5.682856
 42874/100000: episode: 723, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 188.966, mean reward: 1.890 [1.454, 3.144], mean action: 0.000 [0.000, 0.000], mean observation: 1.444 [-0.452, 10.098], loss: 153.750992, mae: 1.345966, mean_q: 5.523298
 42974/100000: episode: 724, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 208.014, mean reward: 2.080 [1.437, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.661, 10.151], loss: 305.857727, mae: 1.762199, mean_q: 5.902292
 43074/100000: episode: 725, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 180.926, mean reward: 1.809 [1.454, 2.566], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.274, 10.147], loss: 153.950912, mae: 1.374964, mean_q: 5.435941
 43174/100000: episode: 726, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 231.868, mean reward: 2.319 [1.470, 4.021], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.399, 10.274], loss: 0.673447, mae: 0.663261, mean_q: 5.155193
 43274/100000: episode: 727, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 189.063, mean reward: 1.891 [1.479, 2.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.209, 10.158], loss: 153.555374, mae: 1.172821, mean_q: 5.364101
 43374/100000: episode: 728, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 198.783, mean reward: 1.988 [1.439, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.136, 10.240], loss: 154.012848, mae: 1.365780, mean_q: 5.392537
 43474/100000: episode: 729, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 190.659, mean reward: 1.907 [1.443, 2.996], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.962, 10.098], loss: 0.586916, mae: 0.615348, mean_q: 5.010915
 43574/100000: episode: 730, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 180.342, mean reward: 1.803 [1.496, 2.744], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.829, 10.197], loss: 0.486959, mae: 0.565848, mean_q: 4.935827
 43674/100000: episode: 731, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 185.283, mean reward: 1.853 [1.440, 3.428], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.225, 10.098], loss: 303.921356, mae: 1.625666, mean_q: 5.497715
 43774/100000: episode: 732, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 210.598, mean reward: 2.106 [1.490, 4.015], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.538, 10.427], loss: 305.304291, mae: 1.601716, mean_q: 5.336318
 43874/100000: episode: 733, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 180.690, mean reward: 1.807 [1.439, 2.803], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.270, 10.221], loss: 1.627834, mae: 0.924767, mean_q: 5.157299
 43974/100000: episode: 734, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 203.689, mean reward: 2.037 [1.457, 3.710], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.531, 10.173], loss: 153.014694, mae: 1.202372, mean_q: 5.174736
 44074/100000: episode: 735, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 197.438, mean reward: 1.974 [1.487, 6.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.610, 10.287], loss: 0.486525, mae: 0.564646, mean_q: 4.760552
 44174/100000: episode: 736, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 195.978, mean reward: 1.960 [1.504, 3.663], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.893, 10.147], loss: 0.430353, mae: 0.509337, mean_q: 4.662462
 44274/100000: episode: 737, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 193.433, mean reward: 1.934 [1.485, 3.550], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.448, 10.352], loss: 0.500924, mae: 0.500975, mean_q: 4.651884
 44374/100000: episode: 738, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 190.721, mean reward: 1.907 [1.447, 3.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.260, 10.253], loss: 153.338531, mae: 1.463432, mean_q: 5.263616
 44474/100000: episode: 739, duration: 0.489s, episode steps: 100, steps per second: 205, episode reward: 196.513, mean reward: 1.965 [1.461, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.542, 10.349], loss: 152.736298, mae: 1.173063, mean_q: 4.967590
 44574/100000: episode: 740, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 193.102, mean reward: 1.931 [1.478, 4.534], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.920, 10.108], loss: 0.489323, mae: 0.555288, mean_q: 4.529644
 44674/100000: episode: 741, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 255.483, mean reward: 2.555 [1.485, 7.347], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.322, 10.547], loss: 152.795273, mae: 1.144011, mean_q: 4.818673
 44774/100000: episode: 742, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 193.254, mean reward: 1.933 [1.466, 3.340], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.082, 10.098], loss: 0.414550, mae: 0.472886, mean_q: 4.372199
 44874/100000: episode: 743, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 193.102, mean reward: 1.931 [1.447, 4.188], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.308, 10.110], loss: 0.254304, mae: 0.440460, mean_q: 4.339934
 44974/100000: episode: 744, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 198.544, mean reward: 1.985 [1.444, 3.247], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.149, 10.117], loss: 152.244720, mae: 0.953173, mean_q: 4.597451
 45074/100000: episode: 745, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 229.785, mean reward: 2.298 [1.505, 3.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.398, 10.413], loss: 0.334845, mae: 0.422025, mean_q: 4.283187
 45174/100000: episode: 746, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 188.051, mean reward: 1.881 [1.483, 3.161], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.171, 10.182], loss: 151.989517, mae: 0.854166, mean_q: 4.386107
 45274/100000: episode: 747, duration: 0.533s, episode steps: 100, steps per second: 188, episode reward: 187.636, mean reward: 1.876 [1.477, 3.003], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.360, 10.098], loss: 152.440338, mae: 1.231732, mean_q: 4.695596
 45374/100000: episode: 748, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 188.200, mean reward: 1.882 [1.487, 3.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.586, 10.261], loss: 0.444110, mae: 0.459281, mean_q: 4.097159
 45474/100000: episode: 749, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 197.089, mean reward: 1.971 [1.457, 4.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.303, 10.146], loss: 0.202557, mae: 0.379260, mean_q: 3.991456
 45574/100000: episode: 750, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 194.206, mean reward: 1.942 [1.481, 4.349], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.078, 10.149], loss: 0.122976, mae: 0.331940, mean_q: 3.931244
 45674/100000: episode: 751, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 198.364, mean reward: 1.984 [1.457, 2.940], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.942, 10.348], loss: 0.129861, mae: 0.329748, mean_q: 3.935953
 45774/100000: episode: 752, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 192.586, mean reward: 1.926 [1.460, 8.423], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.904, 10.220], loss: 0.138166, mae: 0.328531, mean_q: 3.936536
 45874/100000: episode: 753, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 177.415, mean reward: 1.774 [1.478, 2.789], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.730, 10.150], loss: 0.118282, mae: 0.311600, mean_q: 3.907376
 45974/100000: episode: 754, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 222.355, mean reward: 2.224 [1.514, 3.677], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.145, 10.241], loss: 0.141168, mae: 0.331005, mean_q: 3.912476
 46074/100000: episode: 755, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 186.998, mean reward: 1.870 [1.465, 3.467], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.468, 10.098], loss: 0.126392, mae: 0.315747, mean_q: 3.894339
 46174/100000: episode: 756, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 176.038, mean reward: 1.760 [1.468, 2.600], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.209, 10.148], loss: 0.104438, mae: 0.318175, mean_q: 3.910820
 46274/100000: episode: 757, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 213.188, mean reward: 2.132 [1.563, 3.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.661, 10.233], loss: 0.110783, mae: 0.310144, mean_q: 3.909938
 46374/100000: episode: 758, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 202.358, mean reward: 2.024 [1.476, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.644, 10.098], loss: 0.143525, mae: 0.336864, mean_q: 3.920601
 46474/100000: episode: 759, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 196.637, mean reward: 1.966 [1.469, 3.769], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.486, 10.135], loss: 0.100323, mae: 0.303478, mean_q: 3.920049
 46574/100000: episode: 760, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 193.298, mean reward: 1.933 [1.472, 3.141], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.454, 10.098], loss: 0.110261, mae: 0.308277, mean_q: 3.904675
 46674/100000: episode: 761, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 183.975, mean reward: 1.840 [1.442, 3.019], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.533, 10.098], loss: 0.122371, mae: 0.314287, mean_q: 3.934322
 46774/100000: episode: 762, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 186.632, mean reward: 1.866 [1.509, 4.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.201, 10.098], loss: 0.100407, mae: 0.305674, mean_q: 3.900268
 46874/100000: episode: 763, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 189.982, mean reward: 1.900 [1.454, 2.853], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.507, 10.098], loss: 0.111665, mae: 0.318741, mean_q: 3.916149
 46974/100000: episode: 764, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 186.681, mean reward: 1.867 [1.471, 3.167], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.898, 10.147], loss: 0.096880, mae: 0.296944, mean_q: 3.894422
 47074/100000: episode: 765, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 209.233, mean reward: 2.092 [1.495, 3.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.806, 10.217], loss: 0.088301, mae: 0.295103, mean_q: 3.903872
 47174/100000: episode: 766, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 172.356, mean reward: 1.724 [1.468, 2.779], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.312, 10.098], loss: 0.111270, mae: 0.297054, mean_q: 3.897276
 47274/100000: episode: 767, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 183.077, mean reward: 1.831 [1.473, 5.084], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.051, 10.098], loss: 0.127814, mae: 0.309171, mean_q: 3.917454
 47374/100000: episode: 768, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 191.973, mean reward: 1.920 [1.461, 3.005], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.196, 10.098], loss: 0.093339, mae: 0.288941, mean_q: 3.896751
 47474/100000: episode: 769, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 179.782, mean reward: 1.798 [1.441, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.738, 10.098], loss: 0.089582, mae: 0.289462, mean_q: 3.891310
 47574/100000: episode: 770, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 205.415, mean reward: 2.054 [1.518, 3.464], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.299, 10.098], loss: 0.094879, mae: 0.284644, mean_q: 3.872290
 47674/100000: episode: 771, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 190.320, mean reward: 1.903 [1.461, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.684, 10.135], loss: 0.122555, mae: 0.297846, mean_q: 3.886151
 47774/100000: episode: 772, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 173.576, mean reward: 1.736 [1.443, 2.660], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.228, 10.186], loss: 0.095270, mae: 0.285744, mean_q: 3.875429
 47874/100000: episode: 773, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 183.175, mean reward: 1.832 [1.462, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.017, 10.134], loss: 0.096648, mae: 0.281730, mean_q: 3.853625
 47974/100000: episode: 774, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 181.986, mean reward: 1.820 [1.451, 2.911], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.818, 10.098], loss: 0.089529, mae: 0.278003, mean_q: 3.854971
 48074/100000: episode: 775, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 180.675, mean reward: 1.807 [1.448, 3.315], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.916, 10.133], loss: 0.090505, mae: 0.286713, mean_q: 3.865237
 48174/100000: episode: 776, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 181.905, mean reward: 1.819 [1.434, 3.140], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.681, 10.098], loss: 0.076002, mae: 0.269580, mean_q: 3.837286
 48274/100000: episode: 777, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 191.877, mean reward: 1.919 [1.443, 3.022], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.584, 10.098], loss: 0.076827, mae: 0.270068, mean_q: 3.831049
 48374/100000: episode: 778, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 193.452, mean reward: 1.935 [1.454, 3.857], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.917, 10.120], loss: 0.087973, mae: 0.274796, mean_q: 3.821581
 48474/100000: episode: 779, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 192.874, mean reward: 1.929 [1.461, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.952, 10.098], loss: 0.081385, mae: 0.281830, mean_q: 3.837534
 48574/100000: episode: 780, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 179.022, mean reward: 1.790 [1.447, 3.053], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.659, 10.133], loss: 0.080362, mae: 0.276301, mean_q: 3.840182
 48674/100000: episode: 781, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 183.509, mean reward: 1.835 [1.472, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.504, 10.106], loss: 0.076762, mae: 0.277367, mean_q: 3.816236
 48774/100000: episode: 782, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 198.751, mean reward: 1.988 [1.530, 3.268], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.370, 10.245], loss: 0.066969, mae: 0.263520, mean_q: 3.803395
 48874/100000: episode: 783, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 195.676, mean reward: 1.957 [1.455, 3.421], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.011, 10.237], loss: 0.092635, mae: 0.282104, mean_q: 3.826638
 48974/100000: episode: 784, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 209.226, mean reward: 2.092 [1.462, 3.400], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.346, 10.159], loss: 0.091210, mae: 0.290236, mean_q: 3.851605
 49074/100000: episode: 785, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 188.312, mean reward: 1.883 [1.443, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.888, 10.180], loss: 0.083568, mae: 0.282815, mean_q: 3.838167
 49174/100000: episode: 786, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 189.432, mean reward: 1.894 [1.478, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.735, 10.098], loss: 0.076069, mae: 0.273191, mean_q: 3.847135
 49274/100000: episode: 787, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 178.623, mean reward: 1.786 [1.450, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.271, 10.203], loss: 0.071129, mae: 0.263263, mean_q: 3.805345
 49374/100000: episode: 788, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 176.281, mean reward: 1.763 [1.451, 2.686], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.914, 10.098], loss: 0.079119, mae: 0.270670, mean_q: 3.798307
 49474/100000: episode: 789, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 197.057, mean reward: 1.971 [1.455, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.679, 10.098], loss: 0.077153, mae: 0.272186, mean_q: 3.807239
 49574/100000: episode: 790, duration: 0.527s, episode steps: 100, steps per second: 190, episode reward: 208.880, mean reward: 2.089 [1.470, 4.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.723, 10.098], loss: 0.085377, mae: 0.284253, mean_q: 3.824627
 49674/100000: episode: 791, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 191.361, mean reward: 1.914 [1.476, 4.227], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.373, 10.140], loss: 0.080121, mae: 0.277364, mean_q: 3.802516
 49774/100000: episode: 792, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 177.471, mean reward: 1.775 [1.436, 3.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.409, 10.128], loss: 0.074091, mae: 0.268778, mean_q: 3.792644
 49874/100000: episode: 793, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 197.155, mean reward: 1.972 [1.511, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.857, 10.329], loss: 0.095693, mae: 0.278237, mean_q: 3.800786
 49974/100000: episode: 794, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 187.253, mean reward: 1.873 [1.463, 4.709], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.587, 10.098], loss: 0.081196, mae: 0.276629, mean_q: 3.792819
 50074/100000: episode: 795, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 193.128, mean reward: 1.931 [1.467, 3.239], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.680, 10.319], loss: 0.065447, mae: 0.260376, mean_q: 3.782382
 50174/100000: episode: 796, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 193.364, mean reward: 1.934 [1.476, 2.650], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.255, 10.177], loss: 0.081987, mae: 0.267864, mean_q: 3.777808
 50274/100000: episode: 797, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 199.516, mean reward: 1.995 [1.468, 3.639], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.396, 10.098], loss: 0.064826, mae: 0.256854, mean_q: 3.779466
 50374/100000: episode: 798, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 211.804, mean reward: 2.118 [1.472, 3.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.365, 10.314], loss: 0.066021, mae: 0.258576, mean_q: 3.778821
 50474/100000: episode: 799, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 191.533, mean reward: 1.915 [1.465, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.946, 10.335], loss: 0.064644, mae: 0.261800, mean_q: 3.778429
[Info] 1-TH LEVEL FOUND: 4.917568206787109, Considering 10/90 traces
 50574/100000: episode: 800, duration: 4.799s, episode steps: 100, steps per second: 21, episode reward: 193.691, mean reward: 1.937 [1.458, 4.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.165, 10.204], loss: 0.070535, mae: 0.269886, mean_q: 3.783105
 50591/100000: episode: 801, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 34.575, mean reward: 2.034 [1.770, 2.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.572, 10.311], loss: 0.065929, mae: 0.256261, mean_q: 3.782832
 50605/100000: episode: 802, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 25.876, mean reward: 1.848 [1.496, 2.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.346, 10.261], loss: 0.052208, mae: 0.253014, mean_q: 3.789213
 50622/100000: episode: 803, duration: 0.083s, episode steps: 17, steps per second: 205, episode reward: 33.747, mean reward: 1.985 [1.461, 2.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.165], loss: 0.055896, mae: 0.241396, mean_q: 3.754914
 50639/100000: episode: 804, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 43.316, mean reward: 2.548 [2.145, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.407, 10.352], loss: 0.071878, mae: 0.266393, mean_q: 3.771724
 50653/100000: episode: 805, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 34.189, mean reward: 2.442 [1.956, 2.795], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.533, 10.404], loss: 0.066786, mae: 0.261575, mean_q: 3.802724
 50667/100000: episode: 806, duration: 0.074s, episode steps: 14, steps per second: 188, episode reward: 29.893, mean reward: 2.135 [1.841, 2.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.304], loss: 0.120586, mae: 0.294026, mean_q: 3.845234
 50703/100000: episode: 807, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 75.722, mean reward: 2.103 [1.734, 2.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.646, 10.353], loss: 0.073124, mae: 0.278327, mean_q: 3.793903
 50733/100000: episode: 808, duration: 0.145s, episode steps: 30, steps per second: 208, episode reward: 76.026, mean reward: 2.534 [2.042, 3.384], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.203, 10.366], loss: 0.063358, mae: 0.263399, mean_q: 3.796587
 50746/100000: episode: 809, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 29.729, mean reward: 2.287 [2.061, 2.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.414], loss: 0.077484, mae: 0.284167, mean_q: 3.815225
 50776/100000: episode: 810, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 73.028, mean reward: 2.434 [1.784, 3.185], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.035, 10.255], loss: 0.070082, mae: 0.272124, mean_q: 3.814759
 50793/100000: episode: 811, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 70.991, mean reward: 4.176 [2.731, 5.743], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.589], loss: 0.075345, mae: 0.277640, mean_q: 3.868596
 50806/100000: episode: 812, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 31.468, mean reward: 2.421 [1.965, 2.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.328, 10.353], loss: 0.076194, mae: 0.258097, mean_q: 3.856177
 50836/100000: episode: 813, duration: 0.152s, episode steps: 30, steps per second: 198, episode reward: 64.009, mean reward: 2.134 [1.474, 3.815], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.707, 10.144], loss: 0.071597, mae: 0.275502, mean_q: 3.806824
 50849/100000: episode: 814, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 29.595, mean reward: 2.277 [1.761, 2.734], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.704, 10.229], loss: 0.061142, mae: 0.259709, mean_q: 3.805221
 50866/100000: episode: 815, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 44.941, mean reward: 2.644 [2.135, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.438], loss: 0.077529, mae: 0.287306, mean_q: 3.859578
 50898/100000: episode: 816, duration: 0.173s, episode steps: 32, steps per second: 185, episode reward: 97.005, mean reward: 3.031 [2.302, 5.434], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.077, 10.498], loss: 0.081583, mae: 0.284740, mean_q: 3.813874
 50912/100000: episode: 817, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 33.855, mean reward: 2.418 [1.888, 2.949], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.351], loss: 0.060003, mae: 0.268476, mean_q: 3.846521
 50942/100000: episode: 818, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 82.045, mean reward: 2.735 [1.762, 5.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.035, 10.274], loss: 0.080873, mae: 0.278287, mean_q: 3.820853
 50959/100000: episode: 819, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 42.205, mean reward: 2.483 [1.884, 3.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.035, 10.425], loss: 0.076851, mae: 0.284638, mean_q: 3.853638
 50974/100000: episode: 820, duration: 0.077s, episode steps: 15, steps per second: 196, episode reward: 38.138, mean reward: 2.543 [2.187, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.426, 10.340], loss: 0.089064, mae: 0.274949, mean_q: 3.808251
 50988/100000: episode: 821, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 29.758, mean reward: 2.126 [1.768, 2.591], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.658, 10.275], loss: 0.069297, mae: 0.268479, mean_q: 3.819004
 51020/100000: episode: 822, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 77.555, mean reward: 2.424 [1.654, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.774, 10.259], loss: 0.076988, mae: 0.280018, mean_q: 3.879786
 51052/100000: episode: 823, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 77.193, mean reward: 2.412 [1.747, 3.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-1.276, 10.302], loss: 0.075517, mae: 0.282602, mean_q: 3.851961
 51065/100000: episode: 824, duration: 0.071s, episode steps: 13, steps per second: 182, episode reward: 31.537, mean reward: 2.426 [1.992, 2.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.325, 10.426], loss: 0.077917, mae: 0.290985, mean_q: 3.833761
 51079/100000: episode: 825, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 34.000, mean reward: 2.429 [2.006, 3.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.261, 10.354], loss: 0.072843, mae: 0.267845, mean_q: 3.863121
 51093/100000: episode: 826, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 33.667, mean reward: 2.405 [1.800, 5.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.260], loss: 0.089734, mae: 0.293824, mean_q: 3.854199
 51107/100000: episode: 827, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 35.065, mean reward: 2.505 [1.935, 3.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.326, 10.368], loss: 0.093479, mae: 0.307883, mean_q: 3.865383
 51124/100000: episode: 828, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 48.682, mean reward: 2.864 [2.128, 3.889], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.370], loss: 0.079854, mae: 0.283533, mean_q: 3.915876
 51138/100000: episode: 829, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 30.776, mean reward: 2.198 [1.937, 2.459], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.340], loss: 0.084118, mae: 0.272782, mean_q: 3.867078
 51170/100000: episode: 830, duration: 0.155s, episode steps: 32, steps per second: 206, episode reward: 95.569, mean reward: 2.987 [2.192, 5.627], mean action: 0.000 [0.000, 0.000], mean observation: 2.083 [-0.035, 10.408], loss: 0.081923, mae: 0.281130, mean_q: 3.859958
 51185/100000: episode: 831, duration: 0.088s, episode steps: 15, steps per second: 170, episode reward: 33.405, mean reward: 2.227 [1.566, 2.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.072, 10.175], loss: 0.061560, mae: 0.268909, mean_q: 3.856896
 51203/100000: episode: 832, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 41.399, mean reward: 2.300 [1.581, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.293, 10.248], loss: 0.080850, mae: 0.289218, mean_q: 3.836444
 51235/100000: episode: 833, duration: 0.166s, episode steps: 32, steps per second: 192, episode reward: 72.203, mean reward: 2.256 [1.666, 3.091], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.035, 10.224], loss: 0.105363, mae: 0.308692, mean_q: 3.908552
 51249/100000: episode: 834, duration: 0.091s, episode steps: 14, steps per second: 153, episode reward: 35.258, mean reward: 2.518 [1.631, 4.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.261], loss: 0.071755, mae: 0.271794, mean_q: 3.799604
 51264/100000: episode: 835, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 31.965, mean reward: 2.131 [1.547, 3.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.084, 10.183], loss: 0.096328, mae: 0.310179, mean_q: 3.916840
 51277/100000: episode: 836, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 38.315, mean reward: 2.947 [1.887, 4.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.877, 10.572], loss: 0.064563, mae: 0.262319, mean_q: 3.874143
 51295/100000: episode: 837, duration: 0.088s, episode steps: 18, steps per second: 205, episode reward: 41.991, mean reward: 2.333 [1.703, 2.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.035, 10.362], loss: 0.070862, mae: 0.269458, mean_q: 3.933582
 51331/100000: episode: 838, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 87.190, mean reward: 2.422 [1.515, 3.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.017 [-0.832, 10.180], loss: 0.095396, mae: 0.290922, mean_q: 3.897906
 51345/100000: episode: 839, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 32.355, mean reward: 2.311 [2.044, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.485, 10.376], loss: 0.107376, mae: 0.314609, mean_q: 3.908090
 51358/100000: episode: 840, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 34.177, mean reward: 2.629 [2.090, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.220, 10.461], loss: 0.065001, mae: 0.256275, mean_q: 3.883524
 51388/100000: episode: 841, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 99.946, mean reward: 3.332 [2.174, 4.997], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.199, 10.510], loss: 0.078865, mae: 0.293149, mean_q: 3.903781
 51403/100000: episode: 842, duration: 0.074s, episode steps: 15, steps per second: 203, episode reward: 51.005, mean reward: 3.400 [2.557, 4.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.477], loss: 0.120580, mae: 0.309143, mean_q: 3.930060
 51418/100000: episode: 843, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 39.935, mean reward: 2.662 [2.207, 3.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.304, 10.363], loss: 0.081589, mae: 0.275566, mean_q: 3.812104
 51450/100000: episode: 844, duration: 0.183s, episode steps: 32, steps per second: 175, episode reward: 126.285, mean reward: 3.946 [2.161, 12.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.574, 10.418], loss: 0.102221, mae: 0.317291, mean_q: 3.973968
 51465/100000: episode: 845, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 41.132, mean reward: 2.742 [2.160, 3.563], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.035, 10.316], loss: 0.112888, mae: 0.318086, mean_q: 4.021634
 51501/100000: episode: 846, duration: 0.184s, episode steps: 36, steps per second: 195, episode reward: 84.901, mean reward: 2.358 [1.586, 3.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.022 [-0.605, 10.191], loss: 0.135513, mae: 0.323402, mean_q: 3.977816
 51533/100000: episode: 847, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 76.555, mean reward: 2.392 [1.866, 2.881], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.035, 10.300], loss: 0.156304, mae: 0.307173, mean_q: 3.942007
 51547/100000: episode: 848, duration: 0.091s, episode steps: 14, steps per second: 154, episode reward: 32.232, mean reward: 2.302 [1.760, 2.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.403, 10.312], loss: 0.106970, mae: 0.308751, mean_q: 3.981037
 51564/100000: episode: 849, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 39.110, mean reward: 2.301 [1.859, 3.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.316, 10.350], loss: 0.132277, mae: 0.348300, mean_q: 3.976562
 51579/100000: episode: 850, duration: 0.081s, episode steps: 15, steps per second: 185, episode reward: 33.722, mean reward: 2.248 [1.743, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.114, 10.255], loss: 0.091817, mae: 0.305405, mean_q: 3.964242
 51596/100000: episode: 851, duration: 0.086s, episode steps: 17, steps per second: 198, episode reward: 35.539, mean reward: 2.091 [1.655, 3.007], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.228], loss: 0.099387, mae: 0.315629, mean_q: 4.016853
 51611/100000: episode: 852, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 46.162, mean reward: 3.077 [2.417, 5.182], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.235, 10.434], loss: 0.095602, mae: 0.313075, mean_q: 4.000616
 51643/100000: episode: 853, duration: 0.168s, episode steps: 32, steps per second: 190, episode reward: 77.977, mean reward: 2.437 [1.608, 3.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.201, 10.215], loss: 0.085837, mae: 0.298610, mean_q: 3.983742
 51673/100000: episode: 854, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 70.709, mean reward: 2.357 [1.732, 4.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.297, 10.249], loss: 0.092272, mae: 0.300812, mean_q: 3.960369
 51703/100000: episode: 855, duration: 0.160s, episode steps: 30, steps per second: 187, episode reward: 72.219, mean reward: 2.407 [1.974, 2.976], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.155, 10.355], loss: 0.085082, mae: 0.291610, mean_q: 3.967373
 51720/100000: episode: 856, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 40.706, mean reward: 2.394 [2.007, 3.115], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.035, 10.409], loss: 0.115687, mae: 0.337581, mean_q: 4.024822
 51734/100000: episode: 857, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 30.543, mean reward: 2.182 [1.850, 2.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.318], loss: 0.111664, mae: 0.306512, mean_q: 4.033761
 51748/100000: episode: 858, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 33.432, mean reward: 2.388 [1.834, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.149, 10.370], loss: 0.094652, mae: 0.297971, mean_q: 3.994434
 51778/100000: episode: 859, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 69.865, mean reward: 2.329 [1.807, 4.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.230, 10.203], loss: 0.137527, mae: 0.332204, mean_q: 4.073473
 51808/100000: episode: 860, duration: 0.159s, episode steps: 30, steps per second: 188, episode reward: 69.407, mean reward: 2.314 [1.591, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-1.324, 10.288], loss: 0.083425, mae: 0.297905, mean_q: 4.031270
 51840/100000: episode: 861, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 66.900, mean reward: 2.091 [1.563, 3.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.052 [-0.471, 10.182], loss: 0.086178, mae: 0.297453, mean_q: 3.994446
 51872/100000: episode: 862, duration: 0.158s, episode steps: 32, steps per second: 202, episode reward: 78.361, mean reward: 2.449 [1.707, 4.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.035, 10.299], loss: 0.094121, mae: 0.309715, mean_q: 4.002519
 51889/100000: episode: 863, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 51.129, mean reward: 3.008 [2.173, 5.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-1.219, 10.224], loss: 0.079329, mae: 0.298046, mean_q: 4.022091
 51903/100000: episode: 864, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 30.994, mean reward: 2.214 [1.772, 2.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.347], loss: 0.077183, mae: 0.292507, mean_q: 4.031211
 51920/100000: episode: 865, duration: 0.091s, episode steps: 17, steps per second: 186, episode reward: 53.957, mean reward: 3.174 [2.115, 4.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.283, 10.420], loss: 0.089486, mae: 0.300724, mean_q: 4.023347
 51938/100000: episode: 866, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 38.931, mean reward: 2.163 [1.594, 2.773], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.054, 10.141], loss: 0.111762, mae: 0.326245, mean_q: 4.094566
 51952/100000: episode: 867, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 33.435, mean reward: 2.388 [1.869, 2.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.035, 10.448], loss: 0.093443, mae: 0.316888, mean_q: 4.018960
 51982/100000: episode: 868, duration: 0.149s, episode steps: 30, steps per second: 202, episode reward: 61.474, mean reward: 2.049 [1.621, 3.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.035, 10.192], loss: 0.100791, mae: 0.305190, mean_q: 4.016258
 51996/100000: episode: 869, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 27.844, mean reward: 1.989 [1.643, 2.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.295], loss: 0.086113, mae: 0.310709, mean_q: 4.022983
 52010/100000: episode: 870, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 40.219, mean reward: 2.873 [1.863, 7.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.430], loss: 0.092326, mae: 0.319022, mean_q: 4.050745
 52027/100000: episode: 871, duration: 0.083s, episode steps: 17, steps per second: 205, episode reward: 65.169, mean reward: 3.833 [2.003, 5.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.718, 10.610], loss: 0.103801, mae: 0.325080, mean_q: 4.122095
 52044/100000: episode: 872, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 56.875, mean reward: 3.346 [2.208, 5.149], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.102, 10.477], loss: 0.081509, mae: 0.288168, mean_q: 4.037047
 52061/100000: episode: 873, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 41.758, mean reward: 2.456 [2.082, 3.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.409], loss: 0.090550, mae: 0.311686, mean_q: 4.062076
 52093/100000: episode: 874, duration: 0.157s, episode steps: 32, steps per second: 204, episode reward: 83.463, mean reward: 2.608 [2.005, 3.672], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.927, 10.344], loss: 0.095651, mae: 0.314514, mean_q: 4.068589
 52107/100000: episode: 875, duration: 0.092s, episode steps: 14, steps per second: 152, episode reward: 31.516, mean reward: 2.251 [2.058, 2.518], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.329], loss: 0.085624, mae: 0.308518, mean_q: 4.009529
 52137/100000: episode: 876, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 79.180, mean reward: 2.639 [1.737, 3.588], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.201, 10.315], loss: 0.107279, mae: 0.321916, mean_q: 4.117737
 52155/100000: episode: 877, duration: 0.089s, episode steps: 18, steps per second: 202, episode reward: 51.597, mean reward: 2.867 [2.274, 4.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.068, 10.413], loss: 0.109939, mae: 0.349543, mean_q: 4.137468
 52170/100000: episode: 878, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 40.706, mean reward: 2.714 [2.102, 4.386], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.710, 10.489], loss: 0.156514, mae: 0.362823, mean_q: 4.138931
 52185/100000: episode: 879, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 42.380, mean reward: 2.825 [2.299, 3.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.280, 10.414], loss: 0.108762, mae: 0.341396, mean_q: 4.099767
 52203/100000: episode: 880, duration: 0.095s, episode steps: 18, steps per second: 190, episode reward: 39.495, mean reward: 2.194 [1.632, 2.858], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.317], loss: 0.109729, mae: 0.327259, mean_q: 4.136877
 52216/100000: episode: 881, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 27.793, mean reward: 2.138 [1.729, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.474, 10.322], loss: 0.099807, mae: 0.309448, mean_q: 4.102834
 52230/100000: episode: 882, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 31.419, mean reward: 2.244 [2.018, 2.566], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.246, 10.348], loss: 0.099321, mae: 0.327531, mean_q: 4.081531
 52247/100000: episode: 883, duration: 0.104s, episode steps: 17, steps per second: 163, episode reward: 43.769, mean reward: 2.575 [2.143, 3.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.047, 10.440], loss: 0.117520, mae: 0.325717, mean_q: 4.109492
 52261/100000: episode: 884, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 37.167, mean reward: 2.655 [2.154, 3.530], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.511], loss: 0.074871, mae: 0.299604, mean_q: 4.080062
 52291/100000: episode: 885, duration: 0.155s, episode steps: 30, steps per second: 194, episode reward: 70.774, mean reward: 2.359 [1.688, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.035, 10.230], loss: 0.108453, mae: 0.321374, mean_q: 4.103352
 52304/100000: episode: 886, duration: 0.065s, episode steps: 13, steps per second: 201, episode reward: 26.813, mean reward: 2.063 [1.725, 2.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.110, 10.341], loss: 0.098154, mae: 0.317258, mean_q: 4.062025
 52336/100000: episode: 887, duration: 0.156s, episode steps: 32, steps per second: 205, episode reward: 86.684, mean reward: 2.709 [1.876, 4.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.593, 10.388], loss: 0.104787, mae: 0.320794, mean_q: 4.110291
 52366/100000: episode: 888, duration: 0.152s, episode steps: 30, steps per second: 197, episode reward: 70.293, mean reward: 2.343 [1.550, 4.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.612, 10.244], loss: 0.100852, mae: 0.319857, mean_q: 4.117355
 52380/100000: episode: 889, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 28.283, mean reward: 2.020 [1.804, 2.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.223, 10.266], loss: 0.089889, mae: 0.310251, mean_q: 4.145146
[Info] 2-TH LEVEL FOUND: 6.579909801483154, Considering 10/90 traces
 52397/100000: episode: 890, duration: 4.305s, episode steps: 17, steps per second: 4, episode reward: 56.986, mean reward: 3.352 [2.270, 6.665], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.327, 10.578], loss: 0.093382, mae: 0.297570, mean_q: 4.125023
 52425/100000: episode: 891, duration: 0.155s, episode steps: 28, steps per second: 181, episode reward: 94.936, mean reward: 3.391 [2.090, 7.813], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-1.004, 10.384], loss: 0.107569, mae: 0.339683, mean_q: 4.217735
 52440/100000: episode: 892, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 61.513, mean reward: 4.101 [3.146, 6.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-1.277, 10.570], loss: 0.128714, mae: 0.341517, mean_q: 4.201168
 52453/100000: episode: 893, duration: 0.071s, episode steps: 13, steps per second: 183, episode reward: 54.224, mean reward: 4.171 [3.308, 5.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.555], loss: 0.131596, mae: 0.347024, mean_q: 4.125996
 52477/100000: episode: 894, duration: 0.118s, episode steps: 24, steps per second: 203, episode reward: 72.947, mean reward: 3.039 [2.374, 5.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.547], loss: 0.100666, mae: 0.329682, mean_q: 4.177578
 52501/100000: episode: 895, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 83.036, mean reward: 3.460 [2.534, 4.184], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-1.675, 10.531], loss: 0.135696, mae: 0.360852, mean_q: 4.244996
 52512/100000: episode: 896, duration: 0.061s, episode steps: 11, steps per second: 182, episode reward: 32.217, mean reward: 2.929 [2.494, 3.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.035, 10.506], loss: 0.098133, mae: 0.319724, mean_q: 4.144907
 52526/100000: episode: 897, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 36.657, mean reward: 2.618 [2.050, 3.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.150, 10.397], loss: 0.108926, mae: 0.325839, mean_q: 4.212529
 52549/100000: episode: 898, duration: 0.119s, episode steps: 23, steps per second: 194, episode reward: 85.316, mean reward: 3.709 [2.686, 7.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.421], loss: 0.112245, mae: 0.336914, mean_q: 4.188282
 52563/100000: episode: 899, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 53.749, mean reward: 3.839 [3.234, 5.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.479], loss: 0.182699, mae: 0.342639, mean_q: 4.147387
 52591/100000: episode: 900, duration: 0.148s, episode steps: 28, steps per second: 190, episode reward: 155.773, mean reward: 5.563 [2.874, 10.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-1.213, 10.553], loss: 0.168591, mae: 0.360111, mean_q: 4.215672
 52606/100000: episode: 901, duration: 0.079s, episode steps: 15, steps per second: 189, episode reward: 38.799, mean reward: 2.587 [2.185, 2.856], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.410], loss: 0.101638, mae: 0.332363, mean_q: 4.324838
 52630/100000: episode: 902, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 61.173, mean reward: 2.549 [1.841, 3.375], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-0.305, 10.277], loss: 0.148075, mae: 0.343495, mean_q: 4.247889
 52653/100000: episode: 903, duration: 0.127s, episode steps: 23, steps per second: 182, episode reward: 70.090, mean reward: 3.047 [2.183, 4.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-1.153, 10.332], loss: 0.166677, mae: 0.374463, mean_q: 4.277906
 52676/100000: episode: 904, duration: 0.139s, episode steps: 23, steps per second: 166, episode reward: 59.556, mean reward: 2.589 [1.879, 4.758], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.194, 10.316], loss: 0.117052, mae: 0.345010, mean_q: 4.255499
 52690/100000: episode: 905, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 40.551, mean reward: 2.896 [2.410, 3.580], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.745, 10.406], loss: 0.115979, mae: 0.360960, mean_q: 4.214498
 52705/100000: episode: 906, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 44.489, mean reward: 2.966 [2.313, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.478], loss: 0.145768, mae: 0.365054, mean_q: 4.382719
 52718/100000: episode: 907, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 60.237, mean reward: 4.634 [3.069, 7.533], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.569, 10.578], loss: 0.135913, mae: 0.359051, mean_q: 4.311397
 52729/100000: episode: 908, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 33.310, mean reward: 3.028 [2.409, 3.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.200, 10.374], loss: 0.125869, mae: 0.375247, mean_q: 4.359903
 52743/100000: episode: 909, duration: 0.070s, episode steps: 14, steps per second: 201, episode reward: 50.024, mean reward: 3.573 [2.875, 5.191], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.501], loss: 0.131793, mae: 0.358674, mean_q: 4.393689
 52766/100000: episode: 910, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 63.340, mean reward: 2.754 [1.822, 4.131], mean action: 0.000 [0.000, 0.000], mean observation: 2.160 [-0.077, 10.263], loss: 0.139405, mae: 0.339082, mean_q: 4.374742
 52789/100000: episode: 911, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 69.977, mean reward: 3.042 [2.463, 5.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.730, 10.359], loss: 0.149194, mae: 0.389788, mean_q: 4.351202
 52813/100000: episode: 912, duration: 0.126s, episode steps: 24, steps per second: 191, episode reward: 74.261, mean reward: 3.094 [2.470, 5.031], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.709, 10.469], loss: 0.191718, mae: 0.396566, mean_q: 4.373300
 52825/100000: episode: 913, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 37.850, mean reward: 3.154 [2.612, 4.262], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.437], loss: 0.147790, mae: 0.376704, mean_q: 4.307377
 52835/100000: episode: 914, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 34.546, mean reward: 3.455 [2.702, 5.402], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.613], loss: 0.110941, mae: 0.335765, mean_q: 4.303814
 52859/100000: episode: 915, duration: 0.134s, episode steps: 24, steps per second: 180, episode reward: 82.732, mean reward: 3.447 [2.325, 4.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.068, 10.492], loss: 0.152528, mae: 0.357854, mean_q: 4.338781
 52871/100000: episode: 916, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 66.060, mean reward: 5.505 [3.420, 16.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.649], loss: 0.177607, mae: 0.389028, mean_q: 4.421705
 52886/100000: episode: 917, duration: 0.078s, episode steps: 15, steps per second: 192, episode reward: 37.146, mean reward: 2.476 [2.081, 2.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.035, 10.374], loss: 0.189301, mae: 0.408296, mean_q: 4.484426
 52901/100000: episode: 918, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 37.423, mean reward: 2.495 [2.051, 3.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.352], loss: 0.193666, mae: 0.370258, mean_q: 4.501256
 52925/100000: episode: 919, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 71.575, mean reward: 2.982 [2.193, 3.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.215, 10.402], loss: 0.190004, mae: 0.380016, mean_q: 4.478897
 52937/100000: episode: 920, duration: 0.066s, episode steps: 12, steps per second: 182, episode reward: 42.423, mean reward: 3.535 [3.014, 4.227], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.525], loss: 0.254901, mae: 0.388204, mean_q: 4.413389
 52949/100000: episode: 921, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 44.420, mean reward: 3.702 [2.744, 4.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.686, 10.497], loss: 0.310311, mae: 0.467179, mean_q: 4.413223
 52960/100000: episode: 922, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 35.842, mean reward: 3.258 [2.385, 4.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.240, 10.529], loss: 0.205654, mae: 0.455737, mean_q: 4.569273
 52972/100000: episode: 923, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 42.405, mean reward: 3.534 [2.939, 4.273], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.555], loss: 0.204618, mae: 0.438715, mean_q: 4.548249
 52987/100000: episode: 924, duration: 0.078s, episode steps: 15, steps per second: 193, episode reward: 40.307, mean reward: 2.687 [2.183, 3.198], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.795, 10.405], loss: 0.137813, mae: 0.365041, mean_q: 4.441208
 53015/100000: episode: 925, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 69.602, mean reward: 2.486 [1.823, 3.979], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.362, 10.308], loss: 0.153376, mae: 0.370526, mean_q: 4.376089
 53026/100000: episode: 926, duration: 0.055s, episode steps: 11, steps per second: 199, episode reward: 31.754, mean reward: 2.887 [2.164, 4.569], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.035, 10.336], loss: 0.234068, mae: 0.463757, mean_q: 4.546954
 53041/100000: episode: 927, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 50.676, mean reward: 3.378 [2.701, 4.067], mean action: 0.000 [0.000, 0.000], mean observation: 2.225 [-0.035, 10.467], loss: 0.293634, mae: 0.410665, mean_q: 4.512791
 53054/100000: episode: 928, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 35.006, mean reward: 2.693 [2.175, 4.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.052, 10.369], loss: 0.158284, mae: 0.384798, mean_q: 4.405332
 53065/100000: episode: 929, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 37.768, mean reward: 3.433 [2.738, 5.081], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.443], loss: 0.145319, mae: 0.373328, mean_q: 4.439844
 53080/100000: episode: 930, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 52.903, mean reward: 3.527 [2.326, 4.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.494], loss: 0.152115, mae: 0.377526, mean_q: 4.528121
 53091/100000: episode: 931, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 46.150, mean reward: 4.195 [2.784, 9.607], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.616], loss: 0.153482, mae: 0.387608, mean_q: 4.514154
 53106/100000: episode: 932, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 62.452, mean reward: 4.163 [2.785, 5.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-1.096, 10.498], loss: 0.209650, mae: 0.408363, mean_q: 4.504128
 53118/100000: episode: 933, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 36.322, mean reward: 3.027 [2.194, 3.912], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.417], loss: 0.168827, mae: 0.422772, mean_q: 4.654724
 53130/100000: episode: 934, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 45.314, mean reward: 3.776 [3.026, 5.257], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.575], loss: 0.235984, mae: 0.431061, mean_q: 4.566614
 53145/100000: episode: 935, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 48.209, mean reward: 3.214 [2.606, 4.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.035, 10.397], loss: 0.399593, mae: 0.466359, mean_q: 4.610214
 53157/100000: episode: 936, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 51.937, mean reward: 4.328 [3.219, 5.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.035, 10.557], loss: 0.166836, mae: 0.390469, mean_q: 4.640189
 53185/100000: episode: 937, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 78.119, mean reward: 2.790 [2.031, 3.664], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.672, 10.416], loss: 0.238796, mae: 0.435674, mean_q: 4.642016
 53209/100000: episode: 938, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 84.373, mean reward: 3.516 [2.535, 5.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.138 [-1.229, 10.420], loss: 0.194119, mae: 0.414610, mean_q: 4.469549
 53221/100000: episode: 939, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 45.004, mean reward: 3.750 [2.730, 6.308], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.597], loss: 0.154644, mae: 0.390448, mean_q: 4.589887
 53232/100000: episode: 940, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 35.240, mean reward: 3.204 [2.668, 3.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.035, 10.477], loss: 0.156193, mae: 0.357197, mean_q: 4.474076
 53256/100000: episode: 941, duration: 0.130s, episode steps: 24, steps per second: 184, episode reward: 79.332, mean reward: 3.306 [2.561, 5.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.035, 10.587], loss: 0.152408, mae: 0.384382, mean_q: 4.619386
 53280/100000: episode: 942, duration: 0.119s, episode steps: 24, steps per second: 201, episode reward: 84.446, mean reward: 3.519 [2.896, 4.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.480], loss: 0.206851, mae: 0.418790, mean_q: 4.547321
 53293/100000: episode: 943, duration: 0.069s, episode steps: 13, steps per second: 190, episode reward: 42.279, mean reward: 3.252 [2.648, 4.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-1.040, 10.493], loss: 0.182065, mae: 0.409201, mean_q: 4.634242
 53317/100000: episode: 944, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 68.187, mean reward: 2.841 [2.121, 5.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.494, 10.388], loss: 0.232973, mae: 0.449471, mean_q: 4.641459
 53332/100000: episode: 945, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 111.296, mean reward: 7.420 [2.780, 15.320], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.665], loss: 0.137279, mae: 0.371188, mean_q: 4.598192
 53355/100000: episode: 946, duration: 0.122s, episode steps: 23, steps per second: 189, episode reward: 147.305, mean reward: 6.405 [3.254, 11.474], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.681, 10.694], loss: 0.343718, mae: 0.450828, mean_q: 4.764399
 53370/100000: episode: 947, duration: 0.080s, episode steps: 15, steps per second: 187, episode reward: 93.813, mean reward: 6.254 [2.432, 21.078], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.731], loss: 0.223564, mae: 0.435590, mean_q: 4.775872
 53384/100000: episode: 948, duration: 0.082s, episode steps: 14, steps per second: 170, episode reward: 55.112, mean reward: 3.937 [2.634, 6.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.608], loss: 0.268717, mae: 0.450203, mean_q: 4.670386
 53398/100000: episode: 949, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 46.600, mean reward: 3.329 [2.717, 4.501], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-1.082, 10.569], loss: 0.256036, mae: 0.464442, mean_q: 4.740524
 53410/100000: episode: 950, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 50.876, mean reward: 4.240 [3.200, 5.873], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.136, 10.547], loss: 0.440282, mae: 0.519428, mean_q: 4.707285
 53423/100000: episode: 951, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 49.040, mean reward: 3.772 [2.749, 5.578], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.534], loss: 0.385401, mae: 0.501223, mean_q: 4.885201
 53434/100000: episode: 952, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 44.176, mean reward: 4.016 [2.294, 6.400], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.955, 10.292], loss: 0.242685, mae: 0.447388, mean_q: 4.755989
 53447/100000: episode: 953, duration: 0.084s, episode steps: 13, steps per second: 154, episode reward: 43.886, mean reward: 3.376 [2.333, 4.464], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.418], loss: 0.183968, mae: 0.414633, mean_q: 4.674873
 53475/100000: episode: 954, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 87.230, mean reward: 3.115 [2.287, 4.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.035, 10.420], loss: 0.367610, mae: 0.499763, mean_q: 4.863815
 53499/100000: episode: 955, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 78.464, mean reward: 3.269 [2.288, 4.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.035, 10.370], loss: 0.384342, mae: 0.465355, mean_q: 4.888160
 53514/100000: episode: 956, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 44.690, mean reward: 2.979 [1.958, 4.060], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.402, 10.384], loss: 0.510899, mae: 0.576868, mean_q: 4.839784
 53524/100000: episode: 957, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 39.019, mean reward: 3.902 [2.613, 5.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.535, 10.530], loss: 0.253509, mae: 0.430669, mean_q: 4.625025
 53538/100000: episode: 958, duration: 0.076s, episode steps: 14, steps per second: 184, episode reward: 43.654, mean reward: 3.118 [2.798, 3.505], mean action: 0.000 [0.000, 0.000], mean observation: 2.235 [-0.035, 10.413], loss: 0.468173, mae: 0.496045, mean_q: 4.957167
 53552/100000: episode: 959, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 68.891, mean reward: 4.921 [2.366, 13.291], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.357], loss: 0.336957, mae: 0.459878, mean_q: 4.888068
 53565/100000: episode: 960, duration: 0.067s, episode steps: 13, steps per second: 194, episode reward: 43.241, mean reward: 3.326 [2.566, 4.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.035, 10.586], loss: 0.453103, mae: 0.525545, mean_q: 4.879680
 53579/100000: episode: 961, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 58.721, mean reward: 4.194 [2.768, 9.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.254 [-0.197, 10.545], loss: 0.393992, mae: 0.512715, mean_q: 4.909823
 53607/100000: episode: 962, duration: 0.153s, episode steps: 28, steps per second: 184, episode reward: 74.658, mean reward: 2.666 [1.922, 3.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-1.381, 10.375], loss: 0.294751, mae: 0.507257, mean_q: 4.923261
 53635/100000: episode: 963, duration: 0.140s, episode steps: 28, steps per second: 199, episode reward: 83.320, mean reward: 2.976 [2.137, 4.701], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.537, 10.298], loss: 0.341977, mae: 0.506961, mean_q: 4.973156
 53658/100000: episode: 964, duration: 0.119s, episode steps: 23, steps per second: 194, episode reward: 85.318, mean reward: 3.709 [2.938, 5.523], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-1.137, 10.516], loss: 0.466283, mae: 0.543485, mean_q: 4.853968
 53681/100000: episode: 965, duration: 0.117s, episode steps: 23, steps per second: 196, episode reward: 70.214, mean reward: 3.053 [2.252, 5.285], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.604, 10.363], loss: 0.255360, mae: 0.476696, mean_q: 4.887751
 53695/100000: episode: 966, duration: 0.075s, episode steps: 14, steps per second: 187, episode reward: 37.783, mean reward: 2.699 [2.393, 3.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.440], loss: 0.249610, mae: 0.451675, mean_q: 4.961769
 53706/100000: episode: 967, duration: 0.059s, episode steps: 11, steps per second: 186, episode reward: 31.863, mean reward: 2.897 [2.039, 4.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.307], loss: 0.487726, mae: 0.502807, mean_q: 4.961782
 53720/100000: episode: 968, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 43.925, mean reward: 3.138 [2.442, 3.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.111, 10.424], loss: 0.530703, mae: 0.556780, mean_q: 4.985076
 53744/100000: episode: 969, duration: 0.129s, episode steps: 24, steps per second: 185, episode reward: 74.867, mean reward: 3.119 [2.218, 4.090], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.202, 10.384], loss: 0.640116, mae: 0.567798, mean_q: 4.956425
 53759/100000: episode: 970, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 43.988, mean reward: 2.933 [2.650, 3.515], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.436], loss: 0.394018, mae: 0.562903, mean_q: 4.794470
 53770/100000: episode: 971, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 28.851, mean reward: 2.623 [2.004, 3.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.277], loss: 0.273965, mae: 0.528472, mean_q: 4.850691
 53780/100000: episode: 972, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 33.539, mean reward: 3.354 [2.937, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.134, 10.455], loss: 0.427469, mae: 0.545298, mean_q: 5.095608
 53808/100000: episode: 973, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 64.885, mean reward: 2.317 [1.588, 4.640], mean action: 0.000 [0.000, 0.000], mean observation: 2.101 [-0.495, 10.187], loss: 0.368096, mae: 0.520035, mean_q: 4.959461
 53820/100000: episode: 974, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 39.706, mean reward: 3.309 [2.665, 4.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.415], loss: 0.307317, mae: 0.520069, mean_q: 5.049697
 53848/100000: episode: 975, duration: 0.137s, episode steps: 28, steps per second: 204, episode reward: 74.217, mean reward: 2.651 [1.505, 5.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.282, 10.252], loss: 0.313795, mae: 0.481103, mean_q: 4.966415
 53858/100000: episode: 976, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 28.978, mean reward: 2.898 [2.321, 3.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.448], loss: 0.244376, mae: 0.472507, mean_q: 5.137614
 53873/100000: episode: 977, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 46.585, mean reward: 3.106 [2.605, 3.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.477, 10.504], loss: 0.207724, mae: 0.450980, mean_q: 5.014268
 53883/100000: episode: 978, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 34.563, mean reward: 3.456 [2.937, 4.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.481, 10.351], loss: 0.264070, mae: 0.467613, mean_q: 4.852991
 53895/100000: episode: 979, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 47.544, mean reward: 3.962 [2.970, 5.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.562], loss: 0.293650, mae: 0.481223, mean_q: 4.899556
[Info] 3-TH LEVEL FOUND: 8.618499755859375, Considering 10/90 traces
 53909/100000: episode: 980, duration: 4.539s, episode steps: 14, steps per second: 3, episode reward: 39.577, mean reward: 2.827 [1.867, 3.686], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.389], loss: 0.208051, mae: 0.433991, mean_q: 4.964744
 53919/100000: episode: 981, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 55.359, mean reward: 5.536 [4.065, 7.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.283, 10.592], loss: 0.354465, mae: 0.466953, mean_q: 5.135615
 53940/100000: episode: 982, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 87.768, mean reward: 4.179 [2.256, 6.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.162, 10.403], loss: 0.153068, mae: 0.409476, mean_q: 4.798694
 53944/100000: episode: 983, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 33.273, mean reward: 8.318 [6.010, 10.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.347 [-0.035, 10.701], loss: 1.054736, mae: 0.583655, mean_q: 5.096693
 53964/100000: episode: 984, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 418.216, mean reward: 20.911 [4.197, 122.106], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.782], loss: 0.301106, mae: 0.489694, mean_q: 4.974646
 53975/100000: episode: 985, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 33.163, mean reward: 3.015 [2.276, 4.398], mean action: 0.000 [0.000, 0.000], mean observation: 2.248 [-0.920, 10.322], loss: 0.304489, mae: 0.510237, mean_q: 5.041631
 53985/100000: episode: 986, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 40.602, mean reward: 4.060 [3.311, 4.957], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.230, 10.550], loss: 1.894287, mae: 1.020887, mean_q: 5.250060
 53996/100000: episode: 987, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 79.676, mean reward: 7.243 [2.778, 39.895], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-0.035, 10.330], loss: 0.759407, mae: 0.673302, mean_q: 5.122893
 54016/100000: episode: 988, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 77.623, mean reward: 3.881 [2.390, 5.736], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.173, 10.501], loss: 0.302909, mae: 0.527906, mean_q: 5.032550
 54023/100000: episode: 989, duration: 0.042s, episode steps: 7, steps per second: 169, episode reward: 24.614, mean reward: 3.516 [3.148, 4.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.310 [-0.035, 10.459], loss: 0.673962, mae: 0.595726, mean_q: 5.450123
 54033/100000: episode: 990, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 56.431, mean reward: 5.643 [3.387, 8.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.372, 10.415], loss: 1.051357, mae: 0.672130, mean_q: 5.288020
 54044/100000: episode: 991, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 37.651, mean reward: 3.423 [2.880, 4.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.436], loss: 0.482015, mae: 0.593285, mean_q: 5.107842
 54054/100000: episode: 992, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 96.601, mean reward: 9.660 [5.315, 15.093], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.047, 10.615], loss: 0.319933, mae: 0.505000, mean_q: 5.254273
 54064/100000: episode: 993, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 41.927, mean reward: 4.193 [3.314, 5.335], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-1.524, 10.395], loss: 0.581812, mae: 0.562677, mean_q: 5.282715
 54074/100000: episode: 994, duration: 0.055s, episode steps: 10, steps per second: 180, episode reward: 47.950, mean reward: 4.795 [3.819, 6.388], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.613], loss: 4.749812, mae: 0.841835, mean_q: 5.172357
 54095/100000: episode: 995, duration: 0.109s, episode steps: 21, steps per second: 192, episode reward: 62.182, mean reward: 2.961 [1.731, 5.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.408], loss: 1.170671, mae: 0.673907, mean_q: 5.287914
 54105/100000: episode: 996, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 46.964, mean reward: 4.696 [3.510, 6.213], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.595], loss: 1.016194, mae: 0.669052, mean_q: 5.442116
 54109/100000: episode: 997, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 35.200, mean reward: 8.800 [6.704, 11.946], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.348, 10.520], loss: 5.656051, mae: 0.946396, mean_q: 5.374417
 54120/100000: episode: 998, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 39.575, mean reward: 3.598 [2.901, 4.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.488], loss: 0.951814, mae: 0.657046, mean_q: 4.929280
 54130/100000: episode: 999, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 46.713, mean reward: 4.671 [3.931, 6.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.201, 10.588], loss: 0.501536, mae: 0.664245, mean_q: 5.324832
 54136/100000: episode: 1000, duration: 0.042s, episode steps: 6, steps per second: 141, episode reward: 25.836, mean reward: 4.306 [3.341, 5.762], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.035, 10.624], loss: 0.956286, mae: 0.683194, mean_q: 5.251539
 54140/100000: episode: 1001, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 29.833, mean reward: 7.458 [5.416, 8.932], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.509, 10.489], loss: 0.574774, mae: 0.639597, mean_q: 5.448104
 54161/100000: episode: 1002, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 83.837, mean reward: 3.992 [2.315, 5.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.426, 10.274], loss: 1.413807, mae: 0.723868, mean_q: 5.486149
 54168/100000: episode: 1003, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 26.790, mean reward: 3.827 [3.041, 4.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.470], loss: 31.103931, mae: 1.824153, mean_q: 6.268804
 54189/100000: episode: 1004, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 104.563, mean reward: 4.979 [2.328, 11.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.061, 10.442], loss: 1.487716, mae: 1.161793, mean_q: 5.115671
 54199/100000: episode: 1005, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 41.288, mean reward: 4.129 [3.441, 4.841], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.469], loss: 0.609272, mae: 0.678808, mean_q: 4.969302
 54209/100000: episode: 1006, duration: 0.053s, episode steps: 10, steps per second: 187, episode reward: 43.407, mean reward: 4.341 [3.650, 4.898], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.035, 10.556], loss: 1.701132, mae: 0.873287, mean_q: 5.429580
 54215/100000: episode: 1007, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 22.365, mean reward: 3.727 [3.415, 3.923], mean action: 0.000 [0.000, 0.000], mean observation: 2.309 [-0.035, 10.522], loss: 1.134204, mae: 0.686203, mean_q: 5.255520
 54221/100000: episode: 1008, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 27.594, mean reward: 4.599 [3.581, 6.032], mean action: 0.000 [0.000, 0.000], mean observation: 2.323 [-0.035, 10.616], loss: 0.489493, mae: 0.592046, mean_q: 4.945246
 54225/100000: episode: 1009, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 20.864, mean reward: 5.216 [4.383, 6.458], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.540], loss: 0.268154, mae: 0.573143, mean_q: 5.018991
 54235/100000: episode: 1010, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 35.352, mean reward: 3.535 [2.894, 4.246], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.504], loss: 0.563938, mae: 0.595521, mean_q: 5.052092
 54242/100000: episode: 1011, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 39.612, mean reward: 5.659 [3.656, 6.874], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.652], loss: 0.364781, mae: 0.554864, mean_q: 5.137321
 54262/100000: episode: 1012, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 118.942, mean reward: 5.947 [4.165, 14.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.650], loss: 12.263446, mae: 1.327307, mean_q: 5.308834
 54282/100000: episode: 1013, duration: 0.114s, episode steps: 20, steps per second: 176, episode reward: 104.421, mean reward: 5.221 [2.880, 7.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.406, 10.499], loss: 1.026426, mae: 0.878556, mean_q: 5.583171
 54289/100000: episode: 1014, duration: 0.043s, episode steps: 7, steps per second: 161, episode reward: 24.574, mean reward: 3.511 [2.895, 4.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.434], loss: 1.004701, mae: 0.638549, mean_q: 5.135532
 54299/100000: episode: 1015, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 42.142, mean reward: 4.214 [2.952, 5.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.513, 10.619], loss: 21.745802, mae: 1.484741, mean_q: 6.110540
 54306/100000: episode: 1016, duration: 0.037s, episode steps: 7, steps per second: 191, episode reward: 25.668, mean reward: 3.667 [3.095, 4.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.138, 10.506], loss: 1.109761, mae: 1.148981, mean_q: 4.172701
 54326/100000: episode: 1017, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 91.535, mean reward: 4.577 [2.612, 12.126], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.480], loss: 0.726221, mae: 0.806867, mean_q: 5.591427
 54336/100000: episode: 1018, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 47.740, mean reward: 4.774 [3.375, 6.961], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.035, 10.594], loss: 1.773096, mae: 0.639773, mean_q: 5.244691
 54346/100000: episode: 1019, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 38.409, mean reward: 3.841 [3.270, 4.602], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-0.035, 10.443], loss: 21.838093, mae: 1.600026, mean_q: 6.104416
 54352/100000: episode: 1020, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 23.887, mean reward: 3.981 [3.514, 4.663], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.461], loss: 0.702836, mae: 0.899680, mean_q: 4.369356
 54372/100000: episode: 1021, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 84.936, mean reward: 4.247 [2.410, 6.361], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.402, 10.342], loss: 3.214219, mae: 1.013193, mean_q: 5.518992
 54392/100000: episode: 1022, duration: 0.108s, episode steps: 20, steps per second: 186, episode reward: 102.132, mean reward: 5.107 [3.398, 7.180], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.035, 10.540], loss: 0.859050, mae: 0.684315, mean_q: 5.558830
 54398/100000: episode: 1023, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 22.279, mean reward: 3.713 [3.205, 4.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.439, 10.360], loss: 0.903784, mae: 0.713200, mean_q: 5.307878
 54404/100000: episode: 1024, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: 23.316, mean reward: 3.886 [3.535, 4.279], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.519], loss: 0.486568, mae: 0.575545, mean_q: 5.238971
 54410/100000: episode: 1025, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 29.078, mean reward: 4.846 [3.755, 7.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.519], loss: 4.042789, mae: 0.807752, mean_q: 5.412291
 54417/100000: episode: 1026, duration: 0.038s, episode steps: 7, steps per second: 186, episode reward: 21.992, mean reward: 3.142 [2.590, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.298 [-0.280, 10.492], loss: 2.345359, mae: 1.620288, mean_q: 6.624857
 54438/100000: episode: 1027, duration: 0.113s, episode steps: 21, steps per second: 185, episode reward: 56.447, mean reward: 2.688 [1.575, 4.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-1.358, 10.259], loss: 14.819028, mae: 1.273106, mean_q: 5.628217
 54444/100000: episode: 1028, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 25.663, mean reward: 4.277 [3.318, 5.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.477], loss: 1.423492, mae: 0.763579, mean_q: 4.887389
 54448/100000: episode: 1029, duration: 0.026s, episode steps: 4, steps per second: 153, episode reward: 55.780, mean reward: 13.945 [6.258, 22.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.897, 10.509], loss: 0.478115, mae: 0.664079, mean_q: 5.890712
 54455/100000: episode: 1030, duration: 0.037s, episode steps: 7, steps per second: 192, episode reward: 30.801, mean reward: 4.400 [3.198, 6.070], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-1.354, 10.585], loss: 0.525431, mae: 0.716532, mean_q: 5.626744
 54475/100000: episode: 1031, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 309.468, mean reward: 15.473 [3.579, 156.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.085, 10.624], loss: 2.418267, mae: 0.866318, mean_q: 5.654543
 54481/100000: episode: 1032, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 24.292, mean reward: 4.049 [3.471, 4.724], mean action: 0.000 [0.000, 0.000], mean observation: 2.335 [-0.035, 10.558], loss: 1.728548, mae: 1.232708, mean_q: 6.527170
 54487/100000: episode: 1033, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 20.007, mean reward: 3.334 [2.867, 3.777], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.448], loss: 6.910135, mae: 1.032621, mean_q: 5.546472
 54497/100000: episode: 1034, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 159.061, mean reward: 15.906 [3.211, 99.538], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.562], loss: 1.443036, mae: 0.878973, mean_q: 4.835008
 54518/100000: episode: 1035, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 76.599, mean reward: 3.648 [2.876, 5.245], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.763, 10.480], loss: 8.987883, mae: 1.272804, mean_q: 6.306967
 54539/100000: episode: 1036, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 85.846, mean reward: 4.088 [3.144, 6.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.482, 10.584], loss: 0.917883, mae: 0.834437, mean_q: 5.294929
 54560/100000: episode: 1037, duration: 0.102s, episode steps: 21, steps per second: 206, episode reward: 63.886, mean reward: 3.042 [1.920, 6.637], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-0.035, 10.401], loss: 0.799888, mae: 0.722957, mean_q: 5.781692
 54570/100000: episode: 1038, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 69.235, mean reward: 6.924 [3.982, 14.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.749, 10.494], loss: 0.656402, mae: 0.702031, mean_q: 5.399247
 54581/100000: episode: 1039, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 53.484, mean reward: 4.862 [4.005, 6.435], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.555], loss: 4.534222, mae: 0.995149, mean_q: 5.985653
 54591/100000: episode: 1040, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 48.338, mean reward: 4.834 [2.844, 7.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.268 [-0.581, 10.513], loss: 0.721509, mae: 0.783104, mean_q: 5.831947
 54598/100000: episode: 1041, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 35.674, mean reward: 5.096 [3.641, 8.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.324 [-0.035, 10.674], loss: 0.738732, mae: 0.756857, mean_q: 5.195985
 54618/100000: episode: 1042, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 156.422, mean reward: 7.821 [4.346, 17.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-1.291, 10.562], loss: 0.921629, mae: 0.754323, mean_q: 5.702275
 54625/100000: episode: 1043, duration: 0.042s, episode steps: 7, steps per second: 168, episode reward: 28.788, mean reward: 4.113 [3.091, 5.620], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.084, 10.618], loss: 1.737204, mae: 0.811379, mean_q: 5.544082
 54632/100000: episode: 1044, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 24.079, mean reward: 3.440 [3.031, 4.036], mean action: 0.000 [0.000, 0.000], mean observation: 2.281 [-0.379, 10.495], loss: 1.039357, mae: 0.873531, mean_q: 6.180815
 54652/100000: episode: 1045, duration: 0.105s, episode steps: 20, steps per second: 191, episode reward: 90.921, mean reward: 4.546 [2.492, 7.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.589], loss: 1.095599, mae: 0.792946, mean_q: 5.723309
 54658/100000: episode: 1046, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 18.946, mean reward: 3.158 [2.436, 4.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.344], loss: 1.281779, mae: 0.805339, mean_q: 6.093915
 54664/100000: episode: 1047, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 28.985, mean reward: 4.831 [3.584, 6.545], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.118, 10.565], loss: 0.981478, mae: 0.798203, mean_q: 6.052563
 54671/100000: episode: 1048, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 32.356, mean reward: 4.622 [2.845, 8.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.151, 10.422], loss: 0.613985, mae: 0.715229, mean_q: 5.894146
 54675/100000: episode: 1049, duration: 0.032s, episode steps: 4, steps per second: 126, episode reward: 29.914, mean reward: 7.479 [6.554, 8.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.639], loss: 0.944695, mae: 0.782292, mean_q: 6.111888
 54681/100000: episode: 1050, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 23.444, mean reward: 3.907 [3.149, 4.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.500], loss: 0.790097, mae: 0.714510, mean_q: 5.716059
 54691/100000: episode: 1051, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 44.181, mean reward: 4.418 [3.215, 6.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.595, 10.477], loss: 0.547185, mae: 0.608063, mean_q: 5.459862
 54702/100000: episode: 1052, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 40.995, mean reward: 3.727 [2.759, 5.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.035, 10.545], loss: 4.911871, mae: 1.057553, mean_q: 6.329129
 54708/100000: episode: 1053, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 21.596, mean reward: 3.599 [3.330, 3.842], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.507], loss: 0.717882, mae: 0.711410, mean_q: 5.457888
 54729/100000: episode: 1054, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 107.264, mean reward: 5.108 [4.121, 7.248], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.035, 10.520], loss: 12.164192, mae: 1.204793, mean_q: 6.182443
 54750/100000: episode: 1055, duration: 0.109s, episode steps: 21, steps per second: 193, episode reward: 149.073, mean reward: 7.099 [3.988, 12.068], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.101, 10.662], loss: 18.028135, mae: 1.058174, mean_q: 5.707311
 54771/100000: episode: 1056, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 86.449, mean reward: 4.117 [2.607, 7.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.631, 10.457], loss: 1.889281, mae: 1.129535, mean_q: 5.800395
 54777/100000: episode: 1057, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 24.318, mean reward: 4.053 [3.673, 4.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.447, 10.519], loss: 0.667908, mae: 0.843186, mean_q: 6.492177
 54787/100000: episode: 1058, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 33.072, mean reward: 3.307 [2.819, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.035, 10.514], loss: 1.306599, mae: 0.801346, mean_q: 5.800846
 54798/100000: episode: 1059, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 104.011, mean reward: 9.456 [3.683, 21.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.661], loss: 1.831562, mae: 0.976579, mean_q: 6.360718
 54804/100000: episode: 1060, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 46.117, mean reward: 7.686 [4.166, 10.879], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.814, 10.701], loss: 0.673666, mae: 0.683799, mean_q: 5.642490
 54814/100000: episode: 1061, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 51.748, mean reward: 5.175 [3.364, 7.315], mean action: 0.000 [0.000, 0.000], mean observation: 2.260 [-0.864, 10.560], loss: 0.871671, mae: 0.744355, mean_q: 5.604213
 54824/100000: episode: 1062, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 35.028, mean reward: 3.503 [3.068, 4.041], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.520], loss: 0.951762, mae: 0.846070, mean_q: 6.167642
 54828/100000: episode: 1063, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 24.478, mean reward: 6.119 [5.233, 7.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.591], loss: 1.402199, mae: 0.840971, mean_q: 5.985313
 54834/100000: episode: 1064, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 21.653, mean reward: 3.609 [3.300, 4.012], mean action: 0.000 [0.000, 0.000], mean observation: 2.338 [-0.035, 10.510], loss: 2.617394, mae: 1.061439, mean_q: 6.308937
 54838/100000: episode: 1065, duration: 0.023s, episode steps: 4, steps per second: 171, episode reward: 20.892, mean reward: 5.223 [4.517, 5.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.556], loss: 0.563542, mae: 0.760218, mean_q: 6.063188
 54844/100000: episode: 1066, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 27.457, mean reward: 4.576 [4.167, 5.443], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.606], loss: 0.609590, mae: 0.683096, mean_q: 5.949919
 54865/100000: episode: 1067, duration: 0.106s, episode steps: 21, steps per second: 199, episode reward: 126.190, mean reward: 6.009 [3.116, 22.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.035, 10.472], loss: 0.831733, mae: 0.746433, mean_q: 6.032469
 54885/100000: episode: 1068, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 174.040, mean reward: 8.702 [4.141, 17.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.638, 10.589], loss: 1.606297, mae: 0.949039, mean_q: 6.391783
 54895/100000: episode: 1069, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 40.199, mean reward: 4.020 [2.997, 7.981], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.155, 10.462], loss: 1.213650, mae: 0.797217, mean_q: 5.906437
[Info] 4-TH LEVEL FOUND: 14.781997680664062, Considering 10/90 traces
 54916/100000: episode: 1070, duration: 4.347s, episode steps: 21, steps per second: 5, episode reward: 68.710, mean reward: 3.272 [2.672, 3.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.457], loss: 5.369801, mae: 1.128037, mean_q: 6.595539
 54921/100000: episode: 1071, duration: 0.035s, episode steps: 5, steps per second: 142, episode reward: 22.017, mean reward: 4.403 [3.636, 5.203], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.468], loss: 1.663418, mae: 0.809507, mean_q: 6.172457
 54927/100000: episode: 1072, duration: 0.033s, episode steps: 6, steps per second: 181, episode reward: 43.941, mean reward: 7.323 [5.439, 9.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.574], loss: 0.825103, mae: 0.752182, mean_q: 6.208351
 54941/100000: episode: 1073, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 83.378, mean reward: 5.956 [3.098, 11.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.035, 10.462], loss: 0.912715, mae: 0.769454, mean_q: 6.122686
 54956/100000: episode: 1074, duration: 0.073s, episode steps: 15, steps per second: 205, episode reward: 90.653, mean reward: 6.044 [3.639, 7.553], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.100, 10.593], loss: 1.401214, mae: 0.788134, mean_q: 6.019791
 54958/100000: episode: 1075, duration: 0.013s, episode steps: 2, steps per second: 149, episode reward: 9.513, mean reward: 4.757 [4.225, 5.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.367 [-0.035, 10.579], loss: 0.340685, mae: 0.616079, mean_q: 5.665907
 54966/100000: episode: 1076, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 53.603, mean reward: 6.700 [5.037, 8.111], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.366, 10.568], loss: 0.649909, mae: 0.763789, mean_q: 6.513516
 54970/100000: episode: 1077, duration: 0.025s, episode steps: 4, steps per second: 157, episode reward: 26.164, mean reward: 6.541 [4.342, 8.208], mean action: 0.000 [0.000, 0.000], mean observation: 2.330 [-0.035, 10.654], loss: 0.513119, mae: 0.647035, mean_q: 6.075383
 54984/100000: episode: 1078, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 72.842, mean reward: 5.203 [2.464, 12.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.443], loss: 2.383051, mae: 0.797096, mean_q: 6.043842
 54986/100000: episode: 1079, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 8.791, mean reward: 4.396 [4.195, 4.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.483], loss: 0.770651, mae: 0.966648, mean_q: 6.906593
 54988/100000: episode: 1080, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 9.379, mean reward: 4.690 [4.633, 4.746], mean action: 0.000 [0.000, 0.000], mean observation: 2.364 [-0.035, 10.523], loss: 0.944378, mae: 0.893736, mean_q: 6.455695
 55003/100000: episode: 1081, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 125.709, mean reward: 8.381 [4.522, 16.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.818, 10.625], loss: 6.352814, mae: 1.221043, mean_q: 6.616033
 55011/100000: episode: 1082, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 59.891, mean reward: 7.486 [5.489, 11.510], mean action: 0.000 [0.000, 0.000], mean observation: 2.314 [-0.035, 10.710], loss: 1.438074, mae: 0.887192, mean_q: 6.023643
 55019/100000: episode: 1083, duration: 0.043s, episode steps: 8, steps per second: 187, episode reward: 100.405, mean reward: 12.551 [6.053, 42.287], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.035, 10.769], loss: 19.145969, mae: 1.702110, mean_q: 7.449301
 55024/100000: episode: 1084, duration: 0.030s, episode steps: 5, steps per second: 169, episode reward: 22.043, mean reward: 4.409 [3.533, 4.988], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.120, 10.585], loss: 1.216287, mae: 1.072799, mean_q: 5.975809
 55029/100000: episode: 1085, duration: 0.028s, episode steps: 5, steps per second: 178, episode reward: 25.756, mean reward: 5.151 [4.288, 6.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.351, 10.523], loss: 2.312967, mae: 1.110609, mean_q: 5.946504
 55041/100000: episode: 1086, duration: 0.063s, episode steps: 12, steps per second: 190, episode reward: 61.466, mean reward: 5.122 [3.595, 7.682], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.559], loss: 2.899587, mae: 0.950466, mean_q: 6.619289
 55047/100000: episode: 1087, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 33.856, mean reward: 5.643 [4.570, 8.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.592, 10.665], loss: 3.313169, mae: 1.024420, mean_q: 6.551318
 55055/100000: episode: 1088, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 66.007, mean reward: 8.251 [5.963, 18.854], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.630], loss: 1.475296, mae: 0.895028, mean_q: 6.614984
 55069/100000: episode: 1089, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 68.058, mean reward: 4.861 [3.360, 7.080], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.511], loss: 11.480182, mae: 1.156256, mean_q: 6.531697
 55083/100000: episode: 1090, duration: 0.079s, episode steps: 14, steps per second: 178, episode reward: 104.254, mean reward: 7.447 [5.166, 9.764], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.665], loss: 1.307404, mae: 0.903473, mean_q: 6.289313
 55085/100000: episode: 1091, duration: 0.014s, episode steps: 2, steps per second: 145, episode reward: 10.417, mean reward: 5.209 [4.686, 5.731], mean action: 0.000 [0.000, 0.000], mean observation: 2.365 [-0.035, 10.566], loss: 0.552007, mae: 0.733652, mean_q: 6.653499
 55099/100000: episode: 1092, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 334.140, mean reward: 23.867 [6.571, 100.635], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.640], loss: 1.112422, mae: 0.880167, mean_q: 6.412528
 55113/100000: episode: 1093, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 312.667, mean reward: 22.333 [4.943, 163.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.600], loss: 18.080189, mae: 1.328105, mean_q: 6.581698
 55115/100000: episode: 1094, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 10.786, mean reward: 5.393 [4.557, 6.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.369 [-0.035, 10.626], loss: 1.105348, mae: 1.066264, mean_q: 6.818861
 55130/100000: episode: 1095, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 346.843, mean reward: 23.123 [7.917, 120.824], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.284, 10.678], loss: 25.640575, mae: 2.040207, mean_q: 6.851770
 55145/100000: episode: 1096, duration: 0.081s, episode steps: 15, steps per second: 184, episode reward: 135.655, mean reward: 9.044 [5.875, 16.774], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.446, 10.562], loss: 2.976471, mae: 1.645443, mean_q: 6.778254
[Info] FALSIFICATION!
 55151/100000: episode: 1097, duration: 0.217s, episode steps: 6, steps per second: 28, episode reward: 1083.846, mean reward: 180.641 [7.549, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.015, 10.377], loss: 1.768444, mae: 1.063644, mean_q: 5.959609
 55163/100000: episode: 1098, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 68.683, mean reward: 5.724 [2.723, 9.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.236, 10.556], loss: 21.440414, mae: 1.667311, mean_q: 7.188212
 55177/100000: episode: 1099, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 437.959, mean reward: 31.283 [5.326, 282.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.507], loss: 11.486778, mae: 1.273772, mean_q: 6.838510
[Info] FALSIFICATION!
 55179/100000: episode: 1100, duration: 0.187s, episode steps: 2, steps per second: 11, episode reward: 1030.911, mean reward: 515.455 [30.911, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.019, 10.577], loss: 2.940764, mae: 1.414022, mean_q: 7.543032
 55181/100000: episode: 1101, duration: 0.015s, episode steps: 2, steps per second: 136, episode reward: 9.381, mean reward: 4.691 [4.084, 5.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.332 [-0.035, 10.509], loss: 3.694254, mae: 1.928183, mean_q: 8.421490
 55185/100000: episode: 1102, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: 21.288, mean reward: 5.322 [4.247, 6.671], mean action: 0.000 [0.000, 0.000], mean observation: 2.322 [-0.035, 10.639], loss: 5.501209, mae: 1.544574, mean_q: 7.006464
 55199/100000: episode: 1103, duration: 0.071s, episode steps: 14, steps per second: 198, episode reward: 136.811, mean reward: 9.772 [3.649, 21.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.235, 10.558], loss: 3.285174, mae: 1.151608, mean_q: 6.864629
 55214/100000: episode: 1104, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 86.895, mean reward: 5.793 [2.805, 18.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.631, 10.524], loss: 1.599133, mae: 0.983499, mean_q: 7.149057
 55227/100000: episode: 1105, duration: 0.064s, episode steps: 13, steps per second: 202, episode reward: 150.767, mean reward: 11.597 [6.120, 35.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.640], loss: 4.083600, mae: 1.208495, mean_q: 7.027322
 55229/100000: episode: 1106, duration: 0.014s, episode steps: 2, steps per second: 147, episode reward: 8.969, mean reward: 4.484 [3.746, 5.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.035, 10.599], loss: 1.339506, mae: 1.035589, mean_q: 6.799280
 55242/100000: episode: 1107, duration: 0.068s, episode steps: 13, steps per second: 190, episode reward: 71.378, mean reward: 5.491 [3.597, 7.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.590], loss: 29.273067, mae: 1.617959, mean_q: 6.995542
 55257/100000: episode: 1108, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 316.344, mean reward: 21.090 [3.386, 216.657], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.762], loss: 2.232545, mae: 1.332560, mean_q: 6.698855
 55271/100000: episode: 1109, duration: 0.073s, episode steps: 14, steps per second: 193, episode reward: 109.991, mean reward: 7.857 [3.650, 22.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.224 [-0.778, 10.486], loss: 31.619761, mae: 1.847554, mean_q: 7.670344
 55277/100000: episode: 1110, duration: 0.040s, episode steps: 6, steps per second: 150, episode reward: 53.376, mean reward: 8.896 [7.113, 11.002], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.358, 10.696], loss: 2.645728, mae: 1.214997, mean_q: 6.416788
 55283/100000: episode: 1111, duration: 0.035s, episode steps: 6, steps per second: 173, episode reward: 57.764, mean reward: 9.627 [6.920, 13.722], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-0.035, 10.620], loss: 1.978114, mae: 0.994067, mean_q: 6.594219
 55287/100000: episode: 1112, duration: 0.027s, episode steps: 4, steps per second: 147, episode reward: 68.853, mean reward: 17.213 [6.338, 47.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.341 [-0.051, 10.760], loss: 25.731136, mae: 2.017611, mean_q: 7.627496
 55300/100000: episode: 1113, duration: 0.068s, episode steps: 13, steps per second: 191, episode reward: 101.521, mean reward: 7.809 [5.393, 11.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.515], loss: 71.021767, mae: 1.718099, mean_q: 6.646985
 55308/100000: episode: 1114, duration: 0.045s, episode steps: 8, steps per second: 178, episode reward: 50.843, mean reward: 6.355 [4.578, 9.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.562], loss: 93.764198, mae: 3.623230, mean_q: 9.450234
 55323/100000: episode: 1115, duration: 0.084s, episode steps: 15, steps per second: 178, episode reward: 216.401, mean reward: 14.427 [4.152, 39.127], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.678, 10.520], loss: 1067.823853, mae: 7.662183, mean_q: 11.555102
 55325/100000: episode: 1116, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 6.403, mean reward: 3.201 [3.153, 3.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.327 [-0.035, 10.438], loss: 4.801632, mae: 1.799652, mean_q: 5.473446
[Info] FALSIFICATION!
 55335/100000: episode: 1117, duration: 0.317s, episode steps: 10, steps per second: 32, episode reward: 1651.358, mean reward: 165.136 [13.234, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-0.469, 10.508], loss: 126.187965, mae: 2.981168, mean_q: 5.665931
 55349/100000: episode: 1118, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 127.626, mean reward: 9.116 [6.067, 14.054], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-1.140, 10.510], loss: 5.682453, mae: 1.984512, mean_q: 8.177235
 55354/100000: episode: 1119, duration: 0.028s, episode steps: 5, steps per second: 179, episode reward: 27.170, mean reward: 5.434 [3.431, 6.963], mean action: 0.000 [0.000, 0.000], mean observation: 2.331 [-0.035, 10.645], loss: 1.313241, mae: 1.160491, mean_q: 7.293246
 55356/100000: episode: 1120, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: 10.796, mean reward: 5.398 [4.941, 5.855], mean action: 0.000 [0.000, 0.000], mean observation: 2.386 [-0.035, 10.616], loss: 3.213051, mae: 1.343607, mean_q: 6.358763
 55364/100000: episode: 1121, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 50.810, mean reward: 6.351 [4.480, 13.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.227, 10.441], loss: 14.521324, mae: 1.576185, mean_q: 7.166371
 55368/100000: episode: 1122, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: 24.937, mean reward: 6.234 [4.403, 9.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.337 [-0.035, 10.626], loss: 2.173030, mae: 1.368023, mean_q: 7.656715
 55383/100000: episode: 1123, duration: 0.082s, episode steps: 15, steps per second: 182, episode reward: 91.472, mean reward: 6.098 [4.433, 8.104], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.035, 10.597], loss: 18.602896, mae: 1.763969, mean_q: 7.843330
 55387/100000: episode: 1124, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: 191.008, mean reward: 47.752 [9.910, 66.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.362, 10.710], loss: 1.053850, mae: 0.955298, mean_q: 6.832290
 55391/100000: episode: 1125, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 33.895, mean reward: 8.474 [7.461, 9.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.336 [-0.035, 10.655], loss: 3766.452148, mae: 10.078035, mean_q: 9.094330
 55406/100000: episode: 1126, duration: 0.082s, episode steps: 15, steps per second: 184, episode reward: 174.180, mean reward: 11.612 [4.594, 40.825], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-1.360, 10.653], loss: 13.660428, mae: 3.540591, mean_q: 9.106563
 55414/100000: episode: 1127, duration: 0.041s, episode steps: 8, steps per second: 194, episode reward: 56.606, mean reward: 7.076 [4.208, 13.438], mean action: 0.000 [0.000, 0.000], mean observation: 2.278 [-0.509, 10.562], loss: 206.179047, mae: 3.629533, mean_q: 7.325407
 55421/100000: episode: 1128, duration: 0.044s, episode steps: 7, steps per second: 160, episode reward: 43.869, mean reward: 6.267 [5.656, 7.711], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.849, 10.538], loss: 17.013966, mae: 3.244649, mean_q: 9.834883
 55427/100000: episode: 1129, duration: 0.042s, episode steps: 6, steps per second: 144, episode reward: 42.880, mean reward: 7.147 [5.622, 8.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.078, 10.639], loss: 2.442875, mae: 1.542829, mean_q: 8.370217
 55429/100000: episode: 1130, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 9.873, mean reward: 4.936 [4.149, 5.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-1.215, 10.473], loss: 4.670909, mae: 1.579637, mean_q: 6.801675
 55435/100000: episode: 1131, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 72.073, mean reward: 12.012 [7.227, 20.297], mean action: 0.000 [0.000, 0.000], mean observation: 2.320 [-0.035, 10.697], loss: 200.911728, mae: 3.004718, mean_q: 6.692512
 55448/100000: episode: 1132, duration: 0.071s, episode steps: 13, steps per second: 184, episode reward: 119.870, mean reward: 9.221 [4.986, 25.139], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.325, 10.458], loss: 8.761647, mae: 1.993650, mean_q: 8.799134
 55450/100000: episode: 1133, duration: 0.014s, episode steps: 2, steps per second: 146, episode reward: 8.240, mean reward: 4.120 [3.738, 4.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.441], loss: 2.715951, mae: 1.421894, mean_q: 7.962959
[Info] FALSIFICATION!
 55455/100000: episode: 1134, duration: 0.292s, episode steps: 5, steps per second: 17, episode reward: 1028.108, mean reward: 205.622 [5.311, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.916, 10.525], loss: 2.186417, mae: 1.272859, mean_q: 7.544382
 55469/100000: episode: 1135, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 117.558, mean reward: 8.397 [5.010, 13.300], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.415, 10.642], loss: 74.498703, mae: 2.443468, mean_q: 8.045649
 55481/100000: episode: 1136, duration: 0.064s, episode steps: 12, steps per second: 189, episode reward: 57.095, mean reward: 4.758 [3.372, 5.463], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.503, 10.537], loss: 24.143503, mae: 2.109521, mean_q: 8.846622
 55494/100000: episode: 1137, duration: 0.070s, episode steps: 13, steps per second: 186, episode reward: 101.016, mean reward: 7.770 [4.188, 13.862], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.710], loss: 8.897717, mae: 1.547786, mean_q: 7.917832
 55508/100000: episode: 1138, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 194.086, mean reward: 13.863 [3.634, 79.914], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.562], loss: 92.060013, mae: 2.248437, mean_q: 8.463773
[Info] FALSIFICATION!
 55511/100000: episode: 1139, duration: 0.283s, episode steps: 3, steps per second: 11, episode reward: 1205.396, mean reward: 401.799 [15.772, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.017, 10.425], loss: 483.321899, mae: 5.799934, mean_q: 9.003635
 55523/100000: episode: 1140, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 45.764, mean reward: 3.814 [2.678, 6.035], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.135, 10.483], loss: 2536.940674, mae: 10.950673, mean_q: 13.307466
 55529/100000: episode: 1141, duration: 0.034s, episode steps: 6, steps per second: 175, episode reward: 28.552, mean reward: 4.759 [4.138, 6.018], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.517], loss: 44.992573, mae: 4.737126, mean_q: 11.551186
 55541/100000: episode: 1142, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 68.885, mean reward: 5.740 [4.385, 9.888], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.134, 10.512], loss: 23.943811, mae: 2.623629, mean_q: 5.640343
 55547/100000: episode: 1143, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 50.838, mean reward: 8.473 [6.888, 12.552], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-1.835, 10.594], loss: 31.639734, mae: 2.542915, mean_q: 6.055907
 55552/100000: episode: 1144, duration: 0.028s, episode steps: 5, steps per second: 180, episode reward: 32.046, mean reward: 6.409 [4.413, 8.141], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.309, 10.600], loss: 4.057555, mae: 1.770972, mean_q: 8.047328
 55557/100000: episode: 1145, duration: 0.029s, episode steps: 5, steps per second: 175, episode reward: 23.013, mean reward: 4.603 [4.093, 5.381], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.552, 10.509], loss: 3.631463, mae: 1.869409, mean_q: 8.649866
 55571/100000: episode: 1146, duration: 0.074s, episode steps: 14, steps per second: 189, episode reward: 126.099, mean reward: 9.007 [3.486, 23.013], mean action: 0.000 [0.000, 0.000], mean observation: 2.257 [-0.035, 10.709], loss: 19.835369, mae: 2.528013, mean_q: 9.416188
 55579/100000: episode: 1147, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 45.568, mean reward: 5.696 [4.476, 8.103], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.293, 10.474], loss: 1.748703, mae: 1.296456, mean_q: 8.184822
 55585/100000: episode: 1148, duration: 0.035s, episode steps: 6, steps per second: 172, episode reward: 53.218, mean reward: 8.870 [5.284, 15.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.311 [-0.035, 10.630], loss: 204.610977, mae: 3.086346, mean_q: 8.422787
 55600/100000: episode: 1149, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 95.024, mean reward: 6.335 [3.764, 12.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.321, 10.515], loss: 82.737717, mae: 2.616350, mean_q: 8.731920
[Info] FALSIFICATION!
 55606/100000: episode: 1150, duration: 0.298s, episode steps: 6, steps per second: 20, episode reward: 1084.674, mean reward: 180.779 [6.544, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.016, 10.143], loss: 2.628148, mae: 1.567100, mean_q: 8.435796
 55620/100000: episode: 1151, duration: 0.077s, episode steps: 14, steps per second: 182, episode reward: 78.714, mean reward: 5.622 [3.605, 8.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.730, 10.510], loss: 3236.360840, mae: 13.723198, mean_q: 14.452460
 55627/100000: episode: 1152, duration: 0.037s, episode steps: 7, steps per second: 188, episode reward: 37.730, mean reward: 5.390 [3.891, 6.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.299 [-0.384, 10.647], loss: 2321.739990, mae: 7.886072, mean_q: 8.010145
 55629/100000: episode: 1153, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: 6.916, mean reward: 3.458 [3.420, 3.496], mean action: 0.000 [0.000, 0.000], mean observation: 2.340 [-0.035, 10.438], loss: 142.918030, mae: 3.722708, mean_q: 8.353130
 55641/100000: episode: 1154, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 152.896, mean reward: 12.741 [5.873, 27.811], mean action: 0.000 [0.000, 0.000], mean observation: 2.259 [-0.035, 10.673], loss: 34.827145, mae: 3.444982, mean_q: 10.512071
 55653/100000: episode: 1155, duration: 0.073s, episode steps: 12, steps per second: 164, episode reward: 42.790, mean reward: 3.566 [3.029, 4.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.271 [-0.035, 10.497], loss: 1316.782715, mae: 8.706757, mean_q: 13.326234
 55660/100000: episode: 1156, duration: 0.040s, episode steps: 7, steps per second: 175, episode reward: 49.802, mean reward: 7.115 [5.393, 8.732], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.101, 10.558], loss: 83.060860, mae: 4.973278, mean_q: 12.602995
[Info] FALSIFICATION!
 55664/100000: episode: 1157, duration: 0.295s, episode steps: 4, steps per second: 14, episode reward: 1171.230, mean reward: 292.808 [10.744, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.018, 10.265], loss: 72.421127, mae: 2.465873, mean_q: 7.972075
 55670/100000: episode: 1158, duration: 0.042s, episode steps: 6, steps per second: 142, episode reward: 39.836, mean reward: 6.639 [4.869, 9.084], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.035, 10.578], loss: 4.449821, mae: 1.512700, mean_q: 8.353215
 55674/100000: episode: 1159, duration: 0.025s, episode steps: 4, steps per second: 161, episode reward: 24.069, mean reward: 6.017 [5.160, 7.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.329 [-0.035, 10.612], loss: 314.647125, mae: 4.390939, mean_q: 9.722306
[Info] Complete ISplit Iteration
[Info] Levels: [4.917568, 6.57991, 8.6185, 14.781998, 26.463121]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.1, 0.77]
[Info] Error Prob: 7.700000000000003e-05

 55686/100000: episode: 1160, duration: 4.548s, episode steps: 12, steps per second: 3, episode reward: 82.826, mean reward: 6.902 [3.802, 16.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.229, 10.628], loss: 1297.816772, mae: 6.269107, mean_q: 11.543594
 55786/100000: episode: 1161, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 184.605, mean reward: 1.846 [1.456, 2.970], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.519, 10.155], loss: 336.531525, mae: 3.649104, mean_q: 9.562359
 55886/100000: episode: 1162, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 213.553, mean reward: 2.136 [1.465, 4.324], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.605, 10.383], loss: 952.696777, mae: 6.178951, mean_q: 11.516260
 55986/100000: episode: 1163, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 195.469, mean reward: 1.955 [1.493, 3.049], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.022, 10.098], loss: 319.706635, mae: 2.904215, mean_q: 8.848392
 56086/100000: episode: 1164, duration: 0.486s, episode steps: 100, steps per second: 206, episode reward: 191.525, mean reward: 1.915 [1.444, 2.840], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.121, 10.105], loss: 661.991089, mae: 4.811447, mean_q: 10.371247
 56186/100000: episode: 1165, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 180.354, mean reward: 1.804 [1.498, 2.887], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.538, 10.098], loss: 1222.428345, mae: 6.852453, mean_q: 11.899549
 56286/100000: episode: 1166, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 197.731, mean reward: 1.977 [1.483, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.667, 10.098], loss: 503.004272, mae: 3.849742, mean_q: 9.914967
 56386/100000: episode: 1167, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 198.472, mean reward: 1.985 [1.493, 4.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.222, 10.133], loss: 1072.220337, mae: 6.372341, mean_q: 11.484719
 56486/100000: episode: 1168, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 190.361, mean reward: 1.904 [1.524, 4.708], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.234, 10.098], loss: 502.797424, mae: 4.426247, mean_q: 10.734189
 56586/100000: episode: 1169, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 191.844, mean reward: 1.918 [1.436, 3.564], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.841, 10.126], loss: 931.807129, mae: 6.011897, mean_q: 11.634094
 56686/100000: episode: 1170, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 183.331, mean reward: 1.833 [1.443, 2.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.507, 10.098], loss: 487.328217, mae: 3.706903, mean_q: 9.427608
 56786/100000: episode: 1171, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 197.748, mean reward: 1.977 [1.444, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.156, 10.289], loss: 638.436768, mae: 4.348672, mean_q: 10.401623
 56886/100000: episode: 1172, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 188.120, mean reward: 1.881 [1.465, 3.175], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.127, 10.200], loss: 675.740417, mae: 4.959931, mean_q: 10.777592
 56986/100000: episode: 1173, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 179.954, mean reward: 1.800 [1.493, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.763, 10.120], loss: 773.183350, mae: 4.772504, mean_q: 10.702048
 57086/100000: episode: 1174, duration: 0.489s, episode steps: 100, steps per second: 204, episode reward: 195.334, mean reward: 1.953 [1.460, 3.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.786, 10.147], loss: 1063.902710, mae: 5.915026, mean_q: 11.523903
 57186/100000: episode: 1175, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 188.200, mean reward: 1.882 [1.465, 2.725], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.844, 10.335], loss: 612.575378, mae: 4.247508, mean_q: 10.238540
 57286/100000: episode: 1176, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 182.600, mean reward: 1.826 [1.479, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.381, 10.098], loss: 815.668579, mae: 5.372869, mean_q: 10.429819
 57386/100000: episode: 1177, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 185.137, mean reward: 1.851 [1.436, 3.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.785, 10.107], loss: 888.868103, mae: 5.315102, mean_q: 11.181173
 57486/100000: episode: 1178, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 204.230, mean reward: 2.042 [1.451, 4.150], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.101, 10.098], loss: 771.283386, mae: 4.404399, mean_q: 10.059085
 57586/100000: episode: 1179, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 198.181, mean reward: 1.982 [1.458, 3.549], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.832, 10.098], loss: 751.045288, mae: 4.663519, mean_q: 10.687513
 57686/100000: episode: 1180, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 210.194, mean reward: 2.102 [1.490, 3.094], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.014, 10.098], loss: 1077.707275, mae: 5.717285, mean_q: 10.957825
 57786/100000: episode: 1181, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 192.295, mean reward: 1.923 [1.459, 2.816], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.914, 10.098], loss: 319.215424, mae: 2.872577, mean_q: 9.347663
 57886/100000: episode: 1182, duration: 0.540s, episode steps: 100, steps per second: 185, episode reward: 191.343, mean reward: 1.913 [1.448, 3.238], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.124, 10.371], loss: 1509.175171, mae: 6.877092, mean_q: 11.297107
 57986/100000: episode: 1183, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 180.584, mean reward: 1.806 [1.443, 3.111], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.034, 10.134], loss: 910.918335, mae: 5.220803, mean_q: 10.892326
 58086/100000: episode: 1184, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 184.971, mean reward: 1.850 [1.444, 3.537], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.681, 10.098], loss: 479.440338, mae: 3.718704, mean_q: 10.069448
 58186/100000: episode: 1185, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 213.629, mean reward: 2.136 [1.511, 3.825], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.898, 10.098], loss: 1223.692627, mae: 6.662960, mean_q: 11.200062
 58286/100000: episode: 1186, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 205.771, mean reward: 2.058 [1.445, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.467, 10.374], loss: 1332.331299, mae: 6.468732, mean_q: 11.294150
 58386/100000: episode: 1187, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 191.089, mean reward: 1.911 [1.496, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.927, 10.246], loss: 607.239685, mae: 3.982004, mean_q: 9.757292
 58486/100000: episode: 1188, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 182.844, mean reward: 1.828 [1.490, 3.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.996, 10.192], loss: 324.430237, mae: 3.069204, mean_q: 8.705713
 58586/100000: episode: 1189, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 199.615, mean reward: 1.996 [1.485, 4.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.554, 10.215], loss: 310.928528, mae: 2.758920, mean_q: 8.653820
 58686/100000: episode: 1190, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 190.994, mean reward: 1.910 [1.444, 3.045], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.663, 10.098], loss: 316.378601, mae: 2.850691, mean_q: 8.582109
 58786/100000: episode: 1191, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 203.389, mean reward: 2.034 [1.451, 3.773], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.871, 10.098], loss: 332.880554, mae: 2.796028, mean_q: 8.149953
 58886/100000: episode: 1192, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 190.770, mean reward: 1.908 [1.445, 3.339], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.820, 10.126], loss: 641.386230, mae: 4.227405, mean_q: 9.414857
 58986/100000: episode: 1193, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 189.578, mean reward: 1.896 [1.460, 2.712], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.874, 10.118], loss: 1067.614624, mae: 5.247678, mean_q: 9.636272
 59086/100000: episode: 1194, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 181.903, mean reward: 1.819 [1.463, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.195, 10.098], loss: 363.570068, mae: 3.482946, mean_q: 8.958445
 59186/100000: episode: 1195, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 200.864, mean reward: 2.009 [1.453, 3.775], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.101, 10.098], loss: 328.653900, mae: 2.923585, mean_q: 8.293182
 59286/100000: episode: 1196, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 201.468, mean reward: 2.015 [1.479, 7.824], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.723, 10.224], loss: 1203.710815, mae: 5.550114, mean_q: 9.200884
 59386/100000: episode: 1197, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 207.404, mean reward: 2.074 [1.460, 4.410], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.140, 10.098], loss: 334.282074, mae: 3.190364, mean_q: 8.561433
 59486/100000: episode: 1198, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 219.303, mean reward: 2.193 [1.460, 5.457], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-2.253, 10.098], loss: 326.799042, mae: 2.773141, mean_q: 7.788607
 59586/100000: episode: 1199, duration: 0.506s, episode steps: 100, steps per second: 197, episode reward: 196.599, mean reward: 1.966 [1.476, 3.736], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.886, 10.411], loss: 498.917023, mae: 3.077817, mean_q: 7.519190
 59686/100000: episode: 1200, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 194.002, mean reward: 1.940 [1.458, 3.946], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.602, 10.181], loss: 482.601929, mae: 3.196126, mean_q: 7.856058
 59786/100000: episode: 1201, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 214.521, mean reward: 2.145 [1.458, 4.597], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.970, 10.098], loss: 1067.607666, mae: 5.661876, mean_q: 9.382779
 59886/100000: episode: 1202, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 198.244, mean reward: 1.982 [1.501, 2.985], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.647, 10.098], loss: 472.286133, mae: 3.238227, mean_q: 8.026202
 59986/100000: episode: 1203, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 196.345, mean reward: 1.963 [1.442, 4.432], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.451, 10.098], loss: 311.634430, mae: 2.282094, mean_q: 6.902854
 60086/100000: episode: 1204, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 187.345, mean reward: 1.873 [1.469, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.396, 10.210], loss: 751.696167, mae: 4.356948, mean_q: 8.103078
 60186/100000: episode: 1205, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 197.356, mean reward: 1.974 [1.481, 3.602], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.258, 10.112], loss: 10.383293, mae: 1.100274, mean_q: 5.873903
 60286/100000: episode: 1206, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 201.668, mean reward: 2.017 [1.459, 3.240], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.051, 10.098], loss: 873.301147, mae: 3.744395, mean_q: 6.832982
 60386/100000: episode: 1207, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 193.742, mean reward: 1.937 [1.460, 3.433], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.735, 10.098], loss: 314.181641, mae: 1.949194, mean_q: 5.727493
 60486/100000: episode: 1208, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 192.575, mean reward: 1.926 [1.463, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.028, 10.098], loss: 3.337428, mae: 0.766185, mean_q: 4.830647
 60586/100000: episode: 1209, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 197.945, mean reward: 1.979 [1.460, 4.555], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.298, 10.257], loss: 286.614410, mae: 1.537128, mean_q: 4.766569
 60686/100000: episode: 1210, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 184.952, mean reward: 1.850 [1.463, 6.200], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.595, 10.098], loss: 0.615102, mae: 0.490208, mean_q: 3.976379
 60786/100000: episode: 1211, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 217.794, mean reward: 2.178 [1.439, 7.468], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.988, 10.098], loss: 0.229205, mae: 0.428801, mean_q: 3.924749
 60886/100000: episode: 1212, duration: 0.528s, episode steps: 100, steps per second: 190, episode reward: 186.665, mean reward: 1.867 [1.441, 3.060], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.058, 10.104], loss: 0.195382, mae: 0.417809, mean_q: 3.903838
 60986/100000: episode: 1213, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 188.325, mean reward: 1.883 [1.461, 3.238], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.031, 10.098], loss: 0.206740, mae: 0.412147, mean_q: 3.897014
 61086/100000: episode: 1214, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 187.710, mean reward: 1.877 [1.454, 2.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.461, 10.176], loss: 0.168334, mae: 0.389350, mean_q: 3.864743
 61186/100000: episode: 1215, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 185.410, mean reward: 1.854 [1.446, 2.812], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.323, 10.098], loss: 0.153731, mae: 0.374720, mean_q: 3.873409
 61286/100000: episode: 1216, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 193.700, mean reward: 1.937 [1.432, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.431, 10.191], loss: 0.156484, mae: 0.376641, mean_q: 3.856883
 61386/100000: episode: 1217, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 228.774, mean reward: 2.288 [1.494, 4.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.792, 10.098], loss: 0.160844, mae: 0.386259, mean_q: 3.885942
 61486/100000: episode: 1218, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 211.107, mean reward: 2.111 [1.526, 3.947], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.751, 10.267], loss: 0.146909, mae: 0.380750, mean_q: 3.863175
 61586/100000: episode: 1219, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 183.747, mean reward: 1.837 [1.469, 2.523], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.027, 10.176], loss: 0.143844, mae: 0.374721, mean_q: 3.891585
 61686/100000: episode: 1220, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 190.363, mean reward: 1.904 [1.446, 2.963], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.559, 10.098], loss: 0.142024, mae: 0.371437, mean_q: 3.881692
 61786/100000: episode: 1221, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 200.570, mean reward: 2.006 [1.531, 4.732], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.411, 10.182], loss: 0.137943, mae: 0.367653, mean_q: 3.892809
 61886/100000: episode: 1222, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 190.234, mean reward: 1.902 [1.500, 2.941], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.400, 10.141], loss: 0.138706, mae: 0.363053, mean_q: 3.876756
 61986/100000: episode: 1223, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 221.070, mean reward: 2.211 [1.482, 4.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.821, 10.098], loss: 0.141964, mae: 0.366344, mean_q: 3.896579
 62086/100000: episode: 1224, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 186.345, mean reward: 1.863 [1.467, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.503, 10.098], loss: 0.135965, mae: 0.362489, mean_q: 3.912827
 62186/100000: episode: 1225, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 189.183, mean reward: 1.892 [1.454, 3.405], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.089, 10.147], loss: 0.126662, mae: 0.349563, mean_q: 3.878824
 62286/100000: episode: 1226, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 183.889, mean reward: 1.839 [1.469, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.894, 10.134], loss: 0.134906, mae: 0.359803, mean_q: 3.895030
 62386/100000: episode: 1227, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 192.674, mean reward: 1.927 [1.474, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.506, 10.098], loss: 0.129695, mae: 0.359904, mean_q: 3.897629
 62486/100000: episode: 1228, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 202.767, mean reward: 2.028 [1.441, 3.862], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.866, 10.291], loss: 0.129600, mae: 0.352505, mean_q: 3.923821
 62586/100000: episode: 1229, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 203.557, mean reward: 2.036 [1.475, 3.287], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.277, 10.258], loss: 0.118558, mae: 0.344284, mean_q: 3.903880
 62686/100000: episode: 1230, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 212.984, mean reward: 2.130 [1.495, 6.139], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.887, 10.098], loss: 0.131728, mae: 0.353747, mean_q: 3.911147
 62786/100000: episode: 1231, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 187.120, mean reward: 1.871 [1.442, 2.844], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.241, 10.098], loss: 0.115272, mae: 0.337449, mean_q: 3.889865
 62886/100000: episode: 1232, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 195.314, mean reward: 1.953 [1.470, 5.699], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.648, 10.098], loss: 0.127980, mae: 0.351654, mean_q: 3.927705
 62986/100000: episode: 1233, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 192.426, mean reward: 1.924 [1.476, 4.061], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.031, 10.104], loss: 0.111787, mae: 0.330064, mean_q: 3.896871
 63086/100000: episode: 1234, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 175.831, mean reward: 1.758 [1.451, 2.753], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.582, 10.183], loss: 0.112218, mae: 0.342088, mean_q: 3.904984
 63186/100000: episode: 1235, duration: 0.536s, episode steps: 100, steps per second: 187, episode reward: 182.557, mean reward: 1.826 [1.472, 2.673], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.270, 10.177], loss: 0.118946, mae: 0.337615, mean_q: 3.907849
 63286/100000: episode: 1236, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 186.503, mean reward: 1.865 [1.439, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.836, 10.243], loss: 0.105830, mae: 0.329059, mean_q: 3.883182
 63386/100000: episode: 1237, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 184.503, mean reward: 1.845 [1.443, 2.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.578, 10.131], loss: 0.109983, mae: 0.329699, mean_q: 3.879258
 63486/100000: episode: 1238, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 197.626, mean reward: 1.976 [1.462, 3.685], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-1.627, 10.098], loss: 0.112093, mae: 0.326816, mean_q: 3.874906
 63586/100000: episode: 1239, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 195.442, mean reward: 1.954 [1.463, 3.521], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.623, 10.098], loss: 0.123272, mae: 0.331920, mean_q: 3.890656
 63686/100000: episode: 1240, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 183.458, mean reward: 1.835 [1.505, 2.525], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.233, 10.098], loss: 0.104516, mae: 0.330355, mean_q: 3.883003
 63786/100000: episode: 1241, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 189.659, mean reward: 1.897 [1.447, 3.392], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.321, 10.220], loss: 0.123456, mae: 0.334045, mean_q: 3.886852
 63886/100000: episode: 1242, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 229.510, mean reward: 2.295 [1.472, 3.612], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.621, 10.412], loss: 0.112556, mae: 0.329079, mean_q: 3.877504
 63986/100000: episode: 1243, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 180.103, mean reward: 1.801 [1.487, 2.882], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-2.458, 10.098], loss: 0.112293, mae: 0.316815, mean_q: 3.872549
 64086/100000: episode: 1244, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 204.063, mean reward: 2.041 [1.486, 3.394], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.081, 10.414], loss: 0.104211, mae: 0.317052, mean_q: 3.896613
 64186/100000: episode: 1245, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 190.323, mean reward: 1.903 [1.451, 5.872], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.535, 10.301], loss: 0.106064, mae: 0.318225, mean_q: 3.897771
 64286/100000: episode: 1246, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 183.995, mean reward: 1.840 [1.460, 2.971], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.085, 10.098], loss: 0.101441, mae: 0.313628, mean_q: 3.867804
 64386/100000: episode: 1247, duration: 0.542s, episode steps: 100, steps per second: 184, episode reward: 189.159, mean reward: 1.892 [1.433, 3.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.032, 10.098], loss: 0.113619, mae: 0.325671, mean_q: 3.878989
 64486/100000: episode: 1248, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 190.916, mean reward: 1.909 [1.452, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.372, 10.098], loss: 0.114336, mae: 0.327408, mean_q: 3.876302
 64586/100000: episode: 1249, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 200.053, mean reward: 2.001 [1.501, 5.512], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.744, 10.098], loss: 0.133311, mae: 0.335085, mean_q: 3.889096
 64686/100000: episode: 1250, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 190.412, mean reward: 1.904 [1.469, 3.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.098, 10.098], loss: 0.094904, mae: 0.303505, mean_q: 3.849654
 64786/100000: episode: 1251, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 187.989, mean reward: 1.880 [1.439, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.998, 10.106], loss: 0.095907, mae: 0.300415, mean_q: 3.851771
 64886/100000: episode: 1252, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 199.423, mean reward: 1.994 [1.462, 4.781], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.838, 10.098], loss: 0.095461, mae: 0.305708, mean_q: 3.838577
 64986/100000: episode: 1253, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 194.989, mean reward: 1.950 [1.435, 5.053], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.725, 10.130], loss: 0.094442, mae: 0.301521, mean_q: 3.844033
 65086/100000: episode: 1254, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 224.906, mean reward: 2.249 [1.521, 5.580], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.697, 10.254], loss: 0.118374, mae: 0.330159, mean_q: 3.872360
 65186/100000: episode: 1255, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 193.131, mean reward: 1.931 [1.458, 3.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.607, 10.098], loss: 0.089415, mae: 0.303405, mean_q: 3.848323
 65286/100000: episode: 1256, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 182.453, mean reward: 1.825 [1.442, 3.540], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.625, 10.156], loss: 0.095269, mae: 0.306185, mean_q: 3.857434
 65386/100000: episode: 1257, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 192.035, mean reward: 1.920 [1.459, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.927, 10.118], loss: 0.108419, mae: 0.318451, mean_q: 3.842709
 65486/100000: episode: 1258, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 234.197, mean reward: 2.342 [1.549, 5.034], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.590, 10.098], loss: 0.118943, mae: 0.325681, mean_q: 3.876630
 65586/100000: episode: 1259, duration: 0.480s, episode steps: 100, steps per second: 208, episode reward: 181.168, mean reward: 1.812 [1.450, 3.215], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.391, 10.098], loss: 0.106095, mae: 0.316354, mean_q: 3.851661
[Info] 1-TH LEVEL FOUND: 5.06293249130249, Considering 10/90 traces
 65686/100000: episode: 1260, duration: 4.768s, episode steps: 100, steps per second: 21, episode reward: 199.912, mean reward: 1.999 [1.467, 7.646], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-2.545, 10.269], loss: 0.104964, mae: 0.308371, mean_q: 3.866229
 65698/100000: episode: 1261, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 24.268, mean reward: 2.022 [1.651, 2.331], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.342], loss: 0.080862, mae: 0.288612, mean_q: 3.828275
 65715/100000: episode: 1262, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 42.280, mean reward: 2.487 [1.905, 3.222], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.455], loss: 0.085073, mae: 0.316205, mean_q: 3.880344
 65731/100000: episode: 1263, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 39.421, mean reward: 2.464 [1.744, 4.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.406, 10.100], loss: 0.104633, mae: 0.333607, mean_q: 3.909104
 65763/100000: episode: 1264, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 92.687, mean reward: 2.896 [1.929, 4.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.444, 10.100], loss: 0.120006, mae: 0.316567, mean_q: 3.855682
 65795/100000: episode: 1265, duration: 0.159s, episode steps: 32, steps per second: 201, episode reward: 114.314, mean reward: 3.572 [2.201, 5.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.024 [-0.248, 10.100], loss: 0.081247, mae: 0.297806, mean_q: 3.878666
 65812/100000: episode: 1266, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 34.555, mean reward: 2.033 [1.758, 2.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.035, 10.233], loss: 0.113738, mae: 0.325780, mean_q: 3.896789
 65909/100000: episode: 1267, duration: 0.462s, episode steps: 97, steps per second: 210, episode reward: 189.759, mean reward: 1.956 [1.449, 2.987], mean action: 0.000 [0.000, 0.000], mean observation: 1.474 [-1.066, 10.182], loss: 0.135570, mae: 0.329615, mean_q: 3.912551
 65925/100000: episode: 1268, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 36.464, mean reward: 2.279 [1.769, 2.892], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.633, 10.100], loss: 0.112699, mae: 0.330788, mean_q: 3.866491
 65961/100000: episode: 1269, duration: 0.179s, episode steps: 36, steps per second: 201, episode reward: 87.945, mean reward: 2.443 [1.629, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-0.321, 10.285], loss: 0.106293, mae: 0.331663, mean_q: 3.932597
 65993/100000: episode: 1270, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 78.001, mean reward: 2.438 [1.716, 3.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.173, 10.279], loss: 0.092727, mae: 0.304909, mean_q: 3.894489
 66029/100000: episode: 1271, duration: 0.198s, episode steps: 36, steps per second: 182, episode reward: 95.440, mean reward: 2.651 [2.013, 3.819], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.035, 10.433], loss: 0.098418, mae: 0.319579, mean_q: 3.919824
 66055/100000: episode: 1272, duration: 0.142s, episode steps: 26, steps per second: 184, episode reward: 74.222, mean reward: 2.855 [2.160, 5.045], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.119, 10.424], loss: 0.114820, mae: 0.343350, mean_q: 3.962123
 66152/100000: episode: 1273, duration: 0.494s, episode steps: 97, steps per second: 197, episode reward: 178.552, mean reward: 1.841 [1.444, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 1.479 [-0.720, 10.100], loss: 0.114584, mae: 0.323949, mean_q: 3.947330
 66168/100000: episode: 1274, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 54.562, mean reward: 3.410 [2.087, 7.490], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-0.384, 10.100], loss: 0.095006, mae: 0.314530, mean_q: 3.913724
 66185/100000: episode: 1275, duration: 0.095s, episode steps: 17, steps per second: 178, episode reward: 41.524, mean reward: 2.443 [1.904, 3.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-1.401, 10.247], loss: 0.125461, mae: 0.334838, mean_q: 3.945801
 66202/100000: episode: 1276, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 43.458, mean reward: 2.556 [1.847, 3.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.035, 10.333], loss: 0.134077, mae: 0.342493, mean_q: 3.919312
 66214/100000: episode: 1277, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 23.883, mean reward: 1.990 [1.649, 2.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.332], loss: 0.105255, mae: 0.325017, mean_q: 3.987630
 66250/100000: episode: 1278, duration: 0.180s, episode steps: 36, steps per second: 200, episode reward: 86.982, mean reward: 2.416 [1.587, 3.334], mean action: 0.000 [0.000, 0.000], mean observation: 2.025 [-0.190, 10.278], loss: 0.146077, mae: 0.341076, mean_q: 3.957948
 66347/100000: episode: 1279, duration: 0.478s, episode steps: 97, steps per second: 203, episode reward: 174.237, mean reward: 1.796 [1.461, 2.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.484 [-0.799, 10.232], loss: 0.113213, mae: 0.322253, mean_q: 3.934536
 66359/100000: episode: 1280, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 23.199, mean reward: 1.933 [1.645, 2.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.238], loss: 0.081189, mae: 0.296533, mean_q: 3.905244
 66391/100000: episode: 1281, duration: 0.170s, episode steps: 32, steps per second: 188, episode reward: 89.593, mean reward: 2.800 [1.949, 5.233], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-1.032, 10.285], loss: 0.098232, mae: 0.311961, mean_q: 3.903590
 66403/100000: episode: 1282, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 23.163, mean reward: 1.930 [1.709, 2.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.039, 10.322], loss: 0.080182, mae: 0.297672, mean_q: 3.957920
 66415/100000: episode: 1283, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 24.101, mean reward: 2.008 [1.583, 2.720], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.369], loss: 0.164807, mae: 0.328324, mean_q: 3.916404
 66447/100000: episode: 1284, duration: 0.158s, episode steps: 32, steps per second: 202, episode reward: 136.685, mean reward: 4.271 [2.475, 12.872], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.186, 10.446], loss: 0.157828, mae: 0.328573, mean_q: 4.022382
 66464/100000: episode: 1285, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 44.267, mean reward: 2.604 [1.912, 4.831], mean action: 0.000 [0.000, 0.000], mean observation: 2.196 [-0.046, 10.384], loss: 0.159928, mae: 0.351312, mean_q: 3.943105
 66561/100000: episode: 1286, duration: 0.502s, episode steps: 97, steps per second: 193, episode reward: 185.425, mean reward: 1.912 [1.503, 3.365], mean action: 0.000 [0.000, 0.000], mean observation: 1.477 [-1.359, 10.100], loss: 0.134924, mae: 0.333798, mean_q: 3.951684
 66577/100000: episode: 1287, duration: 0.097s, episode steps: 16, steps per second: 164, episode reward: 36.616, mean reward: 2.288 [1.680, 2.693], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.242, 10.100], loss: 0.145944, mae: 0.342045, mean_q: 4.001520
 66603/100000: episode: 1288, duration: 0.129s, episode steps: 26, steps per second: 201, episode reward: 73.035, mean reward: 2.809 [2.220, 5.235], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.459, 10.394], loss: 0.171033, mae: 0.335855, mean_q: 3.988624
 66635/100000: episode: 1289, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 145.602, mean reward: 4.550 [2.181, 10.126], mean action: 0.000 [0.000, 0.000], mean observation: 1.997 [-1.243, 10.100], loss: 0.150174, mae: 0.353609, mean_q: 3.985405
 66652/100000: episode: 1290, duration: 0.083s, episode steps: 17, steps per second: 205, episode reward: 45.350, mean reward: 2.668 [2.331, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.402], loss: 0.107736, mae: 0.308390, mean_q: 3.968905
 66684/100000: episode: 1291, duration: 0.182s, episode steps: 32, steps per second: 176, episode reward: 151.912, mean reward: 4.747 [2.551, 8.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.155, 10.603], loss: 0.165047, mae: 0.359310, mean_q: 3.987640
 66701/100000: episode: 1292, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 40.537, mean reward: 2.385 [1.948, 2.769], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-0.622, 10.388], loss: 0.175062, mae: 0.363168, mean_q: 4.067351
 66737/100000: episode: 1293, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 145.087, mean reward: 4.030 [2.623, 8.414], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-0.070, 10.576], loss: 0.226744, mae: 0.381030, mean_q: 4.063288
 66749/100000: episode: 1294, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 25.755, mean reward: 2.146 [1.847, 2.692], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.398], loss: 0.151655, mae: 0.345729, mean_q: 4.071826
 66775/100000: episode: 1295, duration: 0.135s, episode steps: 26, steps per second: 192, episode reward: 61.174, mean reward: 2.353 [1.613, 2.966], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.066, 10.161], loss: 0.163541, mae: 0.375909, mean_q: 4.066873
 66811/100000: episode: 1296, duration: 0.198s, episode steps: 36, steps per second: 182, episode reward: 105.591, mean reward: 2.933 [1.999, 4.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.028 [-0.614, 10.284], loss: 0.199158, mae: 0.388007, mean_q: 4.136374
 66818/100000: episode: 1297, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 16.811, mean reward: 2.402 [1.709, 5.083], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.228], loss: 0.138093, mae: 0.356719, mean_q: 4.041295
 66850/100000: episode: 1298, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 141.513, mean reward: 4.422 [2.623, 9.134], mean action: 0.000 [0.000, 0.000], mean observation: 2.003 [-0.682, 10.100], loss: 0.249655, mae: 0.397486, mean_q: 4.167824
 66867/100000: episode: 1299, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 38.186, mean reward: 2.246 [1.808, 2.880], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.652, 10.388], loss: 0.224553, mae: 0.406699, mean_q: 4.130126
 66899/100000: episode: 1300, duration: 0.166s, episode steps: 32, steps per second: 193, episode reward: 93.485, mean reward: 2.921 [2.106, 5.753], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.035, 10.425], loss: 0.172347, mae: 0.367815, mean_q: 4.057529
 66911/100000: episode: 1301, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 28.365, mean reward: 2.364 [1.536, 2.999], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.084, 10.444], loss: 0.201410, mae: 0.407335, mean_q: 4.151075
 66923/100000: episode: 1302, duration: 0.064s, episode steps: 12, steps per second: 189, episode reward: 22.936, mean reward: 1.911 [1.626, 2.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.049, 10.230], loss: 0.136570, mae: 0.336127, mean_q: 3.997335
 66939/100000: episode: 1303, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 40.528, mean reward: 2.533 [1.848, 3.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.462, 10.100], loss: 0.215306, mae: 0.412251, mean_q: 4.105163
 66946/100000: episode: 1304, duration: 0.044s, episode steps: 7, steps per second: 159, episode reward: 17.121, mean reward: 2.446 [2.116, 2.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.394], loss: 0.309321, mae: 0.455676, mean_q: 4.176517
 67043/100000: episode: 1305, duration: 0.493s, episode steps: 97, steps per second: 197, episode reward: 185.805, mean reward: 1.916 [1.446, 5.327], mean action: 0.000 [0.000, 0.000], mean observation: 1.473 [-0.702, 10.132], loss: 0.184116, mae: 0.374971, mean_q: 4.107801
 67079/100000: episode: 1306, duration: 0.173s, episode steps: 36, steps per second: 208, episode reward: 85.154, mean reward: 2.365 [1.629, 3.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.043 [-1.102, 10.202], loss: 0.184962, mae: 0.390406, mean_q: 4.159112
 67096/100000: episode: 1307, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 49.457, mean reward: 2.909 [2.312, 3.574], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.453], loss: 0.256137, mae: 0.415300, mean_q: 4.116764
 67113/100000: episode: 1308, duration: 0.082s, episode steps: 17, steps per second: 207, episode reward: 41.833, mean reward: 2.461 [1.978, 3.284], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.696, 10.449], loss: 0.239176, mae: 0.410568, mean_q: 4.187085
 67129/100000: episode: 1309, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 31.772, mean reward: 1.986 [1.596, 3.390], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.271, 10.100], loss: 0.235953, mae: 0.417172, mean_q: 4.189817
 67161/100000: episode: 1310, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 101.756, mean reward: 3.180 [1.916, 6.779], mean action: 0.000 [0.000, 0.000], mean observation: 2.041 [-0.359, 10.100], loss: 0.206159, mae: 0.385796, mean_q: 4.200199
 67193/100000: episode: 1311, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 76.870, mean reward: 2.402 [1.471, 3.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.035, 10.176], loss: 0.226984, mae: 0.411156, mean_q: 4.140429
 67225/100000: episode: 1312, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 115.185, mean reward: 3.600 [2.312, 5.608], mean action: 0.000 [0.000, 0.000], mean observation: 2.023 [-2.017, 10.100], loss: 0.165299, mae: 0.380988, mean_q: 4.189573
 67257/100000: episode: 1313, duration: 0.159s, episode steps: 32, steps per second: 202, episode reward: 105.698, mean reward: 3.303 [2.516, 4.033], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.899, 10.488], loss: 0.154387, mae: 0.370840, mean_q: 4.162318
 67274/100000: episode: 1314, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 38.207, mean reward: 2.247 [1.872, 2.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.404], loss: 0.119169, mae: 0.338024, mean_q: 4.112397
 67291/100000: episode: 1315, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 42.157, mean reward: 2.480 [1.966, 3.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.453], loss: 0.198824, mae: 0.404578, mean_q: 4.256934
 67303/100000: episode: 1316, duration: 0.063s, episode steps: 12, steps per second: 192, episode reward: 22.687, mean reward: 1.891 [1.684, 2.378], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.310], loss: 0.228818, mae: 0.388670, mean_q: 4.029835
 67329/100000: episode: 1317, duration: 0.137s, episode steps: 26, steps per second: 190, episode reward: 58.017, mean reward: 2.231 [1.769, 3.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.785, 10.314], loss: 0.216292, mae: 0.437130, mean_q: 4.251760
 67336/100000: episode: 1318, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 14.527, mean reward: 2.075 [1.906, 2.377], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.313], loss: 0.278932, mae: 0.437161, mean_q: 4.171207
 67433/100000: episode: 1319, duration: 0.512s, episode steps: 97, steps per second: 189, episode reward: 220.304, mean reward: 2.271 [1.500, 7.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.479 [-1.278, 10.395], loss: 0.241813, mae: 0.420728, mean_q: 4.251729
 67449/100000: episode: 1320, duration: 0.084s, episode steps: 16, steps per second: 191, episode reward: 34.267, mean reward: 2.142 [1.802, 2.559], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.683, 10.100], loss: 0.200332, mae: 0.405335, mean_q: 4.257755
 67475/100000: episode: 1321, duration: 0.130s, episode steps: 26, steps per second: 200, episode reward: 56.333, mean reward: 2.167 [1.462, 2.951], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.974, 10.179], loss: 0.217066, mae: 0.403659, mean_q: 4.236528
 67482/100000: episode: 1322, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 16.553, mean reward: 2.365 [2.151, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.249, 10.341], loss: 0.179298, mae: 0.374532, mean_q: 4.192370
 67489/100000: episode: 1323, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 16.450, mean reward: 2.350 [1.734, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.035, 10.262], loss: 0.385125, mae: 0.504580, mean_q: 4.375394
 67496/100000: episode: 1324, duration: 0.041s, episode steps: 7, steps per second: 172, episode reward: 18.983, mean reward: 2.712 [2.157, 4.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.035, 10.306], loss: 0.127199, mae: 0.363942, mean_q: 4.305768
 67508/100000: episode: 1325, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 23.356, mean reward: 1.946 [1.707, 2.136], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.251], loss: 0.189891, mae: 0.394425, mean_q: 4.190668
 67540/100000: episode: 1326, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 74.592, mean reward: 2.331 [1.710, 3.073], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.788, 10.219], loss: 0.204876, mae: 0.410335, mean_q: 4.252081
 67566/100000: episode: 1327, duration: 0.126s, episode steps: 26, steps per second: 207, episode reward: 60.994, mean reward: 2.346 [1.685, 4.166], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.046, 10.276], loss: 0.234561, mae: 0.426136, mean_q: 4.279124
 67582/100000: episode: 1328, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 29.281, mean reward: 1.830 [1.674, 2.319], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.140, 10.100], loss: 0.250157, mae: 0.425933, mean_q: 4.272319
 67614/100000: episode: 1329, duration: 0.174s, episode steps: 32, steps per second: 184, episode reward: 117.021, mean reward: 3.657 [2.706, 6.576], mean action: 0.000 [0.000, 0.000], mean observation: 2.012 [-0.462, 10.100], loss: 0.225220, mae: 0.423095, mean_q: 4.216581
 67640/100000: episode: 1330, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 65.990, mean reward: 2.538 [1.832, 3.105], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.035, 10.300], loss: 0.188249, mae: 0.387828, mean_q: 4.242023
 67672/100000: episode: 1331, duration: 0.165s, episode steps: 32, steps per second: 194, episode reward: 76.350, mean reward: 2.386 [1.746, 4.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.066 [-0.674, 10.257], loss: 0.186549, mae: 0.401868, mean_q: 4.297194
 67684/100000: episode: 1332, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 26.294, mean reward: 2.191 [1.680, 4.022], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.136, 10.258], loss: 0.174241, mae: 0.389693, mean_q: 4.195582
 67696/100000: episode: 1333, duration: 0.064s, episode steps: 12, steps per second: 189, episode reward: 24.145, mean reward: 2.012 [1.521, 2.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.057, 10.247], loss: 0.161353, mae: 0.386059, mean_q: 4.311199
 67708/100000: episode: 1334, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 24.617, mean reward: 2.051 [1.823, 2.456], mean action: 0.000 [0.000, 0.000], mean observation: 2.249 [-0.035, 10.349], loss: 0.188071, mae: 0.403765, mean_q: 4.284826
 67805/100000: episode: 1335, duration: 0.505s, episode steps: 97, steps per second: 192, episode reward: 186.592, mean reward: 1.924 [1.479, 3.013], mean action: 0.000 [0.000, 0.000], mean observation: 1.468 [-1.058, 10.100], loss: 0.223632, mae: 0.407676, mean_q: 4.269670
 67822/100000: episode: 1336, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 33.584, mean reward: 1.976 [1.697, 2.307], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.324, 10.247], loss: 0.193256, mae: 0.382797, mean_q: 4.223229
 67854/100000: episode: 1337, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 76.421, mean reward: 2.388 [1.596, 5.241], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.515, 10.237], loss: 0.195488, mae: 0.406281, mean_q: 4.273236
 67861/100000: episode: 1338, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 18.224, mean reward: 2.603 [2.008, 3.944], mean action: 0.000 [0.000, 0.000], mean observation: 2.289 [-0.035, 10.352], loss: 0.227400, mae: 0.449511, mean_q: 4.391584
 67868/100000: episode: 1339, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 24.719, mean reward: 3.531 [2.856, 6.075], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.451], loss: 0.205232, mae: 0.369606, mean_q: 4.204698
 67904/100000: episode: 1340, duration: 0.195s, episode steps: 36, steps per second: 184, episode reward: 114.518, mean reward: 3.181 [2.143, 7.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.021 [-1.292, 10.406], loss: 0.230944, mae: 0.431124, mean_q: 4.288329
 67940/100000: episode: 1341, duration: 0.198s, episode steps: 36, steps per second: 182, episode reward: 82.379, mean reward: 2.288 [1.666, 4.543], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.309, 10.191], loss: 0.227057, mae: 0.411198, mean_q: 4.300786
 67956/100000: episode: 1342, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 36.842, mean reward: 2.303 [1.826, 3.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.250, 10.100], loss: 0.346865, mae: 0.452768, mean_q: 4.311448
 67973/100000: episode: 1343, duration: 0.090s, episode steps: 17, steps per second: 190, episode reward: 38.303, mean reward: 2.253 [1.895, 2.857], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.129, 10.308], loss: 0.299760, mae: 0.456288, mean_q: 4.359575
 68005/100000: episode: 1344, duration: 0.155s, episode steps: 32, steps per second: 207, episode reward: 201.658, mean reward: 6.302 [2.481, 30.945], mean action: 0.000 [0.000, 0.000], mean observation: 2.035 [-0.245, 10.100], loss: 0.355100, mae: 0.427424, mean_q: 4.325047
 68022/100000: episode: 1345, duration: 0.096s, episode steps: 17, steps per second: 178, episode reward: 52.675, mean reward: 3.099 [1.973, 4.364], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.551], loss: 0.185747, mae: 0.435245, mean_q: 4.301209
 68034/100000: episode: 1346, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 24.751, mean reward: 2.063 [1.591, 2.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.207], loss: 0.178921, mae: 0.391854, mean_q: 4.212929
 68050/100000: episode: 1347, duration: 0.094s, episode steps: 16, steps per second: 169, episode reward: 31.098, mean reward: 1.944 [1.498, 2.630], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-1.054, 10.100], loss: 1.045129, mae: 0.496619, mean_q: 4.448696
 68147/100000: episode: 1348, duration: 0.475s, episode steps: 97, steps per second: 204, episode reward: 184.416, mean reward: 1.901 [1.475, 2.949], mean action: 0.000 [0.000, 0.000], mean observation: 1.484 [-0.716, 10.166], loss: 0.626173, mae: 0.476520, mean_q: 4.373938
 68244/100000: episode: 1349, duration: 0.486s, episode steps: 97, steps per second: 200, episode reward: 192.827, mean reward: 1.988 [1.457, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.475 [-0.921, 10.137], loss: 0.388004, mae: 0.444582, mean_q: 4.359094
[Info] 2-TH LEVEL FOUND: 7.183562755584717, Considering 10/90 traces
 68270/100000: episode: 1350, duration: 4.326s, episode steps: 26, steps per second: 6, episode reward: 66.939, mean reward: 2.575 [2.101, 3.495], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.729, 10.336], loss: 0.246355, mae: 0.453069, mean_q: 4.436488
 68295/100000: episode: 1351, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 111.467, mean reward: 4.459 [2.637, 7.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.281, 10.543], loss: 0.267357, mae: 0.474364, mean_q: 4.422231
 68322/100000: episode: 1352, duration: 0.157s, episode steps: 27, steps per second: 172, episode reward: 105.619, mean reward: 3.912 [2.724, 6.493], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.255, 10.535], loss: 0.233327, mae: 0.438685, mean_q: 4.397017
 68346/100000: episode: 1353, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 72.276, mean reward: 3.011 [2.285, 4.616], mean action: 0.000 [0.000, 0.000], mean observation: 2.078 [-0.301, 10.100], loss: 0.184389, mae: 0.432371, mean_q: 4.436470
 68373/100000: episode: 1354, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 89.561, mean reward: 3.317 [2.035, 6.477], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.080, 10.393], loss: 0.235120, mae: 0.442019, mean_q: 4.403121
 68400/100000: episode: 1355, duration: 0.145s, episode steps: 27, steps per second: 187, episode reward: 79.037, mean reward: 2.927 [1.542, 11.461], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.075, 10.173], loss: 0.390826, mae: 0.453334, mean_q: 4.430937
 68428/100000: episode: 1356, duration: 0.146s, episode steps: 28, steps per second: 192, episode reward: 102.542, mean reward: 3.662 [1.979, 6.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.757, 10.341], loss: 0.648851, mae: 0.510583, mean_q: 4.562325
 68458/100000: episode: 1357, duration: 0.160s, episode steps: 30, steps per second: 188, episode reward: 161.826, mean reward: 5.394 [2.453, 11.376], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-1.078, 10.582], loss: 0.268056, mae: 0.445965, mean_q: 4.492686
 68488/100000: episode: 1358, duration: 0.161s, episode steps: 30, steps per second: 186, episode reward: 145.970, mean reward: 4.866 [2.907, 6.744], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.160, 10.615], loss: 0.237024, mae: 0.455174, mean_q: 4.460248
 68516/100000: episode: 1359, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 106.018, mean reward: 3.786 [2.350, 6.252], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.691, 10.461], loss: 0.323093, mae: 0.443557, mean_q: 4.496016
 68546/100000: episode: 1360, duration: 0.153s, episode steps: 30, steps per second: 197, episode reward: 93.758, mean reward: 3.125 [1.953, 6.941], mean action: 0.000 [0.000, 0.000], mean observation: 2.104 [-0.035, 10.311], loss: 0.470622, mae: 0.476814, mean_q: 4.488039
 68573/100000: episode: 1361, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 90.976, mean reward: 3.369 [2.105, 4.864], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.317, 10.379], loss: 0.447130, mae: 0.510986, mean_q: 4.582514
 68598/100000: episode: 1362, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 123.027, mean reward: 4.921 [2.914, 8.342], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.437, 10.577], loss: 0.945118, mae: 0.525655, mean_q: 4.615135
 68626/100000: episode: 1363, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 92.017, mean reward: 3.286 [2.304, 4.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.035, 10.541], loss: 0.284370, mae: 0.471649, mean_q: 4.596834
 68654/100000: episode: 1364, duration: 0.137s, episode steps: 28, steps per second: 204, episode reward: 91.530, mean reward: 3.269 [2.461, 6.479], mean action: 0.000 [0.000, 0.000], mean observation: 2.110 [-0.353, 10.473], loss: 0.382071, mae: 0.487740, mean_q: 4.596251
 68666/100000: episode: 1365, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 52.763, mean reward: 4.397 [2.623, 7.346], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.419], loss: 0.292486, mae: 0.484381, mean_q: 4.713577
 68693/100000: episode: 1366, duration: 0.129s, episode steps: 27, steps per second: 209, episode reward: 103.636, mean reward: 3.838 [2.338, 6.867], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-1.115, 10.467], loss: 0.275898, mae: 0.497613, mean_q: 4.639266
 68721/100000: episode: 1367, duration: 0.138s, episode steps: 28, steps per second: 203, episode reward: 115.178, mean reward: 4.114 [2.271, 8.337], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.035, 10.345], loss: 0.229928, mae: 0.460281, mean_q: 4.740644
 68748/100000: episode: 1368, duration: 0.136s, episode steps: 27, steps per second: 198, episode reward: 74.478, mean reward: 2.758 [2.126, 3.816], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.512, 10.376], loss: 0.684351, mae: 0.502491, mean_q: 4.683629
 68775/100000: episode: 1369, duration: 0.137s, episode steps: 27, steps per second: 198, episode reward: 137.154, mean reward: 5.080 [2.907, 10.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.181, 10.546], loss: 1.120046, mae: 0.586607, mean_q: 4.828214
 68803/100000: episode: 1370, duration: 0.148s, episode steps: 28, steps per second: 189, episode reward: 84.956, mean reward: 3.034 [2.233, 5.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.124, 10.376], loss: 0.710803, mae: 0.543504, mean_q: 4.795587
 68830/100000: episode: 1371, duration: 0.132s, episode steps: 27, steps per second: 205, episode reward: 115.694, mean reward: 4.285 [3.078, 7.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.525, 10.472], loss: 0.599176, mae: 0.554596, mean_q: 4.771933
 68857/100000: episode: 1372, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 90.636, mean reward: 3.357 [2.140, 5.349], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.709, 10.273], loss: 0.319160, mae: 0.516829, mean_q: 4.770593
 68881/100000: episode: 1373, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 174.959, mean reward: 7.290 [2.888, 62.705], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-1.162, 10.100], loss: 1.038512, mae: 0.625601, mean_q: 4.950001
 68906/100000: episode: 1374, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 201.545, mean reward: 8.062 [3.394, 20.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.315, 10.606], loss: 3.014461, mae: 0.724511, mean_q: 4.911147
 68936/100000: episode: 1375, duration: 0.176s, episode steps: 30, steps per second: 171, episode reward: 122.089, mean reward: 4.070 [2.890, 5.934], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.523, 10.515], loss: 0.669705, mae: 0.572170, mean_q: 4.894687
 68963/100000: episode: 1376, duration: 0.148s, episode steps: 27, steps per second: 182, episode reward: 95.001, mean reward: 3.519 [1.846, 6.129], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.504, 10.400], loss: 2.649042, mae: 0.740831, mean_q: 5.132383
 68987/100000: episode: 1377, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 142.351, mean reward: 5.931 [3.765, 11.905], mean action: 0.000 [0.000, 0.000], mean observation: 2.059 [-0.293, 10.100], loss: 2.587447, mae: 0.668351, mean_q: 4.979943
 69017/100000: episode: 1378, duration: 0.149s, episode steps: 30, steps per second: 201, episode reward: 78.353, mean reward: 2.612 [1.938, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.318, 10.350], loss: 0.738372, mae: 0.585692, mean_q: 4.960402
 69044/100000: episode: 1379, duration: 0.137s, episode steps: 27, steps per second: 197, episode reward: 87.156, mean reward: 3.228 [2.308, 8.168], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.834, 10.477], loss: 3.301750, mae: 0.737636, mean_q: 4.993808
 69074/100000: episode: 1380, duration: 0.146s, episode steps: 30, steps per second: 205, episode reward: 122.354, mean reward: 4.078 [2.425, 9.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.169, 10.454], loss: 0.439289, mae: 0.588725, mean_q: 5.029334
 69098/100000: episode: 1381, duration: 0.134s, episode steps: 24, steps per second: 179, episode reward: 117.884, mean reward: 4.912 [3.539, 7.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.079 [-0.915, 10.100], loss: 0.394742, mae: 0.556696, mean_q: 4.984879
 69122/100000: episode: 1382, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 151.731, mean reward: 6.322 [3.534, 30.980], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.995, 10.100], loss: 2.662721, mae: 0.677016, mean_q: 5.184615
 69146/100000: episode: 1383, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 181.971, mean reward: 7.582 [3.552, 35.778], mean action: 0.000 [0.000, 0.000], mean observation: 2.056 [-0.523, 10.100], loss: 0.556425, mae: 0.598527, mean_q: 5.016116
 69173/100000: episode: 1384, duration: 0.135s, episode steps: 27, steps per second: 199, episode reward: 103.124, mean reward: 3.819 [2.782, 5.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.035, 10.524], loss: 1.037019, mae: 0.684047, mean_q: 5.109191
 69203/100000: episode: 1385, duration: 0.162s, episode steps: 30, steps per second: 185, episode reward: 173.830, mean reward: 5.794 [2.308, 10.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.035, 10.677], loss: 0.385995, mae: 0.590219, mean_q: 5.132447
 69233/100000: episode: 1386, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 65.846, mean reward: 2.195 [1.465, 3.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.360, 10.291], loss: 0.927151, mae: 0.602187, mean_q: 5.094057
 69245/100000: episode: 1387, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 35.898, mean reward: 2.991 [2.362, 4.152], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.035, 10.441], loss: 0.421648, mae: 0.590764, mean_q: 5.147167
 69269/100000: episode: 1388, duration: 0.115s, episode steps: 24, steps per second: 208, episode reward: 108.967, mean reward: 4.540 [2.779, 9.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.614, 10.100], loss: 2.833510, mae: 0.720737, mean_q: 5.246620
 69296/100000: episode: 1389, duration: 0.135s, episode steps: 27, steps per second: 201, episode reward: 86.415, mean reward: 3.201 [2.473, 4.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.117 [-0.564, 10.442], loss: 0.486747, mae: 0.613545, mean_q: 5.207581
 69326/100000: episode: 1390, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 135.149, mean reward: 4.505 [2.852, 7.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.838, 10.477], loss: 0.829982, mae: 0.662905, mean_q: 5.317217
 69343/100000: episode: 1391, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 119.118, mean reward: 7.007 [4.006, 16.869], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.431, 10.100], loss: 1.479889, mae: 0.701943, mean_q: 5.247025
 69373/100000: episode: 1392, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 107.535, mean reward: 3.585 [2.237, 5.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.035, 10.526], loss: 0.958658, mae: 0.670941, mean_q: 5.242463
 69398/100000: episode: 1393, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 79.577, mean reward: 3.183 [2.028, 6.689], mean action: 0.000 [0.000, 0.000], mean observation: 2.124 [-0.698, 10.294], loss: 0.886589, mae: 0.635961, mean_q: 5.284914
 69422/100000: episode: 1394, duration: 0.124s, episode steps: 24, steps per second: 193, episode reward: 113.268, mean reward: 4.719 [2.598, 11.935], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.227, 10.100], loss: 0.928428, mae: 0.708741, mean_q: 5.329943
 69439/100000: episode: 1395, duration: 0.093s, episode steps: 17, steps per second: 183, episode reward: 106.443, mean reward: 6.261 [3.278, 10.844], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.458, 10.100], loss: 0.809582, mae: 0.642606, mean_q: 5.279979
 69463/100000: episode: 1396, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 80.237, mean reward: 3.343 [2.304, 4.606], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.466, 10.100], loss: 0.759869, mae: 0.725448, mean_q: 5.309167
 69487/100000: episode: 1397, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 140.789, mean reward: 5.866 [2.838, 12.772], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.712, 10.100], loss: 2.879153, mae: 0.816489, mean_q: 5.592714
 69499/100000: episode: 1398, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 31.559, mean reward: 2.630 [2.174, 3.647], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.936, 10.394], loss: 0.408784, mae: 0.557028, mean_q: 4.989707
 69529/100000: episode: 1399, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 75.997, mean reward: 2.533 [1.690, 4.440], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-1.054, 10.267], loss: 0.639617, mae: 0.697237, mean_q: 5.411967
 69556/100000: episode: 1400, duration: 0.136s, episode steps: 27, steps per second: 199, episode reward: 144.763, mean reward: 5.362 [3.427, 15.567], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.380, 10.530], loss: 1.576979, mae: 0.830740, mean_q: 5.538952
 69568/100000: episode: 1401, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 34.870, mean reward: 2.906 [2.564, 3.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.258 [-0.035, 10.367], loss: 5.116965, mae: 0.992787, mean_q: 5.856667
 69585/100000: episode: 1402, duration: 0.087s, episode steps: 17, steps per second: 196, episode reward: 57.229, mean reward: 3.366 [2.619, 3.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.096 [-0.510, 10.100], loss: 0.856020, mae: 0.715474, mean_q: 5.508669
 69615/100000: episode: 1403, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 87.287, mean reward: 2.910 [1.726, 4.859], mean action: 0.000 [0.000, 0.000], mean observation: 2.092 [-0.142, 10.248], loss: 0.687802, mae: 0.674030, mean_q: 5.432810
 69643/100000: episode: 1404, duration: 0.134s, episode steps: 28, steps per second: 209, episode reward: 103.744, mean reward: 3.705 [2.832, 10.650], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.624, 10.450], loss: 1.085372, mae: 0.735464, mean_q: 5.521672
 69667/100000: episode: 1405, duration: 0.127s, episode steps: 24, steps per second: 189, episode reward: 81.913, mean reward: 3.413 [2.760, 4.617], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.427, 10.100], loss: 0.906244, mae: 0.668062, mean_q: 5.430395
 69695/100000: episode: 1406, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 71.600, mean reward: 2.557 [1.892, 4.008], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.229, 10.309], loss: 0.931536, mae: 0.699476, mean_q: 5.556675
 69720/100000: episode: 1407, duration: 0.125s, episode steps: 25, steps per second: 199, episode reward: 115.732, mean reward: 4.629 [3.111, 5.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.035, 10.560], loss: 0.700755, mae: 0.709434, mean_q: 5.531637
 69747/100000: episode: 1408, duration: 0.135s, episode steps: 27, steps per second: 199, episode reward: 71.983, mean reward: 2.666 [1.987, 3.826], mean action: 0.000 [0.000, 0.000], mean observation: 2.116 [-0.173, 10.392], loss: 1.146243, mae: 0.742731, mean_q: 5.523006
 69771/100000: episode: 1409, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 116.274, mean reward: 4.845 [3.163, 8.930], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.211, 10.100], loss: 1.669997, mae: 0.774877, mean_q: 5.638111
 69801/100000: episode: 1410, duration: 0.153s, episode steps: 30, steps per second: 197, episode reward: 78.877, mean reward: 2.629 [2.207, 4.046], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.113, 10.363], loss: 0.703605, mae: 0.747802, mean_q: 5.638740
 69829/100000: episode: 1411, duration: 0.145s, episode steps: 28, steps per second: 193, episode reward: 77.248, mean reward: 2.759 [1.576, 4.327], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.035, 10.180], loss: 0.686657, mae: 0.690298, mean_q: 5.610945
 69846/100000: episode: 1412, duration: 0.110s, episode steps: 17, steps per second: 155, episode reward: 60.504, mean reward: 3.559 [2.900, 5.053], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.463, 10.100], loss: 2.534150, mae: 0.816244, mean_q: 5.593103
 69873/100000: episode: 1413, duration: 0.139s, episode steps: 27, steps per second: 194, episode reward: 135.545, mean reward: 5.020 [2.629, 13.187], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.579, 10.450], loss: 0.808238, mae: 0.726488, mean_q: 5.555105
 69900/100000: episode: 1414, duration: 0.130s, episode steps: 27, steps per second: 208, episode reward: 109.506, mean reward: 4.056 [1.965, 13.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.705, 10.272], loss: 1.301335, mae: 0.857127, mean_q: 5.790668
[Info] FALSIFICATION!
 69911/100000: episode: 1415, duration: 0.309s, episode steps: 11, steps per second: 36, episode reward: 1054.580, mean reward: 95.871 [3.516, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.957 [-0.273, 9.695], loss: 2.432113, mae: 0.900314, mean_q: 5.911206
 69938/100000: episode: 1416, duration: 0.140s, episode steps: 27, steps per second: 193, episode reward: 124.882, mean reward: 4.625 [3.166, 9.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.455, 10.545], loss: 1.291512, mae: 0.739717, mean_q: 5.595640
 69968/100000: episode: 1417, duration: 0.157s, episode steps: 30, steps per second: 192, episode reward: 84.140, mean reward: 2.805 [1.881, 3.924], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.233, 10.439], loss: 2.563536, mae: 0.832367, mean_q: 5.852396
 69992/100000: episode: 1418, duration: 0.128s, episode steps: 24, steps per second: 188, episode reward: 82.558, mean reward: 3.440 [2.751, 5.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.946, 10.100], loss: 1.338117, mae: 0.745718, mean_q: 5.832270
 70016/100000: episode: 1419, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 111.102, mean reward: 4.629 [2.920, 10.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.054 [-0.558, 10.100], loss: 0.762775, mae: 0.732920, mean_q: 5.784377
 70043/100000: episode: 1420, duration: 0.141s, episode steps: 27, steps per second: 191, episode reward: 118.263, mean reward: 4.380 [2.622, 7.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.035, 10.419], loss: 1.440743, mae: 0.785280, mean_q: 5.870905
[Info] FALSIFICATION!
 70049/100000: episode: 1421, duration: 0.196s, episode steps: 6, steps per second: 31, episode reward: 1026.228, mean reward: 171.038 [3.536, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.985 [-0.025, 9.679], loss: 0.426782, mae: 0.680012, mean_q: 5.542936
 70061/100000: episode: 1422, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 53.298, mean reward: 4.442 [2.903, 8.405], mean action: 0.000 [0.000, 0.000], mean observation: 2.250 [-0.480, 10.624], loss: 0.864089, mae: 0.765382, mean_q: 5.807101
 70086/100000: episode: 1423, duration: 0.144s, episode steps: 25, steps per second: 173, episode reward: 91.771, mean reward: 3.671 [3.022, 4.745], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.082, 10.525], loss: 1.505852, mae: 0.841098, mean_q: 5.858478
 70098/100000: episode: 1424, duration: 0.063s, episode steps: 12, steps per second: 189, episode reward: 43.239, mean reward: 3.603 [3.162, 3.972], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.035, 10.477], loss: 2.707969, mae: 0.910938, mean_q: 5.772261
 70115/100000: episode: 1425, duration: 0.096s, episode steps: 17, steps per second: 177, episode reward: 88.876, mean reward: 5.228 [3.279, 7.958], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.521, 10.100], loss: 0.707342, mae: 0.823966, mean_q: 5.934559
 70132/100000: episode: 1426, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 88.003, mean reward: 5.177 [3.824, 6.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.524, 10.100], loss: 1.545625, mae: 0.800891, mean_q: 5.858462
 70162/100000: episode: 1427, duration: 0.159s, episode steps: 30, steps per second: 188, episode reward: 73.135, mean reward: 2.438 [1.617, 3.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.035, 10.283], loss: 517.243469, mae: 3.330900, mean_q: 6.751403
 70189/100000: episode: 1428, duration: 0.138s, episode steps: 27, steps per second: 195, episode reward: 81.429, mean reward: 3.016 [2.495, 3.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.611, 10.339], loss: 1.538387, mae: 1.271933, mean_q: 6.070743
 70214/100000: episode: 1429, duration: 0.130s, episode steps: 25, steps per second: 192, episode reward: 119.137, mean reward: 4.765 [2.990, 13.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.830, 10.433], loss: 0.746722, mae: 0.890728, mean_q: 6.026903
 70244/100000: episode: 1430, duration: 0.167s, episode steps: 30, steps per second: 180, episode reward: 86.247, mean reward: 2.875 [2.233, 3.853], mean action: 0.000 [0.000, 0.000], mean observation: 2.095 [-1.158, 10.361], loss: 2.069835, mae: 0.918624, mean_q: 5.828650
 70269/100000: episode: 1431, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 63.829, mean reward: 2.553 [1.800, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.099, 10.257], loss: 5.312756, mae: 0.972521, mean_q: 6.035771
 70294/100000: episode: 1432, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 113.940, mean reward: 4.558 [2.381, 7.748], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.654, 10.499], loss: 2.720180, mae: 0.852296, mean_q: 5.878538
 70324/100000: episode: 1433, duration: 0.156s, episode steps: 30, steps per second: 192, episode reward: 130.238, mean reward: 4.341 [2.627, 7.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.960, 10.480], loss: 0.833263, mae: 0.806663, mean_q: 6.087494
 70348/100000: episode: 1434, duration: 0.130s, episode steps: 24, steps per second: 185, episode reward: 108.742, mean reward: 4.531 [2.495, 13.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.492, 10.100], loss: 0.739908, mae: 0.775083, mean_q: 5.954702
 70365/100000: episode: 1435, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 133.238, mean reward: 7.838 [4.768, 19.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-1.064, 10.100], loss: 2.160170, mae: 0.867642, mean_q: 6.206276
 70392/100000: episode: 1436, duration: 0.133s, episode steps: 27, steps per second: 203, episode reward: 137.645, mean reward: 5.098 [2.189, 23.822], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-0.389, 10.399], loss: 1.352798, mae: 0.862828, mean_q: 5.974441
 70416/100000: episode: 1437, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 76.758, mean reward: 3.198 [2.149, 3.971], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.665, 10.100], loss: 2.920895, mae: 0.866066, mean_q: 6.009096
 70440/100000: episode: 1438, duration: 0.126s, episode steps: 24, steps per second: 190, episode reward: 94.770, mean reward: 3.949 [3.057, 6.055], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.351, 10.100], loss: 1.463828, mae: 0.851163, mean_q: 6.052834
 70464/100000: episode: 1439, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 94.159, mean reward: 3.923 [2.954, 4.925], mean action: 0.000 [0.000, 0.000], mean observation: 2.051 [-0.427, 10.100], loss: 0.716557, mae: 0.794742, mean_q: 6.169177
[Info] Complete ISplit Iteration
[Info] Levels: [5.0629325, 7.1835628, 10.880886]
[Info] Cond. Prob: [0.1, 0.1, 0.08]
[Info] Error Prob: 0.0008000000000000001

 70488/100000: episode: 1440, duration: 4.484s, episode steps: 24, steps per second: 5, episode reward: 82.735, mean reward: 3.447 [2.415, 6.541], mean action: 0.000 [0.000, 0.000], mean observation: 2.064 [-0.960, 10.100], loss: 0.810423, mae: 0.777040, mean_q: 6.171886
 70588/100000: episode: 1441, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 187.229, mean reward: 1.872 [1.444, 3.074], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.805, 10.108], loss: 307.851227, mae: 2.135333, mean_q: 6.803419
 70688/100000: episode: 1442, duration: 0.522s, episode steps: 100, steps per second: 191, episode reward: 192.933, mean reward: 1.929 [1.472, 4.755], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.842, 10.098], loss: 154.160599, mae: 1.520439, mean_q: 6.645362
 70788/100000: episode: 1443, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 196.704, mean reward: 1.967 [1.507, 3.086], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.704, 10.259], loss: 462.203217, mae: 2.608928, mean_q: 7.123388
 70888/100000: episode: 1444, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 209.596, mean reward: 2.096 [1.450, 3.582], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.636, 10.489], loss: 154.729446, mae: 1.776266, mean_q: 6.891622
 70988/100000: episode: 1445, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 207.680, mean reward: 2.077 [1.456, 3.638], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.157, 10.291], loss: 307.749786, mae: 2.259868, mean_q: 7.104763
 71088/100000: episode: 1446, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 226.108, mean reward: 2.261 [1.519, 3.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.652, 10.343], loss: 153.413315, mae: 1.485346, mean_q: 6.820068
 71188/100000: episode: 1447, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 211.602, mean reward: 2.116 [1.448, 3.691], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.054, 10.098], loss: 459.024597, mae: 2.650334, mean_q: 7.364859
 71288/100000: episode: 1448, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 187.609, mean reward: 1.876 [1.435, 2.878], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.343, 10.108], loss: 306.507019, mae: 2.065441, mean_q: 7.185709
 71388/100000: episode: 1449, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 200.751, mean reward: 2.008 [1.492, 3.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.010, 10.181], loss: 457.753540, mae: 2.091468, mean_q: 6.781268
 71488/100000: episode: 1450, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: 183.690, mean reward: 1.837 [1.450, 3.040], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.626, 10.105], loss: 3.291690, mae: 1.501170, mean_q: 7.129159
 71588/100000: episode: 1451, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 193.796, mean reward: 1.938 [1.440, 3.501], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.849, 10.292], loss: 2.335064, mae: 0.967033, mean_q: 6.386928
 71688/100000: episode: 1452, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 195.178, mean reward: 1.952 [1.458, 3.491], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.219, 10.098], loss: 459.743408, mae: 2.416122, mean_q: 6.846711
 71788/100000: episode: 1453, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 179.706, mean reward: 1.797 [1.453, 2.784], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.316, 10.143], loss: 306.722351, mae: 2.267885, mean_q: 7.097361
 71888/100000: episode: 1454, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 203.515, mean reward: 2.035 [1.448, 5.570], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.367, 10.336], loss: 153.686417, mae: 1.448504, mean_q: 6.548063
 71988/100000: episode: 1455, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 205.830, mean reward: 2.058 [1.474, 4.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.966, 10.098], loss: 455.118256, mae: 2.205717, mean_q: 6.872216
 72088/100000: episode: 1456, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 205.495, mean reward: 2.055 [1.449, 3.477], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.653, 10.098], loss: 2.157745, mae: 1.152885, mean_q: 6.529163
 72188/100000: episode: 1457, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 182.035, mean reward: 1.820 [1.472, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.071, 10.259], loss: 1.058132, mae: 0.820073, mean_q: 6.057049
 72288/100000: episode: 1458, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 178.668, mean reward: 1.787 [1.463, 2.745], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.861, 10.115], loss: 306.280884, mae: 1.533571, mean_q: 6.159385
 72388/100000: episode: 1459, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 185.238, mean reward: 1.852 [1.463, 3.552], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.983, 10.126], loss: 2.889637, mae: 1.251264, mean_q: 6.382514
 72488/100000: episode: 1460, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 183.668, mean reward: 1.837 [1.459, 2.858], mean action: 0.000 [0.000, 0.000], mean observation: 1.442 [-1.149, 10.274], loss: 156.073441, mae: 1.486323, mean_q: 6.348975
 72588/100000: episode: 1461, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 186.455, mean reward: 1.865 [1.467, 3.733], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.796, 10.098], loss: 306.777069, mae: 1.915954, mean_q: 6.701431
 72688/100000: episode: 1462, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 189.182, mean reward: 1.892 [1.450, 3.535], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.392, 10.098], loss: 1.607465, mae: 0.929055, mean_q: 5.997643
 72788/100000: episode: 1463, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 200.329, mean reward: 2.003 [1.500, 3.245], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.996, 10.098], loss: 0.974025, mae: 0.752779, mean_q: 5.957882
 72888/100000: episode: 1464, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 192.031, mean reward: 1.920 [1.469, 3.026], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.493, 10.104], loss: 307.893372, mae: 1.967650, mean_q: 6.650164
 72988/100000: episode: 1465, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 181.542, mean reward: 1.815 [1.443, 3.531], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.589, 10.154], loss: 154.775940, mae: 1.368569, mean_q: 6.278085
 73088/100000: episode: 1466, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 214.045, mean reward: 2.140 [1.484, 4.615], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.371, 10.241], loss: 153.797668, mae: 1.311177, mean_q: 6.244873
 73188/100000: episode: 1467, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 210.993, mean reward: 2.110 [1.493, 9.373], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.655, 10.098], loss: 305.811523, mae: 1.900803, mean_q: 6.616525
 73288/100000: episode: 1468, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 263.631, mean reward: 2.636 [1.445, 7.381], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.882, 10.601], loss: 153.949203, mae: 1.480538, mean_q: 6.189692
 73388/100000: episode: 1469, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 184.814, mean reward: 1.848 [1.441, 3.356], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.288, 10.122], loss: 153.519104, mae: 1.312603, mean_q: 6.043736
 73488/100000: episode: 1470, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 189.442, mean reward: 1.894 [1.479, 3.635], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.212, 10.196], loss: 306.215637, mae: 1.958034, mean_q: 6.445074
 73588/100000: episode: 1471, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 191.001, mean reward: 1.910 [1.459, 3.025], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.611, 10.120], loss: 152.952316, mae: 1.231796, mean_q: 5.852579
 73688/100000: episode: 1472, duration: 0.544s, episode steps: 100, steps per second: 184, episode reward: 182.190, mean reward: 1.822 [1.439, 3.058], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.031, 10.100], loss: 306.756165, mae: 1.930630, mean_q: 6.371541
 73788/100000: episode: 1473, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 191.572, mean reward: 1.916 [1.471, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.888, 10.204], loss: 607.915222, mae: 2.671021, mean_q: 6.591966
 73888/100000: episode: 1474, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: 189.571, mean reward: 1.896 [1.493, 3.096], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.605, 10.098], loss: 2.344635, mae: 1.243066, mean_q: 5.965238
 73988/100000: episode: 1475, duration: 0.484s, episode steps: 100, steps per second: 206, episode reward: 219.140, mean reward: 2.191 [1.477, 4.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.412 [-0.635, 10.098], loss: 153.185364, mae: 1.331078, mean_q: 5.710850
 74088/100000: episode: 1476, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 182.879, mean reward: 1.829 [1.437, 2.982], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.811, 10.098], loss: 0.835853, mae: 0.707493, mean_q: 5.211996
 74188/100000: episode: 1477, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 203.884, mean reward: 2.039 [1.435, 3.363], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.700, 10.098], loss: 152.946518, mae: 1.212980, mean_q: 5.367652
 74288/100000: episode: 1478, duration: 0.495s, episode steps: 100, steps per second: 202, episode reward: 179.585, mean reward: 1.796 [1.466, 3.008], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.490, 10.101], loss: 305.331757, mae: 1.714865, mean_q: 5.655349
 74388/100000: episode: 1479, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 178.189, mean reward: 1.782 [1.463, 2.697], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.676, 10.132], loss: 455.649841, mae: 2.019150, mean_q: 5.606428
 74488/100000: episode: 1480, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 187.215, mean reward: 1.872 [1.514, 4.083], mean action: 0.000 [0.000, 0.000], mean observation: 1.418 [-0.826, 10.098], loss: 151.923813, mae: 1.588288, mean_q: 5.515506
 74588/100000: episode: 1481, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 194.319, mean reward: 1.943 [1.512, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.692, 10.152], loss: 0.550584, mae: 0.647072, mean_q: 4.731146
 74688/100000: episode: 1482, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 210.887, mean reward: 2.109 [1.471, 3.186], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.988, 10.450], loss: 0.517207, mae: 0.591196, mean_q: 4.631297
 74788/100000: episode: 1483, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 196.577, mean reward: 1.966 [1.458, 3.097], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.257, 10.098], loss: 0.417663, mae: 0.545284, mean_q: 4.529444
 74888/100000: episode: 1484, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 183.383, mean reward: 1.834 [1.490, 2.574], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.108, 10.279], loss: 152.533539, mae: 1.093598, mean_q: 4.697821
 74988/100000: episode: 1485, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 187.480, mean reward: 1.875 [1.466, 3.131], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.843, 10.263], loss: 0.398894, mae: 0.504443, mean_q: 4.335414
 75088/100000: episode: 1486, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 181.596, mean reward: 1.816 [1.471, 4.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.683, 10.139], loss: 0.536961, mae: 0.490542, mean_q: 4.236183
 75188/100000: episode: 1487, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 179.096, mean reward: 1.791 [1.459, 2.463], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.254, 10.149], loss: 0.378299, mae: 0.429174, mean_q: 4.144393
 75288/100000: episode: 1488, duration: 0.525s, episode steps: 100, steps per second: 190, episode reward: 183.684, mean reward: 1.837 [1.464, 2.786], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.522, 10.098], loss: 0.217723, mae: 0.385482, mean_q: 4.010749
 75388/100000: episode: 1489, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 179.995, mean reward: 1.800 [1.443, 2.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.588, 10.098], loss: 0.124029, mae: 0.350332, mean_q: 3.944021
 75488/100000: episode: 1490, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 215.680, mean reward: 2.157 [1.459, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.617, 10.273], loss: 0.133773, mae: 0.357755, mean_q: 3.869623
 75588/100000: episode: 1491, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 195.339, mean reward: 1.953 [1.452, 4.236], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-1.105, 10.211], loss: 0.131015, mae: 0.349940, mean_q: 3.862138
 75688/100000: episode: 1492, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 187.475, mean reward: 1.875 [1.479, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.570, 10.098], loss: 0.125830, mae: 0.342096, mean_q: 3.845279
 75788/100000: episode: 1493, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 202.372, mean reward: 2.024 [1.507, 3.274], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.534, 10.098], loss: 0.125898, mae: 0.342860, mean_q: 3.858518
 75888/100000: episode: 1494, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 202.943, mean reward: 2.029 [1.442, 3.483], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.719, 10.123], loss: 0.112318, mae: 0.335251, mean_q: 3.864406
 75988/100000: episode: 1495, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 196.828, mean reward: 1.968 [1.476, 3.543], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.585, 10.098], loss: 0.103153, mae: 0.320130, mean_q: 3.857805
 76088/100000: episode: 1496, duration: 0.496s, episode steps: 100, steps per second: 201, episode reward: 199.560, mean reward: 1.996 [1.460, 6.238], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.264, 10.100], loss: 0.101424, mae: 0.321486, mean_q: 3.842450
 76188/100000: episode: 1497, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 211.224, mean reward: 2.112 [1.511, 3.500], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.718, 10.429], loss: 0.099703, mae: 0.315325, mean_q: 3.829052
 76288/100000: episode: 1498, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 196.705, mean reward: 1.967 [1.493, 3.542], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.755, 10.098], loss: 0.128957, mae: 0.338930, mean_q: 3.839396
 76388/100000: episode: 1499, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 189.480, mean reward: 1.895 [1.484, 4.055], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.320, 10.121], loss: 0.104078, mae: 0.307176, mean_q: 3.852526
 76488/100000: episode: 1500, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 215.995, mean reward: 2.160 [1.496, 2.943], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.590, 10.325], loss: 0.104420, mae: 0.319676, mean_q: 3.853500
 76588/100000: episode: 1501, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 198.413, mean reward: 1.984 [1.489, 5.665], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.748, 10.149], loss: 0.119303, mae: 0.320424, mean_q: 3.853224
 76688/100000: episode: 1502, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 203.086, mean reward: 2.031 [1.461, 3.402], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-0.680, 10.276], loss: 0.113040, mae: 0.319932, mean_q: 3.856261
 76788/100000: episode: 1503, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 202.330, mean reward: 2.023 [1.500, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.055, 10.164], loss: 0.110394, mae: 0.322675, mean_q: 3.877894
 76888/100000: episode: 1504, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 200.904, mean reward: 2.009 [1.466, 3.885], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.718, 10.234], loss: 0.101486, mae: 0.313784, mean_q: 3.864902
 76988/100000: episode: 1505, duration: 0.496s, episode steps: 100, steps per second: 202, episode reward: 197.193, mean reward: 1.972 [1.486, 3.544], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.565, 10.122], loss: 0.094214, mae: 0.310773, mean_q: 3.855807
 77088/100000: episode: 1506, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 198.970, mean reward: 1.990 [1.471, 3.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.867, 10.401], loss: 0.085825, mae: 0.298252, mean_q: 3.851523
 77188/100000: episode: 1507, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 197.569, mean reward: 1.976 [1.464, 3.453], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-1.042, 10.098], loss: 0.108305, mae: 0.315944, mean_q: 3.883147
 77288/100000: episode: 1508, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 181.792, mean reward: 1.818 [1.451, 2.509], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.606, 10.110], loss: 0.109576, mae: 0.318294, mean_q: 3.886636
 77388/100000: episode: 1509, duration: 0.498s, episode steps: 100, steps per second: 201, episode reward: 183.384, mean reward: 1.834 [1.485, 2.541], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.918, 10.338], loss: 0.121647, mae: 0.318732, mean_q: 3.895157
 77488/100000: episode: 1510, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 193.131, mean reward: 1.931 [1.469, 2.968], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.794, 10.098], loss: 0.100052, mae: 0.310162, mean_q: 3.878605
 77588/100000: episode: 1511, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 179.827, mean reward: 1.798 [1.491, 2.651], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.464, 10.239], loss: 0.102208, mae: 0.317895, mean_q: 3.889118
 77688/100000: episode: 1512, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 199.788, mean reward: 1.998 [1.496, 3.771], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.245, 10.137], loss: 0.112708, mae: 0.311385, mean_q: 3.888741
 77788/100000: episode: 1513, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 187.405, mean reward: 1.874 [1.455, 2.774], mean action: 0.000 [0.000, 0.000], mean observation: 1.440 [-0.736, 10.174], loss: 0.091460, mae: 0.305498, mean_q: 3.866971
 77888/100000: episode: 1514, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 234.063, mean reward: 2.341 [1.490, 4.626], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.370, 10.506], loss: 0.081535, mae: 0.292364, mean_q: 3.876635
 77988/100000: episode: 1515, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 191.125, mean reward: 1.911 [1.462, 3.151], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.800, 10.098], loss: 0.099833, mae: 0.312239, mean_q: 3.907848
 78088/100000: episode: 1516, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 192.468, mean reward: 1.925 [1.477, 4.331], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.342, 10.323], loss: 0.090927, mae: 0.301595, mean_q: 3.886627
 78188/100000: episode: 1517, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 201.178, mean reward: 2.012 [1.462, 5.632], mean action: 0.000 [0.000, 0.000], mean observation: 1.445 [-0.360, 10.251], loss: 0.097697, mae: 0.312805, mean_q: 3.902480
 78288/100000: episode: 1518, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 204.710, mean reward: 2.047 [1.504, 3.711], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.297, 10.351], loss: 0.081348, mae: 0.284785, mean_q: 3.859953
 78388/100000: episode: 1519, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 179.538, mean reward: 1.795 [1.438, 3.080], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.380, 10.121], loss: 0.082629, mae: 0.282711, mean_q: 3.857751
 78488/100000: episode: 1520, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 212.343, mean reward: 2.123 [1.455, 3.390], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.786, 10.098], loss: 0.084592, mae: 0.285920, mean_q: 3.861062
 78588/100000: episode: 1521, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 191.783, mean reward: 1.918 [1.449, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.914, 10.098], loss: 0.085350, mae: 0.286188, mean_q: 3.861063
 78688/100000: episode: 1522, duration: 0.525s, episode steps: 100, steps per second: 191, episode reward: 209.460, mean reward: 2.095 [1.464, 5.787], mean action: 0.000 [0.000, 0.000], mean observation: 1.446 [-0.720, 10.506], loss: 0.092235, mae: 0.297962, mean_q: 3.872649
 78788/100000: episode: 1523, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 186.061, mean reward: 1.861 [1.480, 3.288], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.660, 10.242], loss: 0.081629, mae: 0.287125, mean_q: 3.855348
 78888/100000: episode: 1524, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 230.429, mean reward: 2.304 [1.500, 5.113], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.217, 10.098], loss: 0.092328, mae: 0.298389, mean_q: 3.888446
 78988/100000: episode: 1525, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 197.600, mean reward: 1.976 [1.452, 3.260], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.483, 10.105], loss: 0.089145, mae: 0.293353, mean_q: 3.885523
 79088/100000: episode: 1526, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: 187.965, mean reward: 1.880 [1.447, 2.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.157, 10.098], loss: 0.080296, mae: 0.282663, mean_q: 3.859582
 79188/100000: episode: 1527, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 199.111, mean reward: 1.991 [1.468, 3.472], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.535, 10.307], loss: 0.093330, mae: 0.297759, mean_q: 3.877191
 79288/100000: episode: 1528, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 192.319, mean reward: 1.923 [1.454, 3.605], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.088, 10.098], loss: 0.086397, mae: 0.286498, mean_q: 3.861449
 79388/100000: episode: 1529, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 192.256, mean reward: 1.923 [1.447, 3.271], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.061, 10.303], loss: 0.077317, mae: 0.287126, mean_q: 3.888509
 79488/100000: episode: 1530, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 261.827, mean reward: 2.618 [1.475, 12.422], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-0.391, 10.098], loss: 0.084911, mae: 0.293629, mean_q: 3.883795
 79588/100000: episode: 1531, duration: 0.555s, episode steps: 100, steps per second: 180, episode reward: 199.440, mean reward: 1.994 [1.518, 4.726], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.873, 10.098], loss: 0.110835, mae: 0.303006, mean_q: 3.915234
 79688/100000: episode: 1532, duration: 0.529s, episode steps: 100, steps per second: 189, episode reward: 192.337, mean reward: 1.923 [1.443, 3.337], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.755, 10.098], loss: 0.093511, mae: 0.300055, mean_q: 3.900994
 79788/100000: episode: 1533, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 197.585, mean reward: 1.976 [1.452, 3.805], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-1.649, 10.158], loss: 0.096283, mae: 0.296176, mean_q: 3.895373
 79888/100000: episode: 1534, duration: 0.482s, episode steps: 100, steps per second: 207, episode reward: 211.170, mean reward: 2.112 [1.458, 3.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.885, 10.275], loss: 0.092420, mae: 0.299903, mean_q: 3.919921
 79988/100000: episode: 1535, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 177.922, mean reward: 1.779 [1.435, 3.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.460, 10.098], loss: 0.114296, mae: 0.314708, mean_q: 3.914880
 80088/100000: episode: 1536, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 196.847, mean reward: 1.968 [1.476, 2.661], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-1.106, 10.108], loss: 0.097011, mae: 0.302550, mean_q: 3.940881
 80188/100000: episode: 1537, duration: 0.491s, episode steps: 100, steps per second: 204, episode reward: 187.011, mean reward: 1.870 [1.455, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.131, 10.098], loss: 0.094660, mae: 0.300495, mean_q: 3.933497
 80288/100000: episode: 1538, duration: 0.517s, episode steps: 100, steps per second: 193, episode reward: 189.009, mean reward: 1.890 [1.443, 3.130], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.045, 10.221], loss: 0.093423, mae: 0.299301, mean_q: 3.912813
 80388/100000: episode: 1539, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 200.097, mean reward: 2.001 [1.452, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.419 [-0.733, 10.319], loss: 0.113643, mae: 0.314938, mean_q: 3.945113
[Info] 1-TH LEVEL FOUND: 5.713003158569336, Considering 10/90 traces
 80488/100000: episode: 1540, duration: 4.590s, episode steps: 100, steps per second: 22, episode reward: 212.406, mean reward: 2.124 [1.546, 3.647], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.885, 10.098], loss: 0.086168, mae: 0.295718, mean_q: 3.904078
 80524/100000: episode: 1541, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 120.239, mean reward: 3.340 [2.405, 4.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.136, 10.390], loss: 0.099413, mae: 0.319401, mean_q: 3.955022
 80536/100000: episode: 1542, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 26.148, mean reward: 2.179 [1.736, 3.014], mean action: 0.000 [0.000, 0.000], mean observation: 2.187 [-0.221, 10.100], loss: 0.073415, mae: 0.298040, mean_q: 3.998190
 80548/100000: episode: 1543, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 40.095, mean reward: 3.341 [2.301, 4.239], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.296, 10.100], loss: 0.097127, mae: 0.303390, mean_q: 3.925357
 80581/100000: episode: 1544, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 81.318, mean reward: 2.464 [1.560, 3.929], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.889, 10.217], loss: 0.097835, mae: 0.315764, mean_q: 3.948282
 80617/100000: episode: 1545, duration: 0.195s, episode steps: 36, steps per second: 184, episode reward: 102.529, mean reward: 2.848 [1.846, 5.656], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.800, 10.305], loss: 0.086659, mae: 0.292007, mean_q: 3.974991
 80639/100000: episode: 1546, duration: 0.106s, episode steps: 22, steps per second: 207, episode reward: 108.121, mean reward: 4.915 [2.840, 8.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.668, 10.100], loss: 0.103183, mae: 0.326184, mean_q: 4.023424
 80647/100000: episode: 1547, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 22.452, mean reward: 2.806 [2.315, 3.481], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.441], loss: 0.103315, mae: 0.288383, mean_q: 3.958198
 80669/100000: episode: 1548, duration: 0.113s, episode steps: 22, steps per second: 195, episode reward: 74.352, mean reward: 3.380 [1.822, 5.059], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.035, 10.100], loss: 0.118961, mae: 0.316445, mean_q: 4.006384
 80691/100000: episode: 1549, duration: 0.114s, episode steps: 22, steps per second: 193, episode reward: 77.495, mean reward: 3.523 [2.633, 5.072], mean action: 0.000 [0.000, 0.000], mean observation: 2.113 [-0.252, 10.100], loss: 0.147848, mae: 0.340687, mean_q: 4.030252
 80727/100000: episode: 1550, duration: 0.186s, episode steps: 36, steps per second: 194, episode reward: 99.742, mean reward: 2.771 [1.707, 4.238], mean action: 0.000 [0.000, 0.000], mean observation: 2.031 [-0.710, 10.250], loss: 0.146421, mae: 0.353883, mean_q: 4.090582
 80758/100000: episode: 1551, duration: 0.172s, episode steps: 31, steps per second: 181, episode reward: 64.994, mean reward: 2.097 [1.577, 2.799], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.171, 10.233], loss: 0.089296, mae: 0.297154, mean_q: 4.050529
 80774/100000: episode: 1552, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 71.082, mean reward: 4.443 [2.606, 8.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.486, 10.100], loss: 0.152916, mae: 0.320180, mean_q: 4.041391
 80782/100000: episode: 1553, duration: 0.050s, episode steps: 8, steps per second: 158, episode reward: 22.750, mean reward: 2.844 [2.258, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.450], loss: 0.116051, mae: 0.340713, mean_q: 4.098673
 80818/100000: episode: 1554, duration: 0.190s, episode steps: 36, steps per second: 189, episode reward: 101.682, mean reward: 2.824 [1.826, 5.577], mean action: 0.000 [0.000, 0.000], mean observation: 2.039 [-1.906, 10.282], loss: 0.111647, mae: 0.306030, mean_q: 4.049481
 80834/100000: episode: 1555, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 57.136, mean reward: 3.571 [2.645, 5.128], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.407, 10.100], loss: 0.136890, mae: 0.334595, mean_q: 4.128378
 80846/100000: episode: 1556, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 33.540, mean reward: 2.795 [2.470, 3.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.161 [-0.341, 10.100], loss: 0.226936, mae: 0.393969, mean_q: 4.125361
 80877/100000: episode: 1557, duration: 0.157s, episode steps: 31, steps per second: 197, episode reward: 64.858, mean reward: 2.092 [1.489, 2.984], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.375, 10.179], loss: 0.126289, mae: 0.337289, mean_q: 4.091201
 80900/100000: episode: 1558, duration: 0.110s, episode steps: 23, steps per second: 209, episode reward: 76.623, mean reward: 3.331 [2.233, 4.303], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.114, 10.420], loss: 0.116770, mae: 0.325473, mean_q: 4.070850
 80918/100000: episode: 1559, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 55.055, mean reward: 3.059 [2.459, 4.674], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.398, 10.100], loss: 0.123463, mae: 0.324613, mean_q: 4.130739
 80930/100000: episode: 1560, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 27.718, mean reward: 2.310 [1.611, 4.599], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.035, 10.100], loss: 0.132202, mae: 0.356117, mean_q: 4.093112
 80953/100000: episode: 1561, duration: 0.121s, episode steps: 23, steps per second: 191, episode reward: 65.613, mean reward: 2.853 [2.094, 4.781], mean action: 0.000 [0.000, 0.000], mean observation: 2.167 [-0.035, 10.545], loss: 0.156023, mae: 0.344516, mean_q: 4.126935
 80975/100000: episode: 1562, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 138.324, mean reward: 6.287 [3.440, 14.176], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.875, 10.100], loss: 0.131285, mae: 0.355972, mean_q: 4.078441
 80987/100000: episode: 1563, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 41.907, mean reward: 3.492 [2.691, 4.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.655, 10.100], loss: 0.095767, mae: 0.319953, mean_q: 4.174270
 81009/100000: episode: 1564, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 97.306, mean reward: 4.423 [2.854, 6.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.107 [-1.352, 10.100], loss: 0.125949, mae: 0.329102, mean_q: 4.088112
 81038/100000: episode: 1565, duration: 0.145s, episode steps: 29, steps per second: 201, episode reward: 82.897, mean reward: 2.859 [2.377, 3.759], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.042, 10.435], loss: 0.128357, mae: 0.339328, mean_q: 4.231596
 81067/100000: episode: 1566, duration: 0.153s, episode steps: 29, steps per second: 189, episode reward: 82.023, mean reward: 2.828 [1.957, 4.596], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.344, 10.364], loss: 0.104851, mae: 0.320293, mean_q: 4.185560
 81098/100000: episode: 1567, duration: 0.150s, episode steps: 31, steps per second: 206, episode reward: 62.303, mean reward: 2.010 [1.615, 2.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.051, 10.184], loss: 0.206560, mae: 0.374857, mean_q: 4.248716
 81131/100000: episode: 1568, duration: 0.180s, episode steps: 33, steps per second: 183, episode reward: 109.032, mean reward: 3.304 [2.121, 4.670], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.358, 10.390], loss: 0.114500, mae: 0.335837, mean_q: 4.168311
 81153/100000: episode: 1569, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 74.534, mean reward: 3.388 [2.654, 5.283], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.114, 10.100], loss: 0.094544, mae: 0.311477, mean_q: 4.089647
 81184/100000: episode: 1570, duration: 0.156s, episode steps: 31, steps per second: 198, episode reward: 70.104, mean reward: 2.261 [1.624, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 2.097 [-0.035, 10.486], loss: 0.125985, mae: 0.325451, mean_q: 4.204391
 81196/100000: episode: 1571, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 27.233, mean reward: 2.269 [1.945, 2.527], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.322, 10.100], loss: 0.154047, mae: 0.396646, mean_q: 4.273020
 81204/100000: episode: 1572, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 19.328, mean reward: 2.416 [1.954, 3.063], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.413], loss: 0.112108, mae: 0.336332, mean_q: 4.112552
 81226/100000: episode: 1573, duration: 0.122s, episode steps: 22, steps per second: 180, episode reward: 71.182, mean reward: 3.236 [2.511, 4.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.435, 10.100], loss: 0.114744, mae: 0.326714, mean_q: 4.135776
 81248/100000: episode: 1574, duration: 0.106s, episode steps: 22, steps per second: 208, episode reward: 97.547, mean reward: 4.434 [3.142, 11.206], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.638, 10.100], loss: 0.161703, mae: 0.371829, mean_q: 4.271110
 81281/100000: episode: 1575, duration: 0.180s, episode steps: 33, steps per second: 184, episode reward: 106.663, mean reward: 3.232 [1.831, 5.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.046 [-0.592, 10.333], loss: 0.120975, mae: 0.344256, mean_q: 4.191008
 81314/100000: episode: 1576, duration: 0.173s, episode steps: 33, steps per second: 190, episode reward: 97.757, mean reward: 2.962 [2.306, 4.360], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.040, 10.320], loss: 0.217324, mae: 0.398566, mean_q: 4.268609
 81336/100000: episode: 1577, duration: 0.123s, episode steps: 22, steps per second: 178, episode reward: 67.259, mean reward: 3.057 [2.096, 5.296], mean action: 0.000 [0.000, 0.000], mean observation: 2.086 [-0.510, 10.100], loss: 0.249577, mae: 0.410286, mean_q: 4.269816
 81354/100000: episode: 1578, duration: 0.091s, episode steps: 18, steps per second: 197, episode reward: 58.454, mean reward: 3.247 [2.279, 4.998], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-0.286, 10.100], loss: 0.289791, mae: 0.411291, mean_q: 4.342883
 81370/100000: episode: 1579, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 50.267, mean reward: 3.142 [2.872, 3.626], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.509, 10.100], loss: 0.270032, mae: 0.419994, mean_q: 4.405027
 81403/100000: episode: 1580, duration: 0.161s, episode steps: 33, steps per second: 204, episode reward: 79.955, mean reward: 2.423 [1.751, 3.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.549, 10.325], loss: 0.226496, mae: 0.412761, mean_q: 4.342290
 81426/100000: episode: 1581, duration: 0.130s, episode steps: 23, steps per second: 176, episode reward: 88.609, mean reward: 3.853 [2.269, 7.611], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.108, 10.347], loss: 0.201041, mae: 0.415288, mean_q: 4.301849
 81438/100000: episode: 1582, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 28.275, mean reward: 2.356 [2.000, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.197 [-0.430, 10.100], loss: 0.150173, mae: 0.374036, mean_q: 4.330993
 81454/100000: episode: 1583, duration: 0.085s, episode steps: 16, steps per second: 187, episode reward: 51.509, mean reward: 3.219 [2.700, 3.830], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.268, 10.100], loss: 0.205502, mae: 0.413112, mean_q: 4.384998
 81477/100000: episode: 1584, duration: 0.118s, episode steps: 23, steps per second: 194, episode reward: 64.666, mean reward: 2.812 [2.170, 4.379], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-0.035, 10.376], loss: 0.180537, mae: 0.399537, mean_q: 4.305806
 81500/100000: episode: 1585, duration: 0.136s, episode steps: 23, steps per second: 169, episode reward: 61.726, mean reward: 2.684 [2.189, 3.754], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.035, 10.341], loss: 0.145961, mae: 0.356308, mean_q: 4.342706
 81536/100000: episode: 1586, duration: 0.202s, episode steps: 36, steps per second: 178, episode reward: 90.704, mean reward: 2.520 [2.113, 3.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.230, 10.357], loss: 0.181178, mae: 0.382046, mean_q: 4.359635
 81552/100000: episode: 1587, duration: 0.095s, episode steps: 16, steps per second: 169, episode reward: 70.624, mean reward: 4.414 [3.386, 9.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.714, 10.100], loss: 0.168080, mae: 0.372606, mean_q: 4.413914
 81588/100000: episode: 1588, duration: 0.176s, episode steps: 36, steps per second: 204, episode reward: 90.679, mean reward: 2.519 [1.886, 5.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.469, 10.347], loss: 0.219223, mae: 0.402410, mean_q: 4.425820
 81624/100000: episode: 1589, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 80.184, mean reward: 2.227 [1.677, 2.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.037 [-0.193, 10.224], loss: 0.143872, mae: 0.372422, mean_q: 4.337520
 81646/100000: episode: 1590, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 86.807, mean reward: 3.946 [2.773, 4.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.343, 10.100], loss: 0.236237, mae: 0.394988, mean_q: 4.347412
 81654/100000: episode: 1591, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 25.091, mean reward: 3.136 [2.453, 3.615], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.035, 10.437], loss: 0.187838, mae: 0.405499, mean_q: 4.404578
 81683/100000: episode: 1592, duration: 0.146s, episode steps: 29, steps per second: 198, episode reward: 73.751, mean reward: 2.543 [1.990, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.102 [-0.035, 10.356], loss: 0.186584, mae: 0.428597, mean_q: 4.419847
 81701/100000: episode: 1593, duration: 0.091s, episode steps: 18, steps per second: 199, episode reward: 41.527, mean reward: 2.307 [1.893, 2.964], mean action: 0.000 [0.000, 0.000], mean observation: 2.140 [-0.244, 10.100], loss: 0.234019, mae: 0.422823, mean_q: 4.456963
 81737/100000: episode: 1594, duration: 0.182s, episode steps: 36, steps per second: 198, episode reward: 153.340, mean reward: 4.259 [3.343, 6.865], mean action: 0.000 [0.000, 0.000], mean observation: 2.042 [-0.503, 10.565], loss: 0.148217, mae: 0.368080, mean_q: 4.295538
 81759/100000: episode: 1595, duration: 0.105s, episode steps: 22, steps per second: 209, episode reward: 78.341, mean reward: 3.561 [2.808, 4.668], mean action: 0.000 [0.000, 0.000], mean observation: 2.094 [-0.347, 10.100], loss: 0.149788, mae: 0.371132, mean_q: 4.387392
 81790/100000: episode: 1596, duration: 0.154s, episode steps: 31, steps per second: 202, episode reward: 68.123, mean reward: 2.198 [1.506, 5.350], mean action: 0.000 [0.000, 0.000], mean observation: 2.055 [-1.136, 10.100], loss: 0.160769, mae: 0.373204, mean_q: 4.390948
 81802/100000: episode: 1597, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 36.470, mean reward: 3.039 [2.291, 4.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.343, 10.100], loss: 0.188062, mae: 0.417935, mean_q: 4.401858
 81814/100000: episode: 1598, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 28.866, mean reward: 2.406 [2.171, 2.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.407, 10.100], loss: 0.126832, mae: 0.354796, mean_q: 4.347524
 81837/100000: episode: 1599, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 90.104, mean reward: 3.918 [2.725, 6.181], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.035, 10.487], loss: 0.256714, mae: 0.386020, mean_q: 4.458190
 81849/100000: episode: 1600, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 28.481, mean reward: 2.373 [2.138, 2.661], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.235, 10.100], loss: 0.351019, mae: 0.467259, mean_q: 4.580593
 81885/100000: episode: 1601, duration: 0.174s, episode steps: 36, steps per second: 207, episode reward: 100.186, mean reward: 2.783 [1.814, 4.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-1.034, 10.466], loss: 0.143591, mae: 0.358518, mean_q: 4.384792
 81907/100000: episode: 1602, duration: 0.106s, episode steps: 22, steps per second: 208, episode reward: 106.388, mean reward: 4.836 [2.998, 8.568], mean action: 0.000 [0.000, 0.000], mean observation: 2.063 [-1.023, 10.100], loss: 0.212489, mae: 0.429529, mean_q: 4.389839
 81915/100000: episode: 1603, duration: 0.049s, episode steps: 8, steps per second: 162, episode reward: 24.429, mean reward: 3.054 [2.741, 3.442], mean action: 0.000 [0.000, 0.000], mean observation: 2.262 [-0.035, 10.464], loss: 0.152500, mae: 0.389756, mean_q: 4.463868
 81946/100000: episode: 1604, duration: 0.172s, episode steps: 31, steps per second: 181, episode reward: 64.784, mean reward: 2.090 [1.528, 2.985], mean action: 0.000 [0.000, 0.000], mean observation: 2.071 [-0.527, 10.203], loss: 0.177048, mae: 0.382108, mean_q: 4.545813
 81962/100000: episode: 1605, duration: 0.092s, episode steps: 16, steps per second: 174, episode reward: 52.287, mean reward: 3.268 [2.061, 7.260], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-1.036, 10.100], loss: 0.179760, mae: 0.387730, mean_q: 4.449234
 81995/100000: episode: 1606, duration: 0.171s, episode steps: 33, steps per second: 193, episode reward: 88.880, mean reward: 2.693 [1.965, 4.374], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.535, 10.317], loss: 0.163538, mae: 0.383906, mean_q: 4.554801
 82026/100000: episode: 1607, duration: 0.167s, episode steps: 31, steps per second: 186, episode reward: 61.897, mean reward: 1.997 [1.494, 2.544], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-0.479, 10.201], loss: 0.120877, mae: 0.351803, mean_q: 4.419670
 82055/100000: episode: 1608, duration: 0.142s, episode steps: 29, steps per second: 204, episode reward: 103.564, mean reward: 3.571 [2.193, 6.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.484, 10.453], loss: 0.187414, mae: 0.392920, mean_q: 4.510773
 82067/100000: episode: 1609, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 34.166, mean reward: 2.847 [2.309, 3.348], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.413, 10.100], loss: 0.165073, mae: 0.382404, mean_q: 4.288954
 82085/100000: episode: 1610, duration: 0.098s, episode steps: 18, steps per second: 184, episode reward: 44.437, mean reward: 2.469 [1.793, 3.751], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-1.436, 10.100], loss: 0.222114, mae: 0.447870, mean_q: 4.622400
 82103/100000: episode: 1611, duration: 0.090s, episode steps: 18, steps per second: 199, episode reward: 45.037, mean reward: 2.502 [2.039, 3.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.132 [-0.619, 10.100], loss: 0.193109, mae: 0.401578, mean_q: 4.635501
 82119/100000: episode: 1612, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 82.146, mean reward: 5.134 [3.165, 10.302], mean action: 0.000 [0.000, 0.000], mean observation: 2.105 [-1.115, 10.100], loss: 0.214183, mae: 0.421285, mean_q: 4.495481
 82135/100000: episode: 1613, duration: 0.084s, episode steps: 16, steps per second: 189, episode reward: 55.848, mean reward: 3.491 [2.831, 4.584], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.271, 10.100], loss: 0.265564, mae: 0.431168, mean_q: 4.595532
 82166/100000: episode: 1614, duration: 0.149s, episode steps: 31, steps per second: 208, episode reward: 69.502, mean reward: 2.242 [1.794, 3.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.035, 10.403], loss: 0.387207, mae: 0.468449, mean_q: 4.594359
 82184/100000: episode: 1615, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 59.943, mean reward: 3.330 [2.299, 9.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-1.037, 10.100], loss: 0.260976, mae: 0.415684, mean_q: 4.547914
 82200/100000: episode: 1616, duration: 0.079s, episode steps: 16, steps per second: 204, episode reward: 90.749, mean reward: 5.672 [2.885, 11.453], mean action: 0.000 [0.000, 0.000], mean observation: 2.121 [-0.301, 10.100], loss: 0.197069, mae: 0.411533, mean_q: 4.475224
 82223/100000: episode: 1617, duration: 0.111s, episode steps: 23, steps per second: 207, episode reward: 52.772, mean reward: 2.294 [1.742, 3.673], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.829, 10.442], loss: 0.248540, mae: 0.429374, mean_q: 4.636309
 82245/100000: episode: 1618, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 78.479, mean reward: 3.567 [2.564, 4.812], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.278, 10.100], loss: 0.372364, mae: 0.506015, mean_q: 4.690060
 82253/100000: episode: 1619, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 24.329, mean reward: 3.041 [2.468, 3.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.269 [-0.035, 10.445], loss: 0.307248, mae: 0.551162, mean_q: 4.484771
 82269/100000: episode: 1620, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 73.471, mean reward: 4.592 [2.310, 8.062], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.337, 10.100], loss: 0.352057, mae: 0.510678, mean_q: 4.634346
 82305/100000: episode: 1621, duration: 0.183s, episode steps: 36, steps per second: 197, episode reward: 190.777, mean reward: 5.299 [2.429, 14.254], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.048, 10.502], loss: 0.262460, mae: 0.448838, mean_q: 4.634303
 82317/100000: episode: 1622, duration: 0.060s, episode steps: 12, steps per second: 202, episode reward: 36.708, mean reward: 3.059 [2.580, 4.212], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.366, 10.100], loss: 0.196284, mae: 0.404257, mean_q: 4.526378
 82353/100000: episode: 1623, duration: 0.190s, episode steps: 36, steps per second: 190, episode reward: 109.762, mean reward: 3.049 [2.001, 5.242], mean action: 0.000 [0.000, 0.000], mean observation: 2.034 [-0.357, 10.350], loss: 0.235562, mae: 0.441909, mean_q: 4.727563
 82361/100000: episode: 1624, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 33.732, mean reward: 4.217 [3.199, 6.202], mean action: 0.000 [0.000, 0.000], mean observation: 2.287 [-0.035, 10.627], loss: 0.150944, mae: 0.392974, mean_q: 4.566361
 82397/100000: episode: 1625, duration: 0.184s, episode steps: 36, steps per second: 196, episode reward: 109.843, mean reward: 3.051 [2.011, 4.632], mean action: 0.000 [0.000, 0.000], mean observation: 2.036 [-1.029, 10.396], loss: 0.337600, mae: 0.477131, mean_q: 4.727002
 82420/100000: episode: 1626, duration: 0.127s, episode steps: 23, steps per second: 181, episode reward: 61.985, mean reward: 2.695 [2.112, 3.221], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.035, 10.421], loss: 0.253500, mae: 0.464443, mean_q: 4.722290
 82442/100000: episode: 1627, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 185.176, mean reward: 8.417 [3.058, 20.332], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.592, 10.100], loss: 0.233011, mae: 0.434690, mean_q: 4.706641
 82460/100000: episode: 1628, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 52.493, mean reward: 2.916 [2.148, 4.123], mean action: 0.000 [0.000, 0.000], mean observation: 2.134 [-1.357, 10.100], loss: 0.254222, mae: 0.442930, mean_q: 4.722305
 82476/100000: episode: 1629, duration: 0.078s, episode steps: 16, steps per second: 205, episode reward: 50.691, mean reward: 3.168 [2.402, 6.470], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.301, 10.100], loss: 0.270933, mae: 0.434633, mean_q: 4.697856
[Info] 2-TH LEVEL FOUND: 9.99048137664795, Considering 10/90 traces
 82494/100000: episode: 1630, duration: 4.392s, episode steps: 18, steps per second: 4, episode reward: 48.916, mean reward: 2.718 [2.191, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.692, 10.100], loss: 0.867451, mae: 0.579164, mean_q: 4.979346
 82510/100000: episode: 1631, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 134.277, mean reward: 8.392 [3.234, 42.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.286, 10.100], loss: 0.321397, mae: 0.541526, mean_q: 4.682997
 82535/100000: episode: 1632, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 77.345, mean reward: 3.094 [2.077, 4.469], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.424, 10.374], loss: 0.347446, mae: 0.498936, mean_q: 4.763049
 82560/100000: episode: 1633, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 72.594, mean reward: 2.904 [2.423, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.661, 10.458], loss: 0.411653, mae: 0.477200, mean_q: 4.814088
 82569/100000: episode: 1634, duration: 0.050s, episode steps: 9, steps per second: 182, episode reward: 41.609, mean reward: 4.623 [3.565, 6.071], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.514, 10.100], loss: 0.480490, mae: 0.518500, mean_q: 4.859593
 82585/100000: episode: 1635, duration: 0.079s, episode steps: 16, steps per second: 204, episode reward: 127.643, mean reward: 7.978 [4.278, 12.330], mean action: 0.000 [0.000, 0.000], mean observation: 2.232 [-0.122, 10.605], loss: 0.293473, mae: 0.472156, mean_q: 4.853422
 82601/100000: episode: 1636, duration: 0.083s, episode steps: 16, steps per second: 194, episode reward: 118.627, mean reward: 7.414 [3.836, 17.043], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.621, 10.511], loss: 0.286232, mae: 0.457574, mean_q: 4.807130
 82617/100000: episode: 1637, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 61.284, mean reward: 3.830 [2.402, 6.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.209 [-0.035, 10.409], loss: 0.605583, mae: 0.582399, mean_q: 4.953074
 82633/100000: episode: 1638, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 63.564, mean reward: 3.973 [2.632, 6.910], mean action: 0.000 [0.000, 0.000], mean observation: 2.222 [-0.035, 10.398], loss: 1.926529, mae: 0.721143, mean_q: 5.019715
 82653/100000: episode: 1639, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 101.758, mean reward: 5.088 [3.584, 9.444], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.035, 10.660], loss: 0.646307, mae: 0.630990, mean_q: 4.989689
 82669/100000: episode: 1640, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 97.082, mean reward: 6.068 [3.771, 10.247], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.329, 10.100], loss: 0.576560, mae: 0.626551, mean_q: 5.067073
 82685/100000: episode: 1641, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 63.549, mean reward: 3.972 [3.137, 6.843], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.372, 10.100], loss: 0.558403, mae: 0.586109, mean_q: 5.155130
 82699/100000: episode: 1642, duration: 0.072s, episode steps: 14, steps per second: 195, episode reward: 68.348, mean reward: 4.882 [3.704, 7.250], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.464, 10.100], loss: 0.355525, mae: 0.504193, mean_q: 4.844571
 82715/100000: episode: 1643, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 57.583, mean reward: 3.599 [3.146, 4.782], mean action: 0.000 [0.000, 0.000], mean observation: 2.115 [-0.504, 10.100], loss: 0.642666, mae: 0.604974, mean_q: 5.112967
 82740/100000: episode: 1644, duration: 0.127s, episode steps: 25, steps per second: 197, episode reward: 120.108, mean reward: 4.804 [2.606, 7.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.517, 10.611], loss: 0.312108, mae: 0.507224, mean_q: 4.995760
 82762/100000: episode: 1645, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 122.251, mean reward: 5.557 [2.944, 10.706], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.176, 10.555], loss: 0.527939, mae: 0.561060, mean_q: 5.066229
 82771/100000: episode: 1646, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 41.510, mean reward: 4.612 [3.116, 6.418], mean action: 0.000 [0.000, 0.000], mean observation: 2.155 [-0.432, 10.100], loss: 0.632907, mae: 0.557582, mean_q: 5.137790
 82787/100000: episode: 1647, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 94.140, mean reward: 5.884 [4.339, 11.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.593], loss: 0.596208, mae: 0.614002, mean_q: 5.216347
 82809/100000: episode: 1648, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 78.898, mean reward: 3.586 [2.488, 5.525], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.821, 10.407], loss: 0.480413, mae: 0.589332, mean_q: 5.128750
 82834/100000: episode: 1649, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 91.537, mean reward: 3.661 [2.717, 4.585], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.924, 10.492], loss: 0.585185, mae: 0.662452, mean_q: 5.105795
 82859/100000: episode: 1650, duration: 0.137s, episode steps: 25, steps per second: 182, episode reward: 1176.086, mean reward: 47.043 [4.525, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-0.765, 10.561], loss: 0.591361, mae: 0.638347, mean_q: 5.152189
 82875/100000: episode: 1651, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 83.813, mean reward: 5.238 [3.518, 7.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.634], loss: 975.881531, mae: 4.869867, mean_q: 7.149122
 82890/100000: episode: 1652, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 66.758, mean reward: 4.451 [1.872, 9.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.574, 10.100], loss: 2.104839, mae: 1.528152, mean_q: 5.232536
 82906/100000: episode: 1653, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 91.343, mean reward: 5.709 [3.859, 12.288], mean action: 0.000 [0.000, 0.000], mean observation: 2.131 [-0.475, 10.100], loss: 0.879309, mae: 0.885826, mean_q: 5.274639
 82922/100000: episode: 1654, duration: 0.082s, episode steps: 16, steps per second: 195, episode reward: 100.119, mean reward: 6.257 [4.244, 10.500], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.307, 10.609], loss: 1.123346, mae: 0.897147, mean_q: 5.699063
 82947/100000: episode: 1655, duration: 0.128s, episode steps: 25, steps per second: 196, episode reward: 178.700, mean reward: 7.148 [4.095, 11.866], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.685, 10.602], loss: 622.755188, mae: 2.477512, mean_q: 6.123467
 82972/100000: episode: 1656, duration: 0.120s, episode steps: 25, steps per second: 209, episode reward: 87.759, mean reward: 3.510 [2.712, 5.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.035, 10.571], loss: 2.489250, mae: 1.585202, mean_q: 5.946073
 82988/100000: episode: 1657, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 67.062, mean reward: 4.191 [2.339, 8.931], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.266, 10.100], loss: 968.731018, mae: 2.910552, mean_q: 6.318806
 83008/100000: episode: 1658, duration: 0.113s, episode steps: 20, steps per second: 176, episode reward: 66.236, mean reward: 3.312 [2.557, 4.325], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.467], loss: 2.567749, mae: 1.669335, mean_q: 6.241830
 83017/100000: episode: 1659, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 134.777, mean reward: 14.975 [4.517, 91.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.546, 10.100], loss: 0.920162, mae: 0.945543, mean_q: 6.370928
 83042/100000: episode: 1660, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 91.516, mean reward: 3.661 [2.734, 5.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.498, 10.599], loss: 1.841613, mae: 0.892838, mean_q: 6.222392
 83058/100000: episode: 1661, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 173.670, mean reward: 10.854 [4.562, 27.362], mean action: 0.000 [0.000, 0.000], mean observation: 2.123 [-0.489, 10.100], loss: 1.031103, mae: 0.804983, mean_q: 5.984288
 83073/100000: episode: 1662, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 74.860, mean reward: 4.991 [3.651, 6.669], mean action: 0.000 [0.000, 0.000], mean observation: 2.133 [-0.373, 10.100], loss: 0.574594, mae: 0.687764, mean_q: 6.024411
 83088/100000: episode: 1663, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 60.562, mean reward: 4.037 [2.863, 6.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-0.950, 10.100], loss: 0.575531, mae: 0.640146, mean_q: 5.921200
 83104/100000: episode: 1664, duration: 0.079s, episode steps: 16, steps per second: 203, episode reward: 92.474, mean reward: 5.780 [2.724, 11.749], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.569], loss: 0.782318, mae: 0.715326, mean_q: 5.967159
 83120/100000: episode: 1665, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 65.165, mean reward: 4.073 [2.697, 5.408], mean action: 0.000 [0.000, 0.000], mean observation: 2.109 [-0.954, 10.100], loss: 0.904312, mae: 0.769363, mean_q: 6.215072
 83140/100000: episode: 1666, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 89.152, mean reward: 4.458 [3.533, 6.802], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.493, 10.649], loss: 1.839933, mae: 0.811220, mean_q: 6.140398
 83155/100000: episode: 1667, duration: 0.080s, episode steps: 15, steps per second: 186, episode reward: 63.693, mean reward: 4.246 [2.742, 10.936], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.458, 10.100], loss: 8.644891, mae: 0.958632, mean_q: 6.025008
 83180/100000: episode: 1668, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 102.003, mean reward: 4.080 [3.005, 5.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.446, 10.573], loss: 1.087362, mae: 0.684437, mean_q: 5.902165
 83196/100000: episode: 1669, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 40.847, mean reward: 2.553 [2.037, 4.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-1.145, 10.100], loss: 0.841856, mae: 0.700168, mean_q: 5.946846
 83212/100000: episode: 1670, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 112.316, mean reward: 7.020 [3.961, 10.793], mean action: 0.000 [0.000, 0.000], mean observation: 2.106 [-0.361, 10.100], loss: 967.562134, mae: 2.961900, mean_q: 6.268418
 83226/100000: episode: 1671, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 60.450, mean reward: 4.318 [3.227, 5.358], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.393, 10.100], loss: 12.051982, mae: 3.657184, mean_q: 8.759134
 83251/100000: episode: 1672, duration: 0.133s, episode steps: 25, steps per second: 188, episode reward: 102.108, mean reward: 4.084 [2.790, 7.064], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-0.035, 10.528], loss: 2.043878, mae: 1.417545, mean_q: 6.141491
 83271/100000: episode: 1673, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 76.667, mean reward: 3.833 [2.862, 4.913], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.136, 10.475], loss: 0.611348, mae: 0.790657, mean_q: 6.060194
 83296/100000: episode: 1674, duration: 0.121s, episode steps: 25, steps per second: 206, episode reward: 71.618, mean reward: 2.865 [2.279, 3.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.150 [-0.035, 10.433], loss: 5.661778, mae: 0.945653, mean_q: 6.206531
 83321/100000: episode: 1675, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 102.640, mean reward: 4.106 [2.290, 9.201], mean action: 0.000 [0.000, 0.000], mean observation: 2.142 [-0.035, 10.323], loss: 0.822363, mae: 0.795422, mean_q: 5.984910
 83330/100000: episode: 1676, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 80.268, mean reward: 8.919 [2.969, 49.274], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.359, 10.100], loss: 2.950050, mae: 0.810941, mean_q: 6.063002
 83346/100000: episode: 1677, duration: 0.079s, episode steps: 16, steps per second: 204, episode reward: 126.834, mean reward: 7.927 [5.739, 11.471], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.441, 10.100], loss: 2.355257, mae: 0.777447, mean_q: 5.971508
 83368/100000: episode: 1678, duration: 0.121s, episode steps: 22, steps per second: 183, episode reward: 209.218, mean reward: 9.510 [3.655, 34.536], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-1.208, 10.560], loss: 6.518104, mae: 1.156337, mean_q: 6.536450
 83384/100000: episode: 1679, duration: 0.085s, episode steps: 16, steps per second: 189, episode reward: 66.540, mean reward: 4.159 [2.607, 7.411], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.945, 10.368], loss: 1.887522, mae: 0.919613, mean_q: 6.043726
 83400/100000: episode: 1680, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 72.059, mean reward: 4.504 [3.195, 9.429], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.477], loss: 0.631394, mae: 0.754523, mean_q: 6.087916
 83416/100000: episode: 1681, duration: 0.083s, episode steps: 16, steps per second: 193, episode reward: 59.888, mean reward: 3.743 [2.608, 5.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.706, 10.100], loss: 2.828934, mae: 0.929274, mean_q: 6.138131
 83436/100000: episode: 1682, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 72.865, mean reward: 3.643 [2.872, 5.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.035, 10.602], loss: 6.629198, mae: 1.008537, mean_q: 6.350133
 83452/100000: episode: 1683, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 70.508, mean reward: 4.407 [2.852, 7.324], mean action: 0.000 [0.000, 0.000], mean observation: 2.114 [-0.565, 10.100], loss: 0.860390, mae: 0.839460, mean_q: 6.228199
 83474/100000: episode: 1684, duration: 0.111s, episode steps: 22, steps per second: 198, episode reward: 97.335, mean reward: 4.424 [3.192, 7.030], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.035, 10.445], loss: 706.359924, mae: 2.991118, mean_q: 7.080640
 83489/100000: episode: 1685, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 91.100, mean reward: 6.073 [4.130, 9.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.112 [-0.517, 10.100], loss: 1.603571, mae: 1.096357, mean_q: 5.799145
 83509/100000: episode: 1686, duration: 0.103s, episode steps: 20, steps per second: 193, episode reward: 84.152, mean reward: 4.208 [2.567, 8.146], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.188, 10.481], loss: 1.616627, mae: 1.016890, mean_q: 6.430104
 83529/100000: episode: 1687, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 67.838, mean reward: 3.392 [1.938, 8.492], mean action: 0.000 [0.000, 0.000], mean observation: 2.164 [-1.050, 10.231], loss: 779.313721, mae: 3.520379, mean_q: 7.478184
 83544/100000: episode: 1688, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 70.179, mean reward: 4.679 [3.017, 6.697], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.375, 10.100], loss: 11.715934, mae: 2.111405, mean_q: 7.419924
 83569/100000: episode: 1689, duration: 0.127s, episode steps: 25, steps per second: 198, episode reward: 67.875, mean reward: 2.715 [2.029, 6.937], mean action: 0.000 [0.000, 0.000], mean observation: 2.141 [-0.395, 10.361], loss: 2.029963, mae: 1.195902, mean_q: 6.101719
 83594/100000: episode: 1690, duration: 0.129s, episode steps: 25, steps per second: 194, episode reward: 88.735, mean reward: 3.549 [2.789, 5.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.127 [-0.496, 10.401], loss: 1.063248, mae: 0.862816, mean_q: 6.367842
 83619/100000: episode: 1691, duration: 0.153s, episode steps: 25, steps per second: 164, episode reward: 76.912, mean reward: 3.076 [2.147, 4.520], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.035, 10.383], loss: 0.781978, mae: 0.822902, mean_q: 6.239267
 83641/100000: episode: 1692, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 85.324, mean reward: 3.878 [2.839, 5.526], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.035, 10.487], loss: 1.535359, mae: 0.879163, mean_q: 6.332297
 83656/100000: episode: 1693, duration: 0.079s, episode steps: 15, steps per second: 190, episode reward: 191.006, mean reward: 12.734 [4.429, 46.788], mean action: 0.000 [0.000, 0.000], mean observation: 2.103 [-0.403, 10.100], loss: 1.044306, mae: 0.889437, mean_q: 6.275568
 83672/100000: episode: 1694, duration: 0.088s, episode steps: 16, steps per second: 183, episode reward: 148.319, mean reward: 9.270 [2.704, 40.579], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.125, 10.620], loss: 970.320068, mae: 3.738012, mean_q: 7.415650
 83697/100000: episode: 1695, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 175.198, mean reward: 7.008 [3.826, 20.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.130 [-0.035, 10.609], loss: 6.553458, mae: 1.207076, mean_q: 6.360754
 83722/100000: episode: 1696, duration: 0.124s, episode steps: 25, steps per second: 201, episode reward: 85.856, mean reward: 3.434 [2.289, 5.282], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-1.466, 10.391], loss: 2.022926, mae: 1.016000, mean_q: 6.426069
[Info] FALSIFICATION!
 83724/100000: episode: 1697, duration: 0.178s, episode steps: 2, steps per second: 11, episode reward: 1008.834, mean reward: 504.417 [8.834, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.015, 10.303], loss: 1.220226, mae: 1.073643, mean_q: 6.647299
 83740/100000: episode: 1698, duration: 0.084s, episode steps: 16, steps per second: 190, episode reward: 99.092, mean reward: 6.193 [4.253, 9.305], mean action: 0.000 [0.000, 0.000], mean observation: 2.111 [-0.593, 10.100], loss: 1.150731, mae: 0.881401, mean_q: 6.380653
 83762/100000: episode: 1699, duration: 0.117s, episode steps: 22, steps per second: 188, episode reward: 113.619, mean reward: 5.164 [3.509, 8.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.249, 10.638], loss: 1.658020, mae: 1.005278, mean_q: 6.561406
 83777/100000: episode: 1700, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 82.761, mean reward: 5.517 [3.571, 8.896], mean action: 0.000 [0.000, 0.000], mean observation: 2.125 [-0.367, 10.100], loss: 2.556815, mae: 0.935543, mean_q: 6.566016
 83799/100000: episode: 1701, duration: 0.107s, episode steps: 22, steps per second: 206, episode reward: 108.983, mean reward: 4.954 [2.267, 12.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.035, 10.436], loss: 705.035217, mae: 2.464536, mean_q: 6.662980
 83815/100000: episode: 1702, duration: 0.082s, episode steps: 16, steps per second: 196, episode reward: 257.466, mean reward: 16.092 [5.797, 107.507], mean action: 0.000 [0.000, 0.000], mean observation: 2.085 [-0.727, 10.100], loss: 7.874795, mae: 2.927392, mean_q: 9.130537
 83830/100000: episode: 1703, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 75.839, mean reward: 5.056 [3.212, 6.217], mean action: 0.000 [0.000, 0.000], mean observation: 2.119 [-0.558, 10.100], loss: 2.151756, mae: 1.388844, mean_q: 6.054961
 83844/100000: episode: 1704, duration: 0.070s, episode steps: 14, steps per second: 200, episode reward: 73.049, mean reward: 5.218 [3.309, 19.519], mean action: 0.000 [0.000, 0.000], mean observation: 2.139 [-1.670, 10.100], loss: 1.390851, mae: 1.138406, mean_q: 6.869201
 83864/100000: episode: 1705, duration: 0.099s, episode steps: 20, steps per second: 201, episode reward: 93.802, mean reward: 4.690 [2.642, 8.522], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-2.574, 10.544], loss: 768.898132, mae: 3.045107, mean_q: 7.174143
 83880/100000: episode: 1706, duration: 0.096s, episode steps: 16, steps per second: 166, episode reward: 83.848, mean reward: 5.240 [3.833, 8.641], mean action: 0.000 [0.000, 0.000], mean observation: 2.120 [-0.269, 10.100], loss: 970.908630, mae: 4.581234, mean_q: 8.858802
 83900/100000: episode: 1707, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 81.183, mean reward: 4.059 [2.729, 5.868], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.035, 10.495], loss: 3.814513, mae: 1.538584, mean_q: 7.711056
 83922/100000: episode: 1708, duration: 0.112s, episode steps: 22, steps per second: 196, episode reward: 112.084, mean reward: 5.095 [3.844, 6.962], mean action: 0.000 [0.000, 0.000], mean observation: 2.169 [-0.163, 10.509], loss: 9.545976, mae: 1.471902, mean_q: 7.223860
 83947/100000: episode: 1709, duration: 0.124s, episode steps: 25, steps per second: 202, episode reward: 106.574, mean reward: 4.263 [2.457, 7.259], mean action: 0.000 [0.000, 0.000], mean observation: 2.129 [-0.838, 10.428], loss: 621.947754, mae: 2.733323, mean_q: 7.467024
 83967/100000: episode: 1710, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 63.645, mean reward: 3.182 [2.811, 3.767], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.158, 10.474], loss: 767.031982, mae: 5.246505, mean_q: 10.289671
 83983/100000: episode: 1711, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 107.081, mean reward: 6.693 [3.548, 20.502], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.340, 10.100], loss: 1.910789, mae: 1.338973, mean_q: 7.429474
 83998/100000: episode: 1712, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 97.091, mean reward: 6.473 [3.967, 18.430], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.987, 10.100], loss: 1.663631, mae: 1.317414, mean_q: 6.480431
 84014/100000: episode: 1713, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 73.279, mean reward: 4.580 [3.400, 6.037], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.516], loss: 19.694462, mae: 1.721068, mean_q: 7.781053
 84036/100000: episode: 1714, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 145.573, mean reward: 6.617 [2.810, 19.546], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.142, 10.434], loss: 6.453515, mae: 1.205595, mean_q: 7.402208
 84052/100000: episode: 1715, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 62.557, mean reward: 3.910 [2.402, 6.428], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.081, 10.431], loss: 2.241966, mae: 0.994499, mean_q: 6.996043
 84077/100000: episode: 1716, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 108.513, mean reward: 4.341 [2.836, 6.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.128 [-0.693, 10.535], loss: 7.178414, mae: 1.134327, mean_q: 7.449202
 84102/100000: episode: 1717, duration: 0.141s, episode steps: 25, steps per second: 177, episode reward: 97.264, mean reward: 3.891 [2.116, 8.401], mean action: 0.000 [0.000, 0.000], mean observation: 2.137 [-0.619, 10.325], loss: 1.119906, mae: 0.930230, mean_q: 6.857440
 84118/100000: episode: 1718, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 90.717, mean reward: 5.670 [3.504, 10.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.196, 10.412], loss: 1.035644, mae: 0.975386, mean_q: 7.020898
 84143/100000: episode: 1719, duration: 0.140s, episode steps: 25, steps per second: 178, episode reward: 89.662, mean reward: 3.586 [2.815, 4.484], mean action: 0.000 [0.000, 0.000], mean observation: 2.145 [-1.129, 10.468], loss: 1.438557, mae: 1.000393, mean_q: 7.230547
[Info] Complete ISplit Iteration
[Info] Levels: [5.713003, 9.990481, 12.933496]
[Info] Cond. Prob: [0.1, 0.1, 0.23]
[Info] Error Prob: 0.0023000000000000004

 84163/100000: episode: 1720, duration: 4.509s, episode steps: 20, steps per second: 4, episode reward: 104.008, mean reward: 5.200 [3.308, 8.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.190, 10.662], loss: 1.992613, mae: 1.054015, mean_q: 7.288042
 84263/100000: episode: 1721, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 207.015, mean reward: 2.070 [1.432, 3.983], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-0.500, 10.268], loss: 2.060609, mae: 0.998062, mean_q: 6.973080
 84363/100000: episode: 1722, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 194.246, mean reward: 1.942 [1.469, 3.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.364, 10.098], loss: 467.510834, mae: 2.907170, mean_q: 8.094516
 84463/100000: episode: 1723, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 199.276, mean reward: 1.993 [1.455, 3.237], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.800, 10.353], loss: 157.354309, mae: 1.970290, mean_q: 7.793299
 84563/100000: episode: 1724, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 199.132, mean reward: 1.991 [1.464, 3.357], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.890, 10.298], loss: 6.564259, mae: 1.235363, mean_q: 7.302512
 84663/100000: episode: 1725, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 195.992, mean reward: 1.960 [1.477, 3.401], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.332, 10.290], loss: 310.259460, mae: 1.824543, mean_q: 7.200833
 84763/100000: episode: 1726, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 210.200, mean reward: 2.102 [1.475, 5.292], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.031, 10.301], loss: 161.069717, mae: 1.938905, mean_q: 7.711985
 84863/100000: episode: 1727, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 183.138, mean reward: 1.831 [1.457, 2.437], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.849, 10.233], loss: 3.997364, mae: 1.209439, mean_q: 7.179643
 84963/100000: episode: 1728, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 188.649, mean reward: 1.886 [1.447, 3.217], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.237, 10.098], loss: 3.241798, mae: 1.065484, mean_q: 7.072732
 85063/100000: episode: 1729, duration: 0.509s, episode steps: 100, steps per second: 197, episode reward: 184.826, mean reward: 1.848 [1.442, 3.267], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.729, 10.140], loss: 3.482545, mae: 1.008418, mean_q: 6.961958
 85163/100000: episode: 1730, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 192.170, mean reward: 1.922 [1.458, 2.657], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.270, 10.194], loss: 161.953613, mae: 1.768102, mean_q: 7.413765
 85263/100000: episode: 1731, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 181.473, mean reward: 1.815 [1.468, 3.281], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.575, 10.098], loss: 161.329987, mae: 1.768356, mean_q: 7.366877
 85363/100000: episode: 1732, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 200.223, mean reward: 2.002 [1.506, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.915, 10.098], loss: 311.462494, mae: 2.294492, mean_q: 7.820688
 85463/100000: episode: 1733, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 189.734, mean reward: 1.897 [1.480, 3.389], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.471, 10.223], loss: 311.057739, mae: 2.146363, mean_q: 7.539666
 85563/100000: episode: 1734, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 191.565, mean reward: 1.916 [1.490, 3.332], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-1.135, 10.098], loss: 159.020279, mae: 1.815610, mean_q: 7.675529
 85663/100000: episode: 1735, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 190.472, mean reward: 1.905 [1.441, 2.756], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.363, 10.191], loss: 156.628891, mae: 1.813279, mean_q: 7.471542
 85763/100000: episode: 1736, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 188.854, mean reward: 1.889 [1.455, 5.234], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.515, 10.098], loss: 162.574707, mae: 1.729998, mean_q: 7.330729
 85863/100000: episode: 1737, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 189.267, mean reward: 1.893 [1.463, 3.100], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.482, 10.098], loss: 3.744202, mae: 1.053197, mean_q: 6.887966
 85963/100000: episode: 1738, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 206.729, mean reward: 2.067 [1.454, 3.359], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.554, 10.098], loss: 463.629486, mae: 2.714031, mean_q: 7.747995
 86063/100000: episode: 1739, duration: 0.530s, episode steps: 100, steps per second: 189, episode reward: 180.864, mean reward: 1.809 [1.467, 2.810], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.661, 10.172], loss: 3.208826, mae: 1.013584, mean_q: 6.752826
 86163/100000: episode: 1740, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 189.421, mean reward: 1.894 [1.446, 2.848], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.914, 10.098], loss: 156.225235, mae: 1.652060, mean_q: 7.014175
 86263/100000: episode: 1741, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 191.360, mean reward: 1.914 [1.450, 3.147], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.679, 10.103], loss: 462.563324, mae: 2.537584, mean_q: 7.407863
 86363/100000: episode: 1742, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: 191.696, mean reward: 1.917 [1.453, 3.849], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.547, 10.221], loss: 2.769282, mae: 1.218900, mean_q: 6.847311
 86463/100000: episode: 1743, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 191.314, mean reward: 1.913 [1.463, 3.336], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.290, 10.258], loss: 4.238594, mae: 0.986402, mean_q: 6.528482
 86563/100000: episode: 1744, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 179.648, mean reward: 1.796 [1.456, 2.515], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.349, 10.125], loss: 464.454651, mae: 2.265941, mean_q: 6.895651
 86663/100000: episode: 1745, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 214.161, mean reward: 2.142 [1.531, 4.070], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.717, 10.272], loss: 466.798096, mae: 2.880108, mean_q: 7.668893
 86763/100000: episode: 1746, duration: 0.517s, episode steps: 100, steps per second: 194, episode reward: 198.375, mean reward: 1.984 [1.449, 3.871], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.158, 10.098], loss: 4.080933, mae: 1.303825, mean_q: 7.104778
 86863/100000: episode: 1747, duration: 0.487s, episode steps: 100, steps per second: 205, episode reward: 206.307, mean reward: 2.063 [1.462, 5.207], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.681, 10.098], loss: 159.473160, mae: 1.619801, mean_q: 6.963185
 86963/100000: episode: 1748, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 182.471, mean reward: 1.825 [1.465, 3.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-1.417, 10.098], loss: 157.889206, mae: 1.485983, mean_q: 6.644114
 87063/100000: episode: 1749, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 200.666, mean reward: 2.007 [1.447, 5.905], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.683, 10.174], loss: 312.304596, mae: 2.180535, mean_q: 7.239638
 87163/100000: episode: 1750, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 195.269, mean reward: 1.953 [1.494, 3.778], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.144, 10.098], loss: 158.349365, mae: 1.630067, mean_q: 6.885212
 87263/100000: episode: 1751, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 191.550, mean reward: 1.916 [1.462, 3.189], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.368, 10.098], loss: 2.242452, mae: 0.972620, mean_q: 6.325363
 87363/100000: episode: 1752, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 259.997, mean reward: 2.600 [1.538, 4.561], mean action: 0.000 [0.000, 0.000], mean observation: 1.417 [-0.913, 10.429], loss: 1.723837, mae: 0.843379, mean_q: 6.132963
 87463/100000: episode: 1753, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 193.362, mean reward: 1.934 [1.443, 3.266], mean action: 0.000 [0.000, 0.000], mean observation: 1.416 [-1.082, 10.098], loss: 4.702506, mae: 0.866352, mean_q: 5.944929
 87563/100000: episode: 1754, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 193.349, mean reward: 1.933 [1.469, 3.644], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-1.068, 10.349], loss: 157.361404, mae: 1.295736, mean_q: 6.044752
 87663/100000: episode: 1755, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 193.073, mean reward: 1.931 [1.463, 3.951], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.921, 10.098], loss: 157.856247, mae: 1.452683, mean_q: 6.165207
 87763/100000: episode: 1756, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 183.952, mean reward: 1.840 [1.439, 3.129], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.570, 10.125], loss: 2.437171, mae: 0.780760, mean_q: 5.591388
 87863/100000: episode: 1757, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 194.679, mean reward: 1.947 [1.497, 3.859], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.925, 10.199], loss: 3.844577, mae: 0.801465, mean_q: 5.521504
 87963/100000: episode: 1758, duration: 0.483s, episode steps: 100, steps per second: 207, episode reward: 193.353, mean reward: 1.934 [1.431, 3.452], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.406, 10.157], loss: 3.133730, mae: 0.735492, mean_q: 5.346265
 88063/100000: episode: 1759, duration: 0.485s, episode steps: 100, steps per second: 206, episode reward: 196.733, mean reward: 1.967 [1.507, 4.567], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.178, 10.098], loss: 2.551239, mae: 0.651687, mean_q: 5.123679
 88163/100000: episode: 1760, duration: 0.545s, episode steps: 100, steps per second: 183, episode reward: 193.349, mean reward: 1.933 [1.489, 3.319], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.241, 10.214], loss: 153.764191, mae: 1.013526, mean_q: 5.167076
 88263/100000: episode: 1761, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 179.435, mean reward: 1.794 [1.435, 3.193], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.837, 10.098], loss: 460.466248, mae: 2.104781, mean_q: 5.636260
 88363/100000: episode: 1762, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 187.780, mean reward: 1.878 [1.453, 2.990], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.461, 10.098], loss: 152.424301, mae: 1.352120, mean_q: 5.454616
 88463/100000: episode: 1763, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 183.935, mean reward: 1.839 [1.488, 2.889], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.959, 10.271], loss: 2.069785, mae: 0.638951, mean_q: 4.855011
 88563/100000: episode: 1764, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 208.977, mean reward: 2.090 [1.440, 4.434], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.969, 10.250], loss: 3.565125, mae: 0.651298, mean_q: 4.754020
 88663/100000: episode: 1765, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 220.661, mean reward: 2.207 [1.466, 5.258], mean action: 0.000 [0.000, 0.000], mean observation: 1.441 [-0.208, 10.098], loss: 152.986343, mae: 1.183518, mean_q: 4.842128
 88763/100000: episode: 1766, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 191.897, mean reward: 1.919 [1.463, 3.194], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.862, 10.206], loss: 0.433434, mae: 0.505542, mean_q: 4.432572
 88863/100000: episode: 1767, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 192.808, mean reward: 1.928 [1.476, 3.606], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.849, 10.098], loss: 0.256177, mae: 0.442118, mean_q: 4.201136
 88963/100000: episode: 1768, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 194.735, mean reward: 1.947 [1.458, 4.658], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.632, 10.098], loss: 0.262579, mae: 0.435485, mean_q: 4.114157
 89063/100000: episode: 1769, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 185.727, mean reward: 1.857 [1.471, 3.201], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.908, 10.153], loss: 0.184439, mae: 0.391220, mean_q: 3.970989
 89163/100000: episode: 1770, duration: 0.499s, episode steps: 100, steps per second: 201, episode reward: 177.334, mean reward: 1.773 [1.449, 3.994], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.861, 10.098], loss: 0.134506, mae: 0.353116, mean_q: 3.857710
 89263/100000: episode: 1771, duration: 0.520s, episode steps: 100, steps per second: 192, episode reward: 208.000, mean reward: 2.080 [1.465, 3.585], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.435, 10.359], loss: 0.124372, mae: 0.344804, mean_q: 3.855757
 89363/100000: episode: 1772, duration: 0.484s, episode steps: 100, steps per second: 207, episode reward: 189.186, mean reward: 1.892 [1.486, 3.082], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.055, 10.098], loss: 0.115670, mae: 0.345649, mean_q: 3.844016
 89463/100000: episode: 1773, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 189.207, mean reward: 1.892 [1.461, 2.761], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.504, 10.184], loss: 0.107724, mae: 0.328148, mean_q: 3.841146
 89563/100000: episode: 1774, duration: 0.515s, episode steps: 100, steps per second: 194, episode reward: 181.840, mean reward: 1.818 [1.453, 3.197], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.825, 10.189], loss: 0.106285, mae: 0.325312, mean_q: 3.837853
 89663/100000: episode: 1775, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 182.936, mean reward: 1.829 [1.456, 2.918], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.737, 10.098], loss: 0.116083, mae: 0.329200, mean_q: 3.840080
 89763/100000: episode: 1776, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 193.737, mean reward: 1.937 [1.446, 4.300], mean action: 0.000 [0.000, 0.000], mean observation: 1.439 [-1.054, 10.220], loss: 0.098322, mae: 0.317245, mean_q: 3.818150
 89863/100000: episode: 1777, duration: 0.535s, episode steps: 100, steps per second: 187, episode reward: 186.843, mean reward: 1.868 [1.464, 3.700], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.500, 10.112], loss: 0.104182, mae: 0.318972, mean_q: 3.845295
 89963/100000: episode: 1778, duration: 0.514s, episode steps: 100, steps per second: 195, episode reward: 180.432, mean reward: 1.804 [1.438, 2.750], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.153, 10.120], loss: 0.102342, mae: 0.321826, mean_q: 3.828675
 90063/100000: episode: 1779, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 182.985, mean reward: 1.830 [1.449, 3.265], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.850, 10.161], loss: 0.105626, mae: 0.322287, mean_q: 3.833423
 90163/100000: episode: 1780, duration: 0.504s, episode steps: 100, steps per second: 198, episode reward: 187.445, mean reward: 1.874 [1.452, 4.112], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.013, 10.362], loss: 0.108484, mae: 0.317348, mean_q: 3.818464
 90263/100000: episode: 1781, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 195.473, mean reward: 1.955 [1.437, 4.383], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.024, 10.098], loss: 0.095389, mae: 0.303959, mean_q: 3.808848
 90363/100000: episode: 1782, duration: 0.488s, episode steps: 100, steps per second: 205, episode reward: 194.241, mean reward: 1.942 [1.453, 3.203], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-0.469, 10.125], loss: 0.091571, mae: 0.301297, mean_q: 3.826861
 90463/100000: episode: 1783, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 197.534, mean reward: 1.975 [1.442, 4.485], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.459, 10.258], loss: 0.087615, mae: 0.296130, mean_q: 3.813717
 90563/100000: episode: 1784, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 236.689, mean reward: 2.367 [1.469, 6.556], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.834, 10.098], loss: 0.107523, mae: 0.312860, mean_q: 3.841662
 90663/100000: episode: 1785, duration: 0.504s, episode steps: 100, steps per second: 199, episode reward: 180.279, mean reward: 1.803 [1.477, 2.877], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.144, 10.201], loss: 0.105219, mae: 0.310641, mean_q: 3.853638
 90763/100000: episode: 1786, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 217.219, mean reward: 2.172 [1.576, 3.182], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.825, 10.098], loss: 0.089297, mae: 0.295609, mean_q: 3.836770
 90863/100000: episode: 1787, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 180.934, mean reward: 1.809 [1.449, 2.894], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.598, 10.098], loss: 0.106373, mae: 0.321255, mean_q: 3.858236
 90963/100000: episode: 1788, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 215.708, mean reward: 2.157 [1.508, 4.072], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.266, 10.098], loss: 0.103556, mae: 0.313481, mean_q: 3.874207
 91063/100000: episode: 1789, duration: 0.516s, episode steps: 100, steps per second: 194, episode reward: 185.368, mean reward: 1.854 [1.471, 7.369], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.668, 10.134], loss: 0.085848, mae: 0.299648, mean_q: 3.843879
 91163/100000: episode: 1790, duration: 0.502s, episode steps: 100, steps per second: 199, episode reward: 206.271, mean reward: 2.063 [1.500, 4.655], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-0.912, 10.175], loss: 0.084000, mae: 0.295073, mean_q: 3.842952
 91263/100000: episode: 1791, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 205.961, mean reward: 2.060 [1.441, 2.952], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.396, 10.098], loss: 0.090938, mae: 0.301071, mean_q: 3.845830
 91363/100000: episode: 1792, duration: 0.494s, episode steps: 100, steps per second: 203, episode reward: 194.917, mean reward: 1.949 [1.480, 3.444], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.074, 10.098], loss: 0.104594, mae: 0.311156, mean_q: 3.862347
 91463/100000: episode: 1793, duration: 0.511s, episode steps: 100, steps per second: 196, episode reward: 191.729, mean reward: 1.917 [1.460, 3.252], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-1.384, 10.378], loss: 0.092338, mae: 0.300125, mean_q: 3.866421
 91563/100000: episode: 1794, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: 185.207, mean reward: 1.852 [1.455, 3.448], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-1.552, 10.262], loss: 0.105087, mae: 0.312740, mean_q: 3.888694
 91663/100000: episode: 1795, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 193.474, mean reward: 1.935 [1.437, 3.417], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.592, 10.098], loss: 0.076779, mae: 0.281824, mean_q: 3.828562
 91763/100000: episode: 1796, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: 197.936, mean reward: 1.979 [1.506, 3.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-0.330, 10.098], loss: 0.109404, mae: 0.312973, mean_q: 3.864659
 91863/100000: episode: 1797, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 198.585, mean reward: 1.986 [1.520, 4.942], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-1.064, 10.098], loss: 0.088949, mae: 0.295587, mean_q: 3.855652
 91963/100000: episode: 1798, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: 200.429, mean reward: 2.004 [1.486, 3.923], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.629, 10.346], loss: 0.094767, mae: 0.296694, mean_q: 3.848082
 92063/100000: episode: 1799, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 205.258, mean reward: 2.053 [1.517, 6.001], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.676, 10.202], loss: 0.091369, mae: 0.296364, mean_q: 3.860082
 92163/100000: episode: 1800, duration: 0.505s, episode steps: 100, steps per second: 198, episode reward: 184.071, mean reward: 1.841 [1.466, 2.675], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.266, 10.110], loss: 0.098385, mae: 0.304749, mean_q: 3.863539
 92263/100000: episode: 1801, duration: 0.521s, episode steps: 100, steps per second: 192, episode reward: 187.613, mean reward: 1.876 [1.457, 3.714], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.679, 10.108], loss: 0.085122, mae: 0.288826, mean_q: 3.846689
 92363/100000: episode: 1802, duration: 0.500s, episode steps: 100, steps per second: 200, episode reward: 188.133, mean reward: 1.881 [1.461, 3.607], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.749, 10.119], loss: 0.088261, mae: 0.293652, mean_q: 3.840109
 92463/100000: episode: 1803, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 209.601, mean reward: 2.096 [1.480, 3.180], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-0.600, 10.098], loss: 0.083489, mae: 0.286063, mean_q: 3.826144
 92563/100000: episode: 1804, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 189.386, mean reward: 1.894 [1.484, 3.424], mean action: 0.000 [0.000, 0.000], mean observation: 1.429 [-1.663, 10.261], loss: 0.084126, mae: 0.288523, mean_q: 3.837496
 92663/100000: episode: 1805, duration: 0.524s, episode steps: 100, steps per second: 191, episode reward: 201.195, mean reward: 2.012 [1.438, 4.000], mean action: 0.000 [0.000, 0.000], mean observation: 1.423 [-0.884, 10.098], loss: 0.091786, mae: 0.287537, mean_q: 3.829346
 92763/100000: episode: 1806, duration: 0.509s, episode steps: 100, steps per second: 196, episode reward: 185.438, mean reward: 1.854 [1.466, 3.704], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.684, 10.098], loss: 0.087410, mae: 0.295979, mean_q: 3.838775
 92863/100000: episode: 1807, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 188.442, mean reward: 1.884 [1.459, 3.210], mean action: 0.000 [0.000, 0.000], mean observation: 1.420 [-1.057, 10.098], loss: 0.089097, mae: 0.292657, mean_q: 3.818329
 92963/100000: episode: 1808, duration: 0.497s, episode steps: 100, steps per second: 201, episode reward: 199.017, mean reward: 1.990 [1.447, 3.179], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.701, 10.131], loss: 0.091127, mae: 0.295207, mean_q: 3.853501
 93063/100000: episode: 1809, duration: 0.514s, episode steps: 100, steps per second: 194, episode reward: 197.917, mean reward: 1.979 [1.488, 3.204], mean action: 0.000 [0.000, 0.000], mean observation: 1.438 [-0.669, 10.243], loss: 0.090704, mae: 0.301198, mean_q: 3.842095
 93163/100000: episode: 1810, duration: 0.508s, episode steps: 100, steps per second: 197, episode reward: 190.404, mean reward: 1.904 [1.464, 2.961], mean action: 0.000 [0.000, 0.000], mean observation: 1.435 [-0.445, 10.098], loss: 0.094064, mae: 0.305100, mean_q: 3.865201
 93263/100000: episode: 1811, duration: 0.534s, episode steps: 100, steps per second: 187, episode reward: 259.354, mean reward: 2.594 [1.480, 6.301], mean action: 0.000 [0.000, 0.000], mean observation: 1.436 [-0.592, 10.495], loss: 0.085588, mae: 0.288716, mean_q: 3.833823
 93363/100000: episode: 1812, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 183.730, mean reward: 1.837 [1.452, 3.839], mean action: 0.000 [0.000, 0.000], mean observation: 1.422 [-1.359, 10.098], loss: 0.107687, mae: 0.309615, mean_q: 3.880295
 93463/100000: episode: 1813, duration: 0.499s, episode steps: 100, steps per second: 200, episode reward: 185.509, mean reward: 1.855 [1.472, 2.960], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.462, 10.104], loss: 0.100879, mae: 0.311865, mean_q: 3.880099
 93563/100000: episode: 1814, duration: 0.494s, episode steps: 100, steps per second: 202, episode reward: 192.622, mean reward: 1.926 [1.469, 3.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-0.888, 10.098], loss: 0.093382, mae: 0.299937, mean_q: 3.867573
 93663/100000: episode: 1815, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: 209.857, mean reward: 2.099 [1.477, 3.097], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.125, 10.098], loss: 0.095345, mae: 0.294771, mean_q: 3.861713
 93763/100000: episode: 1816, duration: 0.512s, episode steps: 100, steps per second: 195, episode reward: 188.968, mean reward: 1.890 [1.477, 3.211], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.592, 10.098], loss: 0.096296, mae: 0.296791, mean_q: 3.844889
 93863/100000: episode: 1817, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 181.015, mean reward: 1.810 [1.446, 3.592], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.757, 10.119], loss: 0.092290, mae: 0.296836, mean_q: 3.853565
 93963/100000: episode: 1818, duration: 0.522s, episode steps: 100, steps per second: 192, episode reward: 197.621, mean reward: 1.976 [1.455, 3.716], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.750, 10.319], loss: 0.096286, mae: 0.299733, mean_q: 3.852210
 94063/100000: episode: 1819, duration: 0.492s, episode steps: 100, steps per second: 203, episode reward: 182.094, mean reward: 1.821 [1.444, 2.893], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-0.391, 10.098], loss: 0.099080, mae: 0.304675, mean_q: 3.876989
[Info] 1-TH LEVEL FOUND: 5.402620315551758, Considering 10/90 traces
 94163/100000: episode: 1820, duration: 4.640s, episode steps: 100, steps per second: 22, episode reward: 188.231, mean reward: 1.882 [1.453, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 1.434 [-0.781, 10.167], loss: 0.085594, mae: 0.297663, mean_q: 3.864819
 94195/100000: episode: 1821, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 92.477, mean reward: 2.890 [1.803, 3.750], mean action: 0.000 [0.000, 0.000], mean observation: 2.076 [-0.573, 10.283], loss: 0.092006, mae: 0.296636, mean_q: 3.853533
 94207/100000: episode: 1822, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 33.886, mean reward: 2.824 [2.046, 3.614], mean action: 0.000 [0.000, 0.000], mean observation: 2.166 [-1.010, 10.100], loss: 0.081189, mae: 0.278221, mean_q: 3.842523
 94227/100000: episode: 1823, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 51.472, mean reward: 2.574 [1.724, 4.290], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.775, 10.263], loss: 0.069424, mae: 0.276729, mean_q: 3.849021
 94258/100000: episode: 1824, duration: 0.155s, episode steps: 31, steps per second: 200, episode reward: 84.400, mean reward: 2.723 [2.081, 3.850], mean action: 0.000 [0.000, 0.000], mean observation: 2.089 [-0.325, 10.498], loss: 0.134814, mae: 0.327803, mean_q: 3.869924
 94286/100000: episode: 1825, duration: 0.155s, episode steps: 28, steps per second: 180, episode reward: 58.060, mean reward: 2.074 [1.508, 3.439], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.035, 10.100], loss: 0.096856, mae: 0.295790, mean_q: 3.874814
 94306/100000: episode: 1826, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 40.050, mean reward: 2.002 [1.532, 2.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.035, 10.181], loss: 0.074474, mae: 0.280866, mean_q: 3.876308
 94312/100000: episode: 1827, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 16.812, mean reward: 2.802 [2.458, 3.353], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.875, 10.100], loss: 0.068552, mae: 0.278601, mean_q: 3.869088
 94344/100000: episode: 1828, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 72.045, mean reward: 2.251 [1.604, 3.747], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.639, 10.347], loss: 0.086528, mae: 0.285351, mean_q: 3.865893
 94373/100000: episode: 1829, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 65.282, mean reward: 2.251 [1.540, 3.680], mean action: 0.000 [0.000, 0.000], mean observation: 2.087 [-0.173, 10.244], loss: 0.096804, mae: 0.312415, mean_q: 3.895866
 94379/100000: episode: 1830, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 15.248, mean reward: 2.541 [2.272, 2.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.246, 10.100], loss: 0.071946, mae: 0.300521, mean_q: 3.921111
 94399/100000: episode: 1831, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 42.993, mean reward: 2.150 [1.507, 3.110], mean action: 0.000 [0.000, 0.000], mean observation: 2.170 [-0.206, 10.170], loss: 0.085067, mae: 0.287612, mean_q: 3.874778
 94431/100000: episode: 1832, duration: 0.163s, episode steps: 32, steps per second: 196, episode reward: 88.293, mean reward: 2.759 [2.204, 4.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.729, 10.414], loss: 0.099915, mae: 0.305277, mean_q: 3.883257
 94447/100000: episode: 1833, duration: 0.084s, episode steps: 16, steps per second: 192, episode reward: 35.740, mean reward: 2.234 [1.943, 2.718], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.633, 10.407], loss: 0.071467, mae: 0.280394, mean_q: 3.867857
 94479/100000: episode: 1834, duration: 0.176s, episode steps: 32, steps per second: 182, episode reward: 97.139, mean reward: 3.036 [2.110, 4.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-0.408, 10.348], loss: 0.093939, mae: 0.303712, mean_q: 3.918374
 94510/100000: episode: 1835, duration: 0.162s, episode steps: 31, steps per second: 191, episode reward: 66.362, mean reward: 2.141 [1.585, 3.814], mean action: 0.000 [0.000, 0.000], mean observation: 2.070 [-1.491, 10.210], loss: 0.100162, mae: 0.301449, mean_q: 3.882125
 94539/100000: episode: 1836, duration: 0.140s, episode steps: 29, steps per second: 208, episode reward: 66.726, mean reward: 2.301 [1.572, 2.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.099 [-0.995, 10.181], loss: 0.096429, mae: 0.312164, mean_q: 3.941297
 94570/100000: episode: 1837, duration: 0.158s, episode steps: 31, steps per second: 197, episode reward: 77.934, mean reward: 2.514 [1.983, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.128, 10.369], loss: 0.116744, mae: 0.344233, mean_q: 3.999305
 94590/100000: episode: 1838, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 52.663, mean reward: 2.633 [1.749, 4.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.234, 10.271], loss: 0.132467, mae: 0.330183, mean_q: 3.955616
 94610/100000: episode: 1839, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 50.966, mean reward: 2.548 [2.063, 3.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.605, 10.379], loss: 0.080593, mae: 0.292420, mean_q: 3.974810
 94620/100000: episode: 1840, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 33.992, mean reward: 3.399 [2.242, 5.150], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.435, 10.100], loss: 0.129618, mae: 0.336423, mean_q: 3.925531
 94651/100000: episode: 1841, duration: 0.163s, episode steps: 31, steps per second: 191, episode reward: 91.512, mean reward: 2.952 [1.659, 5.592], mean action: 0.000 [0.000, 0.000], mean observation: 2.080 [-0.361, 10.257], loss: 0.116444, mae: 0.325796, mean_q: 3.984094
 94683/100000: episode: 1842, duration: 0.182s, episode steps: 32, steps per second: 176, episode reward: 75.876, mean reward: 2.371 [1.622, 3.583], mean action: 0.000 [0.000, 0.000], mean observation: 2.068 [-0.155, 10.199], loss: 0.101261, mae: 0.313692, mean_q: 3.955944
 94703/100000: episode: 1843, duration: 0.111s, episode steps: 20, steps per second: 181, episode reward: 51.872, mean reward: 2.594 [1.716, 4.160], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.035, 10.248], loss: 0.121171, mae: 0.332758, mean_q: 3.991339
 94732/100000: episode: 1844, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 71.530, mean reward: 2.467 [1.479, 5.196], mean action: 0.000 [0.000, 0.000], mean observation: 2.093 [-0.993, 10.169], loss: 0.101454, mae: 0.307833, mean_q: 3.970167
 94752/100000: episode: 1845, duration: 0.097s, episode steps: 20, steps per second: 206, episode reward: 45.133, mean reward: 2.257 [1.691, 3.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.180 [-0.273, 10.244], loss: 0.101813, mae: 0.297926, mean_q: 3.968982
 94784/100000: episode: 1846, duration: 0.160s, episode steps: 32, steps per second: 200, episode reward: 78.239, mean reward: 2.445 [1.453, 4.465], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.357, 10.127], loss: 0.115568, mae: 0.320487, mean_q: 4.021310
 94794/100000: episode: 1847, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 24.388, mean reward: 2.439 [2.164, 2.847], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.356, 10.100], loss: 0.087854, mae: 0.307447, mean_q: 3.983427
 94810/100000: episode: 1848, duration: 0.079s, episode steps: 16, steps per second: 201, episode reward: 39.942, mean reward: 2.496 [1.936, 3.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.691, 10.421], loss: 0.080493, mae: 0.304410, mean_q: 3.994119
 94826/100000: episode: 1849, duration: 0.081s, episode steps: 16, steps per second: 197, episode reward: 38.216, mean reward: 2.389 [1.987, 2.820], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.376], loss: 0.139658, mae: 0.321406, mean_q: 3.974001
 94855/100000: episode: 1850, duration: 0.153s, episode steps: 29, steps per second: 190, episode reward: 64.854, mean reward: 2.236 [1.594, 3.915], mean action: 0.000 [0.000, 0.000], mean observation: 2.098 [-0.453, 10.237], loss: 0.125486, mae: 0.336999, mean_q: 4.001477
 94867/100000: episode: 1851, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 40.843, mean reward: 3.404 [2.226, 4.784], mean action: 0.000 [0.000, 0.000], mean observation: 2.151 [-0.437, 10.100], loss: 0.094008, mae: 0.306759, mean_q: 4.008536
 94896/100000: episode: 1852, duration: 0.152s, episode steps: 29, steps per second: 190, episode reward: 99.681, mean reward: 3.437 [1.685, 6.703], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.720, 10.258], loss: 0.118053, mae: 0.342509, mean_q: 4.043225
 94925/100000: episode: 1853, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 81.652, mean reward: 2.816 [2.010, 4.309], mean action: 0.000 [0.000, 0.000], mean observation: 2.118 [-0.035, 10.460], loss: 0.092567, mae: 0.309272, mean_q: 3.978354
 94945/100000: episode: 1854, duration: 0.107s, episode steps: 20, steps per second: 187, episode reward: 42.096, mean reward: 2.105 [1.581, 2.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.559, 10.125], loss: 0.100407, mae: 0.312089, mean_q: 4.024358
 94957/100000: episode: 1855, duration: 0.070s, episode steps: 12, steps per second: 172, episode reward: 32.501, mean reward: 2.708 [1.793, 4.025], mean action: 0.000 [0.000, 0.000], mean observation: 2.148 [-0.978, 10.100], loss: 0.144563, mae: 0.347619, mean_q: 4.044173
 94985/100000: episode: 1856, duration: 0.147s, episode steps: 28, steps per second: 191, episode reward: 62.832, mean reward: 2.244 [1.804, 3.451], mean action: 0.000 [0.000, 0.000], mean observation: 2.122 [-0.035, 10.442], loss: 0.125786, mae: 0.329976, mean_q: 4.066603
 94995/100000: episode: 1857, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 26.736, mean reward: 2.674 [2.320, 3.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.291, 10.100], loss: 0.176434, mae: 0.368830, mean_q: 4.147539
 95026/100000: episode: 1858, duration: 0.174s, episode steps: 31, steps per second: 178, episode reward: 88.879, mean reward: 2.867 [2.146, 3.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.091 [-0.173, 10.332], loss: 0.127652, mae: 0.340889, mean_q: 4.094789
 95036/100000: episode: 1859, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 30.587, mean reward: 3.059 [2.569, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.471, 10.100], loss: 0.201118, mae: 0.382209, mean_q: 4.087408
 95048/100000: episode: 1860, duration: 0.064s, episode steps: 12, steps per second: 189, episode reward: 29.035, mean reward: 2.420 [2.067, 3.019], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.219, 10.100], loss: 0.086977, mae: 0.309485, mean_q: 4.041021
 95064/100000: episode: 1861, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 35.346, mean reward: 2.209 [1.791, 2.983], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.160, 10.337], loss: 0.130308, mae: 0.337264, mean_q: 4.063150
 95070/100000: episode: 1862, duration: 0.032s, episode steps: 6, steps per second: 185, episode reward: 18.018, mean reward: 3.003 [2.677, 3.517], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.229, 10.100], loss: 0.169095, mae: 0.397866, mean_q: 4.090744
 95076/100000: episode: 1863, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 14.938, mean reward: 2.490 [2.039, 3.973], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.200, 10.100], loss: 0.122155, mae: 0.356136, mean_q: 4.248517
 95086/100000: episode: 1864, duration: 0.054s, episode steps: 10, steps per second: 186, episode reward: 24.989, mean reward: 2.499 [2.210, 3.016], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.252, 10.100], loss: 0.101506, mae: 0.320243, mean_q: 4.049018
 95102/100000: episode: 1865, duration: 0.086s, episode steps: 16, steps per second: 187, episode reward: 37.156, mean reward: 2.322 [1.842, 3.598], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-1.295, 10.252], loss: 0.099338, mae: 0.325258, mean_q: 4.096319
 95133/100000: episode: 1866, duration: 0.154s, episode steps: 31, steps per second: 201, episode reward: 70.178, mean reward: 2.264 [1.761, 3.449], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.409, 10.503], loss: 0.130029, mae: 0.354389, mean_q: 4.093869
 95139/100000: episode: 1867, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 18.408, mean reward: 3.068 [2.394, 3.432], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.426, 10.100], loss: 0.118550, mae: 0.347088, mean_q: 4.116459
 95151/100000: episode: 1868, duration: 0.063s, episode steps: 12, steps per second: 192, episode reward: 32.926, mean reward: 2.744 [2.192, 3.950], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.207, 10.100], loss: 0.118382, mae: 0.320446, mean_q: 4.066558
 95182/100000: episode: 1869, duration: 0.177s, episode steps: 31, steps per second: 175, episode reward: 63.047, mean reward: 2.034 [1.568, 2.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.072 [-0.513, 10.241], loss: 0.117435, mae: 0.343271, mean_q: 4.078044
 95213/100000: episode: 1870, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 62.030, mean reward: 2.001 [1.550, 2.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.075 [-0.477, 10.171], loss: 0.106379, mae: 0.331659, mean_q: 4.084569
 95245/100000: episode: 1871, duration: 0.169s, episode steps: 32, steps per second: 189, episode reward: 76.691, mean reward: 2.397 [1.676, 3.572], mean action: 0.000 [0.000, 0.000], mean observation: 2.062 [-0.879, 10.288], loss: 0.123675, mae: 0.343802, mean_q: 4.134784
 95277/100000: episode: 1872, duration: 0.161s, episode steps: 32, steps per second: 199, episode reward: 98.278, mean reward: 3.071 [2.114, 4.839], mean action: 0.000 [0.000, 0.000], mean observation: 2.073 [-0.987, 10.536], loss: 0.113723, mae: 0.328251, mean_q: 4.111857
 95287/100000: episode: 1873, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 24.012, mean reward: 2.401 [2.103, 2.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.182 [-0.644, 10.100], loss: 0.205610, mae: 0.393192, mean_q: 4.247702
 95307/100000: episode: 1874, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 44.126, mean reward: 2.206 [1.590, 3.124], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.458, 10.355], loss: 0.112043, mae: 0.330394, mean_q: 4.145661
 95323/100000: episode: 1875, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 43.452, mean reward: 2.716 [1.964, 3.852], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.435], loss: 0.101837, mae: 0.328315, mean_q: 4.136080
 95352/100000: episode: 1876, duration: 0.145s, episode steps: 29, steps per second: 201, episode reward: 75.817, mean reward: 2.614 [1.769, 4.604], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.321, 10.285], loss: 0.117953, mae: 0.337830, mean_q: 4.016307
 95383/100000: episode: 1877, duration: 0.159s, episode steps: 31, steps per second: 195, episode reward: 65.928, mean reward: 2.127 [1.623, 3.512], mean action: 0.000 [0.000, 0.000], mean observation: 2.088 [-0.539, 10.247], loss: 0.126094, mae: 0.336032, mean_q: 4.109503
 95403/100000: episode: 1878, duration: 0.103s, episode steps: 20, steps per second: 194, episode reward: 36.222, mean reward: 1.811 [1.498, 2.214], mean action: 0.000 [0.000, 0.000], mean observation: 2.185 [-0.035, 10.179], loss: 0.132033, mae: 0.350897, mean_q: 4.152710
 95415/100000: episode: 1879, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 26.469, mean reward: 2.206 [1.894, 2.482], mean action: 0.000 [0.000, 0.000], mean observation: 2.195 [-0.217, 10.100], loss: 0.118044, mae: 0.341237, mean_q: 4.120555
 95446/100000: episode: 1880, duration: 0.176s, episode steps: 31, steps per second: 176, episode reward: 78.123, mean reward: 2.520 [1.746, 4.268], mean action: 0.000 [0.000, 0.000], mean observation: 2.082 [-0.483, 10.451], loss: 0.106717, mae: 0.331824, mean_q: 4.105280
 95458/100000: episode: 1881, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 39.382, mean reward: 3.282 [1.908, 7.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.170, 10.100], loss: 0.106235, mae: 0.332257, mean_q: 4.175858
 95489/100000: episode: 1882, duration: 0.165s, episode steps: 31, steps per second: 187, episode reward: 80.210, mean reward: 2.587 [2.088, 3.099], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.335, 10.322], loss: 0.135695, mae: 0.336307, mean_q: 4.073200
 95505/100000: episode: 1883, duration: 0.080s, episode steps: 16, steps per second: 200, episode reward: 40.026, mean reward: 2.502 [2.141, 3.092], mean action: 0.000 [0.000, 0.000], mean observation: 2.212 [-1.078, 10.394], loss: 0.132234, mae: 0.335610, mean_q: 4.144584
 95521/100000: episode: 1884, duration: 0.094s, episode steps: 16, steps per second: 169, episode reward: 35.687, mean reward: 2.230 [1.803, 2.993], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.178, 10.333], loss: 0.166882, mae: 0.354929, mean_q: 4.213072
 95552/100000: episode: 1885, duration: 0.147s, episode steps: 31, steps per second: 210, episode reward: 80.106, mean reward: 2.584 [1.990, 4.846], mean action: 0.000 [0.000, 0.000], mean observation: 2.077 [-0.774, 10.270], loss: 0.128024, mae: 0.349754, mean_q: 4.177368
 95568/100000: episode: 1886, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 33.744, mean reward: 2.109 [1.768, 2.427], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.035, 10.310], loss: 0.127547, mae: 0.363552, mean_q: 4.155233
 95578/100000: episode: 1887, duration: 0.054s, episode steps: 10, steps per second: 183, episode reward: 32.270, mean reward: 3.227 [2.160, 4.234], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.364, 10.100], loss: 0.088707, mae: 0.316778, mean_q: 4.134413
 95610/100000: episode: 1888, duration: 0.156s, episode steps: 32, steps per second: 205, episode reward: 76.755, mean reward: 2.399 [1.949, 3.281], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.568, 10.330], loss: 0.104904, mae: 0.324699, mean_q: 4.139320
 95616/100000: episode: 1889, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 15.429, mean reward: 2.571 [2.383, 3.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.257, 10.100], loss: 0.128683, mae: 0.347766, mean_q: 4.187956
 95647/100000: episode: 1890, duration: 0.152s, episode steps: 31, steps per second: 205, episode reward: 73.952, mean reward: 2.386 [1.797, 3.391], mean action: 0.000 [0.000, 0.000], mean observation: 2.074 [-0.406, 10.329], loss: 0.135779, mae: 0.354332, mean_q: 4.219059
 95678/100000: episode: 1891, duration: 0.151s, episode steps: 31, steps per second: 205, episode reward: 75.795, mean reward: 2.445 [1.908, 4.269], mean action: 0.000 [0.000, 0.000], mean observation: 2.084 [-0.154, 10.206], loss: 0.111325, mae: 0.334261, mean_q: 4.119776
 95709/100000: episode: 1892, duration: 0.169s, episode steps: 31, steps per second: 184, episode reward: 70.000, mean reward: 2.258 [1.561, 3.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.057 [-0.713, 10.100], loss: 0.112790, mae: 0.332509, mean_q: 4.096155
 95721/100000: episode: 1893, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 36.008, mean reward: 3.001 [2.277, 3.708], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-1.052, 10.100], loss: 0.139930, mae: 0.354089, mean_q: 4.140540
 95733/100000: episode: 1894, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 31.269, mean reward: 2.606 [2.111, 3.870], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.469, 10.100], loss: 0.124193, mae: 0.341055, mean_q: 4.066934
 95745/100000: episode: 1895, duration: 0.060s, episode steps: 12, steps per second: 200, episode reward: 32.227, mean reward: 2.686 [2.010, 4.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.393, 10.100], loss: 0.116119, mae: 0.345968, mean_q: 4.180839
 95755/100000: episode: 1896, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 41.642, mean reward: 4.164 [2.592, 7.017], mean action: 0.000 [0.000, 0.000], mean observation: 2.146 [-0.429, 10.100], loss: 0.185177, mae: 0.351602, mean_q: 4.157086
 95787/100000: episode: 1897, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 62.084, mean reward: 1.940 [1.463, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.058 [-0.257, 10.123], loss: 0.136879, mae: 0.363879, mean_q: 4.185004
 95818/100000: episode: 1898, duration: 0.165s, episode steps: 31, steps per second: 187, episode reward: 65.710, mean reward: 2.120 [1.740, 2.875], mean action: 0.000 [0.000, 0.000], mean observation: 2.081 [-0.602, 10.324], loss: 0.147563, mae: 0.342698, mean_q: 4.164227
 95824/100000: episode: 1899, duration: 0.033s, episode steps: 6, steps per second: 184, episode reward: 18.239, mean reward: 3.040 [2.746, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.222, 10.100], loss: 0.146108, mae: 0.375577, mean_q: 3.997503
 95844/100000: episode: 1900, duration: 0.124s, episode steps: 20, steps per second: 162, episode reward: 45.070, mean reward: 2.254 [1.946, 3.228], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.035, 10.365], loss: 0.126561, mae: 0.347548, mean_q: 4.187732
 95850/100000: episode: 1901, duration: 0.037s, episode steps: 6, steps per second: 161, episode reward: 14.249, mean reward: 2.375 [2.192, 2.667], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.126, 10.100], loss: 0.127376, mae: 0.349082, mean_q: 4.232425
 95881/100000: episode: 1902, duration: 0.180s, episode steps: 31, steps per second: 172, episode reward: 76.499, mean reward: 2.468 [2.101, 3.154], mean action: 0.000 [0.000, 0.000], mean observation: 2.090 [-0.214, 10.451], loss: 0.108971, mae: 0.334664, mean_q: 4.165219
 95913/100000: episode: 1903, duration: 0.161s, episode steps: 32, steps per second: 198, episode reward: 87.854, mean reward: 2.745 [1.812, 3.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.060 [-0.245, 10.264], loss: 0.138673, mae: 0.362330, mean_q: 4.192610
 95925/100000: episode: 1904, duration: 0.060s, episode steps: 12, steps per second: 199, episode reward: 25.578, mean reward: 2.131 [1.886, 2.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.208, 10.100], loss: 0.091730, mae: 0.311829, mean_q: 4.158915
 95954/100000: episode: 1905, duration: 0.149s, episode steps: 29, steps per second: 195, episode reward: 56.411, mean reward: 1.945 [1.590, 3.216], mean action: 0.000 [0.000, 0.000], mean observation: 2.100 [-0.035, 10.242], loss: 0.090395, mae: 0.323608, mean_q: 4.170539
 95985/100000: episode: 1906, duration: 0.173s, episode steps: 31, steps per second: 179, episode reward: 86.025, mean reward: 2.775 [1.947, 4.162], mean action: 0.000 [0.000, 0.000], mean observation: 2.069 [-0.834, 10.488], loss: 0.118675, mae: 0.339676, mean_q: 4.189888
 95991/100000: episode: 1907, duration: 0.038s, episode steps: 6, steps per second: 159, episode reward: 19.527, mean reward: 3.254 [2.454, 4.683], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.264, 10.100], loss: 0.105641, mae: 0.316711, mean_q: 4.157211
 96001/100000: episode: 1908, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 25.307, mean reward: 2.531 [2.096, 3.133], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.528, 10.100], loss: 0.128750, mae: 0.331906, mean_q: 4.236090
 96033/100000: episode: 1909, duration: 0.171s, episode steps: 32, steps per second: 188, episode reward: 76.167, mean reward: 2.380 [1.691, 4.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.065 [-2.070, 10.230], loss: 0.112193, mae: 0.328730, mean_q: 4.178869
[Info] 2-TH LEVEL FOUND: 6.31270170211792, Considering 10/90 traces
 96045/100000: episode: 1910, duration: 4.373s, episode steps: 12, steps per second: 3, episode reward: 26.316, mean reward: 2.193 [1.914, 2.436], mean action: 0.000 [0.000, 0.000], mean observation: 2.183 [-0.333, 10.100], loss: 0.095984, mae: 0.305969, mean_q: 4.176497
 96059/100000: episode: 1911, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 42.136, mean reward: 3.010 [2.528, 4.114], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.513, 10.411], loss: 0.106394, mae: 0.329071, mean_q: 4.258988
 96078/100000: episode: 1912, duration: 0.096s, episode steps: 19, steps per second: 199, episode reward: 56.868, mean reward: 2.993 [2.171, 3.900], mean action: 0.000 [0.000, 0.000], mean observation: 2.178 [-0.180, 10.349], loss: 0.127486, mae: 0.349125, mean_q: 4.232276
 96097/100000: episode: 1913, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 89.041, mean reward: 4.686 [3.211, 7.021], mean action: 0.000 [0.000, 0.000], mean observation: 2.186 [-0.825, 10.494], loss: 0.136564, mae: 0.349740, mean_q: 4.287377
 96114/100000: episode: 1914, duration: 0.090s, episode steps: 17, steps per second: 188, episode reward: 66.934, mean reward: 3.937 [2.886, 5.623], mean action: 0.000 [0.000, 0.000], mean observation: 2.215 [-0.488, 10.533], loss: 0.113771, mae: 0.347278, mean_q: 4.176354
 96136/100000: episode: 1915, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 64.667, mean reward: 2.939 [2.042, 4.624], mean action: 0.000 [0.000, 0.000], mean observation: 2.153 [-0.242, 10.347], loss: 0.133677, mae: 0.355445, mean_q: 4.226493
 96151/100000: episode: 1916, duration: 0.073s, episode steps: 15, steps per second: 204, episode reward: 58.400, mean reward: 3.893 [3.052, 7.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.221 [-0.086, 10.527], loss: 0.112028, mae: 0.347210, mean_q: 4.263940
 96157/100000: episode: 1917, duration: 0.032s, episode steps: 6, steps per second: 189, episode reward: 18.105, mean reward: 3.018 [2.682, 3.446], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.278, 10.100], loss: 0.084616, mae: 0.312072, mean_q: 4.116047
 96176/100000: episode: 1918, duration: 0.096s, episode steps: 19, steps per second: 199, episode reward: 41.726, mean reward: 2.196 [1.719, 3.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.809, 10.221], loss: 0.112815, mae: 0.326558, mean_q: 4.204011
 96194/100000: episode: 1919, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 48.018, mean reward: 2.668 [2.113, 3.651], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.690, 10.345], loss: 0.132750, mae: 0.340089, mean_q: 4.193577
 96218/100000: episode: 1920, duration: 0.115s, episode steps: 24, steps per second: 209, episode reward: 58.705, mean reward: 2.446 [1.543, 3.709], mean action: 0.000 [0.000, 0.000], mean observation: 2.136 [-0.063, 10.210], loss: 0.150386, mae: 0.368278, mean_q: 4.224148
 96232/100000: episode: 1921, duration: 0.088s, episode steps: 14, steps per second: 159, episode reward: 38.501, mean reward: 2.750 [2.303, 3.393], mean action: 0.000 [0.000, 0.000], mean observation: 2.246 [-0.035, 10.426], loss: 0.103699, mae: 0.326408, mean_q: 4.299799
 96251/100000: episode: 1922, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 87.538, mean reward: 4.607 [2.949, 7.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.592], loss: 0.162291, mae: 0.378258, mean_q: 4.282790
 96273/100000: episode: 1923, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 64.551, mean reward: 2.934 [1.986, 6.735], mean action: 0.000 [0.000, 0.000], mean observation: 2.181 [-0.061, 10.347], loss: 0.123451, mae: 0.349292, mean_q: 4.186994
 96287/100000: episode: 1924, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 50.019, mean reward: 3.573 [2.845, 4.757], mean action: 0.000 [0.000, 0.000], mean observation: 2.233 [-0.315, 10.518], loss: 0.152332, mae: 0.367847, mean_q: 4.274682
 96306/100000: episode: 1925, duration: 0.097s, episode steps: 19, steps per second: 197, episode reward: 53.905, mean reward: 2.837 [1.890, 3.967], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.385, 10.309], loss: 0.148418, mae: 0.376977, mean_q: 4.247008
 96325/100000: episode: 1926, duration: 0.097s, episode steps: 19, steps per second: 195, episode reward: 59.683, mean reward: 3.141 [2.116, 5.851], mean action: 0.000 [0.000, 0.000], mean observation: 2.223 [-0.035, 10.401], loss: 0.140892, mae: 0.371252, mean_q: 4.332501
 96343/100000: episode: 1927, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 49.342, mean reward: 2.741 [2.069, 3.840], mean action: 0.000 [0.000, 0.000], mean observation: 2.202 [-0.539, 10.538], loss: 0.160562, mae: 0.394529, mean_q: 4.300231
 96349/100000: episode: 1928, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 24.682, mean reward: 4.114 [3.492, 5.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.344, 10.100], loss: 0.105617, mae: 0.332975, mean_q: 4.318351
 96373/100000: episode: 1929, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 87.785, mean reward: 3.658 [2.070, 9.571], mean action: 0.000 [0.000, 0.000], mean observation: 2.154 [-0.192, 10.400], loss: 0.129706, mae: 0.373755, mean_q: 4.320958
 96388/100000: episode: 1930, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 39.459, mean reward: 2.631 [2.111, 3.770], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.607, 10.437], loss: 0.173692, mae: 0.397582, mean_q: 4.325739
 96407/100000: episode: 1931, duration: 0.096s, episode steps: 19, steps per second: 199, episode reward: 56.911, mean reward: 2.995 [1.689, 3.977], mean action: 0.000 [0.000, 0.000], mean observation: 2.204 [-0.179, 10.380], loss: 0.176589, mae: 0.447345, mean_q: 4.337038
 96425/100000: episode: 1932, duration: 0.099s, episode steps: 18, steps per second: 181, episode reward: 40.986, mean reward: 2.277 [1.515, 3.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.200 [-0.035, 10.176], loss: 0.181422, mae: 0.410718, mean_q: 4.391221
 96444/100000: episode: 1933, duration: 0.104s, episode steps: 19, steps per second: 183, episode reward: 73.037, mean reward: 3.844 [2.839, 5.051], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.035, 10.488], loss: 0.147044, mae: 0.375165, mean_q: 4.388625
 96463/100000: episode: 1934, duration: 0.101s, episode steps: 19, steps per second: 188, episode reward: 58.594, mean reward: 3.084 [2.194, 5.117], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.312, 10.364], loss: 0.143339, mae: 0.382743, mean_q: 4.409289
 96485/100000: episode: 1935, duration: 0.110s, episode steps: 22, steps per second: 199, episode reward: 84.310, mean reward: 3.832 [2.987, 5.794], mean action: 0.000 [0.000, 0.000], mean observation: 2.163 [-0.189, 10.449], loss: 0.242395, mae: 0.450099, mean_q: 4.333122
 96509/100000: episode: 1936, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 88.413, mean reward: 3.684 [2.497, 6.426], mean action: 0.000 [0.000, 0.000], mean observation: 2.144 [-0.373, 10.425], loss: 0.174775, mae: 0.412837, mean_q: 4.445562
 96523/100000: episode: 1937, duration: 0.074s, episode steps: 14, steps per second: 190, episode reward: 53.369, mean reward: 3.812 [2.972, 5.445], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.155, 10.428], loss: 0.119386, mae: 0.355637, mean_q: 4.335221
 96537/100000: episode: 1938, duration: 0.069s, episode steps: 14, steps per second: 202, episode reward: 40.536, mean reward: 2.895 [2.362, 3.229], mean action: 0.000 [0.000, 0.000], mean observation: 2.242 [-0.035, 10.454], loss: 0.189867, mae: 0.396962, mean_q: 4.483531
 96556/100000: episode: 1939, duration: 0.096s, episode steps: 19, steps per second: 198, episode reward: 60.078, mean reward: 3.162 [2.619, 4.448], mean action: 0.000 [0.000, 0.000], mean observation: 2.207 [-0.035, 10.494], loss: 0.158660, mae: 0.392508, mean_q: 4.386564
 96575/100000: episode: 1940, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 54.608, mean reward: 2.874 [1.876, 4.380], mean action: 0.000 [0.000, 0.000], mean observation: 2.190 [-1.029, 10.333], loss: 0.150457, mae: 0.377355, mean_q: 4.354978
 96590/100000: episode: 1941, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 65.180, mean reward: 4.345 [2.701, 6.295], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.527], loss: 0.168454, mae: 0.389139, mean_q: 4.384678
 96607/100000: episode: 1942, duration: 0.093s, episode steps: 17, steps per second: 184, episode reward: 57.463, mean reward: 3.380 [2.842, 4.677], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.222, 10.500], loss: 0.152815, mae: 0.369138, mean_q: 4.481738
 96629/100000: episode: 1943, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 57.478, mean reward: 2.613 [1.815, 4.113], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.197, 10.289], loss: 0.183221, mae: 0.406240, mean_q: 4.477006
 96647/100000: episode: 1944, duration: 0.096s, episode steps: 18, steps per second: 188, episode reward: 44.532, mean reward: 2.474 [1.909, 3.223], mean action: 0.000 [0.000, 0.000], mean observation: 2.208 [-0.035, 10.382], loss: 0.144231, mae: 0.397550, mean_q: 4.519166
 96653/100000: episode: 1945, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: 18.224, mean reward: 3.037 [2.726, 3.894], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.391, 10.100], loss: 0.120502, mae: 0.324804, mean_q: 4.409558
 96670/100000: episode: 1946, duration: 0.086s, episode steps: 17, steps per second: 197, episode reward: 49.997, mean reward: 2.941 [1.632, 4.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.201, 10.243], loss: 0.137161, mae: 0.347573, mean_q: 4.404137
 96676/100000: episode: 1947, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 18.426, mean reward: 3.071 [2.635, 3.643], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.328, 10.100], loss: 0.147396, mae: 0.409017, mean_q: 4.541967
 96700/100000: episode: 1948, duration: 0.127s, episode steps: 24, steps per second: 188, episode reward: 84.913, mean reward: 3.538 [2.422, 5.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.135 [-0.403, 10.475], loss: 0.155147, mae: 0.389622, mean_q: 4.549512
 96719/100000: episode: 1949, duration: 0.099s, episode steps: 19, steps per second: 192, episode reward: 61.298, mean reward: 3.226 [2.199, 5.205], mean action: 0.000 [0.000, 0.000], mean observation: 2.188 [-0.320, 10.347], loss: 0.179354, mae: 0.412265, mean_q: 4.519159
 96725/100000: episode: 1950, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 49.029, mean reward: 8.171 [4.662, 12.135], mean action: 0.000 [0.000, 0.000], mean observation: 2.158 [-0.474, 10.100], loss: 0.134636, mae: 0.360242, mean_q: 4.359054
 96739/100000: episode: 1951, duration: 0.069s, episode steps: 14, steps per second: 203, episode reward: 42.308, mean reward: 3.022 [2.488, 3.760], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.126, 10.471], loss: 0.166568, mae: 0.410325, mean_q: 4.474715
 96763/100000: episode: 1952, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 73.360, mean reward: 3.057 [2.373, 4.118], mean action: 0.000 [0.000, 0.000], mean observation: 2.157 [-0.767, 10.443], loss: 0.164616, mae: 0.399791, mean_q: 4.593119
 96777/100000: episode: 1953, duration: 0.072s, episode steps: 14, steps per second: 194, episode reward: 41.247, mean reward: 2.946 [2.467, 3.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-0.035, 10.411], loss: 0.124913, mae: 0.357709, mean_q: 4.489003
 96791/100000: episode: 1954, duration: 0.081s, episode steps: 14, steps per second: 172, episode reward: 52.258, mean reward: 3.733 [2.575, 4.916], mean action: 0.000 [0.000, 0.000], mean observation: 2.266 [-0.035, 10.593], loss: 0.235278, mae: 0.403816, mean_q: 4.568803
 96813/100000: episode: 1955, duration: 0.124s, episode steps: 22, steps per second: 177, episode reward: 77.404, mean reward: 3.518 [2.640, 4.368], mean action: 0.000 [0.000, 0.000], mean observation: 2.171 [-0.035, 10.486], loss: 0.204579, mae: 0.381823, mean_q: 4.470696
 96827/100000: episode: 1956, duration: 0.068s, episode steps: 14, steps per second: 205, episode reward: 53.986, mean reward: 3.856 [2.825, 6.089], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.047, 10.474], loss: 0.222745, mae: 0.439760, mean_q: 4.622077
 96849/100000: episode: 1957, duration: 0.112s, episode steps: 22, steps per second: 197, episode reward: 70.653, mean reward: 3.211 [2.624, 4.177], mean action: 0.000 [0.000, 0.000], mean observation: 2.173 [-0.404, 10.507], loss: 0.159318, mae: 0.393756, mean_q: 4.466233
 96855/100000: episode: 1958, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 27.763, mean reward: 4.627 [3.550, 6.649], mean action: 0.000 [0.000, 0.000], mean observation: 2.174 [-0.417, 10.100], loss: 0.226899, mae: 0.471329, mean_q: 4.517693
 96879/100000: episode: 1959, duration: 0.140s, episode steps: 24, steps per second: 172, episode reward: 137.729, mean reward: 5.739 [2.722, 10.897], mean action: 0.000 [0.000, 0.000], mean observation: 2.156 [-0.415, 10.546], loss: 0.211626, mae: 0.407348, mean_q: 4.626261
 96893/100000: episode: 1960, duration: 0.075s, episode steps: 14, steps per second: 186, episode reward: 41.419, mean reward: 2.958 [2.411, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.035, 10.450], loss: 0.383833, mae: 0.490409, mean_q: 4.745114
 96911/100000: episode: 1961, duration: 0.088s, episode steps: 18, steps per second: 206, episode reward: 52.052, mean reward: 2.892 [2.243, 4.050], mean action: 0.000 [0.000, 0.000], mean observation: 2.214 [-0.035, 10.386], loss: 0.169716, mae: 0.410894, mean_q: 4.558515
 96933/100000: episode: 1962, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 76.457, mean reward: 3.475 [2.631, 4.366], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.543], loss: 0.241035, mae: 0.469735, mean_q: 4.651649
 96947/100000: episode: 1963, duration: 0.075s, episode steps: 14, steps per second: 185, episode reward: 52.095, mean reward: 3.721 [2.266, 7.075], mean action: 0.000 [0.000, 0.000], mean observation: 2.237 [-0.742, 10.500], loss: 0.231280, mae: 0.435483, mean_q: 4.733810
 96961/100000: episode: 1964, duration: 0.072s, episode steps: 14, steps per second: 196, episode reward: 35.476, mean reward: 2.534 [1.969, 3.726], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.345], loss: 0.193040, mae: 0.426091, mean_q: 4.638302
 96967/100000: episode: 1965, duration: 0.039s, episode steps: 6, steps per second: 152, episode reward: 26.930, mean reward: 4.488 [3.335, 5.537], mean action: 0.000 [0.000, 0.000], mean observation: 2.189 [-0.348, 10.100], loss: 0.164844, mae: 0.433115, mean_q: 4.778080
 96973/100000: episode: 1966, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 21.319, mean reward: 3.553 [2.899, 4.833], mean action: 0.000 [0.000, 0.000], mean observation: 2.177 [-0.430, 10.100], loss: 0.312004, mae: 0.490250, mean_q: 4.553532
 96979/100000: episode: 1967, duration: 0.033s, episode steps: 6, steps per second: 183, episode reward: 27.256, mean reward: 4.543 [3.323, 6.261], mean action: 0.000 [0.000, 0.000], mean observation: 2.168 [-0.386, 10.100], loss: 0.202976, mae: 0.422874, mean_q: 4.683918
 96997/100000: episode: 1968, duration: 0.087s, episode steps: 18, steps per second: 208, episode reward: 82.585, mean reward: 4.588 [3.314, 5.883], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.157, 10.451], loss: 0.296464, mae: 0.454969, mean_q: 4.704453
 97003/100000: episode: 1969, duration: 0.032s, episode steps: 6, steps per second: 188, episode reward: 18.019, mean reward: 3.003 [2.436, 3.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.194 [-1.106, 10.100], loss: 0.376683, mae: 0.512382, mean_q: 4.771796
 97021/100000: episode: 1970, duration: 0.088s, episode steps: 18, steps per second: 206, episode reward: 57.536, mean reward: 3.196 [2.013, 4.691], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.319, 10.472], loss: 0.180023, mae: 0.416768, mean_q: 4.727749
 97039/100000: episode: 1971, duration: 0.104s, episode steps: 18, steps per second: 172, episode reward: 46.284, mean reward: 2.571 [1.872, 3.460], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.093, 10.302], loss: 0.150823, mae: 0.405773, mean_q: 4.672223
 97057/100000: episode: 1972, duration: 0.089s, episode steps: 18, steps per second: 203, episode reward: 71.538, mean reward: 3.974 [2.360, 8.488], mean action: 0.000 [0.000, 0.000], mean observation: 2.198 [-0.035, 10.477], loss: 0.182715, mae: 0.396142, mean_q: 4.598700
 97063/100000: episode: 1973, duration: 0.032s, episode steps: 6, steps per second: 187, episode reward: 22.839, mean reward: 3.806 [3.542, 4.147], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.340, 10.100], loss: 0.275929, mae: 0.474171, mean_q: 4.801425
 97081/100000: episode: 1974, duration: 0.092s, episode steps: 18, steps per second: 196, episode reward: 76.086, mean reward: 4.227 [3.119, 6.681], mean action: 0.000 [0.000, 0.000], mean observation: 2.201 [-0.627, 10.576], loss: 0.180381, mae: 0.406375, mean_q: 4.679362
 97100/100000: episode: 1975, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 47.157, mean reward: 2.482 [1.821, 3.159], mean action: 0.000 [0.000, 0.000], mean observation: 2.179 [-0.417, 10.273], loss: 0.269074, mae: 0.439720, mean_q: 4.749277
 97124/100000: episode: 1976, duration: 0.122s, episode steps: 24, steps per second: 197, episode reward: 73.707, mean reward: 3.071 [1.909, 11.107], mean action: 0.000 [0.000, 0.000], mean observation: 2.147 [-2.001, 10.453], loss: 0.272569, mae: 0.486100, mean_q: 4.770792
 97142/100000: episode: 1977, duration: 0.093s, episode steps: 18, steps per second: 193, episode reward: 57.570, mean reward: 3.198 [2.718, 3.918], mean action: 0.000 [0.000, 0.000], mean observation: 2.205 [-0.035, 10.472], loss: 0.304054, mae: 0.482041, mean_q: 4.765803
 97161/100000: episode: 1978, duration: 0.092s, episode steps: 19, steps per second: 207, episode reward: 46.903, mean reward: 2.469 [1.739, 3.200], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.412], loss: 0.315819, mae: 0.466831, mean_q: 4.730278
 97176/100000: episode: 1979, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 44.407, mean reward: 2.960 [2.476, 3.907], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.447], loss: 0.298627, mae: 0.491850, mean_q: 4.814711
 97195/100000: episode: 1980, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 62.278, mean reward: 3.278 [2.304, 4.340], mean action: 0.000 [0.000, 0.000], mean observation: 2.193 [-0.692, 10.470], loss: 0.204879, mae: 0.430381, mean_q: 4.781439
 97213/100000: episode: 1981, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 96.762, mean reward: 5.376 [3.438, 16.293], mean action: 0.000 [0.000, 0.000], mean observation: 2.219 [-0.480, 10.622], loss: 0.203275, mae: 0.445415, mean_q: 4.821915
 97219/100000: episode: 1982, duration: 0.035s, episode steps: 6, steps per second: 170, episode reward: 25.822, mean reward: 4.304 [3.258, 6.675], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.776, 10.100], loss: 0.169288, mae: 0.423978, mean_q: 4.712488
 97241/100000: episode: 1983, duration: 0.109s, episode steps: 22, steps per second: 202, episode reward: 67.881, mean reward: 3.086 [2.623, 3.786], mean action: 0.000 [0.000, 0.000], mean observation: 2.176 [-0.035, 10.499], loss: 0.224165, mae: 0.443822, mean_q: 4.768399
 97263/100000: episode: 1984, duration: 0.115s, episode steps: 22, steps per second: 192, episode reward: 76.229, mean reward: 3.465 [2.594, 5.723], mean action: 0.000 [0.000, 0.000], mean observation: 2.172 [-0.557, 10.550], loss: 0.276797, mae: 0.444533, mean_q: 4.708834
 97278/100000: episode: 1985, duration: 0.077s, episode steps: 15, steps per second: 194, episode reward: 78.099, mean reward: 5.207 [2.674, 12.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.238 [-0.035, 10.552], loss: 0.471611, mae: 0.525584, mean_q: 4.860370
 97297/100000: episode: 1986, duration: 0.092s, episode steps: 19, steps per second: 207, episode reward: 54.191, mean reward: 2.852 [2.382, 3.480], mean action: 0.000 [0.000, 0.000], mean observation: 2.206 [-0.486, 10.429], loss: 0.177798, mae: 0.428064, mean_q: 4.743805
 97321/100000: episode: 1987, duration: 0.119s, episode steps: 24, steps per second: 202, episode reward: 72.655, mean reward: 3.027 [1.957, 5.652], mean action: 0.000 [0.000, 0.000], mean observation: 2.159 [-0.035, 10.354], loss: 0.225730, mae: 0.439114, mean_q: 4.790421
 97343/100000: episode: 1988, duration: 0.126s, episode steps: 22, steps per second: 174, episode reward: 79.126, mean reward: 3.597 [2.451, 6.780], mean action: 0.000 [0.000, 0.000], mean observation: 2.162 [-0.311, 10.488], loss: 0.372595, mae: 0.530125, mean_q: 4.893258
 97358/100000: episode: 1989, duration: 0.075s, episode steps: 15, steps per second: 201, episode reward: 46.119, mean reward: 3.075 [2.561, 4.097], mean action: 0.000 [0.000, 0.000], mean observation: 2.243 [-0.355, 10.547], loss: 0.421410, mae: 0.539974, mean_q: 4.930717
 97372/100000: episode: 1990, duration: 0.073s, episode steps: 14, steps per second: 192, episode reward: 39.919, mean reward: 2.851 [2.495, 3.256], mean action: 0.000 [0.000, 0.000], mean observation: 2.244 [-0.035, 10.461], loss: 0.263468, mae: 0.451623, mean_q: 4.832006
 97378/100000: episode: 1991, duration: 0.039s, episode steps: 6, steps per second: 155, episode reward: 21.899, mean reward: 3.650 [3.296, 3.954], mean action: 0.000 [0.000, 0.000], mean observation: 2.192 [-0.388, 10.100], loss: 0.257093, mae: 0.462645, mean_q: 5.027311
 97392/100000: episode: 1992, duration: 0.072s, episode steps: 14, steps per second: 193, episode reward: 55.477, mean reward: 3.963 [2.517, 5.926], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.035, 10.450], loss: 0.190749, mae: 0.439706, mean_q: 4.865631
 97407/100000: episode: 1993, duration: 0.081s, episode steps: 15, steps per second: 186, episode reward: 47.953, mean reward: 3.197 [2.270, 6.562], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.679, 10.417], loss: 0.276680, mae: 0.474941, mean_q: 4.901716
 97421/100000: episode: 1994, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 52.717, mean reward: 3.766 [2.733, 5.457], mean action: 0.000 [0.000, 0.000], mean observation: 2.245 [-0.035, 10.608], loss: 0.227746, mae: 0.452279, mean_q: 4.940823
 97440/100000: episode: 1995, duration: 0.105s, episode steps: 19, steps per second: 180, episode reward: 47.834, mean reward: 2.518 [1.532, 4.497], mean action: 0.000 [0.000, 0.000], mean observation: 2.184 [-0.035, 10.402], loss: 0.232971, mae: 0.450035, mean_q: 4.949225
 97446/100000: episode: 1996, duration: 0.036s, episode steps: 6, steps per second: 167, episode reward: 19.612, mean reward: 3.269 [2.807, 3.899], mean action: 0.000 [0.000, 0.000], mean observation: 2.175 [-0.459, 10.100], loss: 0.340134, mae: 0.524634, mean_q: 5.148173
 97463/100000: episode: 1997, duration: 0.087s, episode steps: 17, steps per second: 194, episode reward: 52.329, mean reward: 3.078 [2.269, 4.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.226 [-0.035, 10.490], loss: 0.293810, mae: 0.517344, mean_q: 4.895690
 97482/100000: episode: 1998, duration: 0.102s, episode steps: 19, steps per second: 186, episode reward: 58.883, mean reward: 3.099 [2.073, 4.441], mean action: 0.000 [0.000, 0.000], mean observation: 2.191 [-0.809, 10.349], loss: 0.441633, mae: 0.517455, mean_q: 5.010407
 97501/100000: episode: 1999, duration: 0.106s, episode steps: 19, steps per second: 179, episode reward: 62.476, mean reward: 3.288 [2.302, 4.986], mean action: 0.000 [0.000, 0.000], mean observation: 2.203 [-0.035, 10.506], loss: 0.268086, mae: 0.484663, mean_q: 5.009871
[Info] 3-TH LEVEL FOUND: 7.825524806976318, Considering 10/90 traces
 97518/100000: episode: 2000, duration: 4.276s, episode steps: 17, steps per second: 4, episode reward: 43.343, mean reward: 2.550 [1.714, 5.338], mean action: 0.000 [0.000, 0.000], mean observation: 2.217 [-0.035, 10.313], loss: 0.343751, mae: 0.491913, mean_q: 5.001534
 97527/100000: episode: 2001, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 52.660, mean reward: 5.851 [4.108, 7.558], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-1.242, 10.574], loss: 0.440383, mae: 0.573346, mean_q: 5.020820
 97535/100000: episode: 2002, duration: 0.050s, episode steps: 8, steps per second: 161, episode reward: 50.522, mean reward: 6.315 [3.556, 13.653], mean action: 0.000 [0.000, 0.000], mean observation: 2.305 [-1.414, 10.688], loss: 0.249150, mae: 0.472004, mean_q: 4.935104
 97545/100000: episode: 2003, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 34.858, mean reward: 3.486 [2.792, 4.265], mean action: 0.000 [0.000, 0.000], mean observation: 2.274 [-0.231, 10.456], loss: 0.588751, mae: 0.610544, mean_q: 5.164180
 97552/100000: episode: 2004, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 25.157, mean reward: 3.594 [3.237, 3.933], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.501], loss: 0.153421, mae: 0.407573, mean_q: 4.857738
 97562/100000: episode: 2005, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 38.182, mean reward: 3.818 [3.216, 4.699], mean action: 0.000 [0.000, 0.000], mean observation: 2.301 [-0.035, 10.584], loss: 0.224492, mae: 0.486638, mean_q: 4.973275
 97575/100000: episode: 2006, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 69.978, mean reward: 5.383 [4.375, 7.132], mean action: 0.000 [0.000, 0.000], mean observation: 2.261 [-0.035, 10.646], loss: 0.231810, mae: 0.440914, mean_q: 5.006622
 97584/100000: episode: 2007, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 44.490, mean reward: 4.943 [3.118, 6.034], mean action: 0.000 [0.000, 0.000], mean observation: 2.304 [-0.308, 10.624], loss: 0.499896, mae: 0.596481, mean_q: 4.976928
 97591/100000: episode: 2008, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 30.149, mean reward: 4.307 [3.426, 5.917], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.435, 10.626], loss: 0.214450, mae: 0.469449, mean_q: 5.118556
 97600/100000: episode: 2009, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 42.618, mean reward: 4.735 [3.231, 6.314], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-0.035, 10.543], loss: 0.181405, mae: 0.422989, mean_q: 4.904657
 97607/100000: episode: 2010, duration: 0.037s, episode steps: 7, steps per second: 187, episode reward: 33.691, mean reward: 4.813 [3.322, 6.901], mean action: 0.000 [0.000, 0.000], mean observation: 2.313 [-0.259, 10.642], loss: 0.271550, mae: 0.462894, mean_q: 5.025047
 97620/100000: episode: 2011, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 62.631, mean reward: 4.818 [3.878, 8.009], mean action: 0.000 [0.000, 0.000], mean observation: 2.236 [-0.035, 10.559], loss: 0.576149, mae: 0.601998, mean_q: 5.018901
 97629/100000: episode: 2012, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 57.236, mean reward: 6.360 [4.053, 12.039], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.262, 10.662], loss: 0.192573, mae: 0.435914, mean_q: 5.068084
 97638/100000: episode: 2013, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 53.198, mean reward: 5.911 [4.104, 8.644], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.035, 10.599], loss: 0.279180, mae: 0.446055, mean_q: 4.933435
 97651/100000: episode: 2014, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 114.219, mean reward: 8.786 [4.195, 20.838], mean action: 0.000 [0.000, 0.000], mean observation: 2.265 [-0.035, 10.637], loss: 0.316361, mae: 0.490624, mean_q: 5.033376
 97664/100000: episode: 2015, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 42.832, mean reward: 3.295 [2.136, 4.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.240 [-0.035, 10.406], loss: 0.297615, mae: 0.506938, mean_q: 5.121180
 97681/100000: episode: 2016, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 100.051, mean reward: 5.885 [2.933, 8.710], mean action: 0.000 [0.000, 0.000], mean observation: 2.220 [-0.075, 10.595], loss: 0.550103, mae: 0.550729, mean_q: 5.155826
 97688/100000: episode: 2017, duration: 0.036s, episode steps: 7, steps per second: 192, episode reward: 38.306, mean reward: 5.472 [4.509, 7.678], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.035, 10.445], loss: 1.140336, mae: 0.797572, mean_q: 5.299369
 97705/100000: episode: 2018, duration: 0.089s, episode steps: 17, steps per second: 190, episode reward: 94.565, mean reward: 5.563 [3.388, 11.955], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.046, 10.543], loss: 0.469458, mae: 0.670989, mean_q: 4.930286
 97715/100000: episode: 2019, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 58.295, mean reward: 5.829 [3.856, 10.555], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.650], loss: 0.428939, mae: 0.593644, mean_q: 5.100743
 97726/100000: episode: 2020, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 33.523, mean reward: 3.048 [2.373, 3.785], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.083, 10.429], loss: 0.277613, mae: 0.503850, mean_q: 5.093756
 97737/100000: episode: 2021, duration: 0.054s, episode steps: 11, steps per second: 202, episode reward: 35.227, mean reward: 3.202 [2.739, 4.516], mean action: 0.000 [0.000, 0.000], mean observation: 2.264 [-0.035, 10.474], loss: 0.508101, mae: 0.594806, mean_q: 5.361442
 97747/100000: episode: 2022, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 53.953, mean reward: 5.395 [3.752, 6.494], mean action: 0.000 [0.000, 0.000], mean observation: 2.288 [-0.453, 10.539], loss: 0.214203, mae: 0.450282, mean_q: 4.932243
 97764/100000: episode: 2023, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 124.203, mean reward: 7.306 [4.498, 16.738], mean action: 0.000 [0.000, 0.000], mean observation: 2.231 [-0.035, 10.587], loss: 0.488845, mae: 0.561725, mean_q: 5.179689
 97781/100000: episode: 2024, duration: 0.088s, episode steps: 17, steps per second: 192, episode reward: 50.192, mean reward: 2.952 [2.154, 4.164], mean action: 0.000 [0.000, 0.000], mean observation: 2.234 [-0.035, 10.460], loss: 0.626824, mae: 0.639900, mean_q: 5.297582
 97798/100000: episode: 2025, duration: 0.098s, episode steps: 17, steps per second: 173, episode reward: 86.986, mean reward: 5.117 [3.559, 7.587], mean action: 0.000 [0.000, 0.000], mean observation: 2.210 [-0.063, 10.591], loss: 0.394948, mae: 0.590591, mean_q: 5.328472
 97809/100000: episode: 2026, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 56.163, mean reward: 5.106 [3.324, 8.790], mean action: 0.000 [0.000, 0.000], mean observation: 2.239 [-1.244, 10.447], loss: 0.631014, mae: 0.563461, mean_q: 5.281106
 97818/100000: episode: 2027, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 32.693, mean reward: 3.633 [3.238, 3.886], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.821, 10.503], loss: 0.387227, mae: 0.589178, mean_q: 5.372258
 97835/100000: episode: 2028, duration: 0.090s, episode steps: 17, steps per second: 189, episode reward: 156.662, mean reward: 9.215 [3.431, 28.660], mean action: 0.000 [0.000, 0.000], mean observation: 2.199 [-0.098, 10.469], loss: 0.479520, mae: 0.611447, mean_q: 5.437360
 97842/100000: episode: 2029, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 26.582, mean reward: 3.797 [3.196, 4.666], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.525], loss: 0.254284, mae: 0.506480, mean_q: 4.998098
 97851/100000: episode: 2030, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 68.200, mean reward: 7.578 [4.966, 12.953], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.601], loss: 0.667678, mae: 0.771684, mean_q: 5.482770
 97859/100000: episode: 2031, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 32.167, mean reward: 4.021 [3.278, 4.662], mean action: 0.000 [0.000, 0.000], mean observation: 2.300 [-0.035, 10.558], loss: 0.312638, mae: 0.521055, mean_q: 5.205303
 97866/100000: episode: 2032, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 71.419, mean reward: 10.203 [4.137, 19.594], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.289, 10.654], loss: 0.258738, mae: 0.483670, mean_q: 5.181750
 97873/100000: episode: 2033, duration: 0.037s, episode steps: 7, steps per second: 189, episode reward: 36.386, mean reward: 5.198 [3.436, 6.306], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.594], loss: 0.694022, mae: 0.658468, mean_q: 5.484932
 97883/100000: episode: 2034, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 51.600, mean reward: 5.160 [3.886, 6.996], mean action: 0.000 [0.000, 0.000], mean observation: 2.241 [-0.386, 10.366], loss: 0.470346, mae: 0.595266, mean_q: 5.218357
 97892/100000: episode: 2035, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 30.441, mean reward: 3.382 [2.532, 5.333], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.527], loss: 0.918470, mae: 0.648687, mean_q: 5.364806
 97899/100000: episode: 2036, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 24.299, mean reward: 3.471 [2.898, 4.455], mean action: 0.000 [0.000, 0.000], mean observation: 2.303 [-0.035, 10.445], loss: 0.658207, mae: 0.684221, mean_q: 5.504291
 97908/100000: episode: 2037, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 58.494, mean reward: 6.499 [4.700, 14.001], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.585], loss: 0.483665, mae: 0.612882, mean_q: 5.459697
 97921/100000: episode: 2038, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 70.717, mean reward: 5.440 [4.018, 7.264], mean action: 0.000 [0.000, 0.000], mean observation: 2.251 [-0.447, 10.603], loss: 0.386015, mae: 0.599940, mean_q: 5.245397
 97934/100000: episode: 2039, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 47.018, mean reward: 3.617 [2.247, 5.095], mean action: 0.000 [0.000, 0.000], mean observation: 2.253 [-0.035, 10.459], loss: 0.244799, mae: 0.514217, mean_q: 5.218932
 97951/100000: episode: 2040, duration: 0.084s, episode steps: 17, steps per second: 202, episode reward: 60.481, mean reward: 3.558 [2.421, 5.299], mean action: 0.000 [0.000, 0.000], mean observation: 2.228 [-0.035, 10.557], loss: 0.472873, mae: 0.644777, mean_q: 5.430124
 97958/100000: episode: 2041, duration: 0.043s, episode steps: 7, steps per second: 163, episode reward: 51.623, mean reward: 7.375 [3.489, 13.450], mean action: 0.000 [0.000, 0.000], mean observation: 2.275 [-0.472, 10.484], loss: 0.406915, mae: 0.556530, mean_q: 5.069133
 97968/100000: episode: 2042, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 37.883, mean reward: 3.788 [3.231, 4.524], mean action: 0.000 [0.000, 0.000], mean observation: 2.295 [-0.035, 10.525], loss: 1.094713, mae: 0.644592, mean_q: 5.289137
 97975/100000: episode: 2043, duration: 0.058s, episode steps: 7, steps per second: 121, episode reward: 32.222, mean reward: 4.603 [3.091, 6.207], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.494], loss: 0.641976, mae: 0.698781, mean_q: 5.383431
 97988/100000: episode: 2044, duration: 0.065s, episode steps: 13, steps per second: 199, episode reward: 186.507, mean reward: 14.347 [4.136, 46.600], mean action: 0.000 [0.000, 0.000], mean observation: 2.252 [-1.490, 10.732], loss: 0.494434, mae: 0.565494, mean_q: 5.370341
 97996/100000: episode: 2045, duration: 0.041s, episode steps: 8, steps per second: 193, episode reward: 28.382, mean reward: 3.548 [2.885, 4.906], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.338, 10.594], loss: 0.747931, mae: 0.709952, mean_q: 5.559061
 98013/100000: episode: 2046, duration: 0.101s, episode steps: 17, steps per second: 169, episode reward: 67.772, mean reward: 3.987 [3.148, 5.098], mean action: 0.000 [0.000, 0.000], mean observation: 2.227 [-0.035, 10.500], loss: 0.368470, mae: 0.584818, mean_q: 5.370008
 98030/100000: episode: 2047, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 62.854, mean reward: 3.697 [2.863, 5.263], mean action: 0.000 [0.000, 0.000], mean observation: 2.216 [-0.035, 10.504], loss: 0.672088, mae: 0.600578, mean_q: 5.333282
 98037/100000: episode: 2048, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 31.737, mean reward: 4.534 [3.645, 5.255], mean action: 0.000 [0.000, 0.000], mean observation: 2.290 [-1.341, 10.511], loss: 0.533613, mae: 0.568279, mean_q: 5.325853
 98044/100000: episode: 2049, duration: 0.046s, episode steps: 7, steps per second: 151, episode reward: 30.187, mean reward: 4.312 [3.539, 6.125], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.035, 10.547], loss: 0.841515, mae: 0.603605, mean_q: 5.506426
 98057/100000: episode: 2050, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 60.757, mean reward: 4.674 [3.477, 7.863], mean action: 0.000 [0.000, 0.000], mean observation: 2.273 [-0.104, 10.584], loss: 3.106552, mae: 0.953409, mean_q: 5.598574
 98067/100000: episode: 2051, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 76.448, mean reward: 7.645 [3.872, 13.372], mean action: 0.000 [0.000, 0.000], mean observation: 2.292 [-0.035, 10.696], loss: 1.150035, mae: 0.909202, mean_q: 5.302732
 98084/100000: episode: 2052, duration: 0.088s, episode steps: 17, steps per second: 192, episode reward: 60.799, mean reward: 3.576 [2.588, 5.714], mean action: 0.000 [0.000, 0.000], mean observation: 2.218 [-0.035, 10.471], loss: 1.096198, mae: 0.803306, mean_q: 5.716518
 98097/100000: episode: 2053, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 50.020, mean reward: 3.848 [3.003, 4.589], mean action: 0.000 [0.000, 0.000], mean observation: 2.229 [-0.788, 10.506], loss: 0.577243, mae: 0.675369, mean_q: 5.468961
 98104/100000: episode: 2054, duration: 0.046s, episode steps: 7, steps per second: 152, episode reward: 29.403, mean reward: 4.200 [3.088, 5.472], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.253, 10.574], loss: 0.428267, mae: 0.589634, mean_q: 5.126740
 98113/100000: episode: 2055, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 31.132, mean reward: 3.459 [2.549, 4.395], mean action: 0.000 [0.000, 0.000], mean observation: 2.284 [-0.316, 10.472], loss: 0.642128, mae: 0.720759, mean_q: 5.640868
 98120/100000: episode: 2056, duration: 0.041s, episode steps: 7, steps per second: 169, episode reward: 29.042, mean reward: 4.149 [2.978, 5.385], mean action: 0.000 [0.000, 0.000], mean observation: 2.285 [-0.186, 10.487], loss: 0.374130, mae: 0.550925, mean_q: 5.590984
 98131/100000: episode: 2057, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 39.547, mean reward: 3.595 [2.926, 4.695], mean action: 0.000 [0.000, 0.000], mean observation: 2.263 [-0.261, 10.491], loss: 0.946079, mae: 0.783539, mean_q: 5.560998
 98140/100000: episode: 2058, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 57.121, mean reward: 6.347 [4.248, 12.659], mean action: 0.000 [0.000, 0.000], mean observation: 2.283 [-0.035, 10.609], loss: 1.096228, mae: 0.891565, mean_q: 6.113364
 98147/100000: episode: 2059, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 24.106, mean reward: 3.444 [3.084, 4.011], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.035, 10.441], loss: 0.464942, mae: 0.656739, mean_q: 5.395754
 98157/100000: episode: 2060, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 36.596, mean reward: 3.660 [3.186, 4.421], mean action: 0.000 [0.000, 0.000], mean observation: 2.296 [-0.035, 10.556], loss: 0.579415, mae: 0.638883, mean_q: 5.690118
 98165/100000: episode: 2061, duration: 0.065s, episode steps: 8, steps per second: 124, episode reward: 38.132, mean reward: 4.766 [3.959, 5.890], mean action: 0.000 [0.000, 0.000], mean observation: 2.319 [-0.035, 10.548], loss: 0.444528, mae: 0.580804, mean_q: 5.488522
 98175/100000: episode: 2062, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 54.405, mean reward: 5.441 [3.860, 6.804], mean action: 0.000 [0.000, 0.000], mean observation: 2.286 [-0.035, 10.642], loss: 0.875392, mae: 0.677470, mean_q: 5.604954
 98183/100000: episode: 2063, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 37.574, mean reward: 4.697 [2.806, 6.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.277 [-0.454, 10.580], loss: 0.710150, mae: 0.662762, mean_q: 5.701795
 98190/100000: episode: 2064, duration: 0.038s, episode steps: 7, steps per second: 185, episode reward: 27.661, mean reward: 3.952 [3.283, 4.557], mean action: 0.000 [0.000, 0.000], mean observation: 2.297 [-0.530, 10.530], loss: 0.760308, mae: 0.662500, mean_q: 5.640416
 98198/100000: episode: 2065, duration: 0.048s, episode steps: 8, steps per second: 167, episode reward: 36.178, mean reward: 4.522 [3.043, 5.690], mean action: 0.000 [0.000, 0.000], mean observation: 2.317 [-0.035, 10.611], loss: 0.510433, mae: 0.633437, mean_q: 5.612010
 98205/100000: episode: 2066, duration: 0.040s, episode steps: 7, steps per second: 173, episode reward: 30.021, mean reward: 4.289 [3.835, 4.939], mean action: 0.000 [0.000, 0.000], mean observation: 2.267 [-1.402, 10.470], loss: 1.121904, mae: 0.710757, mean_q: 5.395463
 98213/100000: episode: 2067, duration: 0.047s, episode steps: 8, steps per second: 169, episode reward: 32.132, mean reward: 4.016 [3.292, 6.119], mean action: 0.000 [0.000, 0.000], mean observation: 2.326 [-0.035, 10.545], loss: 3.816885, mae: 0.954565, mean_q: 5.802634
 98226/100000: episode: 2068, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 61.521, mean reward: 4.732 [3.493, 7.590], mean action: 0.000 [0.000, 0.000], mean observation: 2.230 [-0.924, 10.475], loss: 0.655672, mae: 0.736826, mean_q: 5.669882
 98233/100000: episode: 2069, duration: 0.040s, episode steps: 7, steps per second: 177, episode reward: 40.239, mean reward: 5.748 [4.203, 9.243], mean action: 0.000 [0.000, 0.000], mean observation: 2.279 [-0.035, 10.517], loss: 0.373219, mae: 0.596684, mean_q: 5.338509
 98241/100000: episode: 2070, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 25.957, mean reward: 3.245 [3.044, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.276 [-0.360, 10.455], loss: 0.921365, mae: 0.766063, mean_q: 5.803767
 98254/100000: episode: 2071, duration: 0.074s, episode steps: 13, steps per second: 176, episode reward: 110.812, mean reward: 8.524 [4.317, 20.622], mean action: 0.000 [0.000, 0.000], mean observation: 2.270 [-0.415, 10.717], loss: 3.100673, mae: 0.930682, mean_q: 6.059215
 98262/100000: episode: 2072, duration: 0.042s, episode steps: 8, steps per second: 191, episode reward: 33.545, mean reward: 4.193 [2.897, 5.272], mean action: 0.000 [0.000, 0.000], mean observation: 2.294 [-0.308, 10.503], loss: 1.613197, mae: 0.845201, mean_q: 5.560987
 98271/100000: episode: 2073, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 38.545, mean reward: 4.283 [3.524, 5.258], mean action: 0.000 [0.000, 0.000], mean observation: 2.291 [-0.322, 10.590], loss: 0.766019, mae: 0.764685, mean_q: 5.898571
 98280/100000: episode: 2074, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 38.950, mean reward: 4.328 [3.939, 4.982], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.106, 10.550], loss: 1.430799, mae: 0.902064, mean_q: 6.138801
 98297/100000: episode: 2075, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 81.582, mean reward: 4.799 [2.963, 8.179], mean action: 0.000 [0.000, 0.000], mean observation: 2.211 [-0.438, 10.462], loss: 0.765005, mae: 0.712523, mean_q: 5.922649
 98314/100000: episode: 2076, duration: 0.099s, episode steps: 17, steps per second: 171, episode reward: 45.143, mean reward: 2.655 [1.961, 3.739], mean action: 0.000 [0.000, 0.000], mean observation: 2.213 [-0.035, 10.400], loss: 0.854052, mae: 0.698850, mean_q: 5.836886
 98324/100000: episode: 2077, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 45.655, mean reward: 4.566 [3.730, 6.023], mean action: 0.000 [0.000, 0.000], mean observation: 2.272 [-0.035, 10.535], loss: 0.813497, mae: 0.740682, mean_q: 5.927708
 98332/100000: episode: 2078, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 36.640, mean reward: 4.580 [4.014, 5.848], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.035, 10.621], loss: 0.461307, mae: 0.669214, mean_q: 5.531711
 98343/100000: episode: 2079, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 37.993, mean reward: 3.454 [2.753, 4.145], mean action: 0.000 [0.000, 0.000], mean observation: 2.280 [-0.108, 10.457], loss: 1.376386, mae: 0.762731, mean_q: 5.796855
 98350/100000: episode: 2080, duration: 0.037s, episode steps: 7, steps per second: 190, episode reward: 25.418, mean reward: 3.631 [3.115, 4.100], mean action: 0.000 [0.000, 0.000], mean observation: 2.315 [-0.035, 10.453], loss: 0.766902, mae: 0.677214, mean_q: 5.740394
 98363/100000: episode: 2081, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 68.410, mean reward: 5.262 [3.870, 6.685], mean action: 0.000 [0.000, 0.000], mean observation: 2.256 [-0.035, 10.563], loss: 0.983110, mae: 0.702085, mean_q: 5.954918
[Info] FALSIFICATION!
 98371/100000: episode: 2082, duration: 0.303s, episode steps: 8, steps per second: 26, episode reward: 1036.582, mean reward: 129.573 [3.537, 1000.000], mean action: 0.000 [0.000, 0.000], mean observation: 2.126 [-0.377, 10.214], loss: 0.916467, mae: 0.762470, mean_q: 6.022175
 98381/100000: episode: 2083, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 80.881, mean reward: 8.088 [4.019, 13.818], mean action: 0.000 [0.000, 0.000], mean observation: 2.293 [-0.484, 10.693], loss: 0.773401, mae: 0.694232, mean_q: 5.657137
 98389/100000: episode: 2084, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 36.339, mean reward: 4.542 [3.333, 5.406], mean action: 0.000 [0.000, 0.000], mean observation: 2.306 [-0.035, 10.611], loss: 0.646415, mae: 0.765295, mean_q: 6.046457
 98396/100000: episode: 2085, duration: 0.043s, episode steps: 7, steps per second: 162, episode reward: 41.550, mean reward: 5.936 [4.156, 7.845], mean action: 0.000 [0.000, 0.000], mean observation: 2.282 [-0.171, 10.592], loss: 0.612613, mae: 0.665549, mean_q: 5.904756
 98406/100000: episode: 2086, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 53.607, mean reward: 5.361 [3.461, 8.425], mean action: 0.000 [0.000, 0.000], mean observation: 2.302 [-0.035, 10.568], loss: 0.443120, mae: 0.577578, mean_q: 5.722332
 98419/100000: episode: 2087, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 68.464, mean reward: 5.266 [4.282, 6.074], mean action: 0.000 [0.000, 0.000], mean observation: 2.255 [-0.071, 10.530], loss: 1.047063, mae: 0.662978, mean_q: 5.867291
 98432/100000: episode: 2088, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 66.312, mean reward: 5.101 [3.964, 7.058], mean action: 0.000 [0.000, 0.000], mean observation: 2.247 [-0.633, 10.621], loss: 1185.364380, mae: 4.291275, mean_q: 7.223911
 98441/100000: episode: 2089, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 52.693, mean reward: 5.855 [4.392, 7.328], mean action: 0.000 [0.000, 0.000], mean observation: 2.308 [-0.035, 10.609], loss: 14.416713, mae: 4.305872, mean_q: 7.778317
[Info] Complete ISplit Iteration
[Info] Levels: [5.4026203, 6.3127017, 7.825525, 11.298944]
[Info] Cond. Prob: [0.1, 0.1, 0.1, 0.38]
[Info] Error Prob: 0.0003800000000000001

 98449/100000: episode: 2090, duration: 4.494s, episode steps: 8, steps per second: 2, episode reward: 27.430, mean reward: 3.429 [2.815, 4.397], mean action: 0.000 [0.000, 0.000], mean observation: 2.307 [-0.035, 10.542], loss: 4.258408, mae: 2.214599, mean_q: 5.175715
 98549/100000: episode: 2091, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: 185.682, mean reward: 1.857 [1.514, 2.676], mean action: 0.000 [0.000, 0.000], mean observation: 1.432 [-0.826, 10.114], loss: 155.034836, mae: 1.515311, mean_q: 6.574313
 98649/100000: episode: 2092, duration: 0.501s, episode steps: 100, steps per second: 199, episode reward: 190.535, mean reward: 1.905 [1.446, 4.257], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-0.415, 10.098], loss: 154.714996, mae: 1.458578, mean_q: 6.477310
 98749/100000: episode: 2093, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 188.662, mean reward: 1.887 [1.465, 2.830], mean action: 0.000 [0.000, 0.000], mean observation: 1.425 [-1.351, 10.098], loss: 153.838165, mae: 1.168223, mean_q: 6.204795
 98849/100000: episode: 2094, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 199.176, mean reward: 1.992 [1.492, 3.796], mean action: 0.000 [0.000, 0.000], mean observation: 1.427 [-1.615, 10.103], loss: 307.776062, mae: 2.345126, mean_q: 6.976669
 98949/100000: episode: 2095, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 198.032, mean reward: 1.980 [1.469, 5.085], mean action: 0.000 [0.000, 0.000], mean observation: 1.437 [-1.167, 10.169], loss: 1.452168, mae: 1.068237, mean_q: 6.465882
 99049/100000: episode: 2096, duration: 0.513s, episode steps: 100, steps per second: 195, episode reward: 191.084, mean reward: 1.911 [1.463, 4.480], mean action: 0.000 [0.000, 0.000], mean observation: 1.421 [-1.105, 10.098], loss: 154.122574, mae: 1.120193, mean_q: 6.282390
 99149/100000: episode: 2097, duration: 0.507s, episode steps: 100, steps per second: 197, episode reward: 206.960, mean reward: 2.070 [1.448, 3.694], mean action: 0.000 [0.000, 0.000], mean observation: 1.426 [-0.932, 10.098], loss: 2.545304, mae: 1.241274, mean_q: 6.514111
 99249/100000: episode: 2098, duration: 0.526s, episode steps: 100, steps per second: 190, episode reward: 185.415, mean reward: 1.854 [1.444, 2.852], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.980, 10.100], loss: 0.918634, mae: 0.741688, mean_q: 6.017989
 99349/100000: episode: 2099, duration: 0.519s, episode steps: 100, steps per second: 193, episode reward: 181.755, mean reward: 1.818 [1.441, 2.920], mean action: 0.000 [0.000, 0.000], mean observation: 1.433 [-0.619, 10.098], loss: 0.889845, mae: 0.730225, mean_q: 5.993485
 99449/100000: episode: 2100, duration: 0.501s, episode steps: 100, steps per second: 200, episode reward: 200.014, mean reward: 2.000 [1.466, 4.208], mean action: 0.000 [0.000, 0.000], mean observation: 1.431 [-1.181, 10.098], loss: 1.190948, mae: 0.763167, mean_q: 5.979873
 99549/100000: episode: 2101, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: 203.243, mean reward: 2.032 [1.438, 3.742], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.781, 10.295], loss: 0.875430, mae: 0.711479, mean_q: 5.870511
 99649/100000: episode: 2102, duration: 0.510s, episode steps: 100, steps per second: 196, episode reward: 198.421, mean reward: 1.984 [1.435, 3.407], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-0.707, 10.098], loss: 1.034650, mae: 0.719752, mean_q: 5.834132
 99749/100000: episode: 2103, duration: 0.532s, episode steps: 100, steps per second: 188, episode reward: 193.946, mean reward: 1.939 [1.481, 3.846], mean action: 0.000 [0.000, 0.000], mean observation: 1.430 [-0.232, 10.172], loss: 155.421448, mae: 1.592768, mean_q: 6.281789
 99849/100000: episode: 2104, duration: 0.541s, episode steps: 100, steps per second: 185, episode reward: 206.788, mean reward: 2.068 [1.466, 4.121], mean action: 0.000 [0.000, 0.000], mean observation: 1.424 [-1.082, 10.098], loss: 1.627399, mae: 0.763969, mean_q: 5.909554
 99949/100000: episode: 2105, duration: 0.518s, episode steps: 100, steps per second: 193, episode reward: 186.640, mean reward: 1.866 [1.456, 3.636], mean action: 0.000 [0.000, 0.000], mean observation: 1.428 [-0.842, 10.330], loss: 154.848694, mae: 1.461377, mean_q: 6.184568
done, took 595.214 seconds
[Info] End Importance Splitting. Falsification occurred 13 times.
