Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 918)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                14704     
_________________________________________________________________
dense_2 (Dense)              (None, 102)               1734      
=================================================================
Total params: 16,438
Trainable params: 16,438
Non-trainable params: 0
_________________________________________________________________
None
Training for 100000 steps ...
    10/100000: episode: 1, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 5.093, mean reward: 0.509 [0.432, 0.635], mean action: 45.500 [1.000, 88.000], mean observation: 3.166 [-1.456, 10.480], loss: --, mae: --, mean_q: --
    20/100000: episode: 2, duration: 0.070s, episode steps: 10, steps per second: 144, episode reward: 5.108, mean reward: 0.511 [0.437, 0.544], mean action: 60.700 [36.000, 81.000], mean observation: 3.146 [-1.654, 10.354], loss: --, mae: --, mean_q: --
    30/100000: episode: 3, duration: 0.070s, episode steps: 10, steps per second: 143, episode reward: 3.651, mean reward: 0.365 [0.340, 0.414], mean action: 60.500 [53.000, 80.000], mean observation: 3.149 [-1.529, 10.318], loss: --, mae: --, mean_q: --
    40/100000: episode: 4, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 4.215, mean reward: 0.422 [0.380, 0.490], mean action: 55.000 [3.000, 94.000], mean observation: 3.161 [-1.436, 10.314], loss: --, mae: --, mean_q: --
    50/100000: episode: 5, duration: 0.086s, episode steps: 10, steps per second: 117, episode reward: 4.618, mean reward: 0.462 [0.387, 0.504], mean action: 56.200 [6.000, 66.000], mean observation: 3.161 [-1.054, 10.234], loss: --, mae: --, mean_q: --
    60/100000: episode: 6, duration: 0.085s, episode steps: 10, steps per second: 118, episode reward: 4.100, mean reward: 0.410 [0.342, 0.539], mean action: 49.800 [2.000, 80.000], mean observation: 3.158 [-1.775, 10.330], loss: --, mae: --, mean_q: --
    70/100000: episode: 7, duration: 0.082s, episode steps: 10, steps per second: 122, episode reward: 4.092, mean reward: 0.409 [0.389, 0.456], mean action: 51.100 [18.000, 95.000], mean observation: 3.158 [-1.203, 10.451], loss: --, mae: --, mean_q: --
    80/100000: episode: 8, duration: 0.092s, episode steps: 10, steps per second: 108, episode reward: 3.931, mean reward: 0.393 [0.331, 0.452], mean action: 43.500 [2.000, 97.000], mean observation: 3.149 [-1.547, 10.302], loss: --, mae: --, mean_q: --
    90/100000: episode: 9, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 3.795, mean reward: 0.379 [0.308, 0.440], mean action: 57.000 [4.000, 88.000], mean observation: 3.146 [-1.234, 10.303], loss: --, mae: --, mean_q: --
   100/100000: episode: 10, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 3.912, mean reward: 0.391 [0.367, 0.426], mean action: 47.100 [7.000, 69.000], mean observation: 3.153 [-1.253, 10.413], loss: --, mae: --, mean_q: --
   110/100000: episode: 11, duration: 0.647s, episode steps: 10, steps per second: 15, episode reward: 4.271, mean reward: 0.427 [0.381, 0.447], mean action: 28.200 [1.000, 63.000], mean observation: 3.151 [-2.162, 10.432], loss: 3.080574, mae: 1.231542, mean_q: 3.014288
   120/100000: episode: 12, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.231, mean reward: 0.423 [0.389, 0.453], mean action: 44.300 [14.000, 86.000], mean observation: 3.163 [-0.982, 10.385], loss: 1.280723, mae: 0.108193, mean_q: 0.194862
   130/100000: episode: 13, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.238, mean reward: 0.424 [0.353, 0.561], mean action: 58.200 [4.000, 99.000], mean observation: 3.148 [-2.032, 10.440], loss: 0.312960, mae: 0.008267, mean_q: 0.011747
   140/100000: episode: 14, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.332, mean reward: 0.433 [0.371, 0.478], mean action: 34.600 [1.000, 100.000], mean observation: 3.152 [-0.754, 10.356], loss: 0.048612, mae: 0.005999, mean_q: 0.016416
   150/100000: episode: 15, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.110, mean reward: 0.411 [0.320, 0.517], mean action: 35.400 [13.000, 98.000], mean observation: 3.152 [-1.215, 10.397], loss: 0.040396, mae: 0.006480, mean_q: 0.022638
   160/100000: episode: 16, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 3.864, mean reward: 0.386 [0.322, 0.489], mean action: 29.900 [7.000, 73.000], mean observation: 3.168 [-1.496, 10.469], loss: 0.075691, mae: 0.008605, mean_q: 0.032147
   170/100000: episode: 17, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.052, mean reward: 0.405 [0.351, 0.469], mean action: 37.300 [20.000, 71.000], mean observation: 3.144 [-1.698, 10.365], loss: 0.089220, mae: 0.010182, mean_q: 0.047161
   180/100000: episode: 18, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.159, mean reward: 0.416 [0.334, 0.464], mean action: 39.800 [30.000, 98.000], mean observation: 3.164 [-1.820, 10.311], loss: 0.088445, mae: 0.011673, mean_q: 0.064415
   190/100000: episode: 19, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.277, mean reward: 0.428 [0.363, 0.500], mean action: 43.200 [2.000, 100.000], mean observation: 3.153 [-1.240, 10.417], loss: 0.089751, mae: 0.013785, mean_q: 0.080710
   191/100000: episode: 20, duration: 0.040s, episode steps: 1, steps per second: 25, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 39.000 [39.000, 39.000], mean observation: 3.161 [-1.580, 10.100], loss: 0.095429, mae: 0.015326, mean_q: 0.090241
   201/100000: episode: 21, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.902, mean reward: 0.390 [0.369, 0.460], mean action: 40.400 [9.000, 86.000], mean observation: 3.150 [-1.151, 10.286], loss: 0.245573, mae: 0.016817, mean_q: 0.098726
   211/100000: episode: 22, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 5.063, mean reward: 0.506 [0.491, 0.534], mean action: 36.100 [6.000, 87.000], mean observation: 3.161 [-1.462, 10.456], loss: 0.556955, mae: 0.019961, mean_q: 0.114259
   221/100000: episode: 23, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.868, mean reward: 0.487 [0.487, 0.487], mean action: 57.100 [30.000, 99.000], mean observation: 3.139 [-1.483, 10.357], loss: 0.244815, mae: 0.021948, mean_q: 0.129454
   231/100000: episode: 24, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.177, mean reward: 0.418 [0.350, 0.514], mean action: 49.500 [0.000, 90.000], mean observation: 3.152 [-1.347, 10.308], loss: 0.400742, mae: 0.024973, mean_q: 0.145694
   241/100000: episode: 25, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.896, mean reward: 0.390 [0.321, 0.477], mean action: 46.600 [29.000, 83.000], mean observation: 3.148 [-1.420, 10.346], loss: 0.252083, mae: 0.027572, mean_q: 0.161979
   251/100000: episode: 26, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.059, mean reward: 0.406 [0.374, 0.492], mean action: 27.000 [8.000, 30.000], mean observation: 3.149 [-1.212, 10.272], loss: 0.249624, mae: 0.030467, mean_q: 0.176692
   261/100000: episode: 27, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.469, mean reward: 0.447 [0.374, 0.531], mean action: 37.400 [30.000, 67.000], mean observation: 3.167 [-1.404, 10.317], loss: 0.095730, mae: 0.033163, mean_q: 0.191123
   271/100000: episode: 28, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 3.738, mean reward: 0.374 [0.325, 0.443], mean action: 27.800 [10.000, 39.000], mean observation: 3.152 [-1.826, 10.316], loss: 0.098578, mae: 0.036682, mean_q: 0.205836
   281/100000: episode: 29, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.045, mean reward: 0.404 [0.354, 0.467], mean action: 29.700 [8.000, 55.000], mean observation: 3.163 [-1.686, 10.396], loss: 0.098255, mae: 0.040099, mean_q: 0.220575
   291/100000: episode: 30, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.268, mean reward: 0.427 [0.409, 0.530], mean action: 53.800 [28.000, 94.000], mean observation: 3.156 [-1.631, 10.357], loss: 0.408870, mae: 0.044109, mean_q: 0.235399
   301/100000: episode: 31, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.188, mean reward: 0.419 [0.399, 0.448], mean action: 35.900 [8.000, 81.000], mean observation: 3.146 [-0.996, 10.341], loss: 0.103838, mae: 0.046828, mean_q: 0.249426
   311/100000: episode: 32, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.952, mean reward: 0.395 [0.328, 0.483], mean action: 45.500 [6.000, 88.000], mean observation: 3.155 [-2.151, 10.253], loss: 0.256222, mae: 0.050490, mean_q: 0.263595
   321/100000: episode: 33, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.149, mean reward: 0.415 [0.380, 0.526], mean action: 35.900 [18.000, 83.000], mean observation: 3.155 [-1.055, 10.333], loss: 0.106033, mae: 0.053763, mean_q: 0.277719
   331/100000: episode: 34, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 3.911, mean reward: 0.391 [0.321, 0.454], mean action: 31.800 [0.000, 72.000], mean observation: 3.145 [-1.667, 10.313], loss: 0.107996, mae: 0.057270, mean_q: 0.291071
   341/100000: episode: 35, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.687, mean reward: 0.369 [0.335, 0.421], mean action: 31.000 [2.000, 57.000], mean observation: 3.155 [-1.571, 10.368], loss: 0.109976, mae: 0.060872, mean_q: 0.305210
   351/100000: episode: 36, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.197, mean reward: 0.420 [0.363, 0.510], mean action: 43.400 [30.000, 98.000], mean observation: 3.147 [-1.945, 10.450], loss: 0.264225, mae: 0.064561, mean_q: 0.318853
   361/100000: episode: 37, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.342, mean reward: 0.434 [0.311, 0.493], mean action: 30.800 [0.000, 73.000], mean observation: 3.160 [-1.164, 10.349], loss: 0.111090, mae: 0.067761, mean_q: 0.332734
   371/100000: episode: 38, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.050, mean reward: 0.405 [0.342, 0.488], mean action: 40.800 [9.000, 100.000], mean observation: 3.163 [-1.382, 10.450], loss: 0.273239, mae: 0.072084, mean_q: 0.346917
   381/100000: episode: 39, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.953, mean reward: 0.395 [0.337, 0.478], mean action: 33.900 [7.000, 81.000], mean observation: 3.150 [-1.273, 10.332], loss: 0.575392, mae: 0.076260, mean_q: 0.360357
   391/100000: episode: 40, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.626, mean reward: 0.363 [0.314, 0.434], mean action: 47.300 [6.000, 96.000], mean observation: 3.139 [-1.555, 10.421], loss: 0.121511, mae: 0.079014, mean_q: 0.373689
   401/100000: episode: 41, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.130, mean reward: 0.413 [0.340, 0.590], mean action: 41.000 [30.000, 79.000], mean observation: 3.148 [-1.017, 10.339], loss: 0.115431, mae: 0.082330, mean_q: 0.387923
   411/100000: episode: 42, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.164, mean reward: 0.416 [0.311, 0.461], mean action: 46.200 [6.000, 94.000], mean observation: 3.158 [-1.437, 10.397], loss: 0.275671, mae: 0.086573, mean_q: 0.402523
   421/100000: episode: 43, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.815, mean reward: 0.382 [0.332, 0.566], mean action: 37.300 [8.000, 91.000], mean observation: 3.148 [-1.907, 10.396], loss: 0.585361, mae: 0.091561, mean_q: 0.415917
   431/100000: episode: 44, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.063, mean reward: 0.406 [0.344, 0.483], mean action: 28.300 [13.000, 30.000], mean observation: 3.144 [-1.446, 10.280], loss: 0.130342, mae: 0.095185, mean_q: 0.428440
   441/100000: episode: 45, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.216, mean reward: 0.422 [0.418, 0.452], mean action: 44.400 [25.000, 81.000], mean observation: 3.168 [-1.315, 10.213], loss: 0.128359, mae: 0.099189, mean_q: 0.440849
   451/100000: episode: 46, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.707, mean reward: 0.371 [0.297, 0.489], mean action: 44.000 [1.000, 98.000], mean observation: 3.160 [-0.934, 10.357], loss: 0.436752, mae: 0.104232, mean_q: 0.453793
   461/100000: episode: 47, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.970, mean reward: 0.397 [0.333, 0.472], mean action: 37.300 [14.000, 70.000], mean observation: 3.160 [-1.467, 10.211], loss: 0.283588, mae: 0.108092, mean_q: 0.466350
   471/100000: episode: 48, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.867, mean reward: 0.387 [0.324, 0.504], mean action: 41.900 [13.000, 97.000], mean observation: 3.158 [-1.294, 10.489], loss: 0.141440, mae: 0.112227, mean_q: 0.479319
   481/100000: episode: 49, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.160, mean reward: 0.416 [0.398, 0.449], mean action: 40.700 [1.000, 97.000], mean observation: 3.150 [-1.790, 10.430], loss: 0.297674, mae: 0.116897, mean_q: 0.492420
   491/100000: episode: 50, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.847, mean reward: 0.385 [0.325, 0.464], mean action: 38.900 [19.000, 68.000], mean observation: 3.159 [-1.528, 10.330], loss: 0.298604, mae: 0.121364, mean_q: 0.505518
   501/100000: episode: 51, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.261, mean reward: 0.426 [0.395, 0.556], mean action: 37.400 [15.000, 80.000], mean observation: 3.180 [-2.028, 10.341], loss: 0.456394, mae: 0.126030, mean_q: 0.517989
   511/100000: episode: 52, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.261, mean reward: 0.426 [0.365, 0.519], mean action: 33.700 [0.000, 79.000], mean observation: 3.152 [-1.072, 10.417], loss: 0.156574, mae: 0.129707, mean_q: 0.530213
   521/100000: episode: 53, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.980, mean reward: 0.398 [0.369, 0.453], mean action: 53.100 [30.000, 99.000], mean observation: 3.164 [-1.060, 10.362], loss: 0.158683, mae: 0.134406, mean_q: 0.542400
   531/100000: episode: 54, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.003, mean reward: 0.400 [0.291, 0.495], mean action: 27.300 [13.000, 52.000], mean observation: 3.154 [-1.931, 10.260], loss: 0.313039, mae: 0.139453, mean_q: 0.554161
   541/100000: episode: 55, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.060, mean reward: 0.406 [0.376, 0.470], mean action: 39.800 [30.000, 95.000], mean observation: 3.145 [-1.630, 10.341], loss: 0.318964, mae: 0.144327, mean_q: 0.565763
   551/100000: episode: 56, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.976, mean reward: 0.398 [0.338, 0.491], mean action: 41.200 [11.000, 100.000], mean observation: 3.147 [-1.121, 10.309], loss: 0.469089, mae: 0.149078, mean_q: 0.577679
   561/100000: episode: 57, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.407, mean reward: 0.441 [0.355, 0.518], mean action: 52.300 [29.000, 80.000], mean observation: 3.161 [-1.483, 10.343], loss: 0.169147, mae: 0.152830, mean_q: 0.589581
   571/100000: episode: 58, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.348, mean reward: 0.435 [0.368, 0.544], mean action: 46.900 [5.000, 98.000], mean observation: 3.167 [-1.045, 10.261], loss: 0.471798, mae: 0.157777, mean_q: 0.602326
   581/100000: episode: 59, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.283, mean reward: 0.428 [0.384, 0.472], mean action: 39.500 [1.000, 84.000], mean observation: 3.151 [-1.147, 10.263], loss: 0.171381, mae: 0.161285, mean_q: 0.614521
   591/100000: episode: 60, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.140, mean reward: 0.414 [0.334, 0.522], mean action: 30.300 [0.000, 69.000], mean observation: 3.169 [-1.643, 10.398], loss: 0.171111, mae: 0.165443, mean_q: 0.626754
   601/100000: episode: 61, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.302, mean reward: 0.430 [0.328, 0.485], mean action: 39.200 [23.000, 83.000], mean observation: 3.164 [-1.472, 10.371], loss: 0.335811, mae: 0.170475, mean_q: 0.639620
   611/100000: episode: 62, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.282, mean reward: 0.428 [0.426, 0.446], mean action: 49.100 [30.000, 94.000], mean observation: 3.174 [-1.102, 10.266], loss: 0.176610, mae: 0.174340, mean_q: 0.652155
   617/100000: episode: 63, duration: 0.107s, episode steps: 6, steps per second: 56, episode reward: 12.552, mean reward: 2.092 [0.510, 10.000], mean action: 31.833 [12.000, 59.000], mean observation: 3.152 [-1.507, 10.425], loss: 0.185131, mae: 0.178376, mean_q: 0.662027
   627/100000: episode: 64, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.883, mean reward: 0.388 [0.336, 0.442], mean action: 44.800 [30.000, 89.000], mean observation: 3.150 [-1.298, 10.354], loss: 0.198570, mae: 0.182760, mean_q: 0.670755
   637/100000: episode: 65, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.783, mean reward: 0.378 [0.348, 0.403], mean action: 50.100 [30.000, 94.000], mean observation: 3.152 [-1.613, 10.415], loss: 0.328477, mae: 0.187036, mean_q: 0.680876
   647/100000: episode: 66, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.275, mean reward: 0.427 [0.402, 0.501], mean action: 47.400 [21.000, 100.000], mean observation: 3.156 [-1.276, 10.322], loss: 0.197884, mae: 0.191843, mean_q: 0.691934
   657/100000: episode: 67, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.382, mean reward: 0.438 [0.399, 0.509], mean action: 29.000 [0.000, 73.000], mean observation: 3.147 [-1.993, 10.584], loss: 0.490977, mae: 0.196931, mean_q: 0.702481
   667/100000: episode: 68, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.072, mean reward: 0.407 [0.357, 0.504], mean action: 45.800 [2.000, 93.000], mean observation: 3.142 [-1.869, 10.453], loss: 0.647436, mae: 0.202168, mean_q: 0.714808
   677/100000: episode: 69, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.127, mean reward: 0.413 [0.344, 0.480], mean action: 41.200 [5.000, 80.000], mean observation: 3.152 [-1.476, 10.322], loss: 0.344903, mae: 0.206230, mean_q: 0.727462
   687/100000: episode: 70, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.984, mean reward: 0.398 [0.302, 0.595], mean action: 37.800 [28.000, 74.000], mean observation: 3.157 [-1.833, 10.173], loss: 0.214235, mae: 0.211015, mean_q: 0.738808
   697/100000: episode: 71, duration: 0.153s, episode steps: 10, steps per second: 66, episode reward: 4.302, mean reward: 0.430 [0.428, 0.450], mean action: 50.400 [29.000, 85.000], mean observation: 3.163 [-1.340, 10.386], loss: 0.211290, mae: 0.215685, mean_q: 0.749422
   707/100000: episode: 72, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.973, mean reward: 0.397 [0.305, 0.479], mean action: 40.100 [13.000, 91.000], mean observation: 3.158 [-1.664, 10.211], loss: 0.232521, mae: 0.221235, mean_q: 0.760729
   717/100000: episode: 73, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.322, mean reward: 0.432 [0.336, 0.540], mean action: 51.000 [30.000, 97.000], mean observation: 3.161 [-1.448, 10.333], loss: 0.217055, mae: 0.225434, mean_q: 0.772142
   727/100000: episode: 74, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.306, mean reward: 0.431 [0.431, 0.431], mean action: 43.400 [18.000, 81.000], mean observation: 3.156 [-1.166, 10.289], loss: 0.226674, mae: 0.230785, mean_q: 0.784596
   737/100000: episode: 75, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.848, mean reward: 0.385 [0.317, 0.438], mean action: 36.000 [8.000, 91.000], mean observation: 3.153 [-1.323, 10.286], loss: 0.379654, mae: 0.236121, mean_q: 0.796996
   747/100000: episode: 76, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.770, mean reward: 0.377 [0.298, 0.484], mean action: 32.200 [30.000, 47.000], mean observation: 3.151 [-1.817, 10.375], loss: 0.371146, mae: 0.240590, mean_q: 0.808816
   757/100000: episode: 77, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.703, mean reward: 0.470 [0.470, 0.470], mean action: 34.500 [16.000, 69.000], mean observation: 3.157 [-1.558, 10.327], loss: 0.377805, mae: 0.245646, mean_q: 0.820483
   767/100000: episode: 78, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.226, mean reward: 0.423 [0.349, 0.501], mean action: 36.700 [2.000, 88.000], mean observation: 3.165 [-1.703, 10.470], loss: 0.239980, mae: 0.250417, mean_q: 0.832061
   777/100000: episode: 79, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.223, mean reward: 0.422 [0.315, 0.523], mean action: 34.500 [30.000, 75.000], mean observation: 3.167 [-1.629, 10.368], loss: 0.817915, mae: 0.256337, mean_q: 0.843678
   787/100000: episode: 80, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.671, mean reward: 0.467 [0.374, 0.534], mean action: 36.800 [10.000, 97.000], mean observation: 3.150 [-1.349, 10.537], loss: 0.386650, mae: 0.260547, mean_q: 0.855787
   797/100000: episode: 81, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.607, mean reward: 0.361 [0.332, 0.400], mean action: 49.200 [3.000, 101.000], mean observation: 3.161 [-1.522, 10.382], loss: 0.212102, mae: 0.264286, mean_q: 0.867815
   807/100000: episode: 82, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.042, mean reward: 0.404 [0.323, 0.532], mean action: 39.200 [25.000, 88.000], mean observation: 3.156 [-1.542, 10.298], loss: 0.525706, mae: 0.269671, mean_q: 0.881110
   808/100000: episode: 83, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 30.000 [30.000, 30.000], mean observation: 3.176 [-0.704, 10.100], loss: 0.212509, mae: 0.270699, mean_q: 0.889235
   818/100000: episode: 84, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.096, mean reward: 0.410 [0.357, 0.471], mean action: 34.700 [9.000, 87.000], mean observation: 3.156 [-1.597, 10.224], loss: 0.518570, mae: 0.275227, mean_q: 0.897209
   828/100000: episode: 85, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.157, mean reward: 0.416 [0.343, 0.489], mean action: 43.800 [3.000, 87.000], mean observation: 3.159 [-1.130, 10.416], loss: 0.537126, mae: 0.280017, mean_q: 0.911538
   838/100000: episode: 86, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.195, mean reward: 0.420 [0.375, 0.453], mean action: 30.700 [10.000, 57.000], mean observation: 3.160 [-1.035, 10.387], loss: 0.231388, mae: 0.283650, mean_q: 0.923803
   848/100000: episode: 87, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.796, mean reward: 0.480 [0.480, 0.480], mean action: 40.500 [9.000, 95.000], mean observation: 3.166 [-1.001, 10.275], loss: 0.228048, mae: 0.287694, mean_q: 0.934904
   858/100000: episode: 88, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 3.945, mean reward: 0.394 [0.324, 0.495], mean action: 60.300 [0.000, 99.000], mean observation: 3.146 [-1.202, 10.287], loss: 0.246593, mae: 0.292823, mean_q: 0.946782
   868/100000: episode: 89, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.918, mean reward: 0.392 [0.362, 0.457], mean action: 44.800 [30.000, 88.000], mean observation: 3.167 [-0.812, 10.340], loss: 0.489087, mae: 0.297387, mean_q: 0.959568
   878/100000: episode: 90, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.294, mean reward: 0.429 [0.415, 0.480], mean action: 28.500 [15.000, 30.000], mean observation: 3.148 [-1.360, 10.259], loss: 0.404631, mae: 0.302437, mean_q: 0.975053
   888/100000: episode: 91, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.988, mean reward: 0.399 [0.345, 0.487], mean action: 38.000 [1.000, 93.000], mean observation: 3.157 [-1.028, 10.271], loss: 0.280600, mae: 0.307138, mean_q: 0.988337
   898/100000: episode: 92, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.784, mean reward: 0.378 [0.301, 0.468], mean action: 45.200 [9.000, 89.000], mean observation: 3.156 [-1.100, 10.285], loss: 0.266282, mae: 0.311474, mean_q: 0.998406
   908/100000: episode: 93, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.978, mean reward: 0.398 [0.352, 0.526], mean action: 51.700 [30.000, 101.000], mean observation: 3.156 [-1.127, 10.257], loss: 0.375921, mae: 0.315981, mean_q: 1.008353
   918/100000: episode: 94, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.221, mean reward: 0.422 [0.421, 0.431], mean action: 50.100 [14.000, 92.000], mean observation: 3.158 [-1.577, 10.318], loss: 0.271981, mae: 0.320790, mean_q: 1.020536
   928/100000: episode: 95, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.833, mean reward: 0.383 [0.329, 0.473], mean action: 45.300 [0.000, 84.000], mean observation: 3.153 [-1.126, 10.305], loss: 0.387229, mae: 0.325233, mean_q: 1.032053
   935/100000: episode: 96, duration: 0.122s, episode steps: 7, steps per second: 58, episode reward: 12.338, mean reward: 1.763 [0.375, 10.000], mean action: 45.429 [30.000, 99.000], mean observation: 3.155 [-1.101, 10.308], loss: 0.893649, mae: 0.330046, mean_q: 1.042624
   945/100000: episode: 97, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.374, mean reward: 0.437 [0.437, 0.437], mean action: 50.300 [30.000, 81.000], mean observation: 3.165 [-1.655, 10.320], loss: 0.419093, mae: 0.333534, mean_q: 1.053818
   955/100000: episode: 98, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.643, mean reward: 0.364 [0.293, 0.425], mean action: 46.200 [21.000, 76.000], mean observation: 3.152 [-0.890, 10.399], loss: 0.431557, mae: 0.338515, mean_q: 1.066926
   965/100000: episode: 99, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.010, mean reward: 0.401 [0.308, 0.534], mean action: 31.800 [15.000, 45.000], mean observation: 3.149 [-1.654, 10.280], loss: 0.443630, mae: 0.343496, mean_q: 1.078954
   975/100000: episode: 100, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.264, mean reward: 0.426 [0.349, 0.501], mean action: 34.200 [5.000, 73.000], mean observation: 3.154 [-0.793, 10.431], loss: 0.419191, mae: 0.348091, mean_q: 1.089184
   985/100000: episode: 101, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.973, mean reward: 0.397 [0.345, 0.465], mean action: 31.600 [7.000, 57.000], mean observation: 3.154 [-1.605, 10.375], loss: 0.400075, mae: 0.352619, mean_q: 1.100399
   995/100000: episode: 102, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.796, mean reward: 0.380 [0.341, 0.447], mean action: 32.200 [5.000, 58.000], mean observation: 3.154 [-1.534, 10.335], loss: 0.556777, mae: 0.357849, mean_q: 1.112951
  1005/100000: episode: 103, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.103, mean reward: 0.410 [0.358, 0.556], mean action: 35.100 [10.000, 77.000], mean observation: 3.150 [-1.289, 10.413], loss: 0.710021, mae: 0.362242, mean_q: 1.126230
  1015/100000: episode: 104, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.117, mean reward: 0.412 [0.306, 0.486], mean action: 50.700 [12.000, 94.000], mean observation: 3.165 [-1.255, 10.372], loss: 0.432468, mae: 0.366220, mean_q: 1.138513
  1025/100000: episode: 105, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.047, mean reward: 0.405 [0.294, 0.493], mean action: 49.000 [22.000, 94.000], mean observation: 3.154 [-1.760, 10.299], loss: 0.436939, mae: 0.371372, mean_q: 1.149490
  1035/100000: episode: 106, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.265, mean reward: 0.426 [0.421, 0.477], mean action: 39.600 [13.000, 86.000], mean observation: 3.169 [-2.450, 10.333], loss: 0.469013, mae: 0.376997, mean_q: 1.160707
  1045/100000: episode: 107, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.111, mean reward: 0.411 [0.404, 0.460], mean action: 40.800 [23.000, 95.000], mean observation: 3.155 [-1.068, 10.277], loss: 0.315349, mae: 0.380905, mean_q: 1.171529
  1055/100000: episode: 108, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.884, mean reward: 0.388 [0.358, 0.440], mean action: 37.800 [1.000, 90.000], mean observation: 3.151 [-1.646, 10.259], loss: 0.438141, mae: 0.385616, mean_q: 1.181957
  1065/100000: episode: 109, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.322, mean reward: 0.432 [0.313, 0.536], mean action: 27.500 [1.000, 61.000], mean observation: 3.169 [-1.344, 10.490], loss: 0.467839, mae: 0.390572, mean_q: 1.192477
  1075/100000: episode: 110, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.205, mean reward: 0.420 [0.376, 0.541], mean action: 33.700 [3.000, 101.000], mean observation: 3.144 [-1.136, 10.261], loss: 0.584097, mae: 0.395549, mean_q: 1.203185
  1085/100000: episode: 111, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.028, mean reward: 0.403 [0.377, 0.480], mean action: 46.500 [18.000, 97.000], mean observation: 3.158 [-1.019, 10.277], loss: 0.465379, mae: 0.400391, mean_q: 1.213846
  1088/100000: episode: 112, duration: 0.055s, episode steps: 3, steps per second: 54, episode reward: 10.759, mean reward: 3.586 [0.367, 10.000], mean action: 43.000 [30.000, 54.000], mean observation: 3.162 [-1.008, 10.144], loss: 0.319479, mae: 0.402900, mean_q: 1.220834
  1098/100000: episode: 113, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.229, mean reward: 0.423 [0.406, 0.470], mean action: 34.800 [6.000, 64.000], mean observation: 3.147 [-1.427, 10.340], loss: 0.601380, mae: 0.406913, mean_q: 1.227545
  1108/100000: episode: 114, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.864, mean reward: 0.386 [0.372, 0.414], mean action: 44.100 [30.000, 91.000], mean observation: 3.179 [-1.494, 10.271], loss: 0.611202, mae: 0.411442, mean_q: 1.237676
  1118/100000: episode: 115, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 4.053, mean reward: 0.405 [0.321, 0.491], mean action: 26.000 [4.000, 30.000], mean observation: 3.165 [-1.752, 10.316], loss: 0.624794, mae: 0.416270, mean_q: 1.247269
  1128/100000: episode: 116, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.856, mean reward: 0.386 [0.326, 0.519], mean action: 39.300 [30.000, 70.000], mean observation: 3.170 [-1.437, 10.307], loss: 0.496424, mae: 0.420958, mean_q: 1.256691
  1138/100000: episode: 117, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.731, mean reward: 0.473 [0.469, 0.509], mean action: 33.800 [6.000, 84.000], mean observation: 3.156 [-1.428, 10.243], loss: 0.345748, mae: 0.425012, mean_q: 1.266206
  1148/100000: episode: 118, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.264, mean reward: 0.426 [0.406, 0.513], mean action: 43.400 [23.000, 98.000], mean observation: 3.144 [-1.645, 10.309], loss: 0.482665, mae: 0.430388, mean_q: 1.276000
  1149/100000: episode: 119, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 40.000 [40.000, 40.000], mean observation: 3.197 [-0.954, 10.424], loss: 0.397735, mae: 0.433326, mean_q: 1.281814
  1159/100000: episode: 120, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 5.212, mean reward: 0.521 [0.510, 0.532], mean action: 39.000 [29.000, 71.000], mean observation: 3.171 [-1.523, 10.231], loss: 0.347545, mae: 0.435109, mean_q: 1.288805
  1169/100000: episode: 121, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.609, mean reward: 0.461 [0.451, 0.536], mean action: 52.300 [13.000, 83.000], mean observation: 3.145 [-1.062, 10.276], loss: 0.759463, mae: 0.440867, mean_q: 1.300357
  1179/100000: episode: 122, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.479, mean reward: 0.448 [0.390, 0.528], mean action: 44.500 [21.000, 85.000], mean observation: 3.158 [-1.375, 10.344], loss: 0.744815, mae: 0.445822, mean_q: 1.310279
  1189/100000: episode: 123, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.240, mean reward: 0.424 [0.354, 0.553], mean action: 31.200 [2.000, 71.000], mean observation: 3.152 [-2.687, 10.279], loss: 0.532175, mae: 0.450630, mean_q: 1.320108
  1199/100000: episode: 124, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.208, mean reward: 0.421 [0.398, 0.461], mean action: 30.900 [20.000, 50.000], mean observation: 3.145 [-1.484, 10.332], loss: 1.107765, mae: 0.456699, mean_q: 1.329375
  1209/100000: episode: 125, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.748, mean reward: 0.375 [0.284, 0.502], mean action: 34.800 [22.000, 60.000], mean observation: 3.150 [-1.422, 10.325], loss: 0.727586, mae: 0.459654, mean_q: 1.344367
  1219/100000: episode: 126, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.233, mean reward: 0.423 [0.412, 0.488], mean action: 38.100 [30.000, 79.000], mean observation: 3.157 [-1.485, 10.277], loss: 0.492307, mae: 0.464513, mean_q: 1.358300
  1229/100000: episode: 127, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.034, mean reward: 0.403 [0.381, 0.445], mean action: 42.400 [4.000, 101.000], mean observation: 3.145 [-1.290, 10.444], loss: 0.813896, mae: 0.469989, mean_q: 1.371547
  1239/100000: episode: 128, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.369, mean reward: 0.437 [0.347, 0.528], mean action: 45.400 [1.000, 95.000], mean observation: 3.163 [-1.890, 10.457], loss: 0.485071, mae: 0.473629, mean_q: 1.384844
  1249/100000: episode: 129, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.716, mean reward: 0.472 [0.472, 0.472], mean action: 34.500 [30.000, 75.000], mean observation: 3.167 [-1.475, 10.546], loss: 1.084168, mae: 0.480900, mean_q: 1.396759
  1259/100000: episode: 130, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.064, mean reward: 0.406 [0.323, 0.582], mean action: 54.900 [30.000, 99.000], mean observation: 3.155 [-1.943, 10.287], loss: 0.654362, mae: 0.484748, mean_q: 1.406528
  1269/100000: episode: 131, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.514, mean reward: 0.451 [0.358, 0.504], mean action: 27.800 [3.000, 55.000], mean observation: 3.155 [-1.187, 10.425], loss: 0.918394, mae: 0.488974, mean_q: 1.415994
  1279/100000: episode: 132, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.555, mean reward: 0.455 [0.455, 0.455], mean action: 49.000 [30.000, 100.000], mean observation: 3.173 [-1.169, 10.439], loss: 1.188951, mae: 0.494804, mean_q: 1.424678
  1289/100000: episode: 133, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.344, mean reward: 0.434 [0.355, 0.551], mean action: 34.900 [15.000, 64.000], mean observation: 3.142 [-1.235, 10.396], loss: 0.685199, mae: 0.499054, mean_q: 1.434371
  1299/100000: episode: 134, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.263, mean reward: 0.426 [0.346, 0.581], mean action: 40.000 [30.000, 87.000], mean observation: 3.150 [-1.846, 10.220], loss: 0.565686, mae: 0.503286, mean_q: 1.444847
  1309/100000: episode: 135, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.935, mean reward: 0.393 [0.386, 0.436], mean action: 48.600 [30.000, 92.000], mean observation: 3.165 [-1.412, 10.346], loss: 0.434365, mae: 0.508312, mean_q: 1.453200
  1319/100000: episode: 136, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.833, mean reward: 0.383 [0.337, 0.511], mean action: 40.500 [8.000, 96.000], mean observation: 3.153 [-1.135, 10.289], loss: 0.723250, mae: 0.513730, mean_q: 1.461724
  1329/100000: episode: 137, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.350, mean reward: 0.435 [0.358, 0.537], mean action: 49.600 [30.000, 92.000], mean observation: 3.168 [-1.347, 10.288], loss: 0.605234, mae: 0.518966, mean_q: 1.469170
  1339/100000: episode: 138, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.047, mean reward: 0.405 [0.320, 0.486], mean action: 36.000 [25.000, 66.000], mean observation: 3.156 [-1.664, 10.307], loss: 0.425040, mae: 0.522679, mean_q: 1.476164
  1349/100000: episode: 139, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.187, mean reward: 0.419 [0.368, 0.521], mean action: 39.900 [15.000, 84.000], mean observation: 3.150 [-1.517, 10.375], loss: 0.825943, mae: 0.528700, mean_q: 1.486437
  1359/100000: episode: 140, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.181, mean reward: 0.418 [0.387, 0.466], mean action: 41.400 [21.000, 94.000], mean observation: 3.155 [-1.631, 10.361], loss: 0.573129, mae: 0.532600, mean_q: 1.497974
  1369/100000: episode: 141, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.777, mean reward: 0.378 [0.311, 0.437], mean action: 38.000 [9.000, 97.000], mean observation: 3.152 [-1.687, 10.397], loss: 0.416235, mae: 0.536382, mean_q: 1.506851
  1379/100000: episode: 142, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.499, mean reward: 0.350 [0.318, 0.440], mean action: 42.700 [30.000, 93.000], mean observation: 3.157 [-1.582, 10.249], loss: 0.586451, mae: 0.541809, mean_q: 1.514755
  1389/100000: episode: 143, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.205, mean reward: 0.420 [0.347, 0.489], mean action: 42.000 [3.000, 98.000], mean observation: 3.150 [-1.088, 10.383], loss: 0.841935, mae: 0.547414, mean_q: 1.522770
  1399/100000: episode: 144, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.993, mean reward: 0.399 [0.342, 0.530], mean action: 51.800 [19.000, 95.000], mean observation: 3.149 [-1.137, 10.184], loss: 0.756674, mae: 0.551949, mean_q: 1.531682
  1409/100000: episode: 145, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.597, mean reward: 0.460 [0.460, 0.460], mean action: 57.600 [30.000, 95.000], mean observation: 3.162 [-1.197, 10.295], loss: 0.561441, mae: 0.556115, mean_q: 1.538352
  1419/100000: episode: 146, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.460, mean reward: 0.446 [0.326, 0.535], mean action: 49.700 [24.000, 99.000], mean observation: 3.155 [-1.635, 10.347], loss: 0.626917, mae: 0.559939, mean_q: 1.546119
  1429/100000: episode: 147, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.853, mean reward: 0.385 [0.344, 0.439], mean action: 37.500 [5.000, 76.000], mean observation: 3.146 [-1.326, 10.282], loss: 0.710145, mae: 0.565580, mean_q: 1.556277
  1439/100000: episode: 148, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.071, mean reward: 0.407 [0.330, 0.517], mean action: 39.600 [10.000, 82.000], mean observation: 3.164 [-1.186, 10.378], loss: 0.767022, mae: 0.571094, mean_q: 1.565959
  1449/100000: episode: 149, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.619, mean reward: 0.462 [0.435, 0.493], mean action: 50.700 [8.000, 99.000], mean observation: 3.160 [-1.894, 10.327], loss: 0.607893, mae: 0.575308, mean_q: 1.575347
  1459/100000: episode: 150, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.096, mean reward: 0.410 [0.309, 0.542], mean action: 39.000 [13.000, 96.000], mean observation: 3.153 [-1.530, 10.336], loss: 0.846456, mae: 0.579795, mean_q: 1.584097
  1469/100000: episode: 151, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.861, mean reward: 0.386 [0.342, 0.435], mean action: 35.300 [12.000, 91.000], mean observation: 3.152 [-1.909, 10.288], loss: 0.606888, mae: 0.584191, mean_q: 1.592093
  1479/100000: episode: 152, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.272, mean reward: 0.427 [0.331, 0.540], mean action: 39.800 [15.000, 89.000], mean observation: 3.165 [-1.720, 10.297], loss: 0.718341, mae: 0.588979, mean_q: 1.600399
  1489/100000: episode: 153, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.666, mean reward: 0.367 [0.326, 0.416], mean action: 38.000 [6.000, 95.000], mean observation: 3.157 [-1.797, 10.207], loss: 0.729234, mae: 0.593772, mean_q: 1.610862
  1499/100000: episode: 154, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.410, mean reward: 0.441 [0.346, 0.521], mean action: 37.900 [10.000, 78.000], mean observation: 3.150 [-2.070, 10.387], loss: 0.615076, mae: 0.599004, mean_q: 1.620518
  1509/100000: episode: 155, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.054, mean reward: 0.405 [0.340, 0.453], mean action: 29.400 [15.000, 45.000], mean observation: 3.155 [-1.745, 10.400], loss: 0.819880, mae: 0.603506, mean_q: 1.630209
  1519/100000: episode: 156, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.264, mean reward: 0.426 [0.406, 0.517], mean action: 31.000 [9.000, 44.000], mean observation: 3.168 [-1.545, 10.277], loss: 0.717177, mae: 0.608700, mean_q: 1.641567
  1529/100000: episode: 157, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.957, mean reward: 0.396 [0.360, 0.576], mean action: 47.500 [30.000, 100.000], mean observation: 3.171 [-1.660, 10.349], loss: 0.521937, mae: 0.613264, mean_q: 1.651283
  1539/100000: episode: 158, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.165, mean reward: 0.416 [0.386, 0.500], mean action: 42.000 [6.000, 86.000], mean observation: 3.166 [-1.900, 10.338], loss: 0.473403, mae: 0.617125, mean_q: 1.659001
  1549/100000: episode: 159, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.021, mean reward: 0.402 [0.381, 0.448], mean action: 46.100 [6.000, 95.000], mean observation: 3.151 [-0.992, 10.474], loss: 0.508617, mae: 0.622488, mean_q: 1.667129
  1559/100000: episode: 160, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.860, mean reward: 0.386 [0.300, 0.439], mean action: 35.000 [18.000, 83.000], mean observation: 3.156 [-1.752, 10.326], loss: 0.572021, mae: 0.625785, mean_q: 1.675993
  1569/100000: episode: 161, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.023, mean reward: 0.402 [0.371, 0.458], mean action: 38.500 [4.000, 88.000], mean observation: 3.150 [-1.189, 10.251], loss: 0.522489, mae: 0.631807, mean_q: 1.684559
  1579/100000: episode: 162, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.102, mean reward: 0.410 [0.372, 0.492], mean action: 39.100 [10.000, 93.000], mean observation: 3.148 [-1.784, 10.408], loss: 0.493657, mae: 0.635879, mean_q: 1.691966
  1589/100000: episode: 163, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.022, mean reward: 0.402 [0.325, 0.486], mean action: 36.700 [9.000, 62.000], mean observation: 3.164 [-1.434, 10.282], loss: 0.537353, mae: 0.641117, mean_q: 1.699340
  1599/100000: episode: 164, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.830, mean reward: 0.383 [0.314, 0.465], mean action: 36.800 [30.000, 96.000], mean observation: 3.150 [-1.315, 10.292], loss: 0.558340, mae: 0.646593, mean_q: 1.705552
  1609/100000: episode: 165, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.335, mean reward: 0.434 [0.404, 0.512], mean action: 56.000 [21.000, 100.000], mean observation: 3.163 [-2.476, 10.343], loss: 0.689406, mae: 0.652703, mean_q: 1.711112
  1619/100000: episode: 166, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.622, mean reward: 0.362 [0.343, 0.414], mean action: 34.600 [30.000, 64.000], mean observation: 3.166 [-1.220, 10.346], loss: 0.715599, mae: 0.656076, mean_q: 1.718745
  1629/100000: episode: 167, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.799, mean reward: 0.380 [0.316, 0.430], mean action: 41.300 [30.000, 94.000], mean observation: 3.170 [-1.211, 10.468], loss: 0.664987, mae: 0.660573, mean_q: 1.728087
  1639/100000: episode: 168, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.697, mean reward: 0.470 [0.457, 0.531], mean action: 39.300 [30.000, 84.000], mean observation: 3.148 [-1.529, 10.423], loss: 0.652651, mae: 0.665689, mean_q: 1.734997
  1649/100000: episode: 169, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.950, mean reward: 0.495 [0.316, 0.571], mean action: 36.800 [1.000, 69.000], mean observation: 3.156 [-1.253, 10.337], loss: 0.518543, mae: 0.669631, mean_q: 1.742970
  1659/100000: episode: 170, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.631, mean reward: 0.463 [0.450, 0.579], mean action: 44.700 [20.000, 101.000], mean observation: 3.146 [-1.377, 10.328], loss: 0.633437, mae: 0.674007, mean_q: 1.749638
  1669/100000: episode: 171, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.348, mean reward: 0.435 [0.365, 0.482], mean action: 32.700 [17.000, 51.000], mean observation: 3.154 [-1.727, 10.309], loss: 0.817723, mae: 0.679652, mean_q: 1.755858
  1679/100000: episode: 172, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.274, mean reward: 0.427 [0.413, 0.534], mean action: 41.200 [30.000, 101.000], mean observation: 3.145 [-0.994, 10.328], loss: 0.593887, mae: 0.684556, mean_q: 1.762042
  1689/100000: episode: 173, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.097, mean reward: 0.410 [0.312, 0.520], mean action: 40.900 [2.000, 88.000], mean observation: 3.158 [-1.234, 10.398], loss: 0.652998, mae: 0.689335, mean_q: 1.767255
  1699/100000: episode: 174, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.881, mean reward: 0.388 [0.331, 0.458], mean action: 38.500 [30.000, 65.000], mean observation: 3.172 [-1.091, 10.255], loss: 0.570017, mae: 0.694004, mean_q: 1.774192
  1709/100000: episode: 175, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.310, mean reward: 0.431 [0.360, 0.543], mean action: 37.300 [30.000, 74.000], mean observation: 3.162 [-0.944, 10.316], loss: 0.511158, mae: 0.697337, mean_q: 1.780945
  1719/100000: episode: 176, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.205, mean reward: 0.421 [0.346, 0.496], mean action: 37.300 [24.000, 72.000], mean observation: 3.165 [-1.642, 10.256], loss: 0.673404, mae: 0.703235, mean_q: 1.787062
  1729/100000: episode: 177, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 5.098, mean reward: 0.510 [0.504, 0.534], mean action: 42.600 [27.000, 91.000], mean observation: 3.168 [-1.347, 10.396], loss: 0.538663, mae: 0.706792, mean_q: 1.794922
  1739/100000: episode: 178, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.219, mean reward: 0.422 [0.407, 0.469], mean action: 43.000 [24.000, 86.000], mean observation: 3.164 [-1.465, 10.397], loss: 0.532311, mae: 0.711471, mean_q: 1.803047
  1749/100000: episode: 179, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.612, mean reward: 0.461 [0.375, 0.579], mean action: 45.600 [3.000, 89.000], mean observation: 3.157 [-1.339, 10.313], loss: 0.675417, mae: 0.716725, mean_q: 1.810346
  1759/100000: episode: 180, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.129, mean reward: 0.413 [0.400, 0.478], mean action: 37.600 [17.000, 88.000], mean observation: 3.161 [-1.611, 10.483], loss: 0.734857, mae: 0.720108, mean_q: 1.817164
  1769/100000: episode: 181, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.874, mean reward: 0.487 [0.486, 0.498], mean action: 36.500 [11.000, 77.000], mean observation: 3.160 [-1.686, 10.313], loss: 0.792926, mae: 0.725497, mean_q: 1.826459
  1779/100000: episode: 182, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.991, mean reward: 0.399 [0.325, 0.485], mean action: 42.500 [30.000, 98.000], mean observation: 3.162 [-1.085, 10.328], loss: 0.745557, mae: 0.728819, mean_q: 1.835648
  1789/100000: episode: 183, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.563, mean reward: 0.456 [0.371, 0.594], mean action: 48.800 [10.000, 100.000], mean observation: 3.156 [-1.166, 10.373], loss: 0.701309, mae: 0.733897, mean_q: 1.844427
  1799/100000: episode: 184, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.005, mean reward: 0.400 [0.346, 0.503], mean action: 49.800 [20.000, 101.000], mean observation: 3.157 [-0.922, 10.422], loss: 0.657919, mae: 0.737704, mean_q: 1.851535
  1809/100000: episode: 185, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 3.929, mean reward: 0.393 [0.328, 0.486], mean action: 35.400 [0.000, 85.000], mean observation: 3.162 [-1.298, 10.347], loss: 0.664763, mae: 0.742559, mean_q: 1.859291
  1819/100000: episode: 186, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.000, mean reward: 0.400 [0.305, 0.448], mean action: 50.300 [9.000, 86.000], mean observation: 3.147 [-1.613, 10.225], loss: 0.666826, mae: 0.746657, mean_q: 1.868265
  1829/100000: episode: 187, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.875, mean reward: 0.388 [0.322, 0.466], mean action: 38.100 [11.000, 79.000], mean observation: 3.162 [-1.517, 10.200], loss: 0.691794, mae: 0.751707, mean_q: 1.875548
  1839/100000: episode: 188, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.223, mean reward: 0.422 [0.398, 0.551], mean action: 53.100 [17.000, 101.000], mean observation: 3.174 [-1.762, 10.339], loss: 0.552928, mae: 0.755407, mean_q: 1.881811
  1849/100000: episode: 189, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.977, mean reward: 0.398 [0.318, 0.505], mean action: 42.600 [24.000, 92.000], mean observation: 3.151 [-1.253, 10.537], loss: 0.580191, mae: 0.760171, mean_q: 1.888752
  1859/100000: episode: 190, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.137, mean reward: 0.414 [0.344, 0.528], mean action: 48.800 [30.000, 97.000], mean observation: 3.165 [-1.569, 10.492], loss: 0.600330, mae: 0.764848, mean_q: 1.895172
  1869/100000: episode: 191, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.939, mean reward: 0.394 [0.315, 0.438], mean action: 36.400 [16.000, 87.000], mean observation: 3.176 [-1.955, 10.493], loss: 0.894704, mae: 0.770378, mean_q: 1.899063
  1879/100000: episode: 192, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.285, mean reward: 0.428 [0.396, 0.501], mean action: 42.200 [30.000, 86.000], mean observation: 3.178 [-0.775, 10.195], loss: 0.626479, mae: 0.773011, mean_q: 1.903342
  1889/100000: episode: 193, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.892, mean reward: 0.389 [0.339, 0.435], mean action: 50.900 [0.000, 98.000], mean observation: 3.153 [-1.195, 10.299], loss: 0.713505, mae: 0.778323, mean_q: 1.911632
  1899/100000: episode: 194, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 4.193, mean reward: 0.419 [0.333, 0.522], mean action: 25.200 [4.000, 30.000], mean observation: 3.155 [-1.569, 10.299], loss: 0.723505, mae: 0.783031, mean_q: 1.920181
  1909/100000: episode: 195, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.186, mean reward: 0.419 [0.330, 0.518], mean action: 31.900 [2.000, 70.000], mean observation: 3.158 [-1.209, 10.204], loss: 0.615343, mae: 0.787648, mean_q: 1.927657
  1919/100000: episode: 196, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 6.528, mean reward: 0.653 [0.653, 0.653], mean action: 28.600 [2.000, 90.000], mean observation: 3.149 [-1.252, 10.397], loss: 0.775300, mae: 0.792713, mean_q: 1.932633
  1929/100000: episode: 197, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.815, mean reward: 0.381 [0.319, 0.463], mean action: 37.300 [30.000, 76.000], mean observation: 3.159 [-0.997, 10.375], loss: 0.752225, mae: 0.795975, mean_q: 1.938150
  1939/100000: episode: 198, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.012, mean reward: 0.401 [0.396, 0.406], mean action: 52.000 [30.000, 89.000], mean observation: 3.154 [-0.836, 10.318], loss: 0.595101, mae: 0.801108, mean_q: 1.946975
  1949/100000: episode: 199, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.459, mean reward: 0.446 [0.443, 0.460], mean action: 52.100 [30.000, 96.000], mean observation: 3.150 [-1.260, 10.217], loss: 0.510485, mae: 0.803560, mean_q: 1.953517
  1959/100000: episode: 200, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.128, mean reward: 0.413 [0.387, 0.446], mean action: 45.400 [15.000, 86.000], mean observation: 3.160 [-1.369, 10.237], loss: 0.529511, mae: 0.807424, mean_q: 1.958779
  1969/100000: episode: 201, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.153, mean reward: 0.415 [0.335, 0.535], mean action: 29.100 [5.000, 62.000], mean observation: 3.160 [-2.248, 10.261], loss: 0.835700, mae: 0.813653, mean_q: 1.966045
  1979/100000: episode: 202, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.340, mean reward: 0.434 [0.361, 0.572], mean action: 38.800 [22.000, 68.000], mean observation: 3.165 [-1.545, 10.471], loss: 0.613723, mae: 0.817363, mean_q: 1.975690
  1989/100000: episode: 203, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.253, mean reward: 0.425 [0.415, 0.490], mean action: 35.900 [7.000, 82.000], mean observation: 3.146 [-1.431, 10.390], loss: 0.712509, mae: 0.821740, mean_q: 1.983430
  1999/100000: episode: 204, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.037, mean reward: 0.404 [0.352, 0.496], mean action: 32.500 [9.000, 83.000], mean observation: 3.159 [-1.331, 10.425], loss: 0.611517, mae: 0.826466, mean_q: 1.991678
  2009/100000: episode: 205, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.481, mean reward: 0.448 [0.422, 0.544], mean action: 49.700 [29.000, 89.000], mean observation: 3.148 [-1.737, 10.461], loss: 0.732419, mae: 0.830706, mean_q: 1.999173
  2019/100000: episode: 206, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.367, mean reward: 0.437 [0.295, 0.507], mean action: 53.200 [8.000, 92.000], mean observation: 3.163 [-1.318, 10.340], loss: 0.765123, mae: 0.835565, mean_q: 2.006597
  2029/100000: episode: 207, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.772, mean reward: 0.377 [0.324, 0.534], mean action: 37.900 [12.000, 92.000], mean observation: 3.146 [-1.623, 10.295], loss: 0.679474, mae: 0.841076, mean_q: 2.012585
  2039/100000: episode: 208, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.059, mean reward: 0.406 [0.387, 0.431], mean action: 46.300 [1.000, 95.000], mean observation: 3.159 [-1.196, 10.236], loss: 0.660455, mae: 0.845697, mean_q: 2.016952
  2049/100000: episode: 209, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.395, mean reward: 0.439 [0.387, 0.466], mean action: 38.300 [15.000, 94.000], mean observation: 3.158 [-1.583, 10.188], loss: 0.612245, mae: 0.849459, mean_q: 2.021240
  2059/100000: episode: 210, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.085, mean reward: 0.408 [0.358, 0.498], mean action: 38.900 [0.000, 101.000], mean observation: 3.152 [-1.249, 10.298], loss: 0.907797, mae: 0.854644, mean_q: 2.026271
  2069/100000: episode: 211, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.383, mean reward: 0.438 [0.377, 0.512], mean action: 36.400 [11.000, 78.000], mean observation: 3.164 [-1.447, 10.260], loss: 0.625808, mae: 0.857892, mean_q: 2.031685
  2079/100000: episode: 212, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 3.973, mean reward: 0.397 [0.342, 0.539], mean action: 35.900 [8.000, 95.000], mean observation: 3.153 [-1.946, 10.206], loss: 0.747022, mae: 0.862573, mean_q: 2.036932
  2089/100000: episode: 213, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.311, mean reward: 0.431 [0.405, 0.538], mean action: 25.500 [6.000, 87.000], mean observation: 3.157 [-2.150, 10.298], loss: 0.624396, mae: 0.866995, mean_q: 2.043977
  2099/100000: episode: 214, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.374, mean reward: 0.437 [0.433, 0.471], mean action: 39.400 [30.000, 76.000], mean observation: 3.159 [-1.138, 10.292], loss: 0.810721, mae: 0.871037, mean_q: 2.050974
  2109/100000: episode: 215, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.055, mean reward: 0.405 [0.345, 0.482], mean action: 37.800 [22.000, 101.000], mean observation: 3.155 [-1.328, 10.316], loss: 1.067083, mae: 0.877310, mean_q: 2.059072
  2119/100000: episode: 216, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.978, mean reward: 0.398 [0.332, 0.497], mean action: 39.000 [2.000, 92.000], mean observation: 3.153 [-2.396, 10.219], loss: 0.877632, mae: 0.879448, mean_q: 2.064528
  2129/100000: episode: 217, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.247, mean reward: 0.425 [0.387, 0.495], mean action: 38.200 [18.000, 82.000], mean observation: 3.147 [-1.758, 10.348], loss: 0.778078, mae: 0.884927, mean_q: 2.069355
  2139/100000: episode: 218, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.255, mean reward: 0.425 [0.303, 0.512], mean action: 34.500 [2.000, 100.000], mean observation: 3.149 [-1.854, 10.251], loss: 0.942987, mae: 0.889104, mean_q: 2.074119
  2149/100000: episode: 219, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.254, mean reward: 0.425 [0.358, 0.571], mean action: 39.000 [1.000, 86.000], mean observation: 3.154 [-1.669, 10.291], loss: 0.761673, mae: 0.893270, mean_q: 2.082410
  2159/100000: episode: 220, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.651, mean reward: 0.365 [0.292, 0.468], mean action: 38.600 [30.000, 82.000], mean observation: 3.155 [-1.371, 10.358], loss: 0.839162, mae: 0.898399, mean_q: 2.087776
  2169/100000: episode: 221, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.099, mean reward: 0.410 [0.337, 0.512], mean action: 54.100 [2.000, 99.000], mean observation: 3.151 [-0.666, 10.252], loss: 0.755618, mae: 0.902660, mean_q: 2.090531
  2179/100000: episode: 222, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.037, mean reward: 0.404 [0.311, 0.502], mean action: 43.300 [10.000, 100.000], mean observation: 3.155 [-1.669, 10.231], loss: 0.783767, mae: 0.907488, mean_q: 2.096874
  2189/100000: episode: 223, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.886, mean reward: 0.389 [0.333, 0.454], mean action: 42.600 [28.000, 80.000], mean observation: 3.162 [-1.193, 10.284], loss: 0.621948, mae: 0.910276, mean_q: 2.103447
  2199/100000: episode: 224, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.391, mean reward: 0.439 [0.379, 0.543], mean action: 40.600 [8.000, 93.000], mean observation: 3.164 [-1.139, 10.358], loss: 0.645315, mae: 0.914940, mean_q: 2.108880
  2209/100000: episode: 225, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.108, mean reward: 0.411 [0.332, 0.468], mean action: 49.600 [25.000, 100.000], mean observation: 3.164 [-0.617, 10.283], loss: 0.652072, mae: 0.919552, mean_q: 2.114529
  2219/100000: episode: 226, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.574, mean reward: 0.357 [0.271, 0.540], mean action: 49.800 [30.000, 86.000], mean observation: 3.151 [-0.889, 10.304], loss: 0.797769, mae: 0.924027, mean_q: 2.119979
  2229/100000: episode: 227, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.907, mean reward: 0.391 [0.313, 0.544], mean action: 41.600 [11.000, 86.000], mean observation: 3.160 [-1.449, 10.441], loss: 0.644802, mae: 0.927329, mean_q: 2.126328
  2239/100000: episode: 228, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.986, mean reward: 0.399 [0.345, 0.511], mean action: 54.400 [30.000, 94.000], mean observation: 3.161 [-1.537, 10.399], loss: 0.809625, mae: 0.932583, mean_q: 2.132306
  2249/100000: episode: 229, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.142, mean reward: 0.414 [0.320, 0.499], mean action: 37.200 [9.000, 87.000], mean observation: 3.163 [-1.668, 10.156], loss: 0.604899, mae: 0.935630, mean_q: 2.137048
  2259/100000: episode: 230, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.831, mean reward: 0.383 [0.333, 0.453], mean action: 43.800 [14.000, 100.000], mean observation: 3.159 [-0.940, 10.273], loss: 0.610246, mae: 0.939503, mean_q: 2.143461
  2269/100000: episode: 231, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 3.841, mean reward: 0.384 [0.352, 0.514], mean action: 44.800 [2.000, 99.000], mean observation: 3.162 [-1.068, 10.296], loss: 0.735773, mae: 0.944548, mean_q: 2.150515
  2279/100000: episode: 232, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.064, mean reward: 0.406 [0.376, 0.570], mean action: 41.900 [8.000, 87.000], mean observation: 3.144 [-1.875, 10.231], loss: 0.709961, mae: 0.949176, mean_q: 2.157364
  2289/100000: episode: 233, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.316, mean reward: 0.432 [0.382, 0.506], mean action: 41.600 [3.000, 101.000], mean observation: 3.159 [-1.763, 10.359], loss: 0.716273, mae: 0.951619, mean_q: 2.161742
  2299/100000: episode: 234, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.906, mean reward: 0.391 [0.351, 0.441], mean action: 55.300 [16.000, 98.000], mean observation: 3.164 [-1.651, 10.346], loss: 0.689519, mae: 0.957590, mean_q: 2.168828
  2309/100000: episode: 235, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.055, mean reward: 0.406 [0.346, 0.489], mean action: 35.100 [13.000, 92.000], mean observation: 3.155 [-1.790, 10.502], loss: 0.685768, mae: 0.962199, mean_q: 2.175303
  2319/100000: episode: 236, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.578, mean reward: 0.458 [0.380, 0.545], mean action: 38.400 [1.000, 88.000], mean observation: 3.155 [-1.999, 10.292], loss: 0.729865, mae: 0.965757, mean_q: 2.182563
  2329/100000: episode: 237, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.815, mean reward: 0.481 [0.395, 0.510], mean action: 43.500 [6.000, 97.000], mean observation: 3.149 [-0.993, 10.328], loss: 0.622625, mae: 0.969758, mean_q: 2.191152
  2339/100000: episode: 238, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 3.723, mean reward: 0.372 [0.315, 0.432], mean action: 34.300 [30.000, 72.000], mean observation: 3.164 [-1.567, 10.438], loss: 0.803228, mae: 0.974876, mean_q: 2.198890
  2349/100000: episode: 239, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.602, mean reward: 0.460 [0.385, 0.479], mean action: 54.200 [0.000, 101.000], mean observation: 3.158 [-1.521, 10.237], loss: 0.723867, mae: 0.977607, mean_q: 2.204092
  2359/100000: episode: 240, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.144, mean reward: 0.414 [0.359, 0.496], mean action: 34.000 [1.000, 81.000], mean observation: 3.158 [-1.086, 10.145], loss: 0.754406, mae: 0.982524, mean_q: 2.211096
  2369/100000: episode: 241, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.015, mean reward: 0.402 [0.329, 0.461], mean action: 36.200 [8.000, 63.000], mean observation: 3.156 [-1.430, 10.340], loss: 0.792026, mae: 0.987478, mean_q: 2.218627
  2379/100000: episode: 242, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.035, mean reward: 0.404 [0.350, 0.470], mean action: 47.700 [9.000, 98.000], mean observation: 3.146 [-1.822, 10.236], loss: 0.648965, mae: 0.989924, mean_q: 2.226202
  2389/100000: episode: 243, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.104, mean reward: 0.410 [0.299, 0.516], mean action: 47.500 [30.000, 93.000], mean observation: 3.168 [-1.243, 10.421], loss: 0.614361, mae: 0.994454, mean_q: 2.233437
  2399/100000: episode: 244, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.056, mean reward: 0.406 [0.305, 0.530], mean action: 41.000 [10.000, 76.000], mean observation: 3.161 [-1.213, 10.419], loss: 0.711967, mae: 0.999546, mean_q: 2.242959
  2409/100000: episode: 245, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.982, mean reward: 0.398 [0.334, 0.453], mean action: 33.700 [21.000, 58.000], mean observation: 3.164 [-1.441, 10.420], loss: 0.657189, mae: 1.002373, mean_q: 2.249772
  2419/100000: episode: 246, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.097, mean reward: 0.410 [0.362, 0.469], mean action: 51.400 [17.000, 101.000], mean observation: 3.154 [-1.845, 10.455], loss: 0.956729, mae: 1.007856, mean_q: 2.255908
  2429/100000: episode: 247, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.645, mean reward: 0.365 [0.312, 0.404], mean action: 36.300 [4.000, 91.000], mean observation: 3.167 [-1.241, 10.418], loss: 0.655452, mae: 1.011391, mean_q: 2.262816
  2439/100000: episode: 248, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.561, mean reward: 0.456 [0.442, 0.523], mean action: 32.700 [16.000, 59.000], mean observation: 3.166 [-1.773, 10.421], loss: 0.780674, mae: 1.015732, mean_q: 2.270532
  2449/100000: episode: 249, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 3.891, mean reward: 0.389 [0.375, 0.450], mean action: 33.700 [8.000, 74.000], mean observation: 3.149 [-1.507, 10.409], loss: 0.789615, mae: 1.019104, mean_q: 2.278516
  2459/100000: episode: 250, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.911, mean reward: 0.391 [0.345, 0.479], mean action: 38.200 [6.000, 94.000], mean observation: 3.159 [-1.571, 10.270], loss: 0.662008, mae: 1.023104, mean_q: 2.285676
  2469/100000: episode: 251, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.139, mean reward: 0.414 [0.359, 0.473], mean action: 33.600 [2.000, 63.000], mean observation: 3.143 [-1.364, 10.385], loss: 0.691806, mae: 1.027507, mean_q: 2.292919
  2479/100000: episode: 252, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.493, mean reward: 0.449 [0.389, 0.522], mean action: 39.200 [0.000, 85.000], mean observation: 3.157 [-1.769, 10.309], loss: 0.774493, mae: 1.033302, mean_q: 2.297484
  2489/100000: episode: 253, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.361, mean reward: 0.436 [0.354, 0.564], mean action: 31.700 [7.000, 73.000], mean observation: 3.169 [-1.527, 10.308], loss: 0.882534, mae: 1.036977, mean_q: 2.300026
  2499/100000: episode: 254, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.148, mean reward: 0.415 [0.328, 0.553], mean action: 54.400 [30.000, 100.000], mean observation: 3.153 [-1.435, 10.517], loss: 0.700426, mae: 1.041473, mean_q: 2.306749
  2509/100000: episode: 255, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.804, mean reward: 0.380 [0.323, 0.448], mean action: 40.300 [30.000, 83.000], mean observation: 3.150 [-1.409, 10.403], loss: 0.962198, mae: 1.045647, mean_q: 2.312922
  2519/100000: episode: 256, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.937, mean reward: 0.394 [0.311, 0.438], mean action: 47.800 [22.000, 101.000], mean observation: 3.155 [-1.022, 10.217], loss: 0.832918, mae: 1.050090, mean_q: 2.317348
  2529/100000: episode: 257, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.037, mean reward: 0.404 [0.348, 0.507], mean action: 39.400 [2.000, 92.000], mean observation: 3.168 [-1.584, 10.430], loss: 0.732615, mae: 1.054579, mean_q: 2.321251
  2539/100000: episode: 258, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.917, mean reward: 0.392 [0.357, 0.496], mean action: 53.600 [9.000, 101.000], mean observation: 3.145 [-1.142, 10.295], loss: 0.746385, mae: 1.058999, mean_q: 2.325779
  2549/100000: episode: 259, duration: 0.112s, episode steps: 10, steps per second: 90, episode reward: 4.899, mean reward: 0.490 [0.340, 0.554], mean action: 74.300 [20.000, 101.000], mean observation: 3.147 [-0.962, 10.320], loss: 0.814390, mae: 1.063249, mean_q: 2.331308
  2556/100000: episode: 260, duration: 0.118s, episode steps: 7, steps per second: 59, episode reward: 12.607, mean reward: 1.801 [0.427, 10.000], mean action: 43.286 [30.000, 82.000], mean observation: 3.155 [-1.092, 10.465], loss: 1.056411, mae: 1.065975, mean_q: 2.335817
  2566/100000: episode: 261, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.228, mean reward: 0.423 [0.366, 0.593], mean action: 38.800 [3.000, 76.000], mean observation: 3.145 [-1.773, 10.316], loss: 0.783260, mae: 1.070989, mean_q: 2.340415
  2576/100000: episode: 262, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.803, mean reward: 0.380 [0.358, 0.461], mean action: 46.600 [30.000, 97.000], mean observation: 3.151 [-2.596, 10.148], loss: 0.713194, mae: 1.074145, mean_q: 2.344157
  2586/100000: episode: 263, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.845, mean reward: 0.384 [0.322, 0.487], mean action: 41.600 [12.000, 87.000], mean observation: 3.163 [-1.148, 10.368], loss: 0.721472, mae: 1.077739, mean_q: 2.346976
  2596/100000: episode: 264, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.710, mean reward: 0.371 [0.306, 0.453], mean action: 42.700 [30.000, 101.000], mean observation: 3.163 [-1.190, 10.319], loss: 0.769326, mae: 1.082975, mean_q: 2.349906
  2606/100000: episode: 265, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.776, mean reward: 0.378 [0.293, 0.501], mean action: 34.900 [8.000, 97.000], mean observation: 3.154 [-2.149, 10.240], loss: 0.723150, mae: 1.086733, mean_q: 2.354192
  2616/100000: episode: 266, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.361, mean reward: 0.436 [0.324, 0.553], mean action: 35.900 [11.000, 76.000], mean observation: 3.148 [-1.197, 10.325], loss: 0.749577, mae: 1.089732, mean_q: 2.361753
  2626/100000: episode: 267, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.947, mean reward: 0.395 [0.364, 0.459], mean action: 31.400 [2.000, 75.000], mean observation: 3.157 [-1.638, 10.304], loss: 0.706978, mae: 1.094379, mean_q: 2.370212
  2636/100000: episode: 268, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.043, mean reward: 0.404 [0.352, 0.524], mean action: 42.700 [10.000, 78.000], mean observation: 3.160 [-1.589, 10.326], loss: 0.946750, mae: 1.099399, mean_q: 2.378262
  2646/100000: episode: 269, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.091, mean reward: 0.409 [0.407, 0.430], mean action: 35.000 [17.000, 81.000], mean observation: 3.157 [-1.094, 10.315], loss: 0.773263, mae: 1.103804, mean_q: 2.386516
  2656/100000: episode: 270, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.433, mean reward: 0.443 [0.372, 0.573], mean action: 36.500 [30.000, 70.000], mean observation: 3.149 [-1.619, 10.266], loss: 0.871521, mae: 1.107967, mean_q: 2.392153
  2666/100000: episode: 271, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.254, mean reward: 0.425 [0.338, 0.549], mean action: 42.000 [16.000, 87.000], mean observation: 3.159 [-1.613, 10.418], loss: 0.817399, mae: 1.111222, mean_q: 2.398745
  2676/100000: episode: 272, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.130, mean reward: 0.413 [0.385, 0.489], mean action: 37.200 [1.000, 59.000], mean observation: 3.162 [-1.580, 10.357], loss: 0.836808, mae: 1.115279, mean_q: 2.405988
  2686/100000: episode: 273, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.692, mean reward: 0.369 [0.286, 0.464], mean action: 40.900 [30.000, 84.000], mean observation: 3.144 [-1.432, 10.281], loss: 0.744942, mae: 1.120951, mean_q: 2.411615
  2696/100000: episode: 274, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.765, mean reward: 0.376 [0.366, 0.419], mean action: 53.800 [30.000, 97.000], mean observation: 3.152 [-2.180, 10.228], loss: 0.668424, mae: 1.123226, mean_q: 2.417384
  2706/100000: episode: 275, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 13.929, mean reward: 1.393 [0.394, 10.000], mean action: 50.800 [30.000, 88.000], mean observation: 3.167 [-1.921, 10.293], loss: 0.823547, mae: 1.128929, mean_q: 2.423502
  2716/100000: episode: 276, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.316, mean reward: 0.432 [0.299, 0.509], mean action: 43.900 [30.000, 79.000], mean observation: 3.155 [-1.518, 10.325], loss: 0.790659, mae: 1.131294, mean_q: 2.431061
  2726/100000: episode: 277, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.900, mean reward: 0.390 [0.313, 0.454], mean action: 29.700 [3.000, 78.000], mean observation: 3.149 [-1.294, 10.369], loss: 0.946253, mae: 1.136890, mean_q: 2.438744
  2736/100000: episode: 278, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.030, mean reward: 0.403 [0.322, 0.504], mean action: 48.100 [7.000, 100.000], mean observation: 3.158 [-1.030, 10.273], loss: 0.861918, mae: 1.141197, mean_q: 2.447398
  2746/100000: episode: 279, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.378, mean reward: 0.438 [0.300, 0.513], mean action: 32.900 [11.000, 88.000], mean observation: 3.147 [-1.613, 10.364], loss: 0.730803, mae: 1.144079, mean_q: 2.453681
  2756/100000: episode: 280, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.379, mean reward: 0.438 [0.436, 0.459], mean action: 38.300 [30.000, 91.000], mean observation: 3.162 [-2.401, 10.282], loss: 0.787622, mae: 1.149671, mean_q: 2.458111
  2766/100000: episode: 281, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.124, mean reward: 0.412 [0.356, 0.507], mean action: 37.900 [4.000, 83.000], mean observation: 3.153 [-1.739, 10.192], loss: 0.952711, mae: 1.155058, mean_q: 2.463117
  2776/100000: episode: 282, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.061, mean reward: 0.406 [0.380, 0.438], mean action: 39.600 [30.000, 96.000], mean observation: 3.157 [-2.273, 10.312], loss: 0.878768, mae: 1.157469, mean_q: 2.468007
  2786/100000: episode: 283, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.994, mean reward: 0.399 [0.298, 0.541], mean action: 35.700 [5.000, 75.000], mean observation: 3.160 [-1.784, 10.258], loss: 0.852630, mae: 1.161849, mean_q: 2.471737
  2796/100000: episode: 284, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.481, mean reward: 0.448 [0.374, 0.497], mean action: 32.200 [0.000, 81.000], mean observation: 3.161 [-1.481, 10.290], loss: 0.911283, mae: 1.165841, mean_q: 2.477231
  2806/100000: episode: 285, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.511, mean reward: 0.451 [0.386, 0.501], mean action: 48.600 [3.000, 78.000], mean observation: 3.176 [-1.030, 10.340], loss: 0.862463, mae: 1.169796, mean_q: 2.482230
  2807/100000: episode: 286, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 1.000 [1.000, 1.000], mean observation: 3.116 [-0.789, 10.100], loss: 0.464130, mae: 1.166736, mean_q: 2.485077
  2817/100000: episode: 287, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 5.017, mean reward: 0.502 [0.349, 0.556], mean action: 36.100 [4.000, 99.000], mean observation: 3.155 [-1.702, 10.473], loss: 0.729533, mae: 1.173395, mean_q: 2.488179
  2827/100000: episode: 288, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.330, mean reward: 0.433 [0.366, 0.504], mean action: 38.000 [1.000, 89.000], mean observation: 3.157 [-1.531, 10.278], loss: 0.891003, mae: 1.179272, mean_q: 2.492721
  2837/100000: episode: 289, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.739, mean reward: 0.374 [0.275, 0.484], mean action: 40.400 [30.000, 88.000], mean observation: 3.162 [-1.106, 10.422], loss: 0.813377, mae: 1.182926, mean_q: 2.498010
  2847/100000: episode: 290, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.449, mean reward: 0.445 [0.295, 0.503], mean action: 23.300 [3.000, 36.000], mean observation: 3.170 [-1.522, 10.283], loss: 0.767506, mae: 1.186114, mean_q: 2.501353
  2857/100000: episode: 291, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.574, mean reward: 0.457 [0.370, 0.522], mean action: 49.900 [5.000, 94.000], mean observation: 3.152 [-1.209, 10.222], loss: 0.831146, mae: 1.191344, mean_q: 2.503952
  2867/100000: episode: 292, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 5.028, mean reward: 0.503 [0.400, 0.587], mean action: 39.800 [5.000, 84.000], mean observation: 3.172 [-1.054, 10.325], loss: 0.845661, mae: 1.193398, mean_q: 2.508145
  2873/100000: episode: 293, duration: 0.122s, episode steps: 6, steps per second: 49, episode reward: 12.246, mean reward: 2.041 [0.385, 10.000], mean action: 26.167 [7.000, 30.000], mean observation: 3.159 [-3.101, 10.374], loss: 0.788003, mae: 1.197749, mean_q: 2.513339
  2883/100000: episode: 294, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.519, mean reward: 0.452 [0.331, 0.579], mean action: 40.300 [15.000, 81.000], mean observation: 3.140 [-1.121, 10.294], loss: 1.033648, mae: 1.202265, mean_q: 2.518254
  2893/100000: episode: 295, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.370, mean reward: 0.437 [0.339, 0.490], mean action: 45.500 [5.000, 95.000], mean observation: 3.158 [-2.225, 10.259], loss: 0.998963, mae: 1.205903, mean_q: 2.525387
  2903/100000: episode: 296, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.509, mean reward: 0.451 [0.442, 0.532], mean action: 31.500 [7.000, 78.000], mean observation: 3.161 [-1.569, 10.330], loss: 0.910422, mae: 1.209295, mean_q: 2.531163
  2913/100000: episode: 297, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.810, mean reward: 0.381 [0.326, 0.511], mean action: 47.300 [25.000, 101.000], mean observation: 3.150 [-1.336, 10.284], loss: 0.805620, mae: 1.212893, mean_q: 2.535441
  2923/100000: episode: 298, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.445, mean reward: 0.444 [0.443, 0.455], mean action: 44.500 [30.000, 93.000], mean observation: 3.158 [-1.614, 10.473], loss: 1.035641, mae: 1.219123, mean_q: 2.542814
  2933/100000: episode: 299, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.126, mean reward: 0.413 [0.380, 0.501], mean action: 36.700 [29.000, 93.000], mean observation: 3.158 [-1.482, 10.302], loss: 0.863230, mae: 1.220826, mean_q: 2.548943
  2943/100000: episode: 300, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.779, mean reward: 0.378 [0.336, 0.445], mean action: 40.900 [15.000, 89.000], mean observation: 3.146 [-1.549, 10.231], loss: 0.831981, mae: 1.223894, mean_q: 2.555145
  2953/100000: episode: 301, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.207, mean reward: 0.421 [0.280, 0.488], mean action: 38.000 [3.000, 95.000], mean observation: 3.154 [-1.169, 10.427], loss: 0.754301, mae: 1.228069, mean_q: 2.561153
  2963/100000: episode: 302, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.071, mean reward: 0.407 [0.382, 0.524], mean action: 42.600 [30.000, 90.000], mean observation: 3.152 [-1.607, 10.354], loss: 0.835566, mae: 1.231856, mean_q: 2.567153
  2973/100000: episode: 303, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.808, mean reward: 0.381 [0.302, 0.448], mean action: 38.000 [8.000, 76.000], mean observation: 3.159 [-1.310, 10.205], loss: 0.802568, mae: 1.236447, mean_q: 2.573462
  2983/100000: episode: 304, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.041, mean reward: 0.404 [0.306, 0.492], mean action: 32.600 [24.000, 63.000], mean observation: 3.160 [-0.878, 10.275], loss: 0.938117, mae: 1.240121, mean_q: 2.579100
  2993/100000: episode: 305, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.197, mean reward: 0.420 [0.333, 0.582], mean action: 29.600 [3.000, 55.000], mean observation: 3.156 [-1.427, 10.376], loss: 0.961606, mae: 1.243730, mean_q: 2.585936
  3003/100000: episode: 306, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.298, mean reward: 0.430 [0.344, 0.553], mean action: 41.500 [30.000, 71.000], mean observation: 3.156 [-1.159, 10.305], loss: 0.787028, mae: 1.247145, mean_q: 2.590660
  3013/100000: episode: 307, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.147, mean reward: 0.415 [0.352, 0.564], mean action: 51.200 [12.000, 94.000], mean observation: 3.150 [-1.386, 10.375], loss: 0.799728, mae: 1.250176, mean_q: 2.594829
  3023/100000: episode: 308, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.820, mean reward: 0.382 [0.340, 0.504], mean action: 36.400 [17.000, 94.000], mean observation: 3.155 [-1.398, 10.186], loss: 0.930967, mae: 1.255508, mean_q: 2.601279
  3033/100000: episode: 309, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.211, mean reward: 0.421 [0.309, 0.492], mean action: 37.700 [4.000, 66.000], mean observation: 3.151 [-1.650, 10.198], loss: 0.962625, mae: 1.260170, mean_q: 2.606810
  3043/100000: episode: 310, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.321, mean reward: 0.432 [0.342, 0.514], mean action: 47.000 [27.000, 96.000], mean observation: 3.148 [-1.307, 10.369], loss: 1.036411, mae: 1.264297, mean_q: 2.612047
  3053/100000: episode: 311, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.220, mean reward: 0.422 [0.365, 0.550], mean action: 36.200 [2.000, 58.000], mean observation: 3.152 [-1.131, 10.171], loss: 0.947950, mae: 1.268740, mean_q: 2.617824
  3063/100000: episode: 312, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.198, mean reward: 0.420 [0.308, 0.474], mean action: 47.600 [8.000, 93.000], mean observation: 3.159 [-1.349, 10.333], loss: 0.825774, mae: 1.272828, mean_q: 2.620517
  3073/100000: episode: 313, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.266, mean reward: 0.427 [0.404, 0.444], mean action: 43.100 [7.000, 99.000], mean observation: 3.155 [-1.309, 10.396], loss: 0.912541, mae: 1.276240, mean_q: 2.624421
  3083/100000: episode: 314, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.153, mean reward: 0.415 [0.347, 0.486], mean action: 35.800 [1.000, 82.000], mean observation: 3.165 [-1.030, 10.281], loss: 0.887867, mae: 1.280603, mean_q: 2.629993
  3093/100000: episode: 315, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 4.117, mean reward: 0.412 [0.338, 0.457], mean action: 23.500 [1.000, 31.000], mean observation: 3.160 [-1.764, 10.284], loss: 0.884951, mae: 1.283652, mean_q: 2.637320
  3103/100000: episode: 316, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.632, mean reward: 0.363 [0.304, 0.508], mean action: 39.800 [30.000, 81.000], mean observation: 3.156 [-1.235, 10.415], loss: 0.947473, mae: 1.288741, mean_q: 2.644698
  3113/100000: episode: 317, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.959, mean reward: 0.396 [0.338, 0.492], mean action: 44.500 [4.000, 99.000], mean observation: 3.156 [-1.135, 10.289], loss: 0.933070, mae: 1.291797, mean_q: 2.651053
  3123/100000: episode: 318, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.802, mean reward: 0.380 [0.317, 0.441], mean action: 30.500 [1.000, 100.000], mean observation: 3.155 [-1.797, 10.344], loss: 1.040293, mae: 1.297035, mean_q: 2.660242
  3133/100000: episode: 319, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.902, mean reward: 0.390 [0.336, 0.462], mean action: 36.200 [11.000, 60.000], mean observation: 3.141 [-1.603, 10.273], loss: 0.832382, mae: 1.300364, mean_q: 2.666013
  3143/100000: episode: 320, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.102, mean reward: 0.410 [0.351, 0.525], mean action: 27.000 [14.000, 32.000], mean observation: 3.158 [-1.521, 10.299], loss: 1.197843, mae: 1.305009, mean_q: 2.669532
  3153/100000: episode: 321, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.917, mean reward: 0.392 [0.325, 0.508], mean action: 54.200 [30.000, 99.000], mean observation: 3.157 [-1.548, 10.375], loss: 0.826034, mae: 1.307263, mean_q: 2.674645
  3163/100000: episode: 322, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.163, mean reward: 0.416 [0.333, 0.583], mean action: 33.100 [15.000, 81.000], mean observation: 3.149 [-1.210, 10.354], loss: 0.948258, mae: 1.312727, mean_q: 2.681562
  3173/100000: episode: 323, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.023, mean reward: 0.402 [0.388, 0.431], mean action: 43.800 [19.000, 84.000], mean observation: 3.146 [-1.078, 10.372], loss: 0.838121, mae: 1.316336, mean_q: 2.685794
  3183/100000: episode: 324, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.875, mean reward: 0.387 [0.318, 0.477], mean action: 41.400 [19.000, 88.000], mean observation: 3.149 [-1.053, 10.265], loss: 0.954438, mae: 1.321105, mean_q: 2.689693
  3193/100000: episode: 325, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.486, mean reward: 0.449 [0.440, 0.526], mean action: 34.300 [9.000, 93.000], mean observation: 3.161 [-1.356, 10.288], loss: 0.985545, mae: 1.324100, mean_q: 2.693755
  3203/100000: episode: 326, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.998, mean reward: 0.400 [0.340, 0.454], mean action: 40.700 [12.000, 83.000], mean observation: 3.158 [-1.150, 10.378], loss: 0.849059, mae: 1.328596, mean_q: 2.697107
  3204/100000: episode: 327, duration: 0.022s, episode steps: 1, steps per second: 45, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 30.000 [30.000, 30.000], mean observation: 3.176 [-0.862, 10.713], loss: 0.530911, mae: 1.325857, mean_q: 2.699866
  3214/100000: episode: 328, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.101, mean reward: 0.410 [0.361, 0.421], mean action: 31.800 [8.000, 77.000], mean observation: 3.153 [-2.093, 10.355], loss: 1.034150, mae: 1.333565, mean_q: 2.704009
  3224/100000: episode: 329, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.650, mean reward: 0.365 [0.265, 0.490], mean action: 35.400 [24.000, 74.000], mean observation: 3.160 [-1.335, 10.268], loss: 0.936362, mae: 1.337773, mean_q: 2.710776
  3234/100000: episode: 330, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.894, mean reward: 0.389 [0.318, 0.581], mean action: 40.100 [11.000, 92.000], mean observation: 3.149 [-1.420, 10.359], loss: 0.877757, mae: 1.341347, mean_q: 2.716057
  3244/100000: episode: 331, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.065, mean reward: 0.406 [0.317, 0.459], mean action: 54.100 [17.000, 97.000], mean observation: 3.148 [-1.316, 10.174], loss: 1.114758, mae: 1.346175, mean_q: 2.718233
  3254/100000: episode: 332, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.731, mean reward: 0.373 [0.307, 0.453], mean action: 56.900 [7.000, 101.000], mean observation: 3.144 [-1.303, 10.263], loss: 0.975521, mae: 1.350723, mean_q: 2.720509
  3264/100000: episode: 333, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.082, mean reward: 0.408 [0.356, 0.505], mean action: 34.300 [18.000, 63.000], mean observation: 3.163 [-1.018, 10.288], loss: 1.000036, mae: 1.353800, mean_q: 2.724143
  3274/100000: episode: 334, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.511, mean reward: 0.451 [0.297, 0.547], mean action: 38.900 [11.000, 94.000], mean observation: 3.160 [-1.577, 10.275], loss: 0.914699, mae: 1.357065, mean_q: 2.727290
  3284/100000: episode: 335, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.282, mean reward: 0.428 [0.406, 0.566], mean action: 40.900 [30.000, 86.000], mean observation: 3.154 [-1.780, 10.492], loss: 1.085879, mae: 1.361752, mean_q: 2.732516
  3294/100000: episode: 336, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.083, mean reward: 0.408 [0.325, 0.526], mean action: 40.800 [18.000, 95.000], mean observation: 3.152 [-1.463, 10.535], loss: 0.930250, mae: 1.365191, mean_q: 2.737144
  3304/100000: episode: 337, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.785, mean reward: 0.378 [0.291, 0.440], mean action: 44.500 [15.000, 89.000], mean observation: 3.151 [-2.081, 10.358], loss: 1.018997, mae: 1.368839, mean_q: 2.741224
  3314/100000: episode: 338, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.449, mean reward: 0.445 [0.367, 0.578], mean action: 26.700 [3.000, 46.000], mean observation: 3.167 [-1.352, 10.270], loss: 0.906623, mae: 1.372676, mean_q: 2.747382
  3324/100000: episode: 339, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.519, mean reward: 0.452 [0.443, 0.502], mean action: 50.900 [20.000, 101.000], mean observation: 3.146 [-1.469, 10.265], loss: 1.005854, mae: 1.377984, mean_q: 2.752852
  3334/100000: episode: 340, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.790, mean reward: 0.379 [0.343, 0.467], mean action: 53.200 [5.000, 100.000], mean observation: 3.139 [-1.777, 10.462], loss: 0.856974, mae: 1.380563, mean_q: 2.755743
  3338/100000: episode: 341, duration: 0.107s, episode steps: 4, steps per second: 37, episode reward: 11.135, mean reward: 2.784 [0.339, 10.000], mean action: 30.000 [30.000, 30.000], mean observation: 3.154 [-1.735, 10.277], loss: 1.384053, mae: 1.384773, mean_q: 2.758837
  3348/100000: episode: 342, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.068, mean reward: 0.407 [0.317, 0.493], mean action: 33.900 [5.000, 81.000], mean observation: 3.151 [-0.927, 10.285], loss: 0.959716, mae: 1.386657, mean_q: 2.761760
  3358/100000: episode: 343, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.816, mean reward: 0.382 [0.297, 0.497], mean action: 44.300 [30.000, 86.000], mean observation: 3.147 [-1.642, 10.380], loss: 0.835625, mae: 1.389527, mean_q: 2.766992
  3368/100000: episode: 344, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.852, mean reward: 0.385 [0.318, 0.599], mean action: 45.700 [21.000, 88.000], mean observation: 3.151 [-1.816, 10.231], loss: 1.162903, mae: 1.394098, mean_q: 2.771463
  3378/100000: episode: 345, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.753, mean reward: 0.375 [0.282, 0.504], mean action: 39.700 [4.000, 87.000], mean observation: 3.137 [-1.597, 10.305], loss: 0.993817, mae: 1.398291, mean_q: 2.776595
  3388/100000: episode: 346, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.018, mean reward: 0.402 [0.340, 0.469], mean action: 37.900 [1.000, 90.000], mean observation: 3.151 [-1.419, 10.263], loss: 0.905511, mae: 1.401033, mean_q: 2.780406
  3398/100000: episode: 347, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.310, mean reward: 0.431 [0.423, 0.447], mean action: 36.100 [7.000, 93.000], mean observation: 3.153 [-1.462, 10.464], loss: 1.349039, mae: 1.407462, mean_q: 2.785095
  3402/100000: episode: 348, duration: 0.074s, episode steps: 4, steps per second: 54, episode reward: 11.333, mean reward: 2.833 [0.413, 10.000], mean action: 35.750 [30.000, 42.000], mean observation: 3.157 [-1.421, 10.327], loss: 0.894822, mae: 1.409429, mean_q: 2.788723
  3412/100000: episode: 349, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.039, mean reward: 0.404 [0.391, 0.438], mean action: 36.200 [1.000, 101.000], mean observation: 3.153 [-0.937, 10.388], loss: 0.998691, mae: 1.412184, mean_q: 2.791833
  3422/100000: episode: 350, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.910, mean reward: 0.391 [0.305, 0.485], mean action: 40.400 [16.000, 71.000], mean observation: 3.150 [-1.668, 10.248], loss: 1.086579, mae: 1.416320, mean_q: 2.795220
  3432/100000: episode: 351, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 4.375, mean reward: 0.438 [0.384, 0.539], mean action: 51.300 [13.000, 96.000], mean observation: 3.158 [-2.750, 10.299], loss: 0.850594, mae: 1.418732, mean_q: 2.798209
  3442/100000: episode: 352, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.913, mean reward: 0.491 [0.485, 0.549], mean action: 32.000 [11.000, 66.000], mean observation: 3.156 [-1.593, 10.164], loss: 0.955363, mae: 1.423849, mean_q: 2.802370
  3452/100000: episode: 353, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.495, mean reward: 0.349 [0.272, 0.433], mean action: 42.500 [18.000, 89.000], mean observation: 3.157 [-1.192, 10.294], loss: 0.861246, mae: 1.427406, mean_q: 2.807060
  3462/100000: episode: 354, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.714, mean reward: 0.371 [0.328, 0.428], mean action: 48.900 [30.000, 95.000], mean observation: 3.164 [-1.013, 10.317], loss: 0.924385, mae: 1.431044, mean_q: 2.811895
  3472/100000: episode: 355, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.815, mean reward: 0.382 [0.324, 0.468], mean action: 42.400 [30.000, 96.000], mean observation: 3.152 [-1.318, 10.407], loss: 0.941015, mae: 1.435043, mean_q: 2.818208
  3482/100000: episode: 356, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.751, mean reward: 0.475 [0.420, 0.523], mean action: 47.400 [10.000, 91.000], mean observation: 3.159 [-1.391, 10.382], loss: 1.044018, mae: 1.440734, mean_q: 2.824496
  3492/100000: episode: 357, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.857, mean reward: 0.386 [0.360, 0.452], mean action: 28.500 [13.000, 47.000], mean observation: 3.158 [-1.655, 10.271], loss: 0.962372, mae: 1.443460, mean_q: 2.827972
  3502/100000: episode: 358, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.598, mean reward: 0.360 [0.340, 0.399], mean action: 39.300 [16.000, 76.000], mean observation: 3.169 [-0.962, 10.237], loss: 0.844101, mae: 1.446311, mean_q: 2.831650
  3512/100000: episode: 359, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 13.618, mean reward: 1.362 [0.353, 10.000], mean action: 46.900 [0.000, 99.000], mean observation: 3.145 [-1.820, 10.305], loss: 1.072554, mae: 1.450663, mean_q: 2.833436
  3522/100000: episode: 360, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.319, mean reward: 0.432 [0.387, 0.505], mean action: 53.500 [19.000, 101.000], mean observation: 3.171 [-1.331, 10.284], loss: 1.093205, mae: 1.454930, mean_q: 2.837388
  3532/100000: episode: 361, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.766, mean reward: 0.377 [0.306, 0.452], mean action: 34.200 [2.000, 62.000], mean observation: 3.165 [-1.455, 10.418], loss: 1.126326, mae: 1.458455, mean_q: 2.840742
  3542/100000: episode: 362, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.261, mean reward: 0.426 [0.365, 0.488], mean action: 47.800 [13.000, 99.000], mean observation: 3.158 [-1.095, 10.372], loss: 0.886319, mae: 1.461558, mean_q: 2.843557
  3552/100000: episode: 363, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.219, mean reward: 0.422 [0.381, 0.493], mean action: 30.100 [8.000, 56.000], mean observation: 3.155 [-1.467, 10.236], loss: 1.024920, mae: 1.464435, mean_q: 2.846534
  3562/100000: episode: 364, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.352, mean reward: 0.435 [0.390, 0.532], mean action: 38.400 [11.000, 74.000], mean observation: 3.165 [-1.629, 10.345], loss: 0.934297, mae: 1.469502, mean_q: 2.850434
  3572/100000: episode: 365, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.978, mean reward: 0.398 [0.322, 0.572], mean action: 43.000 [1.000, 96.000], mean observation: 3.141 [-1.286, 10.324], loss: 1.238914, mae: 1.474484, mean_q: 2.853881
  3582/100000: episode: 366, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.659, mean reward: 0.466 [0.441, 0.515], mean action: 26.100 [6.000, 45.000], mean observation: 3.166 [-1.469, 10.328], loss: 0.915640, mae: 1.475920, mean_q: 2.857834
  3592/100000: episode: 367, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.162, mean reward: 0.416 [0.317, 0.584], mean action: 39.600 [10.000, 101.000], mean observation: 3.152 [-1.481, 10.254], loss: 0.968800, mae: 1.480499, mean_q: 2.861421
  3602/100000: episode: 368, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.039, mean reward: 0.404 [0.320, 0.519], mean action: 36.800 [30.000, 57.000], mean observation: 3.157 [-1.368, 10.311], loss: 0.992942, mae: 1.484846, mean_q: 2.863506
  3612/100000: episode: 369, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.959, mean reward: 0.396 [0.336, 0.504], mean action: 36.700 [3.000, 69.000], mean observation: 3.158 [-1.905, 10.257], loss: 0.977495, mae: 1.487326, mean_q: 2.866149
  3622/100000: episode: 370, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.534, mean reward: 0.453 [0.417, 0.541], mean action: 48.600 [13.000, 95.000], mean observation: 3.161 [-1.868, 10.484], loss: 1.079265, mae: 1.492101, mean_q: 2.872463
  3632/100000: episode: 371, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.290, mean reward: 0.429 [0.343, 0.573], mean action: 49.400 [28.000, 99.000], mean observation: 3.152 [-1.657, 10.324], loss: 1.282094, mae: 1.495377, mean_q: 2.876793
  3642/100000: episode: 372, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.126, mean reward: 0.413 [0.367, 0.497], mean action: 36.200 [5.000, 97.000], mean observation: 3.154 [-1.317, 10.247], loss: 1.012579, mae: 1.498818, mean_q: 2.880399
  3652/100000: episode: 373, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.140, mean reward: 0.414 [0.341, 0.447], mean action: 36.500 [0.000, 91.000], mean observation: 3.154 [-1.955, 10.422], loss: 0.963838, mae: 1.502222, mean_q: 2.886319
  3662/100000: episode: 374, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.738, mean reward: 0.374 [0.295, 0.466], mean action: 36.800 [10.000, 92.000], mean observation: 3.157 [-1.127, 10.276], loss: 0.959294, mae: 1.506483, mean_q: 2.891132
  3672/100000: episode: 375, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.162, mean reward: 0.416 [0.338, 0.570], mean action: 38.900 [30.000, 84.000], mean observation: 3.148 [-1.669, 10.357], loss: 0.919026, mae: 1.510605, mean_q: 2.897423
  3682/100000: episode: 376, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.287, mean reward: 0.429 [0.363, 0.483], mean action: 40.600 [10.000, 74.000], mean observation: 3.165 [-0.954, 10.366], loss: 0.947360, mae: 1.514210, mean_q: 2.903219
  3692/100000: episode: 377, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.805, mean reward: 0.381 [0.336, 0.423], mean action: 38.400 [15.000, 89.000], mean observation: 3.164 [-1.673, 10.305], loss: 0.923336, mae: 1.517426, mean_q: 2.910767
  3702/100000: episode: 378, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.041, mean reward: 0.404 [0.303, 0.516], mean action: 37.400 [30.000, 96.000], mean observation: 3.150 [-1.137, 10.228], loss: 1.129208, mae: 1.521552, mean_q: 2.917477
  3712/100000: episode: 379, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.801, mean reward: 0.380 [0.340, 0.448], mean action: 32.500 [30.000, 55.000], mean observation: 3.156 [-1.296, 10.433], loss: 1.186955, mae: 1.527143, mean_q: 2.921979
  3722/100000: episode: 380, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.983, mean reward: 0.398 [0.299, 0.455], mean action: 37.000 [4.000, 93.000], mean observation: 3.157 [-2.122, 10.327], loss: 1.065104, mae: 1.529479, mean_q: 2.927639
  3732/100000: episode: 381, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.416, mean reward: 0.442 [0.417, 0.522], mean action: 39.600 [7.000, 96.000], mean observation: 3.167 [-1.068, 10.343], loss: 1.127330, mae: 1.532310, mean_q: 2.932961
  3742/100000: episode: 382, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.132, mean reward: 0.413 [0.313, 0.484], mean action: 41.300 [0.000, 90.000], mean observation: 3.156 [-1.172, 10.260], loss: 1.084555, mae: 1.537244, mean_q: 2.941073
  3752/100000: episode: 383, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.847, mean reward: 0.385 [0.311, 0.463], mean action: 42.500 [19.000, 101.000], mean observation: 3.165 [-1.434, 10.325], loss: 0.887739, mae: 1.539828, mean_q: 2.945491
  3762/100000: episode: 384, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.820, mean reward: 0.482 [0.481, 0.491], mean action: 46.100 [0.000, 95.000], mean observation: 3.152 [-0.969, 10.365], loss: 1.092287, mae: 1.543409, mean_q: 2.948689
  3772/100000: episode: 385, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.461, mean reward: 0.446 [0.440, 0.503], mean action: 32.500 [7.000, 66.000], mean observation: 3.170 [-1.205, 10.390], loss: 1.031583, mae: 1.547805, mean_q: 2.949590
  3782/100000: episode: 386, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 4.302, mean reward: 0.430 [0.380, 0.583], mean action: 33.400 [24.000, 50.000], mean observation: 3.159 [-1.578, 10.325], loss: 1.070335, mae: 1.550924, mean_q: 2.950469
  3792/100000: episode: 387, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.722, mean reward: 0.372 [0.328, 0.443], mean action: 35.500 [8.000, 96.000], mean observation: 3.151 [-0.995, 10.332], loss: 0.894266, mae: 1.554912, mean_q: 2.952440
  3802/100000: episode: 388, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.348, mean reward: 0.435 [0.355, 0.511], mean action: 32.600 [30.000, 56.000], mean observation: 3.151 [-1.584, 10.257], loss: 0.923576, mae: 1.559302, mean_q: 2.956970
  3812/100000: episode: 389, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.015, mean reward: 0.401 [0.383, 0.460], mean action: 29.500 [8.000, 80.000], mean observation: 3.154 [-1.659, 10.336], loss: 1.107118, mae: 1.562954, mean_q: 2.960052
  3822/100000: episode: 390, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.154, mean reward: 0.415 [0.320, 0.562], mean action: 49.800 [11.000, 93.000], mean observation: 3.161 [-1.882, 10.255], loss: 1.244940, mae: 1.565676, mean_q: 2.962250
  3832/100000: episode: 391, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.781, mean reward: 0.378 [0.360, 0.427], mean action: 33.800 [26.000, 60.000], mean observation: 3.153 [-0.902, 10.356], loss: 1.173723, mae: 1.571880, mean_q: 2.965830
  3842/100000: episode: 392, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.833, mean reward: 0.483 [0.376, 0.591], mean action: 44.000 [1.000, 100.000], mean observation: 3.159 [-1.288, 10.268], loss: 1.022071, mae: 1.573462, mean_q: 2.967469
  3852/100000: episode: 393, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.169, mean reward: 0.417 [0.324, 0.570], mean action: 38.200 [29.000, 65.000], mean observation: 3.160 [-2.298, 10.331], loss: 0.944168, mae: 1.577221, mean_q: 2.969667
  3862/100000: episode: 394, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.159, mean reward: 0.416 [0.323, 0.486], mean action: 37.400 [0.000, 100.000], mean observation: 3.147 [-1.502, 10.244], loss: 1.001476, mae: 1.582803, mean_q: 2.973605
  3872/100000: episode: 395, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.775, mean reward: 0.377 [0.348, 0.421], mean action: 34.800 [14.000, 80.000], mean observation: 3.173 [-1.132, 10.313], loss: 0.976305, mae: 1.584514, mean_q: 2.976266
  3882/100000: episode: 396, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.886, mean reward: 0.389 [0.355, 0.484], mean action: 49.800 [10.000, 85.000], mean observation: 3.154 [-1.509, 10.328], loss: 0.990045, mae: 1.589471, mean_q: 2.978427
  3892/100000: episode: 397, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.166, mean reward: 0.417 [0.348, 0.467], mean action: 47.000 [26.000, 96.000], mean observation: 3.154 [-0.984, 10.299], loss: 0.995193, mae: 1.592204, mean_q: 2.981751
  3902/100000: episode: 398, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.319, mean reward: 0.432 [0.322, 0.552], mean action: 45.700 [12.000, 101.000], mean observation: 3.144 [-1.782, 10.303], loss: 1.170918, mae: 1.596460, mean_q: 2.983580
  3912/100000: episode: 399, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.130, mean reward: 0.413 [0.378, 0.456], mean action: 51.100 [30.000, 98.000], mean observation: 3.152 [-1.281, 10.254], loss: 0.936931, mae: 1.599473, mean_q: 2.985356
  3922/100000: episode: 400, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.166, mean reward: 0.417 [0.402, 0.460], mean action: 41.500 [30.000, 84.000], mean observation: 3.162 [-1.875, 10.441], loss: 0.902496, mae: 1.603236, mean_q: 2.990703
  3932/100000: episode: 401, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.272, mean reward: 0.427 [0.373, 0.583], mean action: 42.200 [30.000, 64.000], mean observation: 3.160 [-1.136, 10.385], loss: 1.149829, mae: 1.608575, mean_q: 2.995413
  3942/100000: episode: 402, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.165, mean reward: 0.416 [0.355, 0.506], mean action: 42.400 [11.000, 96.000], mean observation: 3.143 [-1.298, 10.228], loss: 1.274366, mae: 1.612431, mean_q: 3.000062
  3952/100000: episode: 403, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.963, mean reward: 0.396 [0.327, 0.465], mean action: 47.500 [9.000, 96.000], mean observation: 3.150 [-1.314, 10.414], loss: 0.946890, mae: 1.615903, mean_q: 3.004921
  3962/100000: episode: 404, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.362, mean reward: 0.436 [0.353, 0.451], mean action: 39.100 [19.000, 75.000], mean observation: 3.149 [-1.470, 10.215], loss: 0.968040, mae: 1.618721, mean_q: 3.008680
  3972/100000: episode: 405, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.415, mean reward: 0.442 [0.350, 0.497], mean action: 28.300 [21.000, 30.000], mean observation: 3.147 [-1.490, 10.271], loss: 1.008055, mae: 1.623739, mean_q: 3.013584
  3982/100000: episode: 406, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.455, mean reward: 0.445 [0.347, 0.511], mean action: 37.100 [11.000, 97.000], mean observation: 3.159 [-1.321, 10.376], loss: 0.859208, mae: 1.625678, mean_q: 3.019516
  3992/100000: episode: 407, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.012, mean reward: 0.401 [0.324, 0.474], mean action: 31.300 [11.000, 62.000], mean observation: 3.159 [-1.038, 10.316], loss: 0.930659, mae: 1.630430, mean_q: 3.024570
  4002/100000: episode: 408, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.438, mean reward: 0.444 [0.310, 0.541], mean action: 43.000 [15.000, 93.000], mean observation: 3.156 [-0.936, 10.370], loss: 1.077989, mae: 1.633826, mean_q: 3.028280
  4012/100000: episode: 409, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.205, mean reward: 0.420 [0.251, 0.513], mean action: 38.900 [30.000, 83.000], mean observation: 3.151 [-1.373, 10.241], loss: 0.877052, mae: 1.636809, mean_q: 3.031893
  4022/100000: episode: 410, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.227, mean reward: 0.423 [0.373, 0.540], mean action: 34.800 [3.000, 93.000], mean observation: 3.147 [-1.435, 10.341], loss: 1.027597, mae: 1.641271, mean_q: 3.034226
  4025/100000: episode: 411, duration: 0.061s, episode steps: 3, steps per second: 49, episode reward: 10.736, mean reward: 3.579 [0.368, 10.000], mean action: 47.667 [30.000, 57.000], mean observation: 3.168 [-1.016, 10.237], loss: 0.860727, mae: 1.643516, mean_q: 3.035638
  4035/100000: episode: 412, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.213, mean reward: 0.421 [0.396, 0.453], mean action: 46.300 [9.000, 96.000], mean observation: 3.157 [-1.170, 10.307], loss: 1.034499, mae: 1.645054, mean_q: 3.039204
  4045/100000: episode: 413, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.280, mean reward: 0.428 [0.379, 0.466], mean action: 44.500 [30.000, 83.000], mean observation: 3.155 [-0.712, 10.210], loss: 1.009860, mae: 1.649727, mean_q: 3.043515
  4055/100000: episode: 414, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 3.825, mean reward: 0.383 [0.306, 0.448], mean action: 30.100 [9.000, 48.000], mean observation: 3.155 [-1.274, 10.374], loss: 1.053990, mae: 1.652535, mean_q: 3.048668
  4065/100000: episode: 415, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.471, mean reward: 0.347 [0.282, 0.454], mean action: 48.800 [30.000, 98.000], mean observation: 3.154 [-0.964, 10.230], loss: 1.076407, mae: 1.657338, mean_q: 3.053748
  4075/100000: episode: 416, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.215, mean reward: 0.422 [0.408, 0.440], mean action: 46.400 [30.000, 79.000], mean observation: 3.147 [-2.172, 10.293], loss: 0.942576, mae: 1.661206, mean_q: 3.056994
  4085/100000: episode: 417, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.687, mean reward: 0.369 [0.294, 0.522], mean action: 44.700 [7.000, 85.000], mean observation: 3.154 [-1.688, 10.223], loss: 1.042348, mae: 1.664898, mean_q: 3.058908
  4095/100000: episode: 418, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.965, mean reward: 0.396 [0.302, 0.502], mean action: 30.000 [14.000, 40.000], mean observation: 3.154 [-1.054, 10.418], loss: 0.866849, mae: 1.667368, mean_q: 3.061740
  4105/100000: episode: 419, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.215, mean reward: 0.422 [0.311, 0.559], mean action: 40.000 [6.000, 82.000], mean observation: 3.154 [-1.484, 10.308], loss: 0.960057, mae: 1.672784, mean_q: 3.065237
  4115/100000: episode: 420, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.814, mean reward: 0.381 [0.327, 0.440], mean action: 34.300 [7.000, 76.000], mean observation: 3.150 [-1.629, 10.321], loss: 1.318709, mae: 1.676258, mean_q: 3.068183
  4125/100000: episode: 421, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.652, mean reward: 0.365 [0.338, 0.423], mean action: 45.900 [30.000, 85.000], mean observation: 3.143 [-1.203, 10.304], loss: 1.035567, mae: 1.678401, mean_q: 3.072432
  4135/100000: episode: 422, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.355, mean reward: 0.435 [0.351, 0.530], mean action: 40.000 [16.000, 89.000], mean observation: 3.156 [-1.626, 10.283], loss: 1.143475, mae: 1.682663, mean_q: 3.076566
  4145/100000: episode: 423, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.631, mean reward: 0.463 [0.347, 0.546], mean action: 29.000 [2.000, 56.000], mean observation: 3.155 [-1.820, 10.455], loss: 1.010249, mae: 1.686564, mean_q: 3.081376
  4155/100000: episode: 424, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.782, mean reward: 0.378 [0.327, 0.473], mean action: 48.800 [3.000, 99.000], mean observation: 3.160 [-1.606, 10.241], loss: 0.942729, mae: 1.689253, mean_q: 3.086602
  4165/100000: episode: 425, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.172, mean reward: 0.417 [0.330, 0.562], mean action: 40.200 [30.000, 71.000], mean observation: 3.150 [-1.582, 10.307], loss: 1.255790, mae: 1.695035, mean_q: 3.093489
  4175/100000: episode: 426, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.325, mean reward: 0.432 [0.424, 0.460], mean action: 43.300 [30.000, 95.000], mean observation: 3.134 [-1.099, 10.407], loss: 1.171856, mae: 1.697937, mean_q: 3.097743
  4185/100000: episode: 427, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.790, mean reward: 0.379 [0.349, 0.447], mean action: 47.400 [5.000, 95.000], mean observation: 3.138 [-1.465, 10.354], loss: 1.067120, mae: 1.702150, mean_q: 3.099092
  4195/100000: episode: 428, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.225, mean reward: 0.422 [0.343, 0.523], mean action: 37.200 [6.000, 97.000], mean observation: 3.153 [-1.034, 10.306], loss: 1.281150, mae: 1.706458, mean_q: 3.102135
  4205/100000: episode: 429, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.995, mean reward: 0.400 [0.360, 0.480], mean action: 45.600 [30.000, 92.000], mean observation: 3.158 [-1.237, 10.418], loss: 0.867806, mae: 1.708229, mean_q: 3.105679
  4215/100000: episode: 430, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.978, mean reward: 0.498 [0.480, 0.583], mean action: 41.000 [30.000, 91.000], mean observation: 3.151 [-1.201, 10.391], loss: 1.100497, mae: 1.713663, mean_q: 3.109368
  4225/100000: episode: 431, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.000, mean reward: 0.400 [0.341, 0.569], mean action: 49.700 [21.000, 88.000], mean observation: 3.167 [-1.274, 10.385], loss: 0.990807, mae: 1.717736, mean_q: 3.114641
  4235/100000: episode: 432, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.275, mean reward: 0.427 [0.380, 0.574], mean action: 29.700 [1.000, 62.000], mean observation: 3.146 [-0.805, 10.205], loss: 0.952808, mae: 1.719659, mean_q: 3.119287
  4245/100000: episode: 433, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.089, mean reward: 0.409 [0.354, 0.470], mean action: 38.500 [14.000, 69.000], mean observation: 3.169 [-1.467, 10.359], loss: 1.162892, mae: 1.724801, mean_q: 3.124876
  4255/100000: episode: 434, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.053, mean reward: 0.405 [0.329, 0.505], mean action: 28.700 [13.000, 46.000], mean observation: 3.152 [-1.419, 10.294], loss: 1.018652, mae: 1.728407, mean_q: 3.131984
  4265/100000: episode: 435, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.914, mean reward: 0.391 [0.340, 0.427], mean action: 54.000 [16.000, 94.000], mean observation: 3.146 [-1.521, 10.305], loss: 1.042986, mae: 1.731231, mean_q: 3.137385
  4275/100000: episode: 436, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.269, mean reward: 0.427 [0.337, 0.501], mean action: 39.100 [2.000, 89.000], mean observation: 3.145 [-1.503, 10.422], loss: 0.943365, mae: 1.735974, mean_q: 3.141202
  4285/100000: episode: 437, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.799, mean reward: 0.380 [0.351, 0.468], mean action: 45.300 [29.000, 86.000], mean observation: 3.155 [-1.390, 10.458], loss: 1.031884, mae: 1.739580, mean_q: 3.146636
  4295/100000: episode: 438, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.269, mean reward: 0.427 [0.344, 0.509], mean action: 38.200 [20.000, 81.000], mean observation: 3.154 [-1.325, 10.470], loss: 0.818560, mae: 1.741358, mean_q: 3.150680
  4305/100000: episode: 439, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 3.947, mean reward: 0.395 [0.350, 0.516], mean action: 30.600 [11.000, 75.000], mean observation: 3.164 [-1.491, 10.349], loss: 0.914763, mae: 1.746169, mean_q: 3.154685
  4315/100000: episode: 440, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.060, mean reward: 0.406 [0.331, 0.473], mean action: 33.300 [6.000, 69.000], mean observation: 3.155 [-1.721, 10.323], loss: 0.992658, mae: 1.750275, mean_q: 3.156511
  4325/100000: episode: 441, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.807, mean reward: 0.381 [0.323, 0.439], mean action: 47.800 [11.000, 97.000], mean observation: 3.157 [-1.361, 10.294], loss: 0.977546, mae: 1.752681, mean_q: 3.158447
  4331/100000: episode: 442, duration: 0.120s, episode steps: 6, steps per second: 50, episode reward: 12.196, mean reward: 2.033 [0.439, 10.000], mean action: 36.500 [30.000, 69.000], mean observation: 3.181 [-1.342, 10.278], loss: 1.111054, mae: 1.755330, mean_q: 3.160282
  4341/100000: episode: 443, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.301, mean reward: 0.430 [0.330, 0.541], mean action: 31.800 [1.000, 65.000], mean observation: 3.160 [-1.434, 10.354], loss: 1.081030, mae: 1.759634, mean_q: 3.160103
  4351/100000: episode: 444, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.175, mean reward: 0.418 [0.381, 0.464], mean action: 38.600 [8.000, 101.000], mean observation: 3.159 [-1.428, 10.421], loss: 0.945371, mae: 1.762567, mean_q: 3.158798
  4361/100000: episode: 445, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.826, mean reward: 0.483 [0.435, 0.565], mean action: 47.100 [30.000, 99.000], mean observation: 3.149 [-1.365, 10.512], loss: 0.964143, mae: 1.765668, mean_q: 3.160262
  4371/100000: episode: 446, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.788, mean reward: 0.479 [0.451, 0.515], mean action: 34.600 [4.000, 70.000], mean observation: 3.149 [-1.417, 10.390], loss: 1.081829, mae: 1.769897, mean_q: 3.163342
  4381/100000: episode: 447, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.689, mean reward: 0.369 [0.333, 0.421], mean action: 47.900 [30.000, 92.000], mean observation: 3.170 [-1.110, 10.398], loss: 1.044188, mae: 1.774001, mean_q: 3.166229
  4391/100000: episode: 448, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.432, mean reward: 0.443 [0.388, 0.530], mean action: 46.700 [16.000, 98.000], mean observation: 3.154 [-1.145, 10.213], loss: 1.021403, mae: 1.776589, mean_q: 3.167243
  4401/100000: episode: 449, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.033, mean reward: 0.403 [0.332, 0.505], mean action: 37.700 [11.000, 92.000], mean observation: 3.162 [-1.064, 10.283], loss: 0.904271, mae: 1.779398, mean_q: 3.168030
  4411/100000: episode: 450, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.288, mean reward: 0.429 [0.353, 0.483], mean action: 41.900 [5.000, 92.000], mean observation: 3.153 [-1.898, 10.301], loss: 1.016846, mae: 1.784751, mean_q: 3.170783
  4421/100000: episode: 451, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.837, mean reward: 0.484 [0.347, 0.519], mean action: 44.100 [4.000, 96.000], mean observation: 3.149 [-1.715, 10.422], loss: 0.943637, mae: 1.787293, mean_q: 3.172697
  4431/100000: episode: 452, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.292, mean reward: 0.429 [0.311, 0.552], mean action: 50.300 [29.000, 85.000], mean observation: 3.156 [-1.725, 10.505], loss: 0.984413, mae: 1.790510, mean_q: 3.175756
  4441/100000: episode: 453, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.007, mean reward: 0.401 [0.329, 0.489], mean action: 33.300 [3.000, 95.000], mean observation: 3.150 [-1.303, 10.295], loss: 1.015048, mae: 1.794403, mean_q: 3.179223
  4451/100000: episode: 454, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.664, mean reward: 0.366 [0.284, 0.443], mean action: 39.900 [30.000, 95.000], mean observation: 3.154 [-1.850, 10.294], loss: 1.153046, mae: 1.797926, mean_q: 3.183059
  4461/100000: episode: 455, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.357, mean reward: 0.436 [0.416, 0.540], mean action: 29.400 [22.000, 33.000], mean observation: 3.160 [-1.347, 10.324], loss: 0.977046, mae: 1.800845, mean_q: 3.187536
  4471/100000: episode: 456, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.938, mean reward: 0.394 [0.333, 0.483], mean action: 49.900 [15.000, 82.000], mean observation: 3.150 [-0.888, 10.305], loss: 1.099026, mae: 1.804711, mean_q: 3.188344
  4481/100000: episode: 457, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.167, mean reward: 0.417 [0.345, 0.562], mean action: 39.300 [30.000, 89.000], mean observation: 3.155 [-1.358, 10.274], loss: 0.990286, mae: 1.808960, mean_q: 3.191731
  4491/100000: episode: 458, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.139, mean reward: 0.414 [0.414, 0.414], mean action: 53.500 [30.000, 101.000], mean observation: 3.170 [-1.220, 10.302], loss: 1.118085, mae: 1.811949, mean_q: 3.196675
  4501/100000: episode: 459, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.733, mean reward: 0.473 [0.469, 0.489], mean action: 38.100 [27.000, 65.000], mean observation: 3.163 [-1.715, 10.327], loss: 0.984623, mae: 1.815510, mean_q: 3.201503
  4511/100000: episode: 460, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.837, mean reward: 0.384 [0.362, 0.528], mean action: 38.800 [9.000, 98.000], mean observation: 3.166 [-1.518, 10.483], loss: 0.970302, mae: 1.819713, mean_q: 3.206599
  4521/100000: episode: 461, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.266, mean reward: 0.427 [0.426, 0.433], mean action: 42.700 [23.000, 100.000], mean observation: 3.144 [-1.739, 10.327], loss: 0.975825, mae: 1.821749, mean_q: 3.208512
  4525/100000: episode: 462, duration: 0.090s, episode steps: 4, steps per second: 44, episode reward: 11.392, mean reward: 2.848 [0.419, 10.000], mean action: 37.000 [30.000, 58.000], mean observation: 3.171 [-1.475, 10.269], loss: 1.456715, mae: 1.827328, mean_q: 3.207841
  4535/100000: episode: 463, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.158, mean reward: 0.416 [0.329, 0.531], mean action: 34.000 [13.000, 86.000], mean observation: 3.163 [-1.615, 10.435], loss: 0.898115, mae: 1.826394, mean_q: 3.207452
  4545/100000: episode: 464, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.290, mean reward: 0.429 [0.358, 0.464], mean action: 42.800 [0.000, 100.000], mean observation: 3.158 [-1.948, 10.430], loss: 1.115379, mae: 1.830764, mean_q: 3.206977
  4555/100000: episode: 465, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.735, mean reward: 0.373 [0.334, 0.418], mean action: 35.300 [12.000, 78.000], mean observation: 3.154 [-2.138, 10.309], loss: 0.916010, mae: 1.833770, mean_q: 3.205291
  4565/100000: episode: 466, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.024, mean reward: 0.402 [0.316, 0.480], mean action: 34.200 [18.000, 55.000], mean observation: 3.158 [-1.331, 10.228], loss: 0.952009, mae: 1.837254, mean_q: 3.206697
  4575/100000: episode: 467, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.050, mean reward: 0.405 [0.337, 0.505], mean action: 33.900 [12.000, 56.000], mean observation: 3.156 [-1.332, 10.342], loss: 1.119135, mae: 1.842331, mean_q: 3.213230
  4585/100000: episode: 468, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.257, mean reward: 0.426 [0.366, 0.509], mean action: 39.800 [0.000, 101.000], mean observation: 3.163 [-1.388, 10.348], loss: 1.135250, mae: 1.846084, mean_q: 3.223533
  4595/100000: episode: 469, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.947, mean reward: 0.395 [0.341, 0.452], mean action: 46.500 [30.000, 88.000], mean observation: 3.152 [-1.616, 10.404], loss: 1.145705, mae: 1.848866, mean_q: 3.231284
  4605/100000: episode: 470, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.006, mean reward: 0.401 [0.362, 0.460], mean action: 50.800 [14.000, 100.000], mean observation: 3.164 [-0.987, 10.206], loss: 0.933449, mae: 1.851253, mean_q: 3.233381
  4615/100000: episode: 471, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.368, mean reward: 0.437 [0.425, 0.471], mean action: 41.900 [30.000, 93.000], mean observation: 3.154 [-2.259, 10.509], loss: 1.069535, mae: 1.855877, mean_q: 3.234152
  4625/100000: episode: 472, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.333, mean reward: 0.433 [0.352, 0.525], mean action: 40.700 [7.000, 95.000], mean observation: 3.161 [-1.441, 10.472], loss: 0.975276, mae: 1.859596, mean_q: 3.235626
  4635/100000: episode: 473, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.888, mean reward: 0.389 [0.353, 0.439], mean action: 36.300 [2.000, 85.000], mean observation: 3.158 [-1.568, 10.274], loss: 0.986729, mae: 1.861988, mean_q: 3.237496
  4645/100000: episode: 474, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 3.967, mean reward: 0.397 [0.373, 0.473], mean action: 30.200 [12.000, 60.000], mean observation: 3.160 [-1.510, 10.391], loss: 0.947471, mae: 1.865867, mean_q: 3.240134
  4655/100000: episode: 475, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.307, mean reward: 0.431 [0.356, 0.526], mean action: 35.000 [1.000, 78.000], mean observation: 3.158 [-1.115, 10.463], loss: 1.000106, mae: 1.868641, mean_q: 3.242703
  4665/100000: episode: 476, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.437, mean reward: 0.444 [0.414, 0.533], mean action: 37.100 [7.000, 87.000], mean observation: 3.153 [-1.562, 10.142], loss: 0.932424, mae: 1.871452, mean_q: 3.245794
  4675/100000: episode: 477, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.181, mean reward: 0.418 [0.326, 0.537], mean action: 35.800 [23.000, 61.000], mean observation: 3.162 [-1.087, 10.397], loss: 0.962584, mae: 1.875426, mean_q: 3.250450
  4685/100000: episode: 478, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.065, mean reward: 0.407 [0.312, 0.546], mean action: 40.300 [2.000, 97.000], mean observation: 3.163 [-2.371, 10.333], loss: 0.956672, mae: 1.880255, mean_q: 3.256135
  4688/100000: episode: 479, duration: 0.058s, episode steps: 3, steps per second: 52, episode reward: 10.815, mean reward: 3.605 [0.407, 10.000], mean action: 54.000 [30.000, 92.000], mean observation: 3.147 [-1.738, 10.224], loss: 0.938673, mae: 1.882772, mean_q: 3.259809
  4698/100000: episode: 480, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.898, mean reward: 0.390 [0.307, 0.439], mean action: 37.100 [7.000, 79.000], mean observation: 3.160 [-2.132, 10.247], loss: 1.101846, mae: 1.884784, mean_q: 3.263662
  4708/100000: episode: 481, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.156, mean reward: 0.416 [0.344, 0.517], mean action: 47.200 [9.000, 101.000], mean observation: 3.143 [-1.259, 10.309], loss: 1.137663, mae: 1.888970, mean_q: 3.268963
  4718/100000: episode: 482, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.387, mean reward: 0.439 [0.356, 0.563], mean action: 38.100 [11.000, 71.000], mean observation: 3.160 [-1.698, 10.281], loss: 1.006179, mae: 1.890979, mean_q: 3.270936
  4728/100000: episode: 483, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.076, mean reward: 0.408 [0.310, 0.543], mean action: 30.900 [9.000, 57.000], mean observation: 3.160 [-1.038, 10.325], loss: 0.977140, mae: 1.894972, mean_q: 3.273093
  4738/100000: episode: 484, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.121, mean reward: 0.412 [0.369, 0.443], mean action: 33.500 [2.000, 64.000], mean observation: 3.164 [-1.295, 10.381], loss: 1.246751, mae: 1.897907, mean_q: 3.275871
  4748/100000: episode: 485, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.125, mean reward: 0.412 [0.294, 0.551], mean action: 38.400 [27.000, 76.000], mean observation: 3.162 [-1.134, 10.461], loss: 1.142178, mae: 1.901507, mean_q: 3.281853
  4758/100000: episode: 486, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.076, mean reward: 0.408 [0.357, 0.543], mean action: 34.500 [30.000, 75.000], mean observation: 3.165 [-1.083, 10.207], loss: 0.965528, mae: 1.905395, mean_q: 3.288368
  4768/100000: episode: 487, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.982, mean reward: 0.398 [0.353, 0.469], mean action: 42.800 [25.000, 88.000], mean observation: 3.157 [-0.892, 10.290], loss: 1.076067, mae: 1.909198, mean_q: 3.292281
  4778/100000: episode: 488, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.760, mean reward: 0.376 [0.341, 0.424], mean action: 44.100 [4.000, 99.000], mean observation: 3.157 [-1.145, 10.288], loss: 1.212187, mae: 1.912999, mean_q: 3.297499
  4788/100000: episode: 489, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.961, mean reward: 0.396 [0.334, 0.590], mean action: 42.100 [25.000, 97.000], mean observation: 3.163 [-1.372, 10.439], loss: 0.932664, mae: 1.914850, mean_q: 3.298081
  4798/100000: episode: 490, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.381, mean reward: 0.438 [0.408, 0.493], mean action: 55.000 [30.000, 91.000], mean observation: 3.170 [-1.738, 10.270], loss: 0.919111, mae: 1.917238, mean_q: 3.297855
  4808/100000: episode: 491, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.108, mean reward: 0.411 [0.367, 0.456], mean action: 47.300 [0.000, 98.000], mean observation: 3.177 [-1.413, 10.288], loss: 0.973987, mae: 1.920785, mean_q: 3.297875
  4818/100000: episode: 492, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.300, mean reward: 0.430 [0.401, 0.553], mean action: 35.300 [4.000, 90.000], mean observation: 3.134 [-1.521, 10.359], loss: 1.046520, mae: 1.923245, mean_q: 3.297439
  4826/100000: episode: 493, duration: 0.151s, episode steps: 8, steps per second: 53, episode reward: 12.801, mean reward: 1.600 [0.390, 10.000], mean action: 39.250 [20.000, 77.000], mean observation: 3.161 [-1.297, 10.226], loss: 1.089068, mae: 1.927382, mean_q: 3.300351
  4836/100000: episode: 494, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.008, mean reward: 0.401 [0.345, 0.528], mean action: 32.300 [15.000, 63.000], mean observation: 3.155 [-1.528, 10.378], loss: 1.157629, mae: 1.930359, mean_q: 3.302727
  4846/100000: episode: 495, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.585, mean reward: 0.359 [0.335, 0.398], mean action: 30.800 [5.000, 63.000], mean observation: 3.151 [-1.710, 10.294], loss: 1.085761, mae: 1.933363, mean_q: 3.304925
  4856/100000: episode: 496, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.854, mean reward: 0.385 [0.308, 0.510], mean action: 33.500 [4.000, 61.000], mean observation: 3.159 [-1.054, 10.339], loss: 1.251876, mae: 1.938329, mean_q: 3.307193
  4866/100000: episode: 497, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.536, mean reward: 0.454 [0.383, 0.491], mean action: 54.700 [24.000, 101.000], mean observation: 3.156 [-1.113, 10.340], loss: 1.050115, mae: 1.940287, mean_q: 3.310886
  4876/100000: episode: 498, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.036, mean reward: 0.404 [0.329, 0.490], mean action: 38.000 [11.000, 74.000], mean observation: 3.158 [-1.313, 10.316], loss: 0.902878, mae: 1.943532, mean_q: 3.316931
  4886/100000: episode: 499, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.313, mean reward: 0.431 [0.392, 0.494], mean action: 29.500 [8.000, 47.000], mean observation: 3.167 [-1.479, 10.356], loss: 1.280775, mae: 1.949730, mean_q: 3.325286
  4895/100000: episode: 500, duration: 0.143s, episode steps: 9, steps per second: 63, episode reward: 13.426, mean reward: 1.492 [0.352, 10.000], mean action: 46.111 [16.000, 96.000], mean observation: 3.159 [-1.633, 10.514], loss: 1.065954, mae: 1.950763, mean_q: 3.330024
  4905/100000: episode: 501, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.933, mean reward: 0.393 [0.358, 0.461], mean action: 49.800 [1.000, 97.000], mean observation: 3.158 [-2.096, 10.313], loss: 0.977255, mae: 1.954464, mean_q: 3.333614
  4915/100000: episode: 502, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.162, mean reward: 0.416 [0.376, 0.514], mean action: 35.100 [1.000, 82.000], mean observation: 3.164 [-1.293, 10.454], loss: 0.903247, mae: 1.955006, mean_q: 3.336323
  4925/100000: episode: 503, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.331, mean reward: 0.433 [0.413, 0.517], mean action: 45.300 [16.000, 100.000], mean observation: 3.150 [-0.981, 10.473], loss: 0.973305, mae: 1.960153, mean_q: 3.338967
  4935/100000: episode: 504, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.975, mean reward: 0.397 [0.327, 0.457], mean action: 32.500 [4.000, 66.000], mean observation: 3.152 [-1.713, 10.288], loss: 1.075325, mae: 1.963848, mean_q: 3.341610
  4945/100000: episode: 505, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.954, mean reward: 0.395 [0.383, 0.449], mean action: 44.000 [14.000, 81.000], mean observation: 3.154 [-1.647, 10.310], loss: 1.102343, mae: 1.966780, mean_q: 3.346630
  4955/100000: episode: 506, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.030, mean reward: 0.403 [0.369, 0.459], mean action: 36.000 [5.000, 86.000], mean observation: 3.152 [-1.158, 10.277], loss: 1.276285, mae: 1.970188, mean_q: 3.353848
  4965/100000: episode: 507, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 5.305, mean reward: 0.531 [0.388, 0.546], mean action: 36.800 [9.000, 87.000], mean observation: 3.146 [-1.244, 10.247], loss: 1.089012, mae: 1.974111, mean_q: 3.356038
  4969/100000: episode: 508, duration: 0.082s, episode steps: 4, steps per second: 49, episode reward: 11.047, mean reward: 2.762 [0.320, 10.000], mean action: 43.000 [23.000, 89.000], mean observation: 3.155 [-2.242, 10.321], loss: 0.854367, mae: 1.974795, mean_q: 3.356884
  4979/100000: episode: 509, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.203, mean reward: 0.420 [0.347, 0.560], mean action: 34.700 [22.000, 68.000], mean observation: 3.158 [-1.300, 10.299], loss: 1.085897, mae: 1.976978, mean_q: 3.358102
  4980/100000: episode: 510, duration: 0.027s, episode steps: 1, steps per second: 37, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 30.000 [30.000, 30.000], mean observation: 3.160 [-0.923, 10.130], loss: 2.066168, mae: 1.977545, mean_q: 3.359479
  4990/100000: episode: 511, duration: 0.183s, episode steps: 10, steps per second: 54, episode reward: 4.335, mean reward: 0.433 [0.425, 0.454], mean action: 37.300 [10.000, 71.000], mean observation: 3.155 [-2.798, 10.338], loss: 1.222449, mae: 1.982016, mean_q: 3.360098
  5000/100000: episode: 512, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.138, mean reward: 0.414 [0.318, 0.517], mean action: 33.500 [21.000, 52.000], mean observation: 3.163 [-1.588, 10.319], loss: 1.221074, mae: 1.985390, mean_q: 3.360346
  5010/100000: episode: 513, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.483, mean reward: 0.448 [0.361, 0.556], mean action: 40.900 [30.000, 94.000], mean observation: 3.162 [-1.604, 10.556], loss: 1.153412, mae: 1.990613, mean_q: 3.363656
  5018/100000: episode: 514, duration: 0.156s, episode steps: 8, steps per second: 51, episode reward: 13.098, mean reward: 1.637 [0.441, 10.000], mean action: 36.750 [22.000, 72.000], mean observation: 3.156 [-1.387, 10.263], loss: 1.019804, mae: 1.992418, mean_q: 3.364966
  5028/100000: episode: 515, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.807, mean reward: 0.381 [0.308, 0.470], mean action: 38.200 [1.000, 94.000], mean observation: 3.151 [-1.258, 10.248], loss: 1.011451, mae: 1.994923, mean_q: 3.364547
  5038/100000: episode: 516, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.833, mean reward: 0.483 [0.398, 0.585], mean action: 38.800 [1.000, 90.000], mean observation: 3.142 [-1.586, 10.259], loss: 1.296669, mae: 1.999768, mean_q: 3.364516
  5048/100000: episode: 517, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.073, mean reward: 0.407 [0.392, 0.454], mean action: 26.700 [8.000, 37.000], mean observation: 3.155 [-2.601, 10.411], loss: 1.073255, mae: 2.001200, mean_q: 3.363319
  5058/100000: episode: 518, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.826, mean reward: 0.383 [0.364, 0.457], mean action: 58.200 [30.000, 98.000], mean observation: 3.154 [-1.604, 10.273], loss: 0.906950, mae: 2.005221, mean_q: 3.365749
  5068/100000: episode: 519, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 5.021, mean reward: 0.502 [0.501, 0.514], mean action: 43.300 [30.000, 100.000], mean observation: 3.153 [-1.440, 10.497], loss: 1.015450, mae: 2.008831, mean_q: 3.371671
  5078/100000: episode: 520, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.831, mean reward: 0.383 [0.297, 0.490], mean action: 41.100 [30.000, 90.000], mean observation: 3.158 [-1.114, 10.398], loss: 1.133342, mae: 2.012572, mean_q: 3.376357
  5088/100000: episode: 521, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.422, mean reward: 0.442 [0.391, 0.484], mean action: 46.300 [30.000, 87.000], mean observation: 3.150 [-1.439, 10.246], loss: 1.231271, mae: 2.016708, mean_q: 3.377324
  5098/100000: episode: 522, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.999, mean reward: 0.400 [0.345, 0.474], mean action: 53.200 [29.000, 93.000], mean observation: 3.142 [-1.853, 10.390], loss: 1.068223, mae: 2.019806, mean_q: 3.378352
  5108/100000: episode: 523, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.207, mean reward: 0.421 [0.341, 0.516], mean action: 44.000 [28.000, 77.000], mean observation: 3.156 [-0.991, 10.439], loss: 0.996106, mae: 2.023138, mean_q: 3.381150
  5118/100000: episode: 524, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.410, mean reward: 0.441 [0.398, 0.573], mean action: 38.900 [2.000, 84.000], mean observation: 3.142 [-1.302, 10.250], loss: 1.001854, mae: 2.026693, mean_q: 3.384975
  5128/100000: episode: 525, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.374, mean reward: 0.437 [0.399, 0.580], mean action: 42.800 [19.000, 100.000], mean observation: 3.152 [-2.973, 10.309], loss: 0.982089, mae: 2.028242, mean_q: 3.389636
  5138/100000: episode: 526, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.408, mean reward: 0.441 [0.438, 0.466], mean action: 35.500 [27.000, 68.000], mean observation: 3.156 [-1.287, 10.353], loss: 1.081636, mae: 2.034036, mean_q: 3.395374
  5148/100000: episode: 527, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.140, mean reward: 0.414 [0.406, 0.471], mean action: 44.900 [20.000, 91.000], mean observation: 3.154 [-1.590, 10.292], loss: 0.945606, mae: 2.035838, mean_q: 3.399726
  5158/100000: episode: 528, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.608, mean reward: 0.461 [0.371, 0.563], mean action: 46.000 [9.000, 95.000], mean observation: 3.162 [-1.812, 10.403], loss: 1.078703, mae: 2.040387, mean_q: 3.403322
  5168/100000: episode: 529, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.261, mean reward: 0.426 [0.341, 0.575], mean action: 47.800 [30.000, 97.000], mean observation: 3.170 [-1.510, 10.394], loss: 1.308881, mae: 2.045063, mean_q: 3.407457
  5178/100000: episode: 530, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.431, mean reward: 0.443 [0.286, 0.570], mean action: 38.100 [0.000, 90.000], mean observation: 3.158 [-1.719, 10.482], loss: 1.030837, mae: 2.046929, mean_q: 3.412721
  5188/100000: episode: 531, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.917, mean reward: 0.392 [0.375, 0.415], mean action: 36.400 [4.000, 74.000], mean observation: 3.163 [-1.452, 10.303], loss: 1.119275, mae: 2.050269, mean_q: 3.416119
  5198/100000: episode: 532, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.369, mean reward: 0.437 [0.324, 0.531], mean action: 33.100 [2.000, 96.000], mean observation: 3.161 [-1.658, 10.319], loss: 1.062075, mae: 2.054202, mean_q: 3.418180
  5208/100000: episode: 533, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.121, mean reward: 0.412 [0.333, 0.550], mean action: 43.500 [0.000, 91.000], mean observation: 3.145 [-1.103, 10.310], loss: 1.148007, mae: 2.057781, mean_q: 3.419603
  5218/100000: episode: 534, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 3.850, mean reward: 0.385 [0.293, 0.505], mean action: 35.500 [23.000, 81.000], mean observation: 3.154 [-1.499, 10.332], loss: 0.999663, mae: 2.059986, mean_q: 3.421758
  5228/100000: episode: 535, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.986, mean reward: 0.399 [0.331, 0.478], mean action: 41.000 [6.000, 93.000], mean observation: 3.151 [-1.055, 10.331], loss: 0.955426, mae: 2.063708, mean_q: 3.425315
  5238/100000: episode: 536, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.313, mean reward: 0.431 [0.334, 0.514], mean action: 37.200 [4.000, 82.000], mean observation: 3.163 [-1.215, 10.351], loss: 1.091298, mae: 2.068255, mean_q: 3.429790
  5248/100000: episode: 537, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.559, mean reward: 0.456 [0.414, 0.579], mean action: 26.100 [9.000, 30.000], mean observation: 3.153 [-2.123, 10.190], loss: 0.979103, mae: 2.071321, mean_q: 3.432083
  5258/100000: episode: 538, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.270, mean reward: 0.427 [0.371, 0.504], mean action: 38.300 [30.000, 85.000], mean observation: 3.152 [-1.192, 10.315], loss: 0.994283, mae: 2.074881, mean_q: 3.433680
  5268/100000: episode: 539, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 3.958, mean reward: 0.396 [0.344, 0.448], mean action: 34.900 [11.000, 91.000], mean observation: 3.161 [-1.064, 10.366], loss: 1.291392, mae: 2.078298, mean_q: 3.434265
  5278/100000: episode: 540, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.057, mean reward: 0.406 [0.279, 0.495], mean action: 37.400 [19.000, 70.000], mean observation: 3.160 [-1.460, 10.414], loss: 1.097902, mae: 2.081692, mean_q: 3.435422
  5288/100000: episode: 541, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.938, mean reward: 0.394 [0.363, 0.451], mean action: 42.700 [30.000, 84.000], mean observation: 3.174 [-1.679, 10.434], loss: 1.131546, mae: 2.085554, mean_q: 3.436363
  5298/100000: episode: 542, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.621, mean reward: 0.462 [0.401, 0.566], mean action: 54.000 [16.000, 92.000], mean observation: 3.156 [-2.100, 10.289], loss: 0.915636, mae: 2.085628, mean_q: 3.436642
  5308/100000: episode: 543, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 5.032, mean reward: 0.503 [0.491, 0.577], mean action: 46.300 [30.000, 94.000], mean observation: 3.144 [-1.430, 10.314], loss: 1.090957, mae: 2.090596, mean_q: 3.442317
  5318/100000: episode: 544, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.144, mean reward: 0.414 [0.337, 0.518], mean action: 56.600 [11.000, 99.000], mean observation: 3.145 [-1.467, 10.323], loss: 1.138250, mae: 2.093571, mean_q: 3.448924
  5328/100000: episode: 545, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.947, mean reward: 0.395 [0.282, 0.475], mean action: 37.700 [27.000, 101.000], mean observation: 3.154 [-1.408, 10.395], loss: 1.024884, mae: 2.096916, mean_q: 3.453007
  5338/100000: episode: 546, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.814, mean reward: 0.381 [0.347, 0.481], mean action: 28.300 [4.000, 78.000], mean observation: 3.157 [-1.395, 10.288], loss: 1.113134, mae: 2.099795, mean_q: 3.454711
  5348/100000: episode: 547, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.087, mean reward: 0.409 [0.320, 0.488], mean action: 46.500 [13.000, 101.000], mean observation: 3.155 [-0.597, 10.294], loss: 0.940521, mae: 2.102387, mean_q: 3.456306
  5358/100000: episode: 548, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.143, mean reward: 0.414 [0.350, 0.504], mean action: 31.000 [3.000, 74.000], mean observation: 3.153 [-1.869, 10.229], loss: 0.893249, mae: 2.105858, mean_q: 3.457628
  5368/100000: episode: 549, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.899, mean reward: 0.390 [0.293, 0.437], mean action: 41.500 [24.000, 93.000], mean observation: 3.164 [-1.318, 10.283], loss: 1.156519, mae: 2.109549, mean_q: 3.461951
  5378/100000: episode: 550, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.290, mean reward: 0.429 [0.428, 0.440], mean action: 47.200 [28.000, 89.000], mean observation: 3.162 [-1.928, 10.275], loss: 0.982799, mae: 2.112868, mean_q: 3.463650
  5388/100000: episode: 551, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.372, mean reward: 0.437 [0.326, 0.530], mean action: 36.000 [3.000, 75.000], mean observation: 3.165 [-1.395, 10.311], loss: 0.929264, mae: 2.115654, mean_q: 3.463897
  5398/100000: episode: 552, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.219, mean reward: 0.422 [0.390, 0.477], mean action: 47.300 [22.000, 94.000], mean observation: 3.160 [-1.802, 10.258], loss: 0.964652, mae: 2.119091, mean_q: 3.466323
  5408/100000: episode: 553, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 3.907, mean reward: 0.391 [0.303, 0.459], mean action: 27.600 [5.000, 62.000], mean observation: 3.150 [-1.570, 10.329], loss: 0.930469, mae: 2.121748, mean_q: 3.470128
  5418/100000: episode: 554, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.288, mean reward: 0.429 [0.335, 0.507], mean action: 36.700 [2.000, 92.000], mean observation: 3.172 [-1.802, 10.472], loss: 0.974283, mae: 2.126836, mean_q: 3.475148
  5428/100000: episode: 555, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.507, mean reward: 0.451 [0.337, 0.560], mean action: 25.300 [4.000, 46.000], mean observation: 3.153 [-1.330, 10.346], loss: 0.947611, mae: 2.129473, mean_q: 3.477498
  5438/100000: episode: 556, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.374, mean reward: 0.437 [0.349, 0.516], mean action: 40.000 [5.000, 99.000], mean observation: 3.161 [-1.130, 10.322], loss: 1.079392, mae: 2.133558, mean_q: 3.479562
  5448/100000: episode: 557, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.870, mean reward: 0.387 [0.359, 0.419], mean action: 42.500 [30.000, 88.000], mean observation: 3.167 [-1.676, 10.360], loss: 1.128364, mae: 2.137755, mean_q: 3.481318
  5458/100000: episode: 558, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.704, mean reward: 0.370 [0.323, 0.435], mean action: 44.300 [24.000, 88.000], mean observation: 3.147 [-1.160, 10.330], loss: 1.041819, mae: 2.140871, mean_q: 3.482011
  5468/100000: episode: 559, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.606, mean reward: 0.461 [0.350, 0.539], mean action: 33.600 [11.000, 99.000], mean observation: 3.164 [-1.668, 10.459], loss: 1.061027, mae: 2.143615, mean_q: 3.484858
  5478/100000: episode: 560, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.144, mean reward: 0.414 [0.336, 0.530], mean action: 44.100 [27.000, 85.000], mean observation: 3.161 [-1.641, 10.264], loss: 1.055759, mae: 2.147507, mean_q: 3.487654
  5488/100000: episode: 561, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.700, mean reward: 0.370 [0.350, 0.449], mean action: 35.500 [2.000, 84.000], mean observation: 3.152 [-1.821, 10.236], loss: 1.015768, mae: 2.150315, mean_q: 3.492780
  5498/100000: episode: 562, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.164, mean reward: 0.416 [0.346, 0.523], mean action: 36.500 [1.000, 91.000], mean observation: 3.146 [-2.433, 10.268], loss: 1.169650, mae: 2.154504, mean_q: 3.494352
  5508/100000: episode: 563, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.016, mean reward: 0.402 [0.328, 0.530], mean action: 38.500 [5.000, 92.000], mean observation: 3.171 [-1.387, 10.490], loss: 1.018058, mae: 2.157901, mean_q: 3.498479
  5518/100000: episode: 564, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.189, mean reward: 0.419 [0.370, 0.510], mean action: 36.300 [8.000, 98.000], mean observation: 3.157 [-1.710, 10.314], loss: 0.983064, mae: 2.160038, mean_q: 3.502740
  5528/100000: episode: 565, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.188, mean reward: 0.419 [0.338, 0.498], mean action: 38.400 [7.000, 101.000], mean observation: 3.162 [-1.383, 10.327], loss: 0.923239, mae: 2.162485, mean_q: 3.505956
  5538/100000: episode: 566, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.909, mean reward: 0.391 [0.326, 0.454], mean action: 37.300 [13.000, 97.000], mean observation: 3.158 [-1.245, 10.275], loss: 1.035280, mae: 2.168269, mean_q: 3.508247
  5548/100000: episode: 567, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.959, mean reward: 0.396 [0.334, 0.531], mean action: 33.600 [2.000, 93.000], mean observation: 3.163 [-1.131, 10.445], loss: 1.044975, mae: 2.170598, mean_q: 3.508111
  5558/100000: episode: 568, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.168, mean reward: 0.417 [0.333, 0.447], mean action: 49.400 [5.000, 98.000], mean observation: 3.161 [-1.048, 10.277], loss: 1.192315, mae: 2.174708, mean_q: 3.507841
  5568/100000: episode: 569, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.229, mean reward: 0.423 [0.358, 0.486], mean action: 37.800 [20.000, 72.000], mean observation: 3.142 [-2.277, 10.239], loss: 0.952020, mae: 2.175818, mean_q: 3.507591
  5578/100000: episode: 570, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.900, mean reward: 0.390 [0.312, 0.481], mean action: 40.500 [12.000, 96.000], mean observation: 3.137 [-1.511, 10.350], loss: 1.157462, mae: 2.181348, mean_q: 3.506150
  5588/100000: episode: 571, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.751, mean reward: 0.375 [0.338, 0.472], mean action: 47.600 [15.000, 100.000], mean observation: 3.161 [-1.128, 10.197], loss: 0.999173, mae: 2.182843, mean_q: 3.505225
  5598/100000: episode: 572, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.074, mean reward: 0.407 [0.367, 0.453], mean action: 44.700 [11.000, 93.000], mean observation: 3.139 [-1.042, 10.314], loss: 1.073132, mae: 2.187601, mean_q: 3.508448
  5608/100000: episode: 573, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.853, mean reward: 0.385 [0.337, 0.477], mean action: 42.500 [21.000, 99.000], mean observation: 3.148 [-1.325, 10.291], loss: 0.887498, mae: 2.189424, mean_q: 3.510496
  5618/100000: episode: 574, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 5.307, mean reward: 0.531 [0.531, 0.531], mean action: 53.200 [30.000, 100.000], mean observation: 3.144 [-1.154, 10.177], loss: 1.237170, mae: 2.195298, mean_q: 3.512217
  5628/100000: episode: 575, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.928, mean reward: 0.393 [0.351, 0.487], mean action: 45.000 [23.000, 85.000], mean observation: 3.165 [-1.300, 10.320], loss: 1.191557, mae: 2.197900, mean_q: 3.509694
  5638/100000: episode: 576, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.974, mean reward: 0.397 [0.380, 0.444], mean action: 39.100 [30.000, 68.000], mean observation: 3.154 [-1.242, 10.281], loss: 1.058186, mae: 2.200672, mean_q: 3.509294
  5648/100000: episode: 577, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.933, mean reward: 0.393 [0.366, 0.439], mean action: 43.600 [13.000, 94.000], mean observation: 3.157 [-1.855, 10.390], loss: 1.061520, mae: 2.204268, mean_q: 3.512591
  5658/100000: episode: 578, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.950, mean reward: 0.395 [0.377, 0.468], mean action: 41.400 [30.000, 78.000], mean observation: 3.152 [-1.528, 10.356], loss: 1.056247, mae: 2.207191, mean_q: 3.515143
  5668/100000: episode: 579, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.158, mean reward: 0.416 [0.340, 0.519], mean action: 36.200 [13.000, 72.000], mean observation: 3.162 [-1.871, 10.364], loss: 1.012686, mae: 2.209314, mean_q: 3.517410
  5678/100000: episode: 580, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.644, mean reward: 0.364 [0.325, 0.433], mean action: 43.900 [30.000, 99.000], mean observation: 3.159 [-1.355, 10.472], loss: 0.994422, mae: 2.214052, mean_q: 3.520313
  5688/100000: episode: 581, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.090, mean reward: 0.409 [0.283, 0.518], mean action: 42.200 [0.000, 100.000], mean observation: 3.158 [-1.454, 10.255], loss: 1.030178, mae: 2.216892, mean_q: 3.520925
  5698/100000: episode: 582, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.515, mean reward: 0.451 [0.327, 0.599], mean action: 34.800 [19.000, 89.000], mean observation: 3.157 [-1.957, 10.341], loss: 1.076306, mae: 2.220643, mean_q: 3.521344
  5708/100000: episode: 583, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.013, mean reward: 0.401 [0.293, 0.495], mean action: 49.700 [30.000, 98.000], mean observation: 3.160 [-2.236, 10.399], loss: 0.897079, mae: 2.222326, mean_q: 3.524968
  5718/100000: episode: 584, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.806, mean reward: 0.381 [0.322, 0.492], mean action: 31.400 [4.000, 81.000], mean observation: 3.148 [-1.306, 10.384], loss: 1.049597, mae: 2.226484, mean_q: 3.526680
  5728/100000: episode: 585, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.181, mean reward: 0.418 [0.302, 0.495], mean action: 31.700 [15.000, 56.000], mean observation: 3.150 [-1.445, 10.388], loss: 1.026404, mae: 2.230155, mean_q: 3.528140
  5738/100000: episode: 586, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.961, mean reward: 0.396 [0.330, 0.459], mean action: 43.700 [30.000, 79.000], mean observation: 3.160 [-1.755, 10.264], loss: 1.046428, mae: 2.232920, mean_q: 3.530383
  5748/100000: episode: 587, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.718, mean reward: 0.472 [0.448, 0.474], mean action: 38.400 [10.000, 80.000], mean observation: 3.155 [-1.223, 10.369], loss: 1.041052, mae: 2.235298, mean_q: 3.534382
  5758/100000: episode: 588, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.185, mean reward: 0.419 [0.339, 0.575], mean action: 44.800 [30.000, 90.000], mean observation: 3.161 [-1.504, 10.321], loss: 1.084532, mae: 2.240453, mean_q: 3.537764
  5768/100000: episode: 589, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.225, mean reward: 0.422 [0.323, 0.461], mean action: 36.000 [7.000, 59.000], mean observation: 3.178 [-1.694, 10.319], loss: 0.935148, mae: 2.241995, mean_q: 3.538871
  5778/100000: episode: 590, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.996, mean reward: 0.400 [0.361, 0.507], mean action: 43.400 [30.000, 91.000], mean observation: 3.152 [-1.362, 10.329], loss: 1.124401, mae: 2.245909, mean_q: 3.542580
  5788/100000: episode: 591, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.126, mean reward: 0.413 [0.356, 0.525], mean action: 35.500 [30.000, 52.000], mean observation: 3.161 [-1.063, 10.291], loss: 1.152498, mae: 2.249426, mean_q: 3.546608
  5798/100000: episode: 592, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.016, mean reward: 0.402 [0.269, 0.504], mean action: 32.300 [10.000, 97.000], mean observation: 3.151 [-1.364, 10.532], loss: 1.059240, mae: 2.253020, mean_q: 3.552594
  5808/100000: episode: 593, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.800, mean reward: 0.380 [0.325, 0.453], mean action: 49.300 [16.000, 97.000], mean observation: 3.161 [-1.096, 10.355], loss: 1.003888, mae: 2.256218, mean_q: 3.556996
  5818/100000: episode: 594, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.808, mean reward: 0.381 [0.352, 0.424], mean action: 42.600 [0.000, 100.000], mean observation: 3.159 [-2.066, 10.424], loss: 0.950939, mae: 2.258820, mean_q: 3.557343
  5828/100000: episode: 595, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.220, mean reward: 0.422 [0.329, 0.528], mean action: 41.700 [30.000, 68.000], mean observation: 3.154 [-1.213, 10.602], loss: 1.151046, mae: 2.262511, mean_q: 3.556815
  5838/100000: episode: 596, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.524, mean reward: 0.452 [0.450, 0.473], mean action: 36.800 [30.000, 88.000], mean observation: 3.161 [-1.654, 10.307], loss: 1.016406, mae: 2.265302, mean_q: 3.557259
  5848/100000: episode: 597, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.857, mean reward: 0.486 [0.339, 0.523], mean action: 43.200 [8.000, 80.000], mean observation: 3.156 [-1.289, 10.421], loss: 1.317120, mae: 2.270203, mean_q: 3.556396
  5858/100000: episode: 598, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.797, mean reward: 0.380 [0.320, 0.462], mean action: 30.900 [9.000, 65.000], mean observation: 3.145 [-1.990, 10.386], loss: 1.056310, mae: 2.271882, mean_q: 3.552147
  5868/100000: episode: 599, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 3.839, mean reward: 0.384 [0.316, 0.466], mean action: 37.000 [4.000, 79.000], mean observation: 3.154 [-1.913, 10.314], loss: 1.296345, mae: 2.276791, mean_q: 3.551182
  5878/100000: episode: 600, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.962, mean reward: 0.396 [0.386, 0.436], mean action: 49.500 [7.000, 101.000], mean observation: 3.143 [-1.768, 10.262], loss: 0.829405, mae: 2.277824, mean_q: 3.555113
  5888/100000: episode: 601, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.214, mean reward: 0.421 [0.360, 0.490], mean action: 37.000 [6.000, 83.000], mean observation: 3.152 [-0.989, 10.477], loss: 0.887874, mae: 2.280923, mean_q: 3.561278
  5896/100000: episode: 602, duration: 0.130s, episode steps: 8, steps per second: 62, episode reward: 12.676, mean reward: 1.585 [0.348, 10.000], mean action: 46.375 [30.000, 91.000], mean observation: 3.154 [-1.418, 10.297], loss: 1.317871, mae: 2.285896, mean_q: 3.564841
  5906/100000: episode: 603, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.073, mean reward: 0.407 [0.391, 0.468], mean action: 43.600 [21.000, 84.000], mean observation: 3.156 [-1.090, 10.266], loss: 1.269043, mae: 2.289216, mean_q: 3.567162
  5916/100000: episode: 604, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.995, mean reward: 0.400 [0.351, 0.478], mean action: 29.500 [6.000, 76.000], mean observation: 3.151 [-1.746, 10.391], loss: 1.233322, mae: 2.290805, mean_q: 3.565771
  5926/100000: episode: 605, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.279, mean reward: 0.428 [0.420, 0.469], mean action: 41.900 [26.000, 81.000], mean observation: 3.148 [-1.398, 10.286], loss: 1.256217, mae: 2.294466, mean_q: 3.566207
  5936/100000: episode: 606, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.008, mean reward: 0.401 [0.313, 0.484], mean action: 40.400 [4.000, 97.000], mean observation: 3.165 [-1.222, 10.293], loss: 0.923490, mae: 2.296548, mean_q: 3.567393
  5946/100000: episode: 607, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 5.046, mean reward: 0.505 [0.464, 0.515], mean action: 48.800 [5.000, 95.000], mean observation: 3.151 [-1.794, 10.287], loss: 0.896510, mae: 2.299173, mean_q: 3.569451
  5956/100000: episode: 608, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.622, mean reward: 0.362 [0.299, 0.420], mean action: 36.800 [5.000, 92.000], mean observation: 3.158 [-1.339, 10.230], loss: 0.863628, mae: 2.301449, mean_q: 3.575997
  5966/100000: episode: 609, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.271, mean reward: 0.427 [0.423, 0.465], mean action: 48.700 [16.000, 93.000], mean observation: 3.159 [-1.719, 10.287], loss: 0.956189, mae: 2.306229, mean_q: 3.582933
  5976/100000: episode: 610, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.053, mean reward: 0.405 [0.319, 0.468], mean action: 38.000 [11.000, 73.000], mean observation: 3.147 [-1.023, 10.356], loss: 1.009829, mae: 2.309829, mean_q: 3.585055
  5986/100000: episode: 611, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.430, mean reward: 0.443 [0.319, 0.484], mean action: 39.500 [4.000, 99.000], mean observation: 3.150 [-2.071, 10.273], loss: 0.941802, mae: 2.312664, mean_q: 3.582549
  5996/100000: episode: 612, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.968, mean reward: 0.397 [0.325, 0.487], mean action: 38.700 [16.000, 76.000], mean observation: 3.160 [-1.275, 10.360], loss: 0.943040, mae: 2.315984, mean_q: 3.581357
  6006/100000: episode: 613, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.291, mean reward: 0.429 [0.350, 0.593], mean action: 43.800 [11.000, 94.000], mean observation: 3.151 [-1.742, 10.353], loss: 1.125278, mae: 2.318908, mean_q: 3.581731
  6016/100000: episode: 614, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.040, mean reward: 0.404 [0.357, 0.463], mean action: 35.200 [5.000, 65.000], mean observation: 3.155 [-1.244, 10.285], loss: 0.998616, mae: 2.322669, mean_q: 3.585532
  6026/100000: episode: 615, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.920, mean reward: 0.392 [0.342, 0.456], mean action: 31.000 [8.000, 58.000], mean observation: 3.159 [-1.510, 10.366], loss: 1.182731, mae: 2.325420, mean_q: 3.590837
  6036/100000: episode: 616, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 3.650, mean reward: 0.365 [0.313, 0.472], mean action: 29.600 [10.000, 46.000], mean observation: 3.164 [-1.468, 10.270], loss: 0.933646, mae: 2.327337, mean_q: 3.593876
  6046/100000: episode: 617, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.080, mean reward: 0.408 [0.338, 0.559], mean action: 37.700 [1.000, 91.000], mean observation: 3.150 [-1.457, 10.300], loss: 0.962448, mae: 2.329927, mean_q: 3.594715
  6056/100000: episode: 618, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.313, mean reward: 0.431 [0.300, 0.501], mean action: 40.700 [0.000, 101.000], mean observation: 3.144 [-1.218, 10.327], loss: 0.986149, mae: 2.334218, mean_q: 3.595857
  6066/100000: episode: 619, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.997, mean reward: 0.400 [0.326, 0.510], mean action: 30.500 [3.000, 68.000], mean observation: 3.168 [-1.441, 10.357], loss: 1.138596, mae: 2.337368, mean_q: 3.595097
  6076/100000: episode: 620, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.013, mean reward: 0.401 [0.322, 0.557], mean action: 49.000 [30.000, 91.000], mean observation: 3.158 [-1.386, 10.443], loss: 1.112023, mae: 2.340648, mean_q: 3.593730
  6086/100000: episode: 621, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.518, mean reward: 0.452 [0.371, 0.569], mean action: 43.400 [13.000, 85.000], mean observation: 3.161 [-1.486, 10.327], loss: 0.957697, mae: 2.342576, mean_q: 3.591940
  6096/100000: episode: 622, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.210, mean reward: 0.421 [0.376, 0.465], mean action: 34.600 [3.000, 71.000], mean observation: 3.163 [-0.868, 10.414], loss: 0.973937, mae: 2.345576, mean_q: 3.591598
  6106/100000: episode: 623, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.275, mean reward: 0.428 [0.345, 0.599], mean action: 41.500 [19.000, 93.000], mean observation: 3.163 [-1.782, 10.602], loss: 0.978159, mae: 2.348532, mean_q: 3.594240
  6116/100000: episode: 624, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.048, mean reward: 0.405 [0.338, 0.463], mean action: 50.200 [1.000, 99.000], mean observation: 3.161 [-1.378, 10.340], loss: 1.111142, mae: 2.353162, mean_q: 3.597542
  6126/100000: episode: 625, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.021, mean reward: 0.402 [0.331, 0.489], mean action: 34.100 [12.000, 75.000], mean observation: 3.158 [-1.815, 10.481], loss: 1.092642, mae: 2.356341, mean_q: 3.599736
  6136/100000: episode: 626, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.222, mean reward: 0.422 [0.404, 0.520], mean action: 38.800 [6.000, 85.000], mean observation: 3.157 [-1.292, 10.279], loss: 1.068369, mae: 2.359221, mean_q: 3.600364
  6137/100000: episode: 627, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 53.000 [53.000, 53.000], mean observation: 3.142 [-1.829, 10.100], loss: 0.636953, mae: 2.357178, mean_q: 3.600252
  6147/100000: episode: 628, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.413, mean reward: 0.441 [0.357, 0.580], mean action: 44.100 [30.000, 85.000], mean observation: 3.159 [-0.900, 10.421], loss: 1.024095, mae: 2.362993, mean_q: 3.600813
  6157/100000: episode: 629, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.860, mean reward: 0.386 [0.331, 0.482], mean action: 43.600 [28.000, 97.000], mean observation: 3.151 [-1.905, 10.320], loss: 1.002660, mae: 2.364988, mean_q: 3.602559
  6167/100000: episode: 630, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.636, mean reward: 0.364 [0.283, 0.431], mean action: 54.100 [30.000, 96.000], mean observation: 3.161 [-1.767, 10.285], loss: 1.060151, mae: 2.369470, mean_q: 3.605247
  6177/100000: episode: 631, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.661, mean reward: 0.366 [0.311, 0.436], mean action: 46.900 [10.000, 97.000], mean observation: 3.166 [-1.862, 10.400], loss: 0.991797, mae: 2.371913, mean_q: 3.609423
  6187/100000: episode: 632, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.890, mean reward: 0.389 [0.280, 0.477], mean action: 29.800 [1.000, 62.000], mean observation: 3.163 [-1.069, 10.283], loss: 1.067316, mae: 2.374728, mean_q: 3.613429
  6197/100000: episode: 633, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.108, mean reward: 0.411 [0.324, 0.551], mean action: 31.100 [30.000, 39.000], mean observation: 3.148 [-1.215, 10.482], loss: 0.983102, mae: 2.378293, mean_q: 3.614911
  6207/100000: episode: 634, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.021, mean reward: 0.402 [0.361, 0.479], mean action: 63.100 [30.000, 100.000], mean observation: 3.158 [-1.385, 10.312], loss: 1.209766, mae: 2.381887, mean_q: 3.614848
  6217/100000: episode: 635, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.033, mean reward: 0.403 [0.347, 0.501], mean action: 39.300 [20.000, 78.000], mean observation: 3.158 [-1.397, 10.349], loss: 0.947996, mae: 2.384119, mean_q: 3.615083
  6227/100000: episode: 636, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.175, mean reward: 0.418 [0.356, 0.506], mean action: 44.900 [30.000, 93.000], mean observation: 3.148 [-1.274, 10.398], loss: 0.996336, mae: 2.386981, mean_q: 3.615223
  6237/100000: episode: 637, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.871, mean reward: 0.387 [0.299, 0.520], mean action: 48.600 [15.000, 99.000], mean observation: 3.153 [-1.849, 10.364], loss: 1.116695, mae: 2.390681, mean_q: 3.617080
  6247/100000: episode: 638, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.216, mean reward: 0.422 [0.420, 0.427], mean action: 46.600 [3.000, 100.000], mean observation: 3.155 [-0.876, 10.349], loss: 0.912232, mae: 2.391780, mean_q: 3.618615
  6257/100000: episode: 639, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.581, mean reward: 0.458 [0.345, 0.584], mean action: 35.400 [0.000, 60.000], mean observation: 3.159 [-1.340, 10.265], loss: 1.037025, mae: 2.396563, mean_q: 3.620945
  6267/100000: episode: 640, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.945, mean reward: 0.394 [0.364, 0.460], mean action: 40.600 [27.000, 79.000], mean observation: 3.163 [-1.544, 10.369], loss: 1.051118, mae: 2.399864, mean_q: 3.623030
  6277/100000: episode: 641, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.234, mean reward: 0.423 [0.423, 0.423], mean action: 45.300 [4.000, 98.000], mean observation: 3.158 [-1.143, 10.312], loss: 1.112314, mae: 2.403620, mean_q: 3.626773
  6287/100000: episode: 642, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.011, mean reward: 0.401 [0.357, 0.448], mean action: 31.700 [10.000, 69.000], mean observation: 3.164 [-1.895, 10.420], loss: 1.020718, mae: 2.405579, mean_q: 3.628494
  6297/100000: episode: 643, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.227, mean reward: 0.423 [0.358, 0.481], mean action: 39.000 [8.000, 71.000], mean observation: 3.163 [-1.617, 10.424], loss: 0.931037, mae: 2.407470, mean_q: 3.628471
  6307/100000: episode: 644, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.914, mean reward: 0.391 [0.322, 0.536], mean action: 31.900 [13.000, 56.000], mean observation: 3.156 [-1.694, 10.464], loss: 1.031845, mae: 2.412470, mean_q: 3.627791
  6317/100000: episode: 645, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.722, mean reward: 0.372 [0.290, 0.452], mean action: 30.700 [4.000, 57.000], mean observation: 3.159 [-1.500, 10.443], loss: 0.895511, mae: 2.413763, mean_q: 3.625971
  6327/100000: episode: 646, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.907, mean reward: 0.391 [0.297, 0.479], mean action: 41.800 [4.000, 90.000], mean observation: 3.161 [-2.379, 10.340], loss: 1.176862, mae: 2.418188, mean_q: 3.627007
  6337/100000: episode: 647, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.986, mean reward: 0.399 [0.338, 0.515], mean action: 41.400 [12.000, 84.000], mean observation: 3.152 [-1.509, 10.318], loss: 0.873114, mae: 2.420250, mean_q: 3.626422
  6347/100000: episode: 648, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.395, mean reward: 0.440 [0.425, 0.520], mean action: 40.900 [30.000, 100.000], mean observation: 3.164 [-1.638, 10.346], loss: 0.902667, mae: 2.422851, mean_q: 3.627532
  6357/100000: episode: 649, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.678, mean reward: 0.368 [0.297, 0.438], mean action: 60.600 [30.000, 80.000], mean observation: 3.153 [-1.342, 10.335], loss: 1.042391, mae: 2.425917, mean_q: 3.628353
  6367/100000: episode: 650, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.192, mean reward: 0.419 [0.378, 0.442], mean action: 60.100 [6.000, 100.000], mean observation: 3.162 [-1.250, 10.342], loss: 0.875562, mae: 2.429202, mean_q: 3.630821
  6377/100000: episode: 651, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 3.949, mean reward: 0.395 [0.320, 0.500], mean action: 37.300 [13.000, 96.000], mean observation: 3.154 [-1.589, 10.444], loss: 1.011923, mae: 2.433187, mean_q: 3.633653
  6387/100000: episode: 652, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.833, mean reward: 0.383 [0.323, 0.418], mean action: 53.200 [16.000, 96.000], mean observation: 3.161 [-1.625, 10.351], loss: 1.066081, mae: 2.435574, mean_q: 3.636897
  6397/100000: episode: 653, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.957, mean reward: 0.396 [0.362, 0.460], mean action: 48.100 [30.000, 98.000], mean observation: 3.174 [-1.013, 10.327], loss: 0.916658, mae: 2.437875, mean_q: 3.639977
  6400/100000: episode: 654, duration: 0.064s, episode steps: 3, steps per second: 47, episode reward: 10.665, mean reward: 3.555 [0.324, 10.000], mean action: 43.667 [30.000, 51.000], mean observation: 3.158 [-0.744, 10.244], loss: 1.071822, mae: 2.441936, mean_q: 3.640568
  6410/100000: episode: 655, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.250, mean reward: 0.425 [0.309, 0.524], mean action: 31.500 [9.000, 56.000], mean observation: 3.152 [-2.145, 10.337], loss: 0.975069, mae: 2.442245, mean_q: 3.641368
  6420/100000: episode: 656, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.442, mean reward: 0.444 [0.408, 0.578], mean action: 45.100 [28.000, 96.000], mean observation: 3.164 [-1.327, 10.434], loss: 0.874494, mae: 2.443811, mean_q: 3.642521
  6430/100000: episode: 657, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.853, mean reward: 0.385 [0.338, 0.433], mean action: 56.600 [30.000, 95.000], mean observation: 3.143 [-1.028, 10.289], loss: 0.952543, mae: 2.447556, mean_q: 3.642170
  6440/100000: episode: 658, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.079, mean reward: 0.408 [0.334, 0.480], mean action: 49.300 [4.000, 101.000], mean observation: 3.157 [-1.469, 10.335], loss: 1.120218, mae: 2.452168, mean_q: 3.642998
  6450/100000: episode: 659, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.550, mean reward: 0.355 [0.321, 0.432], mean action: 43.000 [6.000, 96.000], mean observation: 3.154 [-1.652, 10.372], loss: 1.084539, mae: 2.454695, mean_q: 3.642687
  6460/100000: episode: 660, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.554, mean reward: 0.355 [0.318, 0.390], mean action: 47.600 [2.000, 94.000], mean observation: 3.146 [-1.600, 10.387], loss: 1.041483, mae: 2.456428, mean_q: 3.640937
  6470/100000: episode: 661, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.220, mean reward: 0.422 [0.348, 0.534], mean action: 32.000 [1.000, 80.000], mean observation: 3.149 [-2.576, 10.298], loss: 1.075299, mae: 2.459219, mean_q: 3.641041
  6480/100000: episode: 662, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 4.350, mean reward: 0.435 [0.421, 0.485], mean action: 39.700 [11.000, 90.000], mean observation: 3.151 [-1.326, 10.474], loss: 1.114536, mae: 2.462633, mean_q: 3.644511
  6490/100000: episode: 663, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.362, mean reward: 0.436 [0.338, 0.568], mean action: 43.900 [12.000, 99.000], mean observation: 3.168 [-1.210, 10.438], loss: 1.077033, mae: 2.466259, mean_q: 3.648500
  6500/100000: episode: 664, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.387, mean reward: 0.439 [0.347, 0.550], mean action: 38.200 [6.000, 80.000], mean observation: 3.152 [-0.951, 10.237], loss: 0.859322, mae: 2.467538, mean_q: 3.651928
  6510/100000: episode: 665, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.057, mean reward: 0.406 [0.362, 0.489], mean action: 33.700 [8.000, 100.000], mean observation: 3.153 [-1.327, 10.330], loss: 1.354122, mae: 2.472153, mean_q: 3.652972
  6520/100000: episode: 666, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.644, mean reward: 0.364 [0.286, 0.571], mean action: 49.200 [30.000, 80.000], mean observation: 3.150 [-1.363, 10.349], loss: 1.024697, mae: 2.474477, mean_q: 3.652930
  6530/100000: episode: 667, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.118, mean reward: 0.412 [0.353, 0.482], mean action: 40.400 [1.000, 71.000], mean observation: 3.147 [-1.511, 10.437], loss: 1.273854, mae: 2.479270, mean_q: 3.654989
  6540/100000: episode: 668, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.067, mean reward: 0.407 [0.372, 0.475], mean action: 31.300 [10.000, 63.000], mean observation: 3.153 [-1.466, 10.352], loss: 1.205113, mae: 2.480803, mean_q: 3.657752
  6550/100000: episode: 669, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.024, mean reward: 0.402 [0.321, 0.481], mean action: 22.300 [1.000, 30.000], mean observation: 3.150 [-2.265, 10.301], loss: 0.911782, mae: 2.483397, mean_q: 3.660112
  6560/100000: episode: 670, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.556, mean reward: 0.456 [0.383, 0.480], mean action: 49.900 [25.000, 99.000], mean observation: 3.145 [-1.507, 10.416], loss: 0.890956, mae: 2.485417, mean_q: 3.662993
  6570/100000: episode: 671, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.792, mean reward: 0.479 [0.470, 0.510], mean action: 29.000 [8.000, 48.000], mean observation: 3.158 [-2.036, 10.363], loss: 1.066568, mae: 2.488869, mean_q: 3.664653
  6580/100000: episode: 672, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.942, mean reward: 0.394 [0.339, 0.471], mean action: 40.300 [30.000, 76.000], mean observation: 3.166 [-1.166, 10.321], loss: 0.963349, mae: 2.491759, mean_q: 3.666857
  6590/100000: episode: 673, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.619, mean reward: 0.362 [0.290, 0.447], mean action: 48.300 [21.000, 93.000], mean observation: 3.159 [-1.172, 10.273], loss: 0.880011, mae: 2.494498, mean_q: 3.666822
  6600/100000: episode: 674, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.809, mean reward: 0.381 [0.325, 0.417], mean action: 38.000 [5.000, 95.000], mean observation: 3.145 [-1.250, 10.398], loss: 1.092664, mae: 2.498318, mean_q: 3.669249
  6610/100000: episode: 675, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.868, mean reward: 0.387 [0.291, 0.474], mean action: 42.200 [30.000, 57.000], mean observation: 3.164 [-1.657, 10.323], loss: 1.269036, mae: 2.501717, mean_q: 3.674480
  6620/100000: episode: 676, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.165, mean reward: 0.417 [0.319, 0.503], mean action: 30.500 [5.000, 55.000], mean observation: 3.154 [-2.290, 10.306], loss: 0.962440, mae: 2.503382, mean_q: 3.679891
  6630/100000: episode: 677, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.904, mean reward: 0.390 [0.307, 0.543], mean action: 37.300 [30.000, 69.000], mean observation: 3.166 [-1.811, 10.407], loss: 1.268261, mae: 2.506940, mean_q: 3.681473
  6640/100000: episode: 678, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.903, mean reward: 0.390 [0.344, 0.456], mean action: 37.100 [6.000, 86.000], mean observation: 3.160 [-1.117, 10.433], loss: 1.160115, mae: 2.510654, mean_q: 3.680352
  6650/100000: episode: 679, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.874, mean reward: 0.387 [0.248, 0.497], mean action: 47.200 [1.000, 90.000], mean observation: 3.156 [-1.798, 10.307], loss: 0.939319, mae: 2.511834, mean_q: 3.680164
  6660/100000: episode: 680, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.278, mean reward: 0.428 [0.362, 0.562], mean action: 36.700 [7.000, 67.000], mean observation: 3.163 [-1.188, 10.402], loss: 0.945009, mae: 2.515248, mean_q: 3.679333
  6670/100000: episode: 681, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.209, mean reward: 0.421 [0.361, 0.495], mean action: 32.200 [0.000, 67.000], mean observation: 3.159 [-1.972, 10.191], loss: 0.858699, mae: 2.517710, mean_q: 3.679778
  6680/100000: episode: 682, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.606, mean reward: 0.461 [0.419, 0.528], mean action: 38.000 [7.000, 87.000], mean observation: 3.163 [-1.434, 10.274], loss: 1.091884, mae: 2.521178, mean_q: 3.683369
  6690/100000: episode: 683, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.855, mean reward: 0.385 [0.320, 0.455], mean action: 50.000 [16.000, 93.000], mean observation: 3.161 [-1.091, 10.400], loss: 1.057904, mae: 2.524222, mean_q: 3.686295
  6700/100000: episode: 684, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.053, mean reward: 0.405 [0.383, 0.420], mean action: 44.300 [9.000, 93.000], mean observation: 3.156 [-1.277, 10.350], loss: 0.929986, mae: 2.526246, mean_q: 3.688276
  6710/100000: episode: 685, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.780, mean reward: 0.378 [0.332, 0.488], mean action: 46.800 [3.000, 99.000], mean observation: 3.162 [-1.607, 10.410], loss: 1.065471, mae: 2.529390, mean_q: 3.687834
  6720/100000: episode: 686, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.411, mean reward: 0.441 [0.318, 0.534], mean action: 35.900 [1.000, 79.000], mean observation: 3.148 [-1.140, 10.243], loss: 1.177608, mae: 2.533324, mean_q: 3.686904
  6730/100000: episode: 687, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.861, mean reward: 0.386 [0.354, 0.434], mean action: 40.500 [28.000, 95.000], mean observation: 3.164 [-1.458, 10.309], loss: 0.927742, mae: 2.534225, mean_q: 3.683542
  6740/100000: episode: 688, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.965, mean reward: 0.396 [0.325, 0.546], mean action: 37.300 [5.000, 76.000], mean observation: 3.153 [-1.801, 10.397], loss: 0.997551, mae: 2.538604, mean_q: 3.681165
  6750/100000: episode: 689, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.790, mean reward: 0.379 [0.340, 0.451], mean action: 37.900 [26.000, 75.000], mean observation: 3.161 [-1.288, 10.412], loss: 0.933716, mae: 2.541028, mean_q: 3.680356
  6760/100000: episode: 690, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 3.868, mean reward: 0.387 [0.287, 0.500], mean action: 29.800 [25.000, 33.000], mean observation: 3.145 [-1.560, 10.319], loss: 0.980190, mae: 2.543708, mean_q: 3.679394
  6770/100000: episode: 691, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.058, mean reward: 0.406 [0.345, 0.459], mean action: 24.100 [0.000, 32.000], mean observation: 3.158 [-1.280, 10.480], loss: 1.015319, mae: 2.547155, mean_q: 3.677749
  6780/100000: episode: 692, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.843, mean reward: 0.384 [0.356, 0.464], mean action: 44.400 [1.000, 90.000], mean observation: 3.145 [-0.985, 10.404], loss: 0.816391, mae: 2.548295, mean_q: 3.678093
  6790/100000: episode: 693, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 5.125, mean reward: 0.513 [0.513, 0.513], mean action: 39.500 [8.000, 85.000], mean observation: 3.143 [-1.340, 10.342], loss: 1.034723, mae: 2.553216, mean_q: 3.680148
  6800/100000: episode: 694, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.150, mean reward: 0.415 [0.364, 0.562], mean action: 44.200 [15.000, 84.000], mean observation: 3.169 [-1.429, 10.305], loss: 0.990632, mae: 2.555359, mean_q: 3.685088
  6810/100000: episode: 695, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.517, mean reward: 0.452 [0.362, 0.563], mean action: 34.800 [30.000, 55.000], mean observation: 3.136 [-1.820, 10.314], loss: 0.935728, mae: 2.557533, mean_q: 3.689405
  6820/100000: episode: 696, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.694, mean reward: 0.369 [0.302, 0.459], mean action: 46.700 [18.000, 99.000], mean observation: 3.153 [-1.415, 10.308], loss: 1.069036, mae: 2.561714, mean_q: 3.692305
  6830/100000: episode: 697, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.548, mean reward: 0.455 [0.380, 0.542], mean action: 42.100 [30.000, 92.000], mean observation: 3.149 [-1.423, 10.279], loss: 1.103385, mae: 2.563684, mean_q: 3.693141
  6840/100000: episode: 698, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.452, mean reward: 0.445 [0.372, 0.522], mean action: 37.700 [9.000, 98.000], mean observation: 3.152 [-1.233, 10.357], loss: 0.966229, mae: 2.566059, mean_q: 3.692263
  6850/100000: episode: 699, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.680, mean reward: 0.468 [0.416, 0.555], mean action: 43.500 [0.000, 100.000], mean observation: 3.158 [-1.141, 10.242], loss: 0.878189, mae: 2.568383, mean_q: 3.693038
  6860/100000: episode: 700, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.936, mean reward: 0.394 [0.317, 0.503], mean action: 35.400 [13.000, 68.000], mean observation: 3.152 [-1.200, 10.228], loss: 1.054429, mae: 2.572002, mean_q: 3.695542
  6870/100000: episode: 701, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.687, mean reward: 0.469 [0.361, 0.496], mean action: 50.800 [3.000, 94.000], mean observation: 3.159 [-1.347, 10.252], loss: 0.951261, mae: 2.573648, mean_q: 3.696897
  6880/100000: episode: 702, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.001, mean reward: 0.400 [0.345, 0.479], mean action: 41.700 [30.000, 85.000], mean observation: 3.159 [-0.851, 10.388], loss: 1.121596, mae: 2.578049, mean_q: 3.695483
  6890/100000: episode: 703, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.782, mean reward: 0.378 [0.325, 0.461], mean action: 34.900 [0.000, 86.000], mean observation: 3.153 [-1.463, 10.419], loss: 0.937874, mae: 2.579732, mean_q: 3.693480
  6900/100000: episode: 704, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.589, mean reward: 0.459 [0.459, 0.459], mean action: 44.000 [30.000, 93.000], mean observation: 3.166 [-1.175, 10.295], loss: 1.078115, mae: 2.582847, mean_q: 3.692884
  6910/100000: episode: 705, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.822, mean reward: 0.382 [0.320, 0.485], mean action: 43.100 [30.000, 72.000], mean observation: 3.154 [-1.423, 10.217], loss: 1.214944, mae: 2.587730, mean_q: 3.691699
  6920/100000: episode: 706, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 5.008, mean reward: 0.501 [0.467, 0.553], mean action: 41.200 [10.000, 93.000], mean observation: 3.159 [-1.209, 10.227], loss: 1.034592, mae: 2.588168, mean_q: 3.689635
  6930/100000: episode: 707, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.179, mean reward: 0.418 [0.409, 0.452], mean action: 35.300 [20.000, 93.000], mean observation: 3.162 [-1.173, 10.351], loss: 0.966067, mae: 2.591348, mean_q: 3.690599
  6940/100000: episode: 708, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.259, mean reward: 0.426 [0.357, 0.506], mean action: 51.100 [22.000, 100.000], mean observation: 3.156 [-1.286, 10.338], loss: 1.117338, mae: 2.594530, mean_q: 3.691470
  6950/100000: episode: 709, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.270, mean reward: 0.427 [0.329, 0.461], mean action: 41.600 [1.000, 90.000], mean observation: 3.164 [-0.873, 10.276], loss: 1.068681, mae: 2.597684, mean_q: 3.692416
  6960/100000: episode: 710, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.088, mean reward: 0.409 [0.382, 0.472], mean action: 41.700 [8.000, 84.000], mean observation: 3.154 [-1.734, 10.328], loss: 1.050926, mae: 2.600733, mean_q: 3.693277
  6970/100000: episode: 711, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.660, mean reward: 0.466 [0.466, 0.467], mean action: 41.800 [30.000, 89.000], mean observation: 3.157 [-2.111, 10.437], loss: 0.988020, mae: 2.603101, mean_q: 3.689386
  6980/100000: episode: 712, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.210, mean reward: 0.421 [0.270, 0.514], mean action: 38.100 [2.000, 100.000], mean observation: 3.155 [-2.072, 10.385], loss: 0.980495, mae: 2.605252, mean_q: 3.688207
  6990/100000: episode: 713, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.071, mean reward: 0.407 [0.323, 0.523], mean action: 32.000 [7.000, 59.000], mean observation: 3.164 [-1.010, 10.248], loss: 0.955329, mae: 2.609030, mean_q: 3.688398
  7000/100000: episode: 714, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.220, mean reward: 0.422 [0.347, 0.546], mean action: 37.500 [9.000, 99.000], mean observation: 3.162 [-1.072, 10.354], loss: 0.909833, mae: 2.610816, mean_q: 3.690622
  7010/100000: episode: 715, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.343, mean reward: 0.434 [0.424, 0.488], mean action: 42.300 [30.000, 69.000], mean observation: 3.163 [-1.861, 10.409], loss: 0.880446, mae: 2.613662, mean_q: 3.694412
  7020/100000: episode: 716, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.875, mean reward: 0.388 [0.347, 0.499], mean action: 42.200 [18.000, 98.000], mean observation: 3.153 [-1.515, 10.303], loss: 0.870408, mae: 2.615373, mean_q: 3.699067
  7030/100000: episode: 717, duration: 0.226s, episode steps: 10, steps per second: 44, episode reward: 4.325, mean reward: 0.433 [0.393, 0.497], mean action: 27.300 [6.000, 71.000], mean observation: 3.143 [-1.280, 10.385], loss: 0.908797, mae: 2.618378, mean_q: 3.700438
  7040/100000: episode: 718, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.922, mean reward: 0.392 [0.340, 0.488], mean action: 41.200 [19.000, 74.000], mean observation: 3.168 [-1.258, 10.442], loss: 0.917068, mae: 2.621261, mean_q: 3.700118
  7050/100000: episode: 719, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.093, mean reward: 0.409 [0.392, 0.493], mean action: 41.700 [7.000, 92.000], mean observation: 3.160 [-1.205, 10.343], loss: 1.025244, mae: 2.624327, mean_q: 3.701123
  7058/100000: episode: 720, duration: 0.158s, episode steps: 8, steps per second: 51, episode reward: 12.634, mean reward: 1.579 [0.334, 10.000], mean action: 35.625 [13.000, 64.000], mean observation: 3.159 [-1.429, 10.258], loss: 0.875776, mae: 2.625416, mean_q: 3.700818
  7068/100000: episode: 721, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.984, mean reward: 0.398 [0.341, 0.491], mean action: 43.900 [6.000, 90.000], mean observation: 3.168 [-1.563, 10.388], loss: 0.893098, mae: 2.629410, mean_q: 3.700824
  7078/100000: episode: 722, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.238, mean reward: 0.424 [0.407, 0.440], mean action: 51.700 [30.000, 101.000], mean observation: 3.154 [-1.178, 10.332], loss: 0.942873, mae: 2.631573, mean_q: 3.702832
  7088/100000: episode: 723, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.070, mean reward: 0.407 [0.341, 0.468], mean action: 49.200 [30.000, 83.000], mean observation: 3.152 [-1.767, 10.259], loss: 0.941235, mae: 2.635551, mean_q: 3.705404
  7098/100000: episode: 724, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.938, mean reward: 0.394 [0.361, 0.493], mean action: 37.900 [1.000, 94.000], mean observation: 3.145 [-1.076, 10.294], loss: 0.953684, mae: 2.637961, mean_q: 3.703434
  7100/100000: episode: 725, duration: 0.067s, episode steps: 2, steps per second: 30, episode reward: 10.423, mean reward: 5.211 [0.423, 10.000], mean action: 23.500 [17.000, 30.000], mean observation: 3.168 [-0.738, 10.100], loss: 0.750588, mae: 2.638110, mean_q: 3.701292
  7110/100000: episode: 726, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.135, mean reward: 0.414 [0.333, 0.548], mean action: 42.600 [15.000, 91.000], mean observation: 3.149 [-1.891, 10.289], loss: 1.040759, mae: 2.642681, mean_q: 3.699747
  7120/100000: episode: 727, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.380, mean reward: 0.438 [0.356, 0.550], mean action: 42.900 [17.000, 86.000], mean observation: 3.143 [-1.962, 10.351], loss: 1.017570, mae: 2.644341, mean_q: 3.696979
  7128/100000: episode: 728, duration: 0.149s, episode steps: 8, steps per second: 54, episode reward: 13.018, mean reward: 1.627 [0.320, 10.000], mean action: 26.625 [13.000, 30.000], mean observation: 3.157 [-1.897, 10.469], loss: 0.706843, mae: 2.645209, mean_q: 3.694280
  7138/100000: episode: 729, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.546, mean reward: 0.455 [0.346, 0.584], mean action: 38.900 [30.000, 87.000], mean observation: 3.147 [-1.301, 10.177], loss: 0.880117, mae: 2.648077, mean_q: 3.696163
  7148/100000: episode: 730, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.644, mean reward: 0.464 [0.458, 0.503], mean action: 37.200 [7.000, 94.000], mean observation: 3.154 [-1.296, 10.378], loss: 0.909165, mae: 2.650944, mean_q: 3.698848
  7158/100000: episode: 731, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.254, mean reward: 0.425 [0.329, 0.534], mean action: 28.900 [3.000, 59.000], mean observation: 3.150 [-1.368, 10.356], loss: 1.081082, mae: 2.655155, mean_q: 3.701560
  7168/100000: episode: 732, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.915, mean reward: 0.392 [0.345, 0.496], mean action: 36.200 [3.000, 95.000], mean observation: 3.145 [-1.318, 10.314], loss: 0.974357, mae: 2.656866, mean_q: 3.702621
  7178/100000: episode: 733, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.032, mean reward: 0.403 [0.324, 0.492], mean action: 41.200 [15.000, 86.000], mean observation: 3.158 [-1.188, 10.202], loss: 0.910555, mae: 2.659652, mean_q: 3.703312
  7188/100000: episode: 734, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 3.742, mean reward: 0.374 [0.330, 0.505], mean action: 33.300 [11.000, 62.000], mean observation: 3.149 [-1.580, 10.387], loss: 1.077855, mae: 2.661788, mean_q: 3.704208
  7192/100000: episode: 735, duration: 0.110s, episode steps: 4, steps per second: 36, episode reward: 11.245, mean reward: 2.811 [0.365, 10.000], mean action: 15.500 [4.000, 30.000], mean observation: 3.140 [-1.350, 10.236], loss: 0.874121, mae: 2.664134, mean_q: 3.707056
  7202/100000: episode: 736, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.379, mean reward: 0.438 [0.314, 0.524], mean action: 53.100 [30.000, 95.000], mean observation: 3.161 [-1.666, 10.495], loss: 1.002730, mae: 2.667272, mean_q: 3.710043
  7212/100000: episode: 737, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.191, mean reward: 0.419 [0.400, 0.473], mean action: 37.900 [28.000, 76.000], mean observation: 3.160 [-1.862, 10.251], loss: 0.851956, mae: 2.669154, mean_q: 3.713818
  7222/100000: episode: 738, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.979, mean reward: 0.398 [0.390, 0.444], mean action: 38.300 [16.000, 88.000], mean observation: 3.157 [-1.731, 10.248], loss: 1.060941, mae: 2.672900, mean_q: 3.715522
  7232/100000: episode: 739, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.175, mean reward: 0.418 [0.322, 0.523], mean action: 41.900 [1.000, 85.000], mean observation: 3.143 [-1.381, 10.305], loss: 1.022402, mae: 2.675108, mean_q: 3.715955
  7242/100000: episode: 740, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.102, mean reward: 0.410 [0.362, 0.502], mean action: 47.500 [30.000, 98.000], mean observation: 3.163 [-1.778, 10.302], loss: 0.896446, mae: 2.678361, mean_q: 3.714354
  7243/100000: episode: 741, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 2.000 [2.000, 2.000], mean observation: 3.167 [-0.523, 10.100], loss: 0.960097, mae: 2.677863, mean_q: 3.714452
  7253/100000: episode: 742, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.506, mean reward: 0.451 [0.450, 0.459], mean action: 40.100 [18.000, 92.000], mean observation: 3.151 [-1.613, 10.241], loss: 1.070535, mae: 2.681448, mean_q: 3.714882
  7263/100000: episode: 743, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.807, mean reward: 0.381 [0.317, 0.454], mean action: 42.700 [30.000, 66.000], mean observation: 3.160 [-1.991, 10.261], loss: 0.912239, mae: 2.684108, mean_q: 3.717434
  7273/100000: episode: 744, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.063, mean reward: 0.406 [0.353, 0.534], mean action: 37.400 [27.000, 69.000], mean observation: 3.159 [-1.202, 10.434], loss: 0.963782, mae: 2.686166, mean_q: 3.720212
  7274/100000: episode: 745, duration: 0.026s, episode steps: 1, steps per second: 39, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 98.000 [98.000, 98.000], mean observation: 3.128 [-0.327, 10.100], loss: 0.578569, mae: 2.686923, mean_q: 3.722448
  7284/100000: episode: 746, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.151, mean reward: 0.415 [0.317, 0.483], mean action: 43.200 [16.000, 80.000], mean observation: 3.169 [-1.690, 10.430], loss: 1.132658, mae: 2.691641, mean_q: 3.724533
  7294/100000: episode: 747, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.089, mean reward: 0.409 [0.362, 0.474], mean action: 33.200 [4.000, 75.000], mean observation: 3.160 [-1.207, 10.353], loss: 0.960977, mae: 2.692919, mean_q: 3.724696
  7304/100000: episode: 748, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.427, mean reward: 0.443 [0.378, 0.558], mean action: 43.900 [13.000, 88.000], mean observation: 3.159 [-1.845, 10.333], loss: 0.845505, mae: 2.694264, mean_q: 3.723476
  7314/100000: episode: 749, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.156, mean reward: 0.416 [0.343, 0.462], mean action: 40.200 [30.000, 83.000], mean observation: 3.156 [-2.052, 10.375], loss: 0.979679, mae: 2.698227, mean_q: 3.724685
  7324/100000: episode: 750, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.132, mean reward: 0.413 [0.331, 0.481], mean action: 37.900 [30.000, 65.000], mean observation: 3.158 [-1.894, 10.291], loss: 1.039734, mae: 2.700186, mean_q: 3.726107
  7334/100000: episode: 751, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 5.098, mean reward: 0.510 [0.508, 0.527], mean action: 39.200 [30.000, 82.000], mean observation: 3.157 [-1.308, 10.285], loss: 0.814384, mae: 2.701645, mean_q: 3.728867
  7344/100000: episode: 752, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.354, mean reward: 0.435 [0.376, 0.508], mean action: 37.500 [10.000, 83.000], mean observation: 3.152 [-0.896, 10.165], loss: 1.093959, mae: 2.706488, mean_q: 3.732887
  7354/100000: episode: 753, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.929, mean reward: 0.393 [0.334, 0.446], mean action: 51.500 [9.000, 101.000], mean observation: 3.149 [-2.272, 10.333], loss: 0.938790, mae: 2.707172, mean_q: 3.735338
  7364/100000: episode: 754, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.946, mean reward: 0.395 [0.304, 0.547], mean action: 39.800 [7.000, 99.000], mean observation: 3.145 [-1.735, 10.271], loss: 0.763272, mae: 2.709395, mean_q: 3.737900
  7371/100000: episode: 755, duration: 0.118s, episode steps: 7, steps per second: 59, episode reward: 12.223, mean reward: 1.746 [0.343, 10.000], mean action: 63.714 [30.000, 98.000], mean observation: 3.150 [-1.219, 10.363], loss: 0.786866, mae: 2.711928, mean_q: 3.740726
  7381/100000: episode: 756, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.905, mean reward: 0.391 [0.370, 0.491], mean action: 40.700 [11.000, 80.000], mean observation: 3.148 [-1.454, 10.248], loss: 0.799065, mae: 2.713369, mean_q: 3.745315
  7391/100000: episode: 757, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.231, mean reward: 0.423 [0.376, 0.524], mean action: 33.300 [16.000, 59.000], mean observation: 3.157 [-2.263, 10.369], loss: 0.928510, mae: 2.717286, mean_q: 3.748146
  7401/100000: episode: 758, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.899, mean reward: 0.390 [0.297, 0.544], mean action: 37.300 [30.000, 83.000], mean observation: 3.165 [-1.512, 10.305], loss: 0.955785, mae: 2.719450, mean_q: 3.749279
  7411/100000: episode: 759, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.213, mean reward: 0.421 [0.367, 0.528], mean action: 37.000 [2.000, 84.000], mean observation: 3.148 [-1.484, 10.208], loss: 0.841754, mae: 2.721087, mean_q: 3.749813
  7421/100000: episode: 760, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.638, mean reward: 0.364 [0.314, 0.427], mean action: 36.000 [0.000, 62.000], mean observation: 3.163 [-0.873, 10.421], loss: 0.944547, mae: 2.724073, mean_q: 3.750684
  7431/100000: episode: 761, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.664, mean reward: 0.466 [0.457, 0.519], mean action: 29.800 [13.000, 61.000], mean observation: 3.159 [-1.796, 10.334], loss: 1.017461, mae: 2.727274, mean_q: 3.750716
  7433/100000: episode: 762, duration: 0.036s, episode steps: 2, steps per second: 55, episode reward: 10.484, mean reward: 5.242 [0.484, 10.000], mean action: 65.000 [30.000, 100.000], mean observation: 3.152 [-0.942, 10.357], loss: 0.793063, mae: 2.728583, mean_q: 3.749583
  7443/100000: episode: 763, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.801, mean reward: 0.380 [0.308, 0.480], mean action: 24.600 [0.000, 49.000], mean observation: 3.153 [-1.349, 10.465], loss: 1.165956, mae: 2.731274, mean_q: 3.747768
  7453/100000: episode: 764, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.096, mean reward: 0.410 [0.346, 0.548], mean action: 43.500 [2.000, 90.000], mean observation: 3.153 [-2.147, 10.324], loss: 1.026215, mae: 2.732450, mean_q: 3.744810
  7463/100000: episode: 765, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.967, mean reward: 0.497 [0.488, 0.540], mean action: 32.500 [0.000, 69.000], mean observation: 3.166 [-0.993, 10.609], loss: 0.941450, mae: 2.735414, mean_q: 3.745102
  7473/100000: episode: 766, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.196, mean reward: 0.420 [0.310, 0.495], mean action: 38.700 [2.000, 86.000], mean observation: 3.167 [-1.529, 10.441], loss: 0.827134, mae: 2.737415, mean_q: 3.747643
  7483/100000: episode: 767, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.989, mean reward: 0.399 [0.394, 0.437], mean action: 25.200 [7.000, 30.000], mean observation: 3.156 [-1.690, 10.335], loss: 0.858750, mae: 2.741094, mean_q: 3.750697
  7493/100000: episode: 768, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.898, mean reward: 0.390 [0.328, 0.553], mean action: 36.100 [30.000, 91.000], mean observation: 3.171 [-0.988, 10.323], loss: 1.233096, mae: 2.744676, mean_q: 3.751328
  7503/100000: episode: 769, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.146, mean reward: 0.415 [0.392, 0.537], mean action: 35.300 [18.000, 74.000], mean observation: 3.158 [-1.773, 10.518], loss: 1.256896, mae: 2.748612, mean_q: 3.747730
  7513/100000: episode: 770, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.952, mean reward: 0.395 [0.341, 0.477], mean action: 30.200 [17.000, 57.000], mean observation: 3.149 [-1.485, 10.291], loss: 0.977490, mae: 2.749341, mean_q: 3.744576
  7523/100000: episode: 771, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.239, mean reward: 0.424 [0.364, 0.446], mean action: 42.100 [11.000, 96.000], mean observation: 3.157 [-1.022, 10.309], loss: 1.096196, mae: 2.752093, mean_q: 3.743478
  7533/100000: episode: 772, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.129, mean reward: 0.413 [0.350, 0.512], mean action: 32.000 [6.000, 83.000], mean observation: 3.155 [-0.984, 10.410], loss: 0.877587, mae: 2.753317, mean_q: 3.744665
  7543/100000: episode: 773, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.810, mean reward: 0.381 [0.326, 0.468], mean action: 45.300 [19.000, 76.000], mean observation: 3.156 [-1.230, 10.266], loss: 0.918551, mae: 2.755997, mean_q: 3.744805
  7553/100000: episode: 774, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.487, mean reward: 0.449 [0.337, 0.534], mean action: 45.500 [10.000, 88.000], mean observation: 3.137 [-1.305, 10.210], loss: 0.893291, mae: 2.758622, mean_q: 3.746497
  7563/100000: episode: 775, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.811, mean reward: 0.381 [0.312, 0.463], mean action: 31.000 [10.000, 60.000], mean observation: 3.147 [-1.600, 10.321], loss: 1.055721, mae: 2.761844, mean_q: 3.748444
  7573/100000: episode: 776, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.738, mean reward: 0.374 [0.358, 0.400], mean action: 36.200 [23.000, 75.000], mean observation: 3.154 [-1.404, 10.230], loss: 0.941518, mae: 2.764480, mean_q: 3.750194
  7583/100000: episode: 777, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.105, mean reward: 0.410 [0.315, 0.521], mean action: 37.200 [17.000, 69.000], mean observation: 3.151 [-1.288, 10.493], loss: 0.862512, mae: 2.765682, mean_q: 3.750083
  7593/100000: episode: 778, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.962, mean reward: 0.396 [0.357, 0.451], mean action: 42.800 [24.000, 85.000], mean observation: 3.158 [-1.683, 10.500], loss: 1.106104, mae: 2.769721, mean_q: 3.748691
  7603/100000: episode: 779, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.589, mean reward: 0.459 [0.387, 0.560], mean action: 46.000 [24.000, 98.000], mean observation: 3.171 [-1.499, 10.511], loss: 0.887691, mae: 2.771322, mean_q: 3.749357
  7613/100000: episode: 780, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.106, mean reward: 0.411 [0.310, 0.494], mean action: 24.900 [1.000, 42.000], mean observation: 3.154 [-1.559, 10.360], loss: 1.037161, mae: 2.774620, mean_q: 3.754108
  7623/100000: episode: 781, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.713, mean reward: 0.371 [0.345, 0.457], mean action: 45.800 [30.000, 82.000], mean observation: 3.144 [-1.671, 10.427], loss: 0.791882, mae: 2.775176, mean_q: 3.759356
  7633/100000: episode: 782, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.433, mean reward: 0.443 [0.367, 0.532], mean action: 35.100 [24.000, 62.000], mean observation: 3.156 [-1.838, 10.279], loss: 0.968238, mae: 2.779308, mean_q: 3.763773
  7643/100000: episode: 783, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.814, mean reward: 0.381 [0.358, 0.398], mean action: 38.900 [4.000, 86.000], mean observation: 3.148 [-2.030, 10.342], loss: 0.768175, mae: 2.780857, mean_q: 3.765079
  7653/100000: episode: 784, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.204, mean reward: 0.420 [0.310, 0.593], mean action: 41.500 [12.000, 97.000], mean observation: 3.164 [-1.569, 10.650], loss: 0.934509, mae: 2.784997, mean_q: 3.766313
  7663/100000: episode: 785, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.040, mean reward: 0.404 [0.338, 0.500], mean action: 27.700 [0.000, 61.000], mean observation: 3.158 [-1.145, 10.320], loss: 0.855704, mae: 2.786168, mean_q: 3.767852
  7673/100000: episode: 786, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.386, mean reward: 0.439 [0.406, 0.586], mean action: 37.000 [15.000, 74.000], mean observation: 3.152 [-1.706, 10.319], loss: 0.859147, mae: 2.789370, mean_q: 3.773570
  7683/100000: episode: 787, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.104, mean reward: 0.410 [0.327, 0.588], mean action: 33.600 [3.000, 67.000], mean observation: 3.158 [-1.633, 10.430], loss: 1.012180, mae: 2.792271, mean_q: 3.777419
  7693/100000: episode: 788, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.849, mean reward: 0.385 [0.351, 0.453], mean action: 37.400 [3.000, 80.000], mean observation: 3.157 [-1.517, 10.371], loss: 0.852951, mae: 2.794933, mean_q: 3.780853
  7703/100000: episode: 789, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.995, mean reward: 0.400 [0.325, 0.488], mean action: 30.400 [12.000, 42.000], mean observation: 3.149 [-1.707, 10.300], loss: 0.919772, mae: 2.797498, mean_q: 3.783825
  7713/100000: episode: 790, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.197, mean reward: 0.420 [0.317, 0.507], mean action: 54.100 [16.000, 99.000], mean observation: 3.155 [-1.506, 10.335], loss: 1.019215, mae: 2.799937, mean_q: 3.783303
  7723/100000: episode: 791, duration: 0.221s, episode steps: 10, steps per second: 45, episode reward: 4.333, mean reward: 0.433 [0.390, 0.482], mean action: 29.900 [4.000, 59.000], mean observation: 3.155 [-1.474, 10.403], loss: 0.866781, mae: 2.802131, mean_q: 3.784051
  7733/100000: episode: 792, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.838, mean reward: 0.384 [0.356, 0.486], mean action: 38.200 [30.000, 69.000], mean observation: 3.154 [-1.331, 10.283], loss: 0.938060, mae: 2.804821, mean_q: 3.784893
  7743/100000: episode: 793, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.291, mean reward: 0.429 [0.374, 0.546], mean action: 43.300 [9.000, 84.000], mean observation: 3.147 [-1.448, 10.339], loss: 0.806877, mae: 2.806500, mean_q: 3.785292
  7753/100000: episode: 794, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.794, mean reward: 0.379 [0.312, 0.421], mean action: 35.600 [5.000, 101.000], mean observation: 3.164 [-1.775, 10.301], loss: 0.904144, mae: 2.809491, mean_q: 3.788092
  7763/100000: episode: 795, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 5.173, mean reward: 0.517 [0.320, 0.553], mean action: 43.900 [1.000, 90.000], mean observation: 3.158 [-1.705, 10.361], loss: 1.067594, mae: 2.812455, mean_q: 3.790466
  7773/100000: episode: 796, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.920, mean reward: 0.392 [0.300, 0.553], mean action: 37.100 [8.000, 86.000], mean observation: 3.159 [-1.246, 10.388], loss: 1.015439, mae: 2.815436, mean_q: 3.793536
  7783/100000: episode: 797, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.404, mean reward: 0.440 [0.353, 0.579], mean action: 42.200 [0.000, 75.000], mean observation: 3.155 [-1.321, 10.324], loss: 0.846105, mae: 2.817621, mean_q: 3.795458
  7793/100000: episode: 798, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.922, mean reward: 0.392 [0.348, 0.505], mean action: 44.700 [7.000, 87.000], mean observation: 3.142 [-1.698, 10.273], loss: 0.806887, mae: 2.820153, mean_q: 3.797882
  7803/100000: episode: 799, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.977, mean reward: 0.398 [0.329, 0.540], mean action: 38.300 [26.000, 82.000], mean observation: 3.141 [-1.533, 10.250], loss: 1.009589, mae: 2.822855, mean_q: 3.799948
  7813/100000: episode: 800, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.003, mean reward: 0.400 [0.324, 0.476], mean action: 26.400 [1.000, 32.000], mean observation: 3.153 [-0.872, 10.350], loss: 0.750298, mae: 2.824867, mean_q: 3.799914
  7823/100000: episode: 801, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 3.713, mean reward: 0.371 [0.323, 0.429], mean action: 31.100 [4.000, 78.000], mean observation: 3.158 [-1.261, 10.324], loss: 0.997683, mae: 2.828018, mean_q: 3.801736
  7833/100000: episode: 802, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.925, mean reward: 0.393 [0.324, 0.419], mean action: 42.400 [19.000, 83.000], mean observation: 3.152 [-1.098, 10.311], loss: 0.951148, mae: 2.830609, mean_q: 3.803859
  7843/100000: episode: 803, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.972, mean reward: 0.397 [0.315, 0.552], mean action: 37.100 [6.000, 79.000], mean observation: 3.148 [-1.265, 10.272], loss: 0.902280, mae: 2.831857, mean_q: 3.804260
  7853/100000: episode: 804, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.808, mean reward: 0.381 [0.345, 0.418], mean action: 46.200 [10.000, 79.000], mean observation: 3.155 [-1.444, 10.248], loss: 0.868599, mae: 2.834726, mean_q: 3.804267
  7863/100000: episode: 805, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.176, mean reward: 0.418 [0.367, 0.497], mean action: 38.000 [10.000, 88.000], mean observation: 3.158 [-1.188, 10.221], loss: 0.959617, mae: 2.837626, mean_q: 3.805444
  7873/100000: episode: 806, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.082, mean reward: 0.408 [0.308, 0.439], mean action: 57.600 [30.000, 97.000], mean observation: 3.161 [-1.645, 10.349], loss: 0.773485, mae: 2.839215, mean_q: 3.807955
  7883/100000: episode: 807, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.881, mean reward: 0.388 [0.375, 0.449], mean action: 43.600 [17.000, 93.000], mean observation: 3.144 [-1.638, 10.275], loss: 0.881563, mae: 2.842462, mean_q: 3.812327
  7893/100000: episode: 808, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.278, mean reward: 0.428 [0.325, 0.579], mean action: 34.100 [1.000, 83.000], mean observation: 3.161 [-1.229, 10.401], loss: 1.001312, mae: 2.845228, mean_q: 3.813700
  7903/100000: episode: 809, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.373, mean reward: 0.437 [0.340, 0.575], mean action: 34.000 [9.000, 79.000], mean observation: 3.154 [-1.665, 10.407], loss: 1.024768, mae: 2.847329, mean_q: 3.814693
  7913/100000: episode: 810, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.352, mean reward: 0.435 [0.383, 0.525], mean action: 36.500 [22.000, 60.000], mean observation: 3.155 [-1.072, 10.282], loss: 0.860243, mae: 2.848576, mean_q: 3.816865
  7923/100000: episode: 811, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 3.967, mean reward: 0.397 [0.382, 0.482], mean action: 30.000 [5.000, 59.000], mean observation: 3.157 [-1.654, 10.306], loss: 1.266258, mae: 2.852891, mean_q: 3.819261
  7933/100000: episode: 812, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.234, mean reward: 0.423 [0.287, 0.541], mean action: 51.800 [8.000, 101.000], mean observation: 3.160 [-1.558, 10.313], loss: 0.829859, mae: 2.853416, mean_q: 3.817399
  7943/100000: episode: 813, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.170, mean reward: 0.417 [0.350, 0.445], mean action: 37.000 [10.000, 99.000], mean observation: 3.165 [-1.156, 10.387], loss: 0.908695, mae: 2.856123, mean_q: 3.815938
  7953/100000: episode: 814, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.874, mean reward: 0.387 [0.351, 0.470], mean action: 46.900 [9.000, 94.000], mean observation: 3.157 [-1.565, 10.345], loss: 1.008492, mae: 2.859210, mean_q: 3.814346
  7963/100000: episode: 815, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.064, mean reward: 0.406 [0.349, 0.490], mean action: 35.100 [17.000, 82.000], mean observation: 3.160 [-1.575, 10.451], loss: 0.851207, mae: 2.860558, mean_q: 3.812825
  7973/100000: episode: 816, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.734, mean reward: 0.373 [0.280, 0.442], mean action: 32.400 [13.000, 85.000], mean observation: 3.157 [-1.307, 10.535], loss: 0.885162, mae: 2.863385, mean_q: 3.813743
  7983/100000: episode: 817, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.796, mean reward: 0.380 [0.342, 0.464], mean action: 30.200 [12.000, 55.000], mean observation: 3.162 [-1.034, 10.288], loss: 0.786124, mae: 2.866164, mean_q: 3.814525
  7993/100000: episode: 818, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.588, mean reward: 0.459 [0.369, 0.529], mean action: 35.600 [6.000, 98.000], mean observation: 3.162 [-1.454, 10.467], loss: 0.952261, mae: 2.870149, mean_q: 3.816433
  8003/100000: episode: 819, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.976, mean reward: 0.398 [0.323, 0.504], mean action: 46.800 [30.000, 82.000], mean observation: 3.161 [-1.142, 10.276], loss: 0.888882, mae: 2.871748, mean_q: 3.819656
  8013/100000: episode: 820, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.128, mean reward: 0.413 [0.348, 0.459], mean action: 54.600 [30.000, 95.000], mean observation: 3.154 [-2.228, 10.291], loss: 1.078422, mae: 2.874354, mean_q: 3.823084
  8023/100000: episode: 821, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 3.901, mean reward: 0.390 [0.338, 0.472], mean action: 34.900 [0.000, 86.000], mean observation: 3.158 [-1.398, 10.309], loss: 0.836699, mae: 2.876220, mean_q: 3.827176
  8033/100000: episode: 822, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.222, mean reward: 0.422 [0.365, 0.512], mean action: 61.800 [30.000, 98.000], mean observation: 3.138 [-1.766, 10.367], loss: 0.990372, mae: 2.880138, mean_q: 3.830142
  8043/100000: episode: 823, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.935, mean reward: 0.393 [0.384, 0.427], mean action: 37.000 [24.000, 83.000], mean observation: 3.145 [-0.990, 10.421], loss: 1.097466, mae: 2.882063, mean_q: 3.828048
  8053/100000: episode: 824, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.745, mean reward: 0.375 [0.300, 0.556], mean action: 37.500 [9.000, 98.000], mean observation: 3.144 [-1.421, 10.213], loss: 0.758858, mae: 2.883271, mean_q: 3.826331
  8063/100000: episode: 825, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.823, mean reward: 0.382 [0.301, 0.454], mean action: 39.400 [1.000, 96.000], mean observation: 3.159 [-1.223, 10.453], loss: 0.906357, mae: 2.887323, mean_q: 3.825429
  8073/100000: episode: 826, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.190, mean reward: 0.419 [0.379, 0.547], mean action: 45.700 [5.000, 88.000], mean observation: 3.151 [-1.759, 10.350], loss: 0.809779, mae: 2.889132, mean_q: 3.824365
  8083/100000: episode: 827, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.681, mean reward: 0.368 [0.270, 0.490], mean action: 34.600 [25.000, 63.000], mean observation: 3.153 [-1.398, 10.194], loss: 1.116124, mae: 2.892641, mean_q: 3.829581
  8093/100000: episode: 828, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.191, mean reward: 0.419 [0.406, 0.476], mean action: 46.500 [24.000, 93.000], mean observation: 3.151 [-1.405, 10.414], loss: 0.943706, mae: 2.895348, mean_q: 3.834918
  8103/100000: episode: 829, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.837, mean reward: 0.384 [0.307, 0.487], mean action: 38.400 [30.000, 99.000], mean observation: 3.145 [-1.277, 10.318], loss: 0.990161, mae: 2.897969, mean_q: 3.836265
  8113/100000: episode: 830, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.015, mean reward: 0.401 [0.299, 0.473], mean action: 35.400 [2.000, 92.000], mean observation: 3.152 [-1.217, 10.379], loss: 0.995868, mae: 2.899620, mean_q: 3.836003
  8123/100000: episode: 831, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.898, mean reward: 0.390 [0.328, 0.461], mean action: 33.400 [5.000, 78.000], mean observation: 3.151 [-1.377, 10.400], loss: 0.930510, mae: 2.901721, mean_q: 3.836496
  8133/100000: episode: 832, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.850, mean reward: 0.385 [0.312, 0.437], mean action: 41.600 [11.000, 95.000], mean observation: 3.157 [-1.704, 10.286], loss: 0.979495, mae: 2.904508, mean_q: 3.835984
  8143/100000: episode: 833, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.963, mean reward: 0.396 [0.351, 0.463], mean action: 45.600 [20.000, 82.000], mean observation: 3.163 [-1.509, 10.437], loss: 0.851851, mae: 2.906363, mean_q: 3.837461
  8153/100000: episode: 834, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.986, mean reward: 0.499 [0.499, 0.499], mean action: 41.500 [11.000, 94.000], mean observation: 3.159 [-1.483, 10.401], loss: 0.796212, mae: 2.908379, mean_q: 3.836830
  8163/100000: episode: 835, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.886, mean reward: 0.389 [0.344, 0.426], mean action: 46.700 [5.000, 101.000], mean observation: 3.160 [-1.763, 10.333], loss: 0.766758, mae: 2.911388, mean_q: 3.838389
  8173/100000: episode: 836, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.263, mean reward: 0.426 [0.354, 0.529], mean action: 30.300 [0.000, 82.000], mean observation: 3.165 [-2.142, 10.406], loss: 0.812579, mae: 2.914356, mean_q: 3.839929
  8183/100000: episode: 837, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.119, mean reward: 0.412 [0.388, 0.479], mean action: 46.100 [22.000, 99.000], mean observation: 3.155 [-2.062, 10.321], loss: 0.879521, mae: 2.916973, mean_q: 3.839068
  8193/100000: episode: 838, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.130, mean reward: 0.413 [0.361, 0.477], mean action: 48.100 [15.000, 93.000], mean observation: 3.161 [-1.882, 10.239], loss: 1.019194, mae: 2.920778, mean_q: 3.838917
  8203/100000: episode: 839, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.253, mean reward: 0.425 [0.339, 0.544], mean action: 38.700 [13.000, 101.000], mean observation: 3.148 [-1.020, 10.427], loss: 0.854464, mae: 2.922675, mean_q: 3.841546
  8213/100000: episode: 840, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.218, mean reward: 0.422 [0.399, 0.495], mean action: 35.000 [21.000, 88.000], mean observation: 3.150 [-1.456, 10.299], loss: 0.792426, mae: 2.924736, mean_q: 3.842694
  8223/100000: episode: 841, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.810, mean reward: 0.381 [0.315, 0.517], mean action: 47.200 [20.000, 101.000], mean observation: 3.161 [-1.418, 10.491], loss: 0.863238, mae: 2.927618, mean_q: 3.844753
  8233/100000: episode: 842, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.988, mean reward: 0.399 [0.367, 0.465], mean action: 54.900 [30.000, 100.000], mean observation: 3.150 [-1.103, 10.254], loss: 1.174078, mae: 2.931203, mean_q: 3.846809
  8243/100000: episode: 843, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.961, mean reward: 0.396 [0.323, 0.489], mean action: 36.100 [8.000, 79.000], mean observation: 3.155 [-1.521, 10.401], loss: 0.820861, mae: 2.932661, mean_q: 3.847747
  8253/100000: episode: 844, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.847, mean reward: 0.385 [0.340, 0.464], mean action: 49.700 [30.000, 99.000], mean observation: 3.145 [-1.578, 10.287], loss: 0.867237, mae: 2.935838, mean_q: 3.847682
  8263/100000: episode: 845, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.823, mean reward: 0.382 [0.296, 0.465], mean action: 39.600 [11.000, 79.000], mean observation: 3.167 [-1.402, 10.330], loss: 1.014247, mae: 2.937666, mean_q: 3.848008
  8273/100000: episode: 846, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.208, mean reward: 0.421 [0.311, 0.562], mean action: 43.300 [16.000, 88.000], mean observation: 3.164 [-1.372, 10.348], loss: 0.915273, mae: 2.939490, mean_q: 3.850917
  8283/100000: episode: 847, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.770, mean reward: 0.377 [0.337, 0.410], mean action: 44.500 [30.000, 85.000], mean observation: 3.152 [-0.969, 10.408], loss: 0.916817, mae: 2.942129, mean_q: 3.850003
  8293/100000: episode: 848, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.289, mean reward: 0.429 [0.391, 0.506], mean action: 43.000 [1.000, 97.000], mean observation: 3.157 [-1.146, 10.350], loss: 1.092404, mae: 2.944667, mean_q: 3.847882
  8303/100000: episode: 849, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.126, mean reward: 0.413 [0.331, 0.514], mean action: 42.100 [9.000, 80.000], mean observation: 3.144 [-1.370, 10.369], loss: 0.816927, mae: 2.947024, mean_q: 3.846736
  8313/100000: episode: 850, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.001, mean reward: 0.400 [0.328, 0.496], mean action: 34.000 [22.000, 56.000], mean observation: 3.139 [-2.192, 10.265], loss: 0.955188, mae: 2.949410, mean_q: 3.847017
  8323/100000: episode: 851, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.020, mean reward: 0.402 [0.393, 0.482], mean action: 46.200 [30.000, 85.000], mean observation: 3.158 [-1.159, 10.311], loss: 1.090335, mae: 2.952399, mean_q: 3.850908
  8333/100000: episode: 852, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.215, mean reward: 0.422 [0.334, 0.499], mean action: 35.000 [24.000, 82.000], mean observation: 3.143 [-1.129, 10.408], loss: 1.011992, mae: 2.954613, mean_q: 3.851311
  8343/100000: episode: 853, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 13.597, mean reward: 1.360 [0.343, 10.000], mean action: 39.800 [22.000, 75.000], mean observation: 3.165 [-1.985, 10.373], loss: 0.941712, mae: 2.956952, mean_q: 3.848004
  8353/100000: episode: 854, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.602, mean reward: 0.360 [0.294, 0.460], mean action: 42.800 [6.000, 93.000], mean observation: 3.172 [-1.205, 10.240], loss: 1.114151, mae: 2.960325, mean_q: 3.844056
  8363/100000: episode: 855, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.812, mean reward: 0.381 [0.329, 0.410], mean action: 52.000 [13.000, 101.000], mean observation: 3.157 [-1.377, 10.277], loss: 0.832750, mae: 2.961262, mean_q: 3.844678
  8373/100000: episode: 856, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.147, mean reward: 0.415 [0.342, 0.575], mean action: 42.900 [25.000, 87.000], mean observation: 3.154 [-1.325, 10.219], loss: 0.713942, mae: 2.962614, mean_q: 3.847604
  8383/100000: episode: 857, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.351, mean reward: 0.435 [0.414, 0.465], mean action: 46.700 [30.000, 81.000], mean observation: 3.142 [-1.445, 10.284], loss: 0.733101, mae: 2.964715, mean_q: 3.851994
  8392/100000: episode: 858, duration: 0.181s, episode steps: 9, steps per second: 50, episode reward: 13.111, mean reward: 1.457 [0.306, 10.000], mean action: 40.778 [4.000, 100.000], mean observation: 3.172 [-1.392, 10.388], loss: 0.871054, mae: 2.968681, mean_q: 3.854800
  8402/100000: episode: 859, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 5.249, mean reward: 0.525 [0.525, 0.525], mean action: 45.800 [28.000, 97.000], mean observation: 3.157 [-1.927, 10.310], loss: 0.741052, mae: 2.969613, mean_q: 3.855493
  8412/100000: episode: 860, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.069, mean reward: 0.407 [0.302, 0.597], mean action: 38.000 [25.000, 76.000], mean observation: 3.154 [-1.350, 10.633], loss: 0.815305, mae: 2.972773, mean_q: 3.857792
  8422/100000: episode: 861, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.389, mean reward: 0.439 [0.333, 0.548], mean action: 33.600 [13.000, 59.000], mean observation: 3.158 [-1.412, 10.302], loss: 0.826480, mae: 2.975100, mean_q: 3.860692
  8432/100000: episode: 862, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.994, mean reward: 0.399 [0.388, 0.443], mean action: 37.400 [18.000, 71.000], mean observation: 3.141 [-1.782, 10.312], loss: 0.774725, mae: 2.977341, mean_q: 3.862424
  8442/100000: episode: 863, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.714, mean reward: 0.471 [0.459, 0.537], mean action: 38.200 [21.000, 81.000], mean observation: 3.146 [-2.449, 10.371], loss: 0.819823, mae: 2.979403, mean_q: 3.863789
  8452/100000: episode: 864, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.122, mean reward: 0.412 [0.261, 0.452], mean action: 37.200 [7.000, 85.000], mean observation: 3.152 [-1.627, 10.505], loss: 1.009863, mae: 2.983249, mean_q: 3.864094
  8462/100000: episode: 865, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.837, mean reward: 0.384 [0.323, 0.444], mean action: 39.300 [28.000, 92.000], mean observation: 3.139 [-1.656, 10.233], loss: 1.102290, mae: 2.986446, mean_q: 3.860692
  8472/100000: episode: 866, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.909, mean reward: 0.491 [0.478, 0.524], mean action: 43.400 [20.000, 91.000], mean observation: 3.146 [-1.374, 10.224], loss: 0.911351, mae: 2.987237, mean_q: 3.856659
  8482/100000: episode: 867, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.105, mean reward: 0.411 [0.333, 0.477], mean action: 32.500 [17.000, 67.000], mean observation: 3.151 [-1.485, 10.459], loss: 0.800053, mae: 2.989705, mean_q: 3.855677
  8492/100000: episode: 868, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.026, mean reward: 0.403 [0.344, 0.484], mean action: 46.300 [20.000, 95.000], mean observation: 3.164 [-1.443, 10.421], loss: 1.092323, mae: 2.993385, mean_q: 3.855795
  8502/100000: episode: 869, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.209, mean reward: 0.421 [0.327, 0.531], mean action: 32.600 [23.000, 67.000], mean observation: 3.165 [-1.872, 10.383], loss: 0.861429, mae: 2.995294, mean_q: 3.854285
  8512/100000: episode: 870, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.099, mean reward: 0.410 [0.363, 0.509], mean action: 42.000 [6.000, 98.000], mean observation: 3.168 [-1.294, 10.475], loss: 0.975926, mae: 2.997731, mean_q: 3.853945
  8522/100000: episode: 871, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.325, mean reward: 0.433 [0.423, 0.516], mean action: 39.600 [16.000, 101.000], mean observation: 3.156 [-1.866, 10.236], loss: 0.925457, mae: 3.000293, mean_q: 3.853102
  8532/100000: episode: 872, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.327, mean reward: 0.433 [0.417, 0.466], mean action: 36.500 [4.000, 69.000], mean observation: 3.150 [-1.174, 10.360], loss: 1.070819, mae: 3.002873, mean_q: 3.854741
  8542/100000: episode: 873, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.074, mean reward: 0.407 [0.332, 0.484], mean action: 29.500 [11.000, 75.000], mean observation: 3.154 [-2.139, 10.235], loss: 0.797651, mae: 3.003906, mean_q: 3.857028
  8552/100000: episode: 874, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.035, mean reward: 0.403 [0.398, 0.433], mean action: 44.000 [30.000, 89.000], mean observation: 3.147 [-1.789, 10.245], loss: 0.773808, mae: 3.006817, mean_q: 3.859453
  8562/100000: episode: 875, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.479, mean reward: 0.448 [0.367, 0.534], mean action: 35.800 [19.000, 100.000], mean observation: 3.154 [-1.311, 10.216], loss: 0.977845, mae: 3.010376, mean_q: 3.862557
  8572/100000: episode: 876, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.318, mean reward: 0.432 [0.350, 0.521], mean action: 38.800 [30.000, 72.000], mean observation: 3.164 [-1.043, 10.323], loss: 0.769408, mae: 3.011269, mean_q: 3.864329
  8575/100000: episode: 877, duration: 0.059s, episode steps: 3, steps per second: 51, episode reward: 10.827, mean reward: 3.609 [0.346, 10.000], mean action: 55.667 [30.000, 87.000], mean observation: 3.163 [-1.454, 10.250], loss: 0.713483, mae: 3.013601, mean_q: 3.865585
  8585/100000: episode: 878, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.993, mean reward: 0.399 [0.379, 0.440], mean action: 50.400 [30.000, 73.000], mean observation: 3.160 [-1.312, 10.188], loss: 0.890226, mae: 3.015093, mean_q: 3.866815
  8595/100000: episode: 879, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.953, mean reward: 0.395 [0.320, 0.528], mean action: 38.800 [21.000, 77.000], mean observation: 3.164 [-1.815, 10.311], loss: 0.886837, mae: 3.016592, mean_q: 3.867775
  8605/100000: episode: 880, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.178, mean reward: 0.418 [0.384, 0.556], mean action: 40.100 [27.000, 99.000], mean observation: 3.150 [-0.801, 10.363], loss: 0.840865, mae: 3.019715, mean_q: 3.868736
  8615/100000: episode: 881, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.173, mean reward: 0.417 [0.337, 0.538], mean action: 45.000 [12.000, 101.000], mean observation: 3.146 [-1.858, 10.255], loss: 0.760796, mae: 3.020759, mean_q: 3.868618
  8625/100000: episode: 882, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.064, mean reward: 0.406 [0.372, 0.499], mean action: 37.800 [22.000, 72.000], mean observation: 3.155 [-1.116, 10.190], loss: 0.862499, mae: 3.023609, mean_q: 3.871084
  8635/100000: episode: 883, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.387, mean reward: 0.439 [0.353, 0.515], mean action: 30.400 [6.000, 89.000], mean observation: 3.161 [-1.800, 10.439], loss: 0.795215, mae: 3.025407, mean_q: 3.872442
  8645/100000: episode: 884, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.192, mean reward: 0.419 [0.350, 0.531], mean action: 51.800 [30.000, 88.000], mean observation: 3.174 [-1.126, 10.408], loss: 1.049960, mae: 3.029965, mean_q: 3.871924
  8655/100000: episode: 885, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.385, mean reward: 0.438 [0.385, 0.486], mean action: 36.700 [18.000, 77.000], mean observation: 3.165 [-1.325, 10.399], loss: 1.180619, mae: 3.032431, mean_q: 3.871430
  8665/100000: episode: 886, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.048, mean reward: 0.405 [0.365, 0.484], mean action: 39.500 [3.000, 101.000], mean observation: 3.157 [-1.502, 10.296], loss: 1.208572, mae: 3.034126, mean_q: 3.870472
  8675/100000: episode: 887, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.026, mean reward: 0.403 [0.348, 0.498], mean action: 33.100 [1.000, 84.000], mean observation: 3.155 [-1.473, 10.320], loss: 0.832064, mae: 3.035258, mean_q: 3.868084
  8685/100000: episode: 888, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.040, mean reward: 0.404 [0.327, 0.534], mean action: 32.000 [30.000, 47.000], mean observation: 3.164 [-1.632, 10.337], loss: 0.780349, mae: 3.037076, mean_q: 3.867070
  8695/100000: episode: 889, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.116, mean reward: 0.412 [0.348, 0.485], mean action: 38.100 [5.000, 101.000], mean observation: 3.154 [-1.330, 10.418], loss: 1.001420, mae: 3.040066, mean_q: 3.867938
  8705/100000: episode: 890, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.794, mean reward: 0.379 [0.329, 0.447], mean action: 48.900 [17.000, 87.000], mean observation: 3.148 [-1.429, 10.399], loss: 0.770725, mae: 3.041555, mean_q: 3.867849
  8715/100000: episode: 891, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.220, mean reward: 0.422 [0.374, 0.437], mean action: 47.400 [21.000, 90.000], mean observation: 3.169 [-1.488, 10.381], loss: 1.005296, mae: 3.045259, mean_q: 3.867709
  8725/100000: episode: 892, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.206, mean reward: 0.421 [0.343, 0.592], mean action: 44.600 [5.000, 80.000], mean observation: 3.157 [-1.620, 10.370], loss: 0.724345, mae: 3.045049, mean_q: 3.867604
  8735/100000: episode: 893, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.908, mean reward: 0.391 [0.314, 0.486], mean action: 42.400 [10.000, 82.000], mean observation: 3.153 [-0.965, 10.199], loss: 1.013410, mae: 3.048094, mean_q: 3.868729
  8745/100000: episode: 894, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.284, mean reward: 0.428 [0.384, 0.448], mean action: 31.900 [2.000, 62.000], mean observation: 3.160 [-1.784, 10.386], loss: 0.782471, mae: 3.049251, mean_q: 3.871473
  8755/100000: episode: 895, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.328, mean reward: 0.433 [0.433, 0.435], mean action: 42.400 [27.000, 84.000], mean observation: 3.157 [-1.524, 10.316], loss: 0.876813, mae: 3.052220, mean_q: 3.874092
  8765/100000: episode: 896, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.319, mean reward: 0.432 [0.366, 0.516], mean action: 43.800 [10.000, 96.000], mean observation: 3.153 [-1.534, 10.340], loss: 0.859056, mae: 3.055204, mean_q: 3.875082
  8775/100000: episode: 897, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.794, mean reward: 0.379 [0.321, 0.530], mean action: 36.500 [4.000, 82.000], mean observation: 3.152 [-1.144, 10.363], loss: 0.716094, mae: 3.056400, mean_q: 3.876073
  8785/100000: episode: 898, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.460, mean reward: 0.446 [0.373, 0.471], mean action: 32.000 [7.000, 66.000], mean observation: 3.162 [-1.016, 10.435], loss: 0.935655, mae: 3.059218, mean_q: 3.879059
  8795/100000: episode: 899, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.550, mean reward: 0.355 [0.293, 0.406], mean action: 37.800 [10.000, 98.000], mean observation: 3.153 [-1.602, 10.236], loss: 0.933863, mae: 3.061984, mean_q: 3.881667
  8805/100000: episode: 900, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.174, mean reward: 0.417 [0.369, 0.491], mean action: 43.700 [30.000, 90.000], mean observation: 3.152 [-2.013, 10.383], loss: 0.909270, mae: 3.064589, mean_q: 3.880735
  8815/100000: episode: 901, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.961, mean reward: 0.396 [0.339, 0.544], mean action: 39.600 [7.000, 87.000], mean observation: 3.158 [-1.287, 10.232], loss: 0.897774, mae: 3.066225, mean_q: 3.881156
  8825/100000: episode: 902, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.802, mean reward: 0.380 [0.314, 0.432], mean action: 39.800 [4.000, 100.000], mean observation: 3.160 [-3.152, 10.251], loss: 0.742789, mae: 3.067844, mean_q: 3.883284
  8835/100000: episode: 903, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.795, mean reward: 0.380 [0.339, 0.423], mean action: 44.600 [26.000, 78.000], mean observation: 3.176 [-1.523, 10.482], loss: 0.720674, mae: 3.069757, mean_q: 3.884844
  8845/100000: episode: 904, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.805, mean reward: 0.380 [0.303, 0.467], mean action: 31.600 [6.000, 48.000], mean observation: 3.157 [-1.253, 10.343], loss: 0.778288, mae: 3.072225, mean_q: 3.886621
  8855/100000: episode: 905, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.098, mean reward: 0.410 [0.375, 0.470], mean action: 40.300 [27.000, 91.000], mean observation: 3.160 [-1.986, 10.302], loss: 1.054952, mae: 3.076319, mean_q: 3.890221
  8865/100000: episode: 906, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.834, mean reward: 0.383 [0.333, 0.476], mean action: 34.700 [2.000, 82.000], mean observation: 3.158 [-1.299, 10.226], loss: 1.047621, mae: 3.078600, mean_q: 3.894825
  8875/100000: episode: 907, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.583, mean reward: 0.458 [0.368, 0.559], mean action: 42.200 [16.000, 80.000], mean observation: 3.160 [-1.489, 10.476], loss: 0.809890, mae: 3.078777, mean_q: 3.895546
  8885/100000: episode: 908, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.277, mean reward: 0.428 [0.363, 0.528], mean action: 38.700 [21.000, 90.000], mean observation: 3.160 [-0.798, 10.276], loss: 0.738122, mae: 3.080184, mean_q: 3.896779
  8895/100000: episode: 909, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.159, mean reward: 0.416 [0.322, 0.489], mean action: 36.300 [12.000, 99.000], mean observation: 3.152 [-1.749, 10.222], loss: 0.976675, mae: 3.083592, mean_q: 3.898767
  8905/100000: episode: 910, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.933, mean reward: 0.393 [0.305, 0.516], mean action: 40.400 [30.000, 97.000], mean observation: 3.157 [-1.724, 10.298], loss: 0.785313, mae: 3.084464, mean_q: 3.900026
  8915/100000: episode: 911, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.438, mean reward: 0.444 [0.396, 0.494], mean action: 45.000 [17.000, 97.000], mean observation: 3.157 [-1.640, 10.291], loss: 0.927784, mae: 3.088549, mean_q: 3.902112
  8925/100000: episode: 912, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.924, mean reward: 0.392 [0.353, 0.430], mean action: 41.400 [30.000, 98.000], mean observation: 3.154 [-2.466, 10.294], loss: 1.068664, mae: 3.091214, mean_q: 3.900789
  8935/100000: episode: 913, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.178, mean reward: 0.418 [0.368, 0.519], mean action: 30.700 [2.000, 65.000], mean observation: 3.160 [-1.450, 10.302], loss: 0.949674, mae: 3.093326, mean_q: 3.901069
  8945/100000: episode: 914, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.464, mean reward: 0.446 [0.394, 0.488], mean action: 30.600 [0.000, 87.000], mean observation: 3.148 [-2.106, 10.379], loss: 0.899446, mae: 3.094552, mean_q: 3.900715
  8955/100000: episode: 915, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 3.888, mean reward: 0.389 [0.294, 0.487], mean action: 38.100 [10.000, 76.000], mean observation: 3.153 [-1.412, 10.319], loss: 0.970547, mae: 3.097296, mean_q: 3.900931
  8965/100000: episode: 916, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.333, mean reward: 0.433 [0.407, 0.532], mean action: 34.000 [30.000, 54.000], mean observation: 3.173 [-0.954, 10.432], loss: 0.708152, mae: 3.098507, mean_q: 3.900940
  8975/100000: episode: 917, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.480, mean reward: 0.448 [0.382, 0.485], mean action: 32.700 [9.000, 81.000], mean observation: 3.172 [-1.353, 10.370], loss: 0.808493, mae: 3.100734, mean_q: 3.902442
  8985/100000: episode: 918, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.495, mean reward: 0.450 [0.355, 0.572], mean action: 41.000 [19.000, 100.000], mean observation: 3.153 [-2.024, 10.351], loss: 0.647909, mae: 3.102475, mean_q: 3.903066
  8995/100000: episode: 919, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 5.302, mean reward: 0.530 [0.530, 0.530], mean action: 43.500 [30.000, 87.000], mean observation: 3.168 [-1.060, 10.337], loss: 0.956517, mae: 3.105657, mean_q: 3.905809
  9005/100000: episode: 920, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.787, mean reward: 0.379 [0.326, 0.443], mean action: 47.500 [30.000, 88.000], mean observation: 3.158 [-1.777, 10.304], loss: 0.635021, mae: 3.106686, mean_q: 3.907852
  9015/100000: episode: 921, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.185, mean reward: 0.418 [0.356, 0.505], mean action: 40.300 [2.000, 77.000], mean observation: 3.148 [-1.192, 10.303], loss: 0.752110, mae: 3.109831, mean_q: 3.910880
  9025/100000: episode: 922, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.942, mean reward: 0.394 [0.351, 0.478], mean action: 64.200 [2.000, 101.000], mean observation: 3.164 [-1.600, 10.311], loss: 0.750557, mae: 3.111427, mean_q: 3.913219
  9035/100000: episode: 923, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.834, mean reward: 0.383 [0.332, 0.498], mean action: 33.500 [22.000, 52.000], mean observation: 3.154 [-1.257, 10.214], loss: 0.922837, mae: 3.115651, mean_q: 3.914786
  9045/100000: episode: 924, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.213, mean reward: 0.421 [0.421, 0.421], mean action: 45.700 [14.000, 76.000], mean observation: 3.162 [-1.316, 10.421], loss: 0.846515, mae: 3.117694, mean_q: 3.915211
  9055/100000: episode: 925, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.784, mean reward: 0.378 [0.317, 0.435], mean action: 40.000 [30.000, 86.000], mean observation: 3.159 [-1.567, 10.249], loss: 0.767225, mae: 3.119181, mean_q: 3.915562
  9065/100000: episode: 926, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.502, mean reward: 0.450 [0.438, 0.527], mean action: 48.300 [14.000, 99.000], mean observation: 3.159 [-0.938, 10.350], loss: 0.756745, mae: 3.121348, mean_q: 3.916770
  9075/100000: episode: 927, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.341, mean reward: 0.434 [0.311, 0.551], mean action: 32.400 [21.000, 57.000], mean observation: 3.149 [-1.649, 10.266], loss: 0.921329, mae: 3.123856, mean_q: 3.919060
  9085/100000: episode: 928, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 5.267, mean reward: 0.527 [0.527, 0.527], mean action: 32.600 [22.000, 64.000], mean observation: 3.168 [-1.940, 10.276], loss: 0.766545, mae: 3.126609, mean_q: 3.922389
  9095/100000: episode: 929, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.552, mean reward: 0.455 [0.371, 0.559], mean action: 41.300 [2.000, 88.000], mean observation: 3.150 [-1.278, 10.325], loss: 1.038412, mae: 3.129781, mean_q: 3.925581
  9105/100000: episode: 930, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.172, mean reward: 0.417 [0.306, 0.504], mean action: 33.200 [7.000, 77.000], mean observation: 3.151 [-2.154, 10.217], loss: 0.960629, mae: 3.131376, mean_q: 3.926856
  9115/100000: episode: 931, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.153, mean reward: 0.415 [0.376, 0.444], mean action: 39.700 [3.000, 84.000], mean observation: 3.154 [-1.563, 10.270], loss: 0.819666, mae: 3.132589, mean_q: 3.926773
  9125/100000: episode: 932, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.758, mean reward: 0.376 [0.275, 0.485], mean action: 35.000 [30.000, 60.000], mean observation: 3.164 [-0.894, 10.508], loss: 0.787200, mae: 3.135064, mean_q: 3.928309
  9135/100000: episode: 933, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.313, mean reward: 0.431 [0.407, 0.584], mean action: 37.500 [9.000, 89.000], mean observation: 3.154 [-1.450, 10.485], loss: 0.979775, mae: 3.137335, mean_q: 3.929185
  9145/100000: episode: 934, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.503, mean reward: 0.450 [0.405, 0.506], mean action: 61.200 [25.000, 101.000], mean observation: 3.159 [-1.410, 10.241], loss: 0.703371, mae: 3.139390, mean_q: 3.929854
  9155/100000: episode: 935, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.084, mean reward: 0.408 [0.375, 0.432], mean action: 33.200 [9.000, 95.000], mean observation: 3.149 [-1.641, 10.347], loss: 0.819159, mae: 3.141527, mean_q: 3.932824
  9165/100000: episode: 936, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.397, mean reward: 0.440 [0.379, 0.578], mean action: 36.300 [15.000, 71.000], mean observation: 3.162 [-1.343, 10.396], loss: 0.775948, mae: 3.144144, mean_q: 3.936862
  9175/100000: episode: 937, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.062, mean reward: 0.406 [0.319, 0.518], mean action: 38.900 [16.000, 97.000], mean observation: 3.154 [-1.282, 10.252], loss: 0.760935, mae: 3.146467, mean_q: 3.939847
  9185/100000: episode: 938, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.096, mean reward: 0.410 [0.387, 0.426], mean action: 43.100 [3.000, 101.000], mean observation: 3.158 [-1.306, 10.463], loss: 0.876694, mae: 3.149481, mean_q: 3.941587
  9195/100000: episode: 939, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.584, mean reward: 0.458 [0.327, 0.551], mean action: 29.600 [2.000, 66.000], mean observation: 3.167 [-0.988, 10.362], loss: 0.703776, mae: 3.150597, mean_q: 3.941864
  9205/100000: episode: 940, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 3.962, mean reward: 0.396 [0.332, 0.570], mean action: 30.900 [30.000, 39.000], mean observation: 3.160 [-0.964, 10.558], loss: 0.779997, mae: 3.153857, mean_q: 3.943808
  9215/100000: episode: 941, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.749, mean reward: 0.375 [0.324, 0.429], mean action: 30.200 [6.000, 66.000], mean observation: 3.159 [-1.344, 10.204], loss: 0.916405, mae: 3.156349, mean_q: 3.946766
  9225/100000: episode: 942, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.690, mean reward: 0.369 [0.345, 0.462], mean action: 43.300 [14.000, 93.000], mean observation: 3.148 [-1.882, 10.285], loss: 0.879576, mae: 3.158876, mean_q: 3.950514
  9226/100000: episode: 943, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 30.000 [30.000, 30.000], mean observation: 3.160 [-1.432, 10.162], loss: 1.160173, mae: 3.162050, mean_q: 3.952670
  9229/100000: episode: 944, duration: 0.067s, episode steps: 3, steps per second: 45, episode reward: 10.678, mean reward: 3.559 [0.337, 10.000], mean action: 34.333 [9.000, 64.000], mean observation: 3.156 [-1.097, 10.195], loss: 0.698064, mae: 3.160503, mean_q: 3.953506
  9239/100000: episode: 945, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.569, mean reward: 0.457 [0.324, 0.524], mean action: 30.700 [5.000, 83.000], mean observation: 3.157 [-1.510, 10.351], loss: 0.772075, mae: 3.162214, mean_q: 3.955284
  9249/100000: episode: 946, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.962, mean reward: 0.396 [0.338, 0.497], mean action: 53.600 [26.000, 93.000], mean observation: 3.163 [-1.269, 10.431], loss: 0.860167, mae: 3.165117, mean_q: 3.958347
  9259/100000: episode: 947, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 3.746, mean reward: 0.375 [0.318, 0.421], mean action: 35.900 [19.000, 57.000], mean observation: 3.148 [-1.162, 10.276], loss: 0.830474, mae: 3.166584, mean_q: 3.958831
  9269/100000: episode: 948, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.096, mean reward: 0.410 [0.378, 0.496], mean action: 59.400 [5.000, 98.000], mean observation: 3.154 [-1.553, 10.302], loss: 0.821402, mae: 3.168596, mean_q: 3.959084
  9279/100000: episode: 949, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.213, mean reward: 0.421 [0.374, 0.540], mean action: 33.300 [27.000, 66.000], mean observation: 3.153 [-2.634, 10.244], loss: 0.960205, mae: 3.172279, mean_q: 3.959508
  9289/100000: episode: 950, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.071, mean reward: 0.407 [0.381, 0.498], mean action: 47.600 [13.000, 97.000], mean observation: 3.164 [-1.893, 10.462], loss: 0.889996, mae: 3.173836, mean_q: 3.959914
  9290/100000: episode: 951, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 30.000 [30.000, 30.000], mean observation: 3.201 [-0.864, 10.241], loss: 0.890730, mae: 3.175378, mean_q: 3.961159
  9300/100000: episode: 952, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.333, mean reward: 0.433 [0.322, 0.501], mean action: 34.000 [2.000, 94.000], mean observation: 3.158 [-1.241, 10.366], loss: 0.850241, mae: 3.176938, mean_q: 3.960637
  9310/100000: episode: 953, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.081, mean reward: 0.408 [0.332, 0.488], mean action: 30.900 [14.000, 47.000], mean observation: 3.145 [-1.461, 10.388], loss: 0.837938, mae: 3.178511, mean_q: 3.960527
  9320/100000: episode: 954, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.320, mean reward: 0.432 [0.406, 0.477], mean action: 47.000 [30.000, 95.000], mean observation: 3.155 [-1.275, 10.261], loss: 1.137350, mae: 3.182095, mean_q: 3.960550
  9330/100000: episode: 955, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.216, mean reward: 0.422 [0.332, 0.527], mean action: 38.600 [30.000, 79.000], mean observation: 3.161 [-1.076, 10.546], loss: 0.977143, mae: 3.182910, mean_q: 3.960234
  9340/100000: episode: 956, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.905, mean reward: 0.390 [0.319, 0.497], mean action: 36.600 [4.000, 82.000], mean observation: 3.163 [-1.422, 10.342], loss: 0.915566, mae: 3.185613, mean_q: 3.959342
  9350/100000: episode: 957, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.954, mean reward: 0.395 [0.284, 0.499], mean action: 33.400 [0.000, 77.000], mean observation: 3.159 [-0.738, 10.465], loss: 0.968722, mae: 3.187998, mean_q: 3.956347
  9360/100000: episode: 958, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 3.796, mean reward: 0.380 [0.345, 0.465], mean action: 31.400 [1.000, 83.000], mean observation: 3.158 [-1.145, 10.271], loss: 0.939813, mae: 3.189960, mean_q: 3.955930
  9365/100000: episode: 959, duration: 0.096s, episode steps: 5, steps per second: 52, episode reward: 11.610, mean reward: 2.322 [0.334, 10.000], mean action: 52.600 [30.000, 72.000], mean observation: 3.146 [-1.497, 10.266], loss: 0.919983, mae: 3.191415, mean_q: 3.956212
  9375/100000: episode: 960, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.593, mean reward: 0.459 [0.457, 0.470], mean action: 34.200 [1.000, 76.000], mean observation: 3.160 [-1.534, 10.252], loss: 0.837488, mae: 3.192367, mean_q: 3.956840
  9385/100000: episode: 961, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.978, mean reward: 0.398 [0.315, 0.562], mean action: 36.900 [30.000, 68.000], mean observation: 3.164 [-1.766, 10.321], loss: 0.855096, mae: 3.193803, mean_q: 3.958568
  9395/100000: episode: 962, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.397, mean reward: 0.440 [0.339, 0.534], mean action: 37.100 [20.000, 93.000], mean observation: 3.165 [-1.152, 10.503], loss: 1.052619, mae: 3.197553, mean_q: 3.960599
  9405/100000: episode: 963, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.830, mean reward: 0.383 [0.346, 0.426], mean action: 50.500 [30.000, 90.000], mean observation: 3.158 [-1.535, 10.236], loss: 0.944313, mae: 3.198933, mean_q: 3.963392
  9415/100000: episode: 964, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.812, mean reward: 0.381 [0.340, 0.414], mean action: 33.800 [9.000, 50.000], mean observation: 3.152 [-1.478, 10.409], loss: 0.882123, mae: 3.201025, mean_q: 3.966683
  9425/100000: episode: 965, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.030, mean reward: 0.403 [0.299, 0.537], mean action: 35.200 [4.000, 98.000], mean observation: 3.147 [-1.678, 10.295], loss: 0.976517, mae: 3.203930, mean_q: 3.967771
  9435/100000: episode: 966, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.266, mean reward: 0.427 [0.395, 0.503], mean action: 31.100 [1.000, 63.000], mean observation: 3.165 [-1.463, 10.329], loss: 1.004347, mae: 3.205377, mean_q: 3.968731
  9445/100000: episode: 967, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.272, mean reward: 0.427 [0.367, 0.488], mean action: 41.000 [19.000, 79.000], mean observation: 3.152 [-1.352, 10.494], loss: 0.870974, mae: 3.208045, mean_q: 3.968247
  9455/100000: episode: 968, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 5.106, mean reward: 0.511 [0.511, 0.511], mean action: 62.100 [35.000, 100.000], mean observation: 3.150 [-1.950, 10.232], loss: 1.058571, mae: 3.211130, mean_q: 3.970271
  9465/100000: episode: 969, duration: 0.153s, episode steps: 10, steps per second: 66, episode reward: 4.729, mean reward: 0.473 [0.460, 0.494], mean action: 59.800 [4.000, 78.000], mean observation: 3.154 [-2.164, 10.317], loss: 0.846871, mae: 3.212140, mean_q: 3.972058
  9475/100000: episode: 970, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.188, mean reward: 0.419 [0.374, 0.484], mean action: 64.600 [6.000, 101.000], mean observation: 3.167 [-1.552, 10.342], loss: 1.020786, mae: 3.215387, mean_q: 3.975325
  9485/100000: episode: 971, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.873, mean reward: 0.387 [0.350, 0.471], mean action: 51.700 [7.000, 76.000], mean observation: 3.150 [-2.317, 10.341], loss: 1.071482, mae: 3.217910, mean_q: 3.971860
  9495/100000: episode: 972, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.085, mean reward: 0.408 [0.396, 0.440], mean action: 59.700 [47.000, 66.000], mean observation: 3.161 [-1.187, 10.383], loss: 0.935678, mae: 3.218661, mean_q: 3.968424
  9505/100000: episode: 973, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.808, mean reward: 0.381 [0.322, 0.420], mean action: 60.100 [6.000, 100.000], mean observation: 3.145 [-2.014, 10.337], loss: 0.815578, mae: 3.220052, mean_q: 3.969703
  9515/100000: episode: 974, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 4.956, mean reward: 0.496 [0.496, 0.496], mean action: 67.700 [43.000, 99.000], mean observation: 3.154 [-0.720, 10.281], loss: 0.984942, mae: 3.223116, mean_q: 3.973688
  9525/100000: episode: 975, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.632, mean reward: 0.463 [0.453, 0.505], mean action: 55.200 [39.000, 63.000], mean observation: 3.154 [-1.461, 10.268], loss: 0.811050, mae: 3.223709, mean_q: 3.977321
  9535/100000: episode: 976, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.926, mean reward: 0.393 [0.363, 0.435], mean action: 46.500 [5.000, 83.000], mean observation: 3.156 [-1.452, 10.286], loss: 0.748015, mae: 3.226310, mean_q: 3.980010
  9545/100000: episode: 977, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.176, mean reward: 0.418 [0.369, 0.477], mean action: 59.200 [13.000, 84.000], mean observation: 3.148 [-1.024, 10.172], loss: 1.015534, mae: 3.229542, mean_q: 3.982190
  9555/100000: episode: 978, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.869, mean reward: 0.387 [0.296, 0.541], mean action: 59.400 [23.000, 89.000], mean observation: 3.161 [-1.145, 10.228], loss: 0.936933, mae: 3.230500, mean_q: 3.985109
  9565/100000: episode: 979, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.632, mean reward: 0.363 [0.294, 0.542], mean action: 54.700 [5.000, 88.000], mean observation: 3.156 [-1.173, 10.271], loss: 0.936527, mae: 3.233157, mean_q: 3.986991
  9575/100000: episode: 980, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.338, mean reward: 0.434 [0.372, 0.534], mean action: 53.900 [8.000, 63.000], mean observation: 3.154 [-1.368, 10.347], loss: 1.008963, mae: 3.235042, mean_q: 3.984250
  9585/100000: episode: 981, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.340, mean reward: 0.434 [0.345, 0.583], mean action: 68.100 [28.000, 95.000], mean observation: 3.149 [-2.464, 10.349], loss: 1.157901, mae: 3.238469, mean_q: 3.984687
  9586/100000: episode: 982, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 85.000 [85.000, 85.000], mean observation: 3.142 [-1.037, 10.100], loss: 0.833061, mae: 3.238384, mean_q: 3.984516
  9589/100000: episode: 983, duration: 0.052s, episode steps: 3, steps per second: 58, episode reward: 10.723, mean reward: 3.574 [0.320, 10.000], mean action: 54.333 [37.000, 63.000], mean observation: 3.166 [-1.615, 10.335], loss: 0.683410, mae: 3.238229, mean_q: 3.984254
  9590/100000: episode: 984, duration: 0.022s, episode steps: 1, steps per second: 45, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 63.000 [63.000, 63.000], mean observation: 3.143 [-1.286, 10.100], loss: 0.861229, mae: 3.240443, mean_q: 3.984564
  9600/100000: episode: 985, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.996, mean reward: 0.400 [0.338, 0.477], mean action: 48.800 [10.000, 74.000], mean observation: 3.154 [-1.898, 10.402], loss: 1.223192, mae: 3.241692, mean_q: 3.985511
  9610/100000: episode: 986, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.493, mean reward: 0.449 [0.299, 0.528], mean action: 50.600 [0.000, 63.000], mean observation: 3.157 [-1.840, 10.452], loss: 0.804885, mae: 3.241927, mean_q: 3.987083
  9620/100000: episode: 987, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.886, mean reward: 0.389 [0.363, 0.460], mean action: 50.100 [1.000, 99.000], mean observation: 3.171 [-0.843, 10.398], loss: 0.879368, mae: 3.244525, mean_q: 3.989928
  9630/100000: episode: 988, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.868, mean reward: 0.387 [0.335, 0.594], mean action: 57.900 [18.000, 81.000], mean observation: 3.142 [-1.952, 10.235], loss: 0.820848, mae: 3.246419, mean_q: 3.994922
  9640/100000: episode: 989, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.977, mean reward: 0.398 [0.300, 0.504], mean action: 49.000 [13.000, 63.000], mean observation: 3.144 [-1.605, 10.226], loss: 0.895963, mae: 3.249243, mean_q: 3.999213
  9650/100000: episode: 990, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.000, mean reward: 0.400 [0.346, 0.419], mean action: 64.900 [62.000, 78.000], mean observation: 3.159 [-1.333, 10.231], loss: 0.836307, mae: 3.251417, mean_q: 4.002670
  9660/100000: episode: 991, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.857, mean reward: 0.386 [0.342, 0.551], mean action: 54.500 [16.000, 79.000], mean observation: 3.161 [-1.609, 10.305], loss: 0.946454, mae: 3.253376, mean_q: 4.006801
  9670/100000: episode: 992, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.703, mean reward: 0.370 [0.332, 0.407], mean action: 49.100 [7.000, 71.000], mean observation: 3.168 [-2.095, 10.253], loss: 1.026222, mae: 3.256023, mean_q: 4.013878
  9680/100000: episode: 993, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.006, mean reward: 0.401 [0.346, 0.466], mean action: 57.900 [11.000, 93.000], mean observation: 3.158 [-1.332, 10.218], loss: 0.819809, mae: 3.256664, mean_q: 4.021024
  9690/100000: episode: 994, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 5.038, mean reward: 0.504 [0.411, 0.596], mean action: 59.600 [29.000, 73.000], mean observation: 3.161 [-1.427, 10.274], loss: 1.046602, mae: 3.260346, mean_q: 4.024207
  9700/100000: episode: 995, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 3.921, mean reward: 0.392 [0.352, 0.485], mean action: 52.600 [14.000, 89.000], mean observation: 3.161 [-1.432, 10.238], loss: 0.974311, mae: 3.262277, mean_q: 4.025076
  9710/100000: episode: 996, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.527, mean reward: 0.453 [0.388, 0.501], mean action: 58.800 [17.000, 84.000], mean observation: 3.166 [-0.931, 10.363], loss: 0.930750, mae: 3.263766, mean_q: 4.022209
  9720/100000: episode: 997, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.141, mean reward: 0.414 [0.364, 0.436], mean action: 54.200 [5.000, 63.000], mean observation: 3.147 [-1.239, 10.264], loss: 1.048362, mae: 3.266228, mean_q: 4.023470
  9723/100000: episode: 998, duration: 0.076s, episode steps: 3, steps per second: 40, episode reward: 10.679, mean reward: 3.560 [0.334, 10.000], mean action: 23.667 [0.000, 63.000], mean observation: 3.150 [-1.389, 10.135], loss: 1.148032, mae: 3.268893, mean_q: 4.025432
  9733/100000: episode: 999, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.532, mean reward: 0.453 [0.453, 0.453], mean action: 61.500 [25.000, 80.000], mean observation: 3.142 [-1.963, 10.310], loss: 0.888860, mae: 3.268576, mean_q: 4.028263
  9743/100000: episode: 1000, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.195, mean reward: 0.419 [0.367, 0.483], mean action: 51.800 [6.000, 78.000], mean observation: 3.148 [-1.475, 10.273], loss: 1.066806, mae: 3.271001, mean_q: 4.030767
  9753/100000: episode: 1001, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.446, mean reward: 0.445 [0.316, 0.562], mean action: 55.300 [0.000, 90.000], mean observation: 3.150 [-1.144, 10.392], loss: 0.929303, mae: 3.272899, mean_q: 4.031436
  9763/100000: episode: 1002, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 5.259, mean reward: 0.526 [0.523, 0.554], mean action: 58.200 [8.000, 67.000], mean observation: 3.158 [-1.552, 10.339], loss: 0.877771, mae: 3.274929, mean_q: 4.032893
  9773/100000: episode: 1003, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.394, mean reward: 0.439 [0.327, 0.502], mean action: 58.100 [28.000, 92.000], mean observation: 3.161 [-1.572, 10.364], loss: 0.884679, mae: 3.276154, mean_q: 4.033925
  9783/100000: episode: 1004, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.203, mean reward: 0.420 [0.353, 0.468], mean action: 43.400 [6.000, 63.000], mean observation: 3.167 [-1.714, 10.392], loss: 1.054317, mae: 3.280322, mean_q: 4.037001
  9793/100000: episode: 1005, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.754, mean reward: 0.375 [0.368, 0.389], mean action: 50.800 [10.000, 85.000], mean observation: 3.151 [-1.287, 10.352], loss: 0.949043, mae: 3.281167, mean_q: 4.038218
  9803/100000: episode: 1006, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.631, mean reward: 0.363 [0.297, 0.426], mean action: 55.700 [21.000, 80.000], mean observation: 3.167 [-1.759, 10.311], loss: 0.958046, mae: 3.284032, mean_q: 4.039855
  9813/100000: episode: 1007, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.183, mean reward: 0.418 [0.398, 0.450], mean action: 50.300 [2.000, 96.000], mean observation: 3.170 [-1.126, 10.294], loss: 0.655217, mae: 3.284708, mean_q: 4.039830
  9823/100000: episode: 1008, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.099, mean reward: 0.410 [0.289, 0.489], mean action: 54.300 [1.000, 77.000], mean observation: 3.151 [-1.326, 10.281], loss: 0.838597, mae: 3.288531, mean_q: 4.040857
  9833/100000: episode: 1009, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.175, mean reward: 0.417 [0.389, 0.517], mean action: 56.600 [6.000, 86.000], mean observation: 3.147 [-1.969, 10.310], loss: 0.987040, mae: 3.291328, mean_q: 4.038218
  9843/100000: episode: 1010, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.650, mean reward: 0.365 [0.337, 0.430], mean action: 63.400 [51.000, 73.000], mean observation: 3.162 [-0.890, 10.478], loss: 0.867192, mae: 3.292409, mean_q: 4.040182
  9853/100000: episode: 1011, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.786, mean reward: 0.379 [0.360, 0.431], mean action: 57.200 [32.000, 71.000], mean observation: 3.145 [-1.058, 10.344], loss: 0.930980, mae: 3.294774, mean_q: 4.038615
  9863/100000: episode: 1012, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.007, mean reward: 0.401 [0.327, 0.459], mean action: 43.000 [0.000, 63.000], mean observation: 3.161 [-1.107, 10.442], loss: 1.011667, mae: 3.296862, mean_q: 4.036441
  9873/100000: episode: 1013, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.890, mean reward: 0.389 [0.319, 0.486], mean action: 58.000 [13.000, 92.000], mean observation: 3.158 [-1.163, 10.351], loss: 1.118718, mae: 3.298742, mean_q: 4.031980
  9883/100000: episode: 1014, duration: 0.131s, episode steps: 10, steps per second: 77, episode reward: 4.453, mean reward: 0.445 [0.445, 0.445], mean action: 60.400 [40.000, 80.000], mean observation: 3.154 [-1.320, 10.310], loss: 0.721150, mae: 3.299488, mean_q: 4.031947
  9893/100000: episode: 1015, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.540, mean reward: 0.454 [0.328, 0.483], mean action: 60.400 [32.000, 94.000], mean observation: 3.147 [-2.108, 10.399], loss: 1.121660, mae: 3.303516, mean_q: 4.033541
  9903/100000: episode: 1016, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.413, mean reward: 0.441 [0.429, 0.460], mean action: 60.600 [22.000, 97.000], mean observation: 3.157 [-1.598, 10.329], loss: 0.818085, mae: 3.304200, mean_q: 4.031473
  9913/100000: episode: 1017, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.231, mean reward: 0.423 [0.340, 0.439], mean action: 51.800 [2.000, 73.000], mean observation: 3.158 [-1.050, 10.244], loss: 0.902970, mae: 3.307384, mean_q: 4.032401
  9923/100000: episode: 1018, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.836, mean reward: 0.384 [0.341, 0.443], mean action: 53.900 [9.000, 93.000], mean observation: 3.148 [-1.420, 10.302], loss: 0.711029, mae: 3.309055, mean_q: 4.036710
  9933/100000: episode: 1019, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.139, mean reward: 0.414 [0.353, 0.489], mean action: 61.400 [47.000, 73.000], mean observation: 3.162 [-1.232, 10.367], loss: 0.901425, mae: 3.312510, mean_q: 4.039680
  9943/100000: episode: 1020, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.828, mean reward: 0.383 [0.301, 0.496], mean action: 69.500 [42.000, 100.000], mean observation: 3.147 [-1.338, 10.274], loss: 1.101998, mae: 3.314594, mean_q: 4.042438
  9953/100000: episode: 1021, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.005, mean reward: 0.401 [0.374, 0.422], mean action: 57.600 [5.000, 79.000], mean observation: 3.146 [-1.245, 10.318], loss: 1.003126, mae: 3.316779, mean_q: 4.045667
  9963/100000: episode: 1022, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.468, mean reward: 0.447 [0.432, 0.578], mean action: 51.800 [28.000, 63.000], mean observation: 3.144 [-2.059, 10.373], loss: 0.910917, mae: 3.318190, mean_q: 4.043765
  9973/100000: episode: 1023, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.285, mean reward: 0.429 [0.344, 0.554], mean action: 62.200 [31.000, 98.000], mean observation: 3.157 [-1.615, 10.321], loss: 0.764084, mae: 3.319465, mean_q: 4.045089
  9983/100000: episode: 1024, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.969, mean reward: 0.397 [0.340, 0.462], mean action: 52.900 [9.000, 82.000], mean observation: 3.156 [-1.071, 10.325], loss: 1.110019, mae: 3.323537, mean_q: 4.045392
  9993/100000: episode: 1025, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.425, mean reward: 0.442 [0.432, 0.468], mean action: 55.100 [2.000, 65.000], mean observation: 3.150 [-0.896, 10.401], loss: 0.740099, mae: 3.323289, mean_q: 4.042959
 10003/100000: episode: 1026, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 5.128, mean reward: 0.513 [0.513, 0.513], mean action: 64.800 [48.000, 84.000], mean observation: 3.146 [-1.659, 10.165], loss: 0.967522, mae: 3.325973, mean_q: 4.045276
 10013/100000: episode: 1027, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 4.705, mean reward: 0.470 [0.470, 0.470], mean action: 58.200 [23.000, 87.000], mean observation: 3.158 [-1.175, 10.407], loss: 0.741397, mae: 3.327048, mean_q: 4.048457
 10023/100000: episode: 1028, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.479, mean reward: 0.448 [0.447, 0.460], mean action: 49.500 [10.000, 78.000], mean observation: 3.164 [-1.970, 10.368], loss: 0.822031, mae: 3.329667, mean_q: 4.048908
 10033/100000: episode: 1029, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.051, mean reward: 0.405 [0.359, 0.553], mean action: 46.200 [6.000, 63.000], mean observation: 3.160 [-1.141, 10.246], loss: 1.238061, mae: 3.332757, mean_q: 4.051687
 10043/100000: episode: 1030, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.354, mean reward: 0.435 [0.332, 0.477], mean action: 54.800 [27.000, 63.000], mean observation: 3.141 [-1.487, 10.240], loss: 0.842059, mae: 3.333585, mean_q: 4.054529
 10053/100000: episode: 1031, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.639, mean reward: 0.464 [0.346, 0.549], mean action: 43.500 [0.000, 83.000], mean observation: 3.161 [-1.285, 10.441], loss: 0.877366, mae: 3.335562, mean_q: 4.060200
 10063/100000: episode: 1032, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.343, mean reward: 0.434 [0.370, 0.527], mean action: 58.000 [11.000, 93.000], mean observation: 3.154 [-1.552, 10.350], loss: 0.851341, mae: 3.337004, mean_q: 4.065417
 10073/100000: episode: 1033, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.423, mean reward: 0.442 [0.380, 0.547], mean action: 53.600 [11.000, 80.000], mean observation: 3.148 [-1.067, 10.262], loss: 0.848898, mae: 3.340033, mean_q: 4.067546
 10083/100000: episode: 1034, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 5.019, mean reward: 0.502 [0.502, 0.502], mean action: 57.900 [6.000, 86.000], mean observation: 3.153 [-1.417, 10.280], loss: 0.858973, mae: 3.342607, mean_q: 4.071515
 10093/100000: episode: 1035, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.994, mean reward: 0.399 [0.295, 0.440], mean action: 57.300 [0.000, 83.000], mean observation: 3.162 [-1.711, 10.264], loss: 0.919790, mae: 3.343878, mean_q: 4.076398
 10103/100000: episode: 1036, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.174, mean reward: 0.417 [0.334, 0.503], mean action: 49.100 [7.000, 63.000], mean observation: 3.150 [-1.189, 10.277], loss: 0.870022, mae: 3.345082, mean_q: 4.072841
 10104/100000: episode: 1037, duration: 0.025s, episode steps: 1, steps per second: 40, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 63.000 [63.000, 63.000], mean observation: 3.154 [-1.009, 10.100], loss: 0.875412, mae: 3.345747, mean_q: 4.070296
 10114/100000: episode: 1038, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.734, mean reward: 0.373 [0.315, 0.486], mean action: 56.600 [23.000, 99.000], mean observation: 3.164 [-1.147, 10.397], loss: 0.731941, mae: 3.347389, mean_q: 4.069338
 10124/100000: episode: 1039, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.592, mean reward: 0.359 [0.317, 0.397], mean action: 64.800 [49.000, 100.000], mean observation: 3.157 [-0.857, 10.250], loss: 1.014487, mae: 3.350621, mean_q: 4.068959
 10134/100000: episode: 1040, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.840, mean reward: 0.484 [0.382, 0.576], mean action: 73.100 [25.000, 99.000], mean observation: 3.161 [-1.730, 10.242], loss: 1.037818, mae: 3.353081, mean_q: 4.069978
 10144/100000: episode: 1041, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.057, mean reward: 0.406 [0.322, 0.543], mean action: 45.900 [9.000, 64.000], mean observation: 3.163 [-0.935, 10.256], loss: 1.090518, mae: 3.355086, mean_q: 4.070105
 10148/100000: episode: 1042, duration: 0.084s, episode steps: 4, steps per second: 47, episode reward: 11.307, mean reward: 2.827 [0.420, 10.000], mean action: 37.500 [10.000, 62.000], mean observation: 3.169 [-1.017, 10.212], loss: 0.795965, mae: 3.354552, mean_q: 4.070298
 10158/100000: episode: 1043, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.010, mean reward: 0.401 [0.315, 0.460], mean action: 52.500 [11.000, 63.000], mean observation: 3.149 [-1.616, 10.385], loss: 1.169274, mae: 3.358370, mean_q: 4.070665
 10168/100000: episode: 1044, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.326, mean reward: 0.433 [0.325, 0.503], mean action: 58.500 [4.000, 79.000], mean observation: 3.165 [-1.575, 10.249], loss: 0.915673, mae: 3.358940, mean_q: 4.075444
 10178/100000: episode: 1045, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.760, mean reward: 0.476 [0.476, 0.479], mean action: 62.000 [53.000, 63.000], mean observation: 3.163 [-1.220, 10.258], loss: 0.904364, mae: 3.361359, mean_q: 4.073453
 10188/100000: episode: 1046, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.938, mean reward: 0.394 [0.356, 0.469], mean action: 58.600 [33.000, 63.000], mean observation: 3.150 [-1.431, 10.284], loss: 0.819563, mae: 3.363029, mean_q: 4.074908
 10198/100000: episode: 1047, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.930, mean reward: 0.393 [0.338, 0.430], mean action: 50.400 [8.000, 81.000], mean observation: 3.160 [-2.045, 10.363], loss: 0.721812, mae: 3.364640, mean_q: 4.078941
 10208/100000: episode: 1048, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.342, mean reward: 0.434 [0.351, 0.555], mean action: 58.100 [26.000, 73.000], mean observation: 3.148 [-1.133, 10.320], loss: 0.908415, mae: 3.367418, mean_q: 4.081650
 10218/100000: episode: 1049, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.097, mean reward: 0.410 [0.351, 0.450], mean action: 51.600 [3.000, 74.000], mean observation: 3.169 [-1.858, 10.204], loss: 0.794831, mae: 3.369376, mean_q: 4.086329
 10228/100000: episode: 1050, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.969, mean reward: 0.397 [0.338, 0.487], mean action: 66.900 [51.000, 89.000], mean observation: 3.162 [-1.092, 10.323], loss: 0.871663, mae: 3.371306, mean_q: 4.088439
 10238/100000: episode: 1051, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.112, mean reward: 0.411 [0.316, 0.474], mean action: 50.100 [3.000, 77.000], mean observation: 3.156 [-0.973, 10.309], loss: 0.846904, mae: 3.373024, mean_q: 4.087973
 10248/100000: episode: 1052, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.767, mean reward: 0.377 [0.363, 0.438], mean action: 58.800 [16.000, 83.000], mean observation: 3.137 [-1.297, 10.266], loss: 0.802934, mae: 3.375570, mean_q: 4.090619
 10258/100000: episode: 1053, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.784, mean reward: 0.378 [0.314, 0.480], mean action: 57.900 [23.000, 92.000], mean observation: 3.148 [-1.062, 10.359], loss: 1.055480, mae: 3.378872, mean_q: 4.092670
 10268/100000: episode: 1054, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.121, mean reward: 0.412 [0.327, 0.441], mean action: 58.900 [16.000, 90.000], mean observation: 3.154 [-1.232, 10.283], loss: 1.042495, mae: 3.380613, mean_q: 4.093922
 10278/100000: episode: 1055, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.329, mean reward: 0.433 [0.412, 0.539], mean action: 59.800 [10.000, 100.000], mean observation: 3.157 [-0.971, 10.284], loss: 1.106791, mae: 3.382861, mean_q: 4.098454
 10288/100000: episode: 1056, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.996, mean reward: 0.400 [0.356, 0.483], mean action: 56.000 [1.000, 98.000], mean observation: 3.169 [-1.867, 10.280], loss: 0.919764, mae: 3.384490, mean_q: 4.103557
 10298/100000: episode: 1057, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.409, mean reward: 0.441 [0.433, 0.496], mean action: 56.400 [11.000, 90.000], mean observation: 3.147 [-1.054, 10.267], loss: 0.843193, mae: 3.385490, mean_q: 4.107277
 10308/100000: episode: 1058, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.480, mean reward: 0.348 [0.297, 0.385], mean action: 68.500 [39.000, 100.000], mean observation: 3.166 [-1.412, 10.259], loss: 0.715812, mae: 3.387368, mean_q: 4.109574
 10318/100000: episode: 1059, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.836, mean reward: 0.484 [0.470, 0.516], mean action: 61.600 [16.000, 97.000], mean observation: 3.157 [-1.414, 10.190], loss: 1.011039, mae: 3.391375, mean_q: 4.113112
 10328/100000: episode: 1060, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.927, mean reward: 0.393 [0.380, 0.459], mean action: 58.000 [18.000, 99.000], mean observation: 3.163 [-1.592, 10.364], loss: 0.814111, mae: 3.391885, mean_q: 4.114150
 10338/100000: episode: 1061, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.181, mean reward: 0.418 [0.354, 0.449], mean action: 58.300 [19.000, 85.000], mean observation: 3.156 [-1.060, 10.163], loss: 0.894555, mae: 3.394423, mean_q: 4.115441
 10348/100000: episode: 1062, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.722, mean reward: 0.472 [0.472, 0.472], mean action: 55.300 [19.000, 90.000], mean observation: 3.155 [-1.864, 10.402], loss: 0.731994, mae: 3.396012, mean_q: 4.118287
 10358/100000: episode: 1063, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.044, mean reward: 0.404 [0.352, 0.459], mean action: 58.100 [11.000, 97.000], mean observation: 3.161 [-1.614, 10.324], loss: 0.872937, mae: 3.398431, mean_q: 4.121541
 10368/100000: episode: 1064, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.521, mean reward: 0.452 [0.440, 0.471], mean action: 63.800 [9.000, 98.000], mean observation: 3.155 [-1.869, 10.371], loss: 0.918524, mae: 3.401028, mean_q: 4.123162
 10378/100000: episode: 1065, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.258, mean reward: 0.426 [0.363, 0.507], mean action: 48.600 [8.000, 92.000], mean observation: 3.148 [-1.148, 10.209], loss: 1.208968, mae: 3.403411, mean_q: 4.122130
 10388/100000: episode: 1066, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.604, mean reward: 0.360 [0.316, 0.440], mean action: 62.400 [37.000, 100.000], mean observation: 3.158 [-1.541, 10.293], loss: 0.862951, mae: 3.404568, mean_q: 4.118183
 10398/100000: episode: 1067, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 5.207, mean reward: 0.521 [0.521, 0.521], mean action: 54.200 [20.000, 93.000], mean observation: 3.149 [-1.514, 10.249], loss: 1.001946, mae: 3.406920, mean_q: 4.116197
 10408/100000: episode: 1068, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.273, mean reward: 0.427 [0.417, 0.438], mean action: 54.900 [2.000, 85.000], mean observation: 3.158 [-0.800, 10.350], loss: 1.095855, mae: 3.409309, mean_q: 4.116021
 10418/100000: episode: 1069, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.330, mean reward: 0.433 [0.420, 0.461], mean action: 61.600 [29.000, 85.000], mean observation: 3.155 [-1.549, 10.251], loss: 1.011123, mae: 3.411878, mean_q: 4.117412
 10428/100000: episode: 1070, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.641, mean reward: 0.464 [0.383, 0.557], mean action: 58.500 [27.000, 83.000], mean observation: 3.135 [-1.367, 10.364], loss: 0.964311, mae: 3.413666, mean_q: 4.117542
 10438/100000: episode: 1071, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 5.510, mean reward: 0.551 [0.473, 0.560], mean action: 57.000 [1.000, 92.000], mean observation: 3.139 [-1.516, 10.379], loss: 0.738879, mae: 3.414423, mean_q: 4.119635
 10448/100000: episode: 1072, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.239, mean reward: 0.424 [0.353, 0.496], mean action: 34.900 [1.000, 63.000], mean observation: 3.158 [-1.880, 10.357], loss: 0.884464, mae: 3.416815, mean_q: 4.123113
 10458/100000: episode: 1073, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.395, mean reward: 0.339 [0.282, 0.442], mean action: 49.400 [0.000, 88.000], mean observation: 3.161 [-1.706, 10.207], loss: 0.908074, mae: 3.419644, mean_q: 4.127226
 10468/100000: episode: 1074, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.319, mean reward: 0.432 [0.349, 0.551], mean action: 45.400 [1.000, 83.000], mean observation: 3.166 [-1.749, 10.421], loss: 0.875382, mae: 3.421069, mean_q: 4.132905
 10478/100000: episode: 1075, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.716, mean reward: 0.372 [0.279, 0.420], mean action: 60.600 [37.000, 92.000], mean observation: 3.145 [-1.210, 10.217], loss: 0.809610, mae: 3.423535, mean_q: 4.137032
 10488/100000: episode: 1076, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.769, mean reward: 0.377 [0.332, 0.512], mean action: 58.800 [21.000, 63.000], mean observation: 3.162 [-2.265, 10.381], loss: 0.932264, mae: 3.425950, mean_q: 4.139592
 10498/100000: episode: 1077, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.925, mean reward: 0.492 [0.492, 0.500], mean action: 63.900 [47.000, 87.000], mean observation: 3.152 [-1.185, 10.350], loss: 1.024663, mae: 3.428659, mean_q: 4.143840
 10508/100000: episode: 1078, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.783, mean reward: 0.378 [0.376, 0.391], mean action: 68.900 [24.000, 96.000], mean observation: 3.169 [-0.990, 10.383], loss: 0.911013, mae: 3.429704, mean_q: 4.144481
 10518/100000: episode: 1079, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 5.494, mean reward: 0.549 [0.549, 0.549], mean action: 57.800 [37.000, 63.000], mean observation: 3.161 [-1.423, 10.320], loss: 0.848509, mae: 3.432967, mean_q: 4.143429
 10528/100000: episode: 1080, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.221, mean reward: 0.422 [0.390, 0.522], mean action: 60.400 [27.000, 92.000], mean observation: 3.149 [-1.526, 10.463], loss: 0.823506, mae: 3.433810, mean_q: 4.146330
 10538/100000: episode: 1081, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.081, mean reward: 0.408 [0.349, 0.533], mean action: 55.700 [5.000, 100.000], mean observation: 3.165 [-1.167, 10.444], loss: 0.953280, mae: 3.436371, mean_q: 4.149913
 10548/100000: episode: 1082, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.973, mean reward: 0.497 [0.409, 0.546], mean action: 61.400 [33.000, 77.000], mean observation: 3.160 [-0.947, 10.299], loss: 1.031239, mae: 3.439137, mean_q: 4.144622
 10558/100000: episode: 1083, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.920, mean reward: 0.392 [0.319, 0.514], mean action: 44.800 [3.000, 94.000], mean observation: 3.169 [-1.803, 10.549], loss: 0.977243, mae: 3.440290, mean_q: 4.138206
 10568/100000: episode: 1084, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.915, mean reward: 0.391 [0.317, 0.465], mean action: 54.400 [21.000, 83.000], mean observation: 3.170 [-0.807, 10.235], loss: 0.807321, mae: 3.440957, mean_q: 4.140463
 10578/100000: episode: 1085, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.949, mean reward: 0.395 [0.384, 0.426], mean action: 63.000 [46.000, 88.000], mean observation: 3.149 [-1.459, 10.375], loss: 0.964524, mae: 3.444668, mean_q: 4.144855
 10588/100000: episode: 1086, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 5.068, mean reward: 0.507 [0.488, 0.525], mean action: 64.200 [30.000, 99.000], mean observation: 3.168 [-1.574, 10.382], loss: 0.931507, mae: 3.445858, mean_q: 4.148221
 10598/100000: episode: 1087, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 3.676, mean reward: 0.368 [0.299, 0.500], mean action: 58.000 [2.000, 93.000], mean observation: 3.153 [-1.104, 10.215], loss: 0.843647, mae: 3.447476, mean_q: 4.149916
 10608/100000: episode: 1088, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.789, mean reward: 0.479 [0.479, 0.479], mean action: 61.600 [8.000, 100.000], mean observation: 3.153 [-1.378, 10.250], loss: 0.791792, mae: 3.448354, mean_q: 4.153974
 10618/100000: episode: 1089, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.908, mean reward: 0.391 [0.334, 0.480], mean action: 44.800 [4.000, 90.000], mean observation: 3.159 [-1.080, 10.234], loss: 0.939598, mae: 3.451496, mean_q: 4.157752
 10628/100000: episode: 1090, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.778, mean reward: 0.378 [0.319, 0.457], mean action: 53.900 [3.000, 91.000], mean observation: 3.163 [-1.787, 10.394], loss: 0.774580, mae: 3.452775, mean_q: 4.161150
 10638/100000: episode: 1091, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.893, mean reward: 0.389 [0.362, 0.454], mean action: 61.700 [41.000, 74.000], mean observation: 3.167 [-1.242, 10.346], loss: 0.825059, mae: 3.454500, mean_q: 4.164020
 10648/100000: episode: 1092, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.557, mean reward: 0.456 [0.434, 0.546], mean action: 63.400 [10.000, 101.000], mean observation: 3.143 [-1.496, 10.231], loss: 0.751061, mae: 3.456191, mean_q: 4.169029
 10658/100000: episode: 1093, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.276, mean reward: 0.428 [0.378, 0.497], mean action: 55.400 [6.000, 101.000], mean observation: 3.174 [-1.347, 10.380], loss: 0.766243, mae: 3.458949, mean_q: 4.173227
 10668/100000: episode: 1094, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.485, mean reward: 0.448 [0.400, 0.499], mean action: 55.300 [0.000, 96.000], mean observation: 3.170 [-1.705, 10.349], loss: 1.024185, mae: 3.461589, mean_q: 4.178297
 10678/100000: episode: 1095, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.075, mean reward: 0.407 [0.385, 0.432], mean action: 58.800 [13.000, 94.000], mean observation: 3.151 [-1.235, 10.239], loss: 0.836864, mae: 3.462847, mean_q: 4.181471
 10688/100000: episode: 1096, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.734, mean reward: 0.373 [0.343, 0.441], mean action: 61.900 [23.000, 99.000], mean observation: 3.153 [-1.093, 10.300], loss: 1.090154, mae: 3.466647, mean_q: 4.181180
 10698/100000: episode: 1097, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.947, mean reward: 0.395 [0.376, 0.437], mean action: 53.400 [14.000, 97.000], mean observation: 3.149 [-1.724, 10.222], loss: 0.860232, mae: 3.466925, mean_q: 4.179343
 10708/100000: episode: 1098, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.224, mean reward: 0.422 [0.336, 0.498], mean action: 47.700 [1.000, 85.000], mean observation: 3.167 [-1.599, 10.295], loss: 0.858275, mae: 3.469784, mean_q: 4.179764
 10718/100000: episode: 1099, duration: 0.132s, episode steps: 10, steps per second: 75, episode reward: 4.048, mean reward: 0.405 [0.397, 0.447], mean action: 65.400 [43.000, 99.000], mean observation: 3.167 [-0.742, 10.329], loss: 1.117472, mae: 3.472176, mean_q: 4.179949
 10728/100000: episode: 1100, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.247, mean reward: 0.425 [0.352, 0.463], mean action: 57.600 [2.000, 96.000], mean observation: 3.147 [-1.580, 10.299], loss: 0.897993, mae: 3.472299, mean_q: 4.177193
 10738/100000: episode: 1101, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.063, mean reward: 0.406 [0.396, 0.432], mean action: 57.300 [20.000, 72.000], mean observation: 3.164 [-1.817, 10.281], loss: 0.853162, mae: 3.474141, mean_q: 4.173221
 10748/100000: episode: 1102, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.903, mean reward: 0.390 [0.291, 0.450], mean action: 53.300 [19.000, 63.000], mean observation: 3.154 [-1.382, 10.286], loss: 0.965155, mae: 3.477657, mean_q: 4.173125
 10758/100000: episode: 1103, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.342, mean reward: 0.434 [0.368, 0.560], mean action: 58.100 [14.000, 94.000], mean observation: 3.155 [-1.656, 10.260], loss: 0.684456, mae: 3.477945, mean_q: 4.177718
 10768/100000: episode: 1104, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.232, mean reward: 0.423 [0.325, 0.515], mean action: 63.300 [11.000, 92.000], mean observation: 3.154 [-0.986, 10.367], loss: 0.983349, mae: 3.481126, mean_q: 4.180221
 10778/100000: episode: 1105, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.105, mean reward: 0.410 [0.389, 0.524], mean action: 69.400 [55.000, 95.000], mean observation: 3.151 [-1.257, 10.345], loss: 0.965125, mae: 3.482934, mean_q: 4.176643
 10788/100000: episode: 1106, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.273, mean reward: 0.327 [0.283, 0.394], mean action: 72.800 [63.000, 96.000], mean observation: 3.149 [-1.052, 10.279], loss: 1.017771, mae: 3.485360, mean_q: 4.171784
 10798/100000: episode: 1107, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.874, mean reward: 0.387 [0.324, 0.491], mean action: 50.200 [11.000, 88.000], mean observation: 3.151 [-1.260, 10.419], loss: 1.152553, mae: 3.487337, mean_q: 4.170597
 10808/100000: episode: 1108, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.169, mean reward: 0.417 [0.367, 0.565], mean action: 64.400 [19.000, 97.000], mean observation: 3.161 [-1.000, 10.382], loss: 1.113852, mae: 3.489477, mean_q: 4.167577
 10818/100000: episode: 1109, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.082, mean reward: 0.408 [0.405, 0.437], mean action: 51.100 [12.000, 71.000], mean observation: 3.160 [-1.497, 10.274], loss: 0.720027, mae: 3.489103, mean_q: 4.162613
 10828/100000: episode: 1110, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.860, mean reward: 0.486 [0.443, 0.494], mean action: 48.700 [4.000, 72.000], mean observation: 3.175 [-1.305, 10.493], loss: 1.051754, mae: 3.492454, mean_q: 4.161911
 10838/100000: episode: 1111, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.567, mean reward: 0.457 [0.457, 0.457], mean action: 64.500 [5.000, 96.000], mean observation: 3.157 [-0.845, 10.304], loss: 0.863557, mae: 3.494068, mean_q: 4.162264
 10848/100000: episode: 1112, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.917, mean reward: 0.392 [0.386, 0.420], mean action: 45.900 [2.000, 87.000], mean observation: 3.143 [-1.284, 10.327], loss: 0.908321, mae: 3.496305, mean_q: 4.159374
 10858/100000: episode: 1113, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.215, mean reward: 0.422 [0.374, 0.487], mean action: 61.200 [18.000, 91.000], mean observation: 3.151 [-1.510, 10.285], loss: 0.942921, mae: 3.497670, mean_q: 4.160033
 10868/100000: episode: 1114, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.782, mean reward: 0.378 [0.337, 0.434], mean action: 58.600 [17.000, 99.000], mean observation: 3.169 [-2.203, 10.317], loss: 1.062568, mae: 3.500130, mean_q: 4.158214
 10878/100000: episode: 1115, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.970, mean reward: 0.397 [0.377, 0.463], mean action: 65.200 [63.000, 73.000], mean observation: 3.134 [-0.852, 10.286], loss: 0.927587, mae: 3.501285, mean_q: 4.159729
 10888/100000: episode: 1116, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.608, mean reward: 0.461 [0.461, 0.461], mean action: 64.600 [40.000, 85.000], mean observation: 3.144 [-1.146, 10.267], loss: 1.029023, mae: 3.502968, mean_q: 4.159377
 10898/100000: episode: 1117, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.024, mean reward: 0.402 [0.338, 0.488], mean action: 57.500 [1.000, 87.000], mean observation: 3.151 [-1.134, 10.165], loss: 0.907679, mae: 3.504633, mean_q: 4.157676
 10908/100000: episode: 1118, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.320, mean reward: 0.432 [0.430, 0.441], mean action: 50.700 [15.000, 93.000], mean observation: 3.140 [-1.004, 10.344], loss: 0.940728, mae: 3.506398, mean_q: 4.153897
 10918/100000: episode: 1119, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.447, mean reward: 0.445 [0.439, 0.470], mean action: 45.800 [18.000, 81.000], mean observation: 3.172 [-1.942, 10.444], loss: 0.805339, mae: 3.508141, mean_q: 4.148148
 10919/100000: episode: 1120, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 63.000 [63.000, 63.000], mean observation: 3.175 [-0.921, 10.362], loss: 1.144443, mae: 3.509994, mean_q: 4.146388
 10929/100000: episode: 1121, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.243, mean reward: 0.424 [0.382, 0.457], mean action: 59.300 [15.000, 101.000], mean observation: 3.153 [-0.825, 10.308], loss: 0.968319, mae: 3.510253, mean_q: 4.146155
 10939/100000: episode: 1122, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.904, mean reward: 0.390 [0.320, 0.512], mean action: 55.100 [16.000, 63.000], mean observation: 3.168 [-1.408, 10.317], loss: 0.838478, mae: 3.512477, mean_q: 4.150117
 10949/100000: episode: 1123, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.899, mean reward: 0.390 [0.367, 0.463], mean action: 50.400 [5.000, 63.000], mean observation: 3.171 [-1.628, 10.374], loss: 1.121162, mae: 3.514798, mean_q: 4.155302
 10959/100000: episode: 1124, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.024, mean reward: 0.402 [0.317, 0.512], mean action: 45.400 [0.000, 63.000], mean observation: 3.159 [-1.576, 10.357], loss: 0.781531, mae: 3.514907, mean_q: 4.155224
 10969/100000: episode: 1125, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.871, mean reward: 0.387 [0.354, 0.422], mean action: 60.200 [40.000, 78.000], mean observation: 3.151 [-1.892, 10.259], loss: 0.903563, mae: 3.516764, mean_q: 4.158014
 10979/100000: episode: 1126, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.789, mean reward: 0.479 [0.364, 0.530], mean action: 57.200 [6.000, 87.000], mean observation: 3.149 [-1.404, 10.294], loss: 0.900104, mae: 3.518947, mean_q: 4.162198
 10989/100000: episode: 1127, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.366, mean reward: 0.437 [0.421, 0.502], mean action: 54.200 [16.000, 82.000], mean observation: 3.145 [-1.534, 10.361], loss: 0.842395, mae: 3.520523, mean_q: 4.165515
 10999/100000: episode: 1128, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.538, mean reward: 0.354 [0.258, 0.409], mean action: 58.300 [3.000, 101.000], mean observation: 3.160 [-1.235, 10.418], loss: 0.973456, mae: 3.522682, mean_q: 4.162289
 11009/100000: episode: 1129, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.053, mean reward: 0.405 [0.285, 0.505], mean action: 63.700 [36.000, 99.000], mean observation: 3.170 [-2.160, 10.215], loss: 0.944407, mae: 3.524859, mean_q: 4.162065
 11019/100000: episode: 1130, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.235, mean reward: 0.423 [0.362, 0.513], mean action: 53.900 [13.000, 92.000], mean observation: 3.164 [-0.997, 10.306], loss: 0.934642, mae: 3.526474, mean_q: 4.161160
 11029/100000: episode: 1131, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.409, mean reward: 0.441 [0.396, 0.529], mean action: 64.600 [35.000, 98.000], mean observation: 3.151 [-1.328, 10.277], loss: 1.021581, mae: 3.528372, mean_q: 4.161836
 11039/100000: episode: 1132, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.788, mean reward: 0.379 [0.307, 0.478], mean action: 50.400 [3.000, 71.000], mean observation: 3.161 [-1.719, 10.421], loss: 1.087401, mae: 3.530591, mean_q: 4.159260
 11049/100000: episode: 1133, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.501, mean reward: 0.350 [0.315, 0.414], mean action: 54.900 [7.000, 96.000], mean observation: 3.148 [-1.131, 10.297], loss: 0.920674, mae: 3.530605, mean_q: 4.157017
 11059/100000: episode: 1134, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 4.106, mean reward: 0.411 [0.394, 0.482], mean action: 70.700 [51.000, 101.000], mean observation: 3.158 [-1.466, 10.385], loss: 1.050691, mae: 3.533142, mean_q: 4.154953
 11069/100000: episode: 1135, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.151, mean reward: 0.415 [0.349, 0.529], mean action: 51.700 [4.000, 79.000], mean observation: 3.159 [-1.450, 10.229], loss: 0.976338, mae: 3.534004, mean_q: 4.154721
 11079/100000: episode: 1136, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.269, mean reward: 0.427 [0.394, 0.517], mean action: 60.800 [9.000, 96.000], mean observation: 3.149 [-1.237, 10.412], loss: 1.021406, mae: 3.535862, mean_q: 4.155519
 11089/100000: episode: 1137, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.056, mean reward: 0.406 [0.297, 0.478], mean action: 59.900 [2.000, 93.000], mean observation: 3.153 [-1.863, 10.287], loss: 0.948759, mae: 3.537373, mean_q: 4.156245
 11099/100000: episode: 1138, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.066, mean reward: 0.407 [0.381, 0.497], mean action: 59.200 [42.000, 63.000], mean observation: 3.153 [-1.250, 10.224], loss: 0.706582, mae: 3.537438, mean_q: 4.157362
 11109/100000: episode: 1139, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.071, mean reward: 0.407 [0.347, 0.540], mean action: 51.700 [11.000, 78.000], mean observation: 3.159 [-1.541, 10.248], loss: 1.242791, mae: 3.542453, mean_q: 4.161474
 11119/100000: episode: 1140, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.407, mean reward: 0.441 [0.346, 0.481], mean action: 51.400 [5.000, 78.000], mean observation: 3.147 [-1.655, 10.301], loss: 0.998763, mae: 3.542989, mean_q: 4.163341
 11129/100000: episode: 1141, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.944, mean reward: 0.394 [0.322, 0.485], mean action: 55.900 [33.000, 83.000], mean observation: 3.149 [-1.610, 10.283], loss: 0.964817, mae: 3.543956, mean_q: 4.159516
 11131/100000: episode: 1142, duration: 0.052s, episode steps: 2, steps per second: 39, episode reward: 10.349, mean reward: 5.174 [0.349, 10.000], mean action: 56.500 [50.000, 63.000], mean observation: 3.140 [-1.291, 10.104], loss: 1.349082, mae: 3.547035, mean_q: 4.159410
 11141/100000: episode: 1143, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.633, mean reward: 0.363 [0.310, 0.460], mean action: 65.300 [46.000, 94.000], mean observation: 3.156 [-2.348, 10.260], loss: 0.803865, mae: 3.544838, mean_q: 4.161245
 11151/100000: episode: 1144, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.930, mean reward: 0.393 [0.367, 0.428], mean action: 60.600 [17.000, 99.000], mean observation: 3.150 [-2.038, 10.278], loss: 0.876410, mae: 3.548086, mean_q: 4.161421
 11161/100000: episode: 1145, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 3.897, mean reward: 0.390 [0.351, 0.485], mean action: 65.800 [42.000, 86.000], mean observation: 3.170 [-1.398, 10.274], loss: 0.957625, mae: 3.549272, mean_q: 4.159057
 11171/100000: episode: 1146, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.762, mean reward: 0.376 [0.324, 0.525], mean action: 63.400 [34.000, 91.000], mean observation: 3.159 [-1.153, 10.313], loss: 1.023201, mae: 3.551119, mean_q: 4.157758
 11181/100000: episode: 1147, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.143, mean reward: 0.414 [0.350, 0.535], mean action: 51.500 [0.000, 70.000], mean observation: 3.150 [-1.343, 10.364], loss: 0.888422, mae: 3.551594, mean_q: 4.156789
 11191/100000: episode: 1148, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.774, mean reward: 0.377 [0.340, 0.448], mean action: 55.200 [6.000, 84.000], mean observation: 3.160 [-1.737, 10.282], loss: 0.977132, mae: 3.554152, mean_q: 4.158681
 11201/100000: episode: 1149, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.595, mean reward: 0.459 [0.396, 0.543], mean action: 68.400 [37.000, 101.000], mean observation: 3.162 [-1.184, 10.212], loss: 0.776885, mae: 3.555584, mean_q: 4.161433
 11211/100000: episode: 1150, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.094, mean reward: 0.409 [0.396, 0.518], mean action: 56.000 [21.000, 87.000], mean observation: 3.154 [-0.994, 10.275], loss: 0.948955, mae: 3.557934, mean_q: 4.163818
 11221/100000: episode: 1151, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.265, mean reward: 0.427 [0.340, 0.533], mean action: 60.900 [14.000, 95.000], mean observation: 3.147 [-1.426, 10.318], loss: 0.899646, mae: 3.559239, mean_q: 4.165799
 11231/100000: episode: 1152, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.891, mean reward: 0.389 [0.331, 0.511], mean action: 45.000 [6.000, 75.000], mean observation: 3.170 [-1.328, 10.349], loss: 0.972264, mae: 3.560985, mean_q: 4.164037
 11241/100000: episode: 1153, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.107, mean reward: 0.411 [0.335, 0.457], mean action: 59.900 [27.000, 94.000], mean observation: 3.166 [-1.219, 10.304], loss: 0.914571, mae: 3.562852, mean_q: 4.162885
 11251/100000: episode: 1154, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.695, mean reward: 0.469 [0.465, 0.487], mean action: 69.600 [28.000, 101.000], mean observation: 3.159 [-0.854, 10.355], loss: 0.875124, mae: 3.564066, mean_q: 4.157493
 11261/100000: episode: 1155, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.453, mean reward: 0.445 [0.366, 0.547], mean action: 51.100 [0.000, 63.000], mean observation: 3.159 [-1.368, 10.178], loss: 1.101922, mae: 3.567445, mean_q: 4.151593
 11271/100000: episode: 1156, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 5.188, mean reward: 0.519 [0.512, 0.526], mean action: 46.400 [3.000, 71.000], mean observation: 3.164 [-0.832, 10.214], loss: 0.831990, mae: 3.568050, mean_q: 4.148530
 11281/100000: episode: 1157, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.213, mean reward: 0.421 [0.329, 0.488], mean action: 50.800 [21.000, 100.000], mean observation: 3.158 [-1.503, 10.194], loss: 0.921139, mae: 3.570206, mean_q: 4.147344
 11291/100000: episode: 1158, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.434, mean reward: 0.443 [0.438, 0.477], mean action: 55.800 [10.000, 86.000], mean observation: 3.157 [-1.312, 10.379], loss: 0.793187, mae: 3.570834, mean_q: 4.142336
 11301/100000: episode: 1159, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.025, mean reward: 0.402 [0.322, 0.477], mean action: 64.900 [47.000, 97.000], mean observation: 3.167 [-1.201, 10.302], loss: 0.886933, mae: 3.572788, mean_q: 4.139634
 11311/100000: episode: 1160, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.928, mean reward: 0.393 [0.364, 0.474], mean action: 66.500 [37.000, 98.000], mean observation: 3.161 [-1.439, 10.353], loss: 0.857753, mae: 3.574719, mean_q: 4.140338
 11321/100000: episode: 1161, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.301, mean reward: 0.430 [0.305, 0.507], mean action: 57.900 [7.000, 98.000], mean observation: 3.171 [-1.543, 10.295], loss: 0.889404, mae: 3.576064, mean_q: 4.141621
 11331/100000: episode: 1162, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.101, mean reward: 0.410 [0.402, 0.482], mean action: 63.900 [24.000, 91.000], mean observation: 3.157 [-1.127, 10.321], loss: 0.985765, mae: 3.577136, mean_q: 4.143012
 11341/100000: episode: 1163, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.323, mean reward: 0.432 [0.379, 0.489], mean action: 58.000 [7.000, 88.000], mean observation: 3.155 [-1.254, 10.375], loss: 0.892999, mae: 3.578935, mean_q: 4.141393
 11351/100000: episode: 1164, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.569, mean reward: 0.357 [0.334, 0.387], mean action: 65.000 [39.000, 93.000], mean observation: 3.145 [-0.946, 10.260], loss: 1.046283, mae: 3.580979, mean_q: 4.141904
 11361/100000: episode: 1165, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.462, mean reward: 0.446 [0.437, 0.525], mean action: 57.600 [10.000, 87.000], mean observation: 3.152 [-1.657, 10.348], loss: 1.056383, mae: 3.582930, mean_q: 4.141273
 11371/100000: episode: 1166, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.182, mean reward: 0.418 [0.329, 0.445], mean action: 45.800 [11.000, 84.000], mean observation: 3.172 [-1.272, 10.358], loss: 0.832076, mae: 3.583884, mean_q: 4.137210
 11381/100000: episode: 1167, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.151, mean reward: 0.415 [0.350, 0.456], mean action: 44.300 [3.000, 98.000], mean observation: 3.145 [-2.446, 10.416], loss: 0.762478, mae: 3.585286, mean_q: 4.135364
 11391/100000: episode: 1168, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 5.167, mean reward: 0.517 [0.517, 0.517], mean action: 56.200 [30.000, 63.000], mean observation: 3.158 [-1.490, 10.186], loss: 0.936896, mae: 3.587704, mean_q: 4.132117
 11401/100000: episode: 1169, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.969, mean reward: 0.397 [0.319, 0.544], mean action: 51.700 [37.000, 95.000], mean observation: 3.161 [-0.929, 10.278], loss: 0.742333, mae: 3.588602, mean_q: 4.129976
 11411/100000: episode: 1170, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.984, mean reward: 0.398 [0.318, 0.477], mean action: 36.700 [17.000, 70.000], mean observation: 3.162 [-1.517, 10.272], loss: 0.939165, mae: 3.591197, mean_q: 4.132714
 11421/100000: episode: 1171, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.826, mean reward: 0.383 [0.303, 0.419], mean action: 29.700 [5.000, 56.000], mean observation: 3.156 [-1.623, 10.165], loss: 1.035691, mae: 3.593346, mean_q: 4.135489
 11431/100000: episode: 1172, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.371, mean reward: 0.437 [0.364, 0.486], mean action: 42.600 [30.000, 78.000], mean observation: 3.150 [-1.923, 10.377], loss: 1.160708, mae: 3.595351, mean_q: 4.136794
 11441/100000: episode: 1173, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.891, mean reward: 0.389 [0.357, 0.505], mean action: 38.800 [8.000, 73.000], mean observation: 3.150 [-1.699, 10.320], loss: 0.712349, mae: 3.595157, mean_q: 4.133203
 11451/100000: episode: 1174, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.270, mean reward: 0.427 [0.380, 0.561], mean action: 31.900 [0.000, 56.000], mean observation: 3.158 [-1.454, 10.384], loss: 0.623186, mae: 3.595741, mean_q: 4.131999
 11461/100000: episode: 1175, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.593, mean reward: 0.459 [0.450, 0.519], mean action: 46.000 [20.000, 73.000], mean observation: 3.156 [-1.292, 10.357], loss: 0.878869, mae: 3.598553, mean_q: 4.132302
 11471/100000: episode: 1176, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.228, mean reward: 0.423 [0.343, 0.486], mean action: 48.700 [37.000, 78.000], mean observation: 3.159 [-1.363, 10.320], loss: 0.719378, mae: 3.599548, mean_q: 4.133147
 11481/100000: episode: 1177, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.120, mean reward: 0.412 [0.322, 0.509], mean action: 46.100 [6.000, 86.000], mean observation: 3.151 [-1.806, 10.346], loss: 0.817952, mae: 3.601266, mean_q: 4.134380
 11491/100000: episode: 1178, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.855, mean reward: 0.385 [0.350, 0.439], mean action: 42.300 [8.000, 93.000], mean observation: 3.161 [-1.827, 10.271], loss: 1.111115, mae: 3.603960, mean_q: 4.136961
 11501/100000: episode: 1179, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.213, mean reward: 0.421 [0.421, 0.421], mean action: 43.600 [14.000, 91.000], mean observation: 3.159 [-0.961, 10.293], loss: 0.724417, mae: 3.603266, mean_q: 4.136530
 11511/100000: episode: 1180, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.602, mean reward: 0.360 [0.305, 0.447], mean action: 44.300 [37.000, 75.000], mean observation: 3.157 [-1.410, 10.194], loss: 1.004542, mae: 3.606248, mean_q: 4.135767
 11521/100000: episode: 1181, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.033, mean reward: 0.403 [0.338, 0.493], mean action: 52.200 [11.000, 87.000], mean observation: 3.159 [-1.222, 10.383], loss: 0.886678, mae: 3.606805, mean_q: 4.136894
 11531/100000: episode: 1182, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.740, mean reward: 0.374 [0.312, 0.445], mean action: 44.400 [1.000, 96.000], mean observation: 3.166 [-1.208, 10.348], loss: 0.803000, mae: 3.608731, mean_q: 4.139724
 11541/100000: episode: 1183, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.995, mean reward: 0.400 [0.296, 0.556], mean action: 36.500 [32.000, 37.000], mean observation: 3.161 [-1.809, 10.366], loss: 0.743682, mae: 3.609981, mean_q: 4.142662
 11551/100000: episode: 1184, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.685, mean reward: 0.369 [0.332, 0.449], mean action: 53.800 [36.000, 93.000], mean observation: 3.162 [-0.837, 10.368], loss: 0.866644, mae: 3.611857, mean_q: 4.149711
 11561/100000: episode: 1185, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.597, mean reward: 0.360 [0.294, 0.495], mean action: 54.200 [17.000, 87.000], mean observation: 3.144 [-1.140, 10.297], loss: 0.810494, mae: 3.613370, mean_q: 4.157094
 11571/100000: episode: 1186, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.048, mean reward: 0.405 [0.357, 0.476], mean action: 37.800 [1.000, 100.000], mean observation: 3.160 [-1.355, 10.349], loss: 0.852882, mae: 3.615643, mean_q: 4.159071
 11581/100000: episode: 1187, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.376, mean reward: 0.438 [0.407, 0.515], mean action: 40.900 [33.000, 82.000], mean observation: 3.150 [-1.219, 10.417], loss: 0.833260, mae: 3.617118, mean_q: 4.160525
 11591/100000: episode: 1188, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.220, mean reward: 0.422 [0.417, 0.438], mean action: 45.400 [37.000, 84.000], mean observation: 3.153 [-1.429, 10.372], loss: 0.875674, mae: 3.618933, mean_q: 4.156809
 11601/100000: episode: 1189, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.889, mean reward: 0.389 [0.315, 0.475], mean action: 45.900 [21.000, 87.000], mean observation: 3.157 [-0.985, 10.358], loss: 0.994672, mae: 3.620274, mean_q: 4.152411
 11611/100000: episode: 1190, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 5.194, mean reward: 0.519 [0.519, 0.519], mean action: 41.100 [8.000, 79.000], mean observation: 3.171 [-1.295, 10.349], loss: 0.920191, mae: 3.621013, mean_q: 4.153084
 11621/100000: episode: 1191, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.856, mean reward: 0.386 [0.350, 0.433], mean action: 40.300 [12.000, 64.000], mean observation: 3.154 [-1.264, 10.213], loss: 0.804297, mae: 3.622069, mean_q: 4.152295
 11631/100000: episode: 1192, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.661, mean reward: 0.366 [0.294, 0.427], mean action: 47.200 [4.000, 91.000], mean observation: 3.154 [-1.464, 10.424], loss: 1.050852, mae: 3.624714, mean_q: 4.152706
 11641/100000: episode: 1193, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.454, mean reward: 0.445 [0.369, 0.579], mean action: 50.100 [10.000, 98.000], mean observation: 3.144 [-1.487, 10.392], loss: 0.827918, mae: 3.625332, mean_q: 4.155967
 11651/100000: episode: 1194, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.214, mean reward: 0.421 [0.369, 0.467], mean action: 36.900 [6.000, 55.000], mean observation: 3.166 [-1.450, 10.383], loss: 0.888349, mae: 3.626911, mean_q: 4.157083
 11661/100000: episode: 1195, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.911, mean reward: 0.491 [0.491, 0.491], mean action: 51.800 [4.000, 97.000], mean observation: 3.160 [-1.232, 10.250], loss: 0.963759, mae: 3.628240, mean_q: 4.159211
 11671/100000: episode: 1196, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.269, mean reward: 0.427 [0.318, 0.507], mean action: 42.900 [1.000, 96.000], mean observation: 3.159 [-1.316, 10.313], loss: 0.781020, mae: 3.629271, mean_q: 4.161440
 11681/100000: episode: 1197, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.828, mean reward: 0.383 [0.352, 0.430], mean action: 45.300 [37.000, 76.000], mean observation: 3.158 [-1.809, 10.265], loss: 1.131471, mae: 3.632030, mean_q: 4.157198
 11691/100000: episode: 1198, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.556, mean reward: 0.456 [0.408, 0.461], mean action: 74.200 [5.000, 95.000], mean observation: 3.171 [-0.826, 10.379], loss: 0.723683, mae: 3.630988, mean_q: 4.155065
 11701/100000: episode: 1199, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 4.511, mean reward: 0.451 [0.411, 0.456], mean action: 82.700 [0.000, 95.000], mean observation: 3.144 [-1.757, 10.350], loss: 0.897126, mae: 3.633286, mean_q: 4.157181
 11711/100000: episode: 1200, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.232, mean reward: 0.423 [0.378, 0.470], mean action: 56.000 [13.000, 95.000], mean observation: 3.163 [-1.610, 10.326], loss: 0.818016, mae: 3.634880, mean_q: 4.160099
 11721/100000: episode: 1201, duration: 0.099s, episode steps: 10, steps per second: 101, episode reward: 4.311, mean reward: 0.431 [0.358, 0.449], mean action: 85.700 [8.000, 95.000], mean observation: 3.161 [-1.057, 10.264], loss: 0.781513, mae: 3.636542, mean_q: 4.161944
 11731/100000: episode: 1202, duration: 0.117s, episode steps: 10, steps per second: 86, episode reward: 4.242, mean reward: 0.424 [0.383, 0.473], mean action: 75.100 [4.000, 99.000], mean observation: 3.165 [-1.063, 10.488], loss: 1.036532, mae: 3.638017, mean_q: 4.164691
 11741/100000: episode: 1203, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.804, mean reward: 0.380 [0.363, 0.401], mean action: 74.700 [26.000, 99.000], mean observation: 3.156 [-1.443, 10.388], loss: 0.882818, mae: 3.638768, mean_q: 4.166873
 11751/100000: episode: 1204, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 4.174, mean reward: 0.417 [0.372, 0.480], mean action: 81.200 [14.000, 96.000], mean observation: 3.163 [-1.521, 10.263], loss: 0.846268, mae: 3.639670, mean_q: 4.167702
 11761/100000: episode: 1205, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.155, mean reward: 0.416 [0.407, 0.441], mean action: 71.600 [18.000, 95.000], mean observation: 3.157 [-1.777, 10.309], loss: 0.912411, mae: 3.641682, mean_q: 4.168643
 11771/100000: episode: 1206, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.418, mean reward: 0.442 [0.395, 0.479], mean action: 54.500 [4.000, 95.000], mean observation: 3.150 [-1.707, 10.435], loss: 0.913148, mae: 3.642981, mean_q: 4.170308
 11781/100000: episode: 1207, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.095, mean reward: 0.410 [0.309, 0.598], mean action: 65.400 [15.000, 95.000], mean observation: 3.162 [-0.890, 10.555], loss: 0.903856, mae: 3.643775, mean_q: 4.167971
 11791/100000: episode: 1208, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.021, mean reward: 0.402 [0.347, 0.469], mean action: 58.500 [0.000, 95.000], mean observation: 3.153 [-1.399, 10.379], loss: 0.708118, mae: 3.644930, mean_q: 4.164838
 11801/100000: episode: 1209, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.395, mean reward: 0.439 [0.352, 0.583], mean action: 70.300 [19.000, 95.000], mean observation: 3.169 [-1.713, 10.284], loss: 1.094208, mae: 3.647378, mean_q: 4.167200
 11811/100000: episode: 1210, duration: 0.107s, episode steps: 10, steps per second: 93, episode reward: 4.578, mean reward: 0.458 [0.435, 0.467], mean action: 75.600 [0.000, 101.000], mean observation: 3.155 [-1.573, 10.299], loss: 0.840721, mae: 3.647044, mean_q: 4.166426
 11821/100000: episode: 1211, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 3.924, mean reward: 0.392 [0.346, 0.445], mean action: 70.600 [9.000, 95.000], mean observation: 3.172 [-2.671, 10.349], loss: 1.141215, mae: 3.650062, mean_q: 4.163808
 11831/100000: episode: 1212, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.120, mean reward: 0.412 [0.389, 0.549], mean action: 69.100 [12.000, 95.000], mean observation: 3.150 [-1.243, 10.301], loss: 0.964656, mae: 3.651031, mean_q: 4.166401
 11841/100000: episode: 1213, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.324, mean reward: 0.432 [0.341, 0.521], mean action: 73.800 [22.000, 95.000], mean observation: 3.159 [-1.385, 10.362], loss: 0.783056, mae: 3.651602, mean_q: 4.170089
 11851/100000: episode: 1214, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.984, mean reward: 0.398 [0.336, 0.486], mean action: 53.700 [1.000, 95.000], mean observation: 3.153 [-1.426, 10.350], loss: 1.092042, mae: 3.654345, mean_q: 4.173801
 11861/100000: episode: 1215, duration: 0.107s, episode steps: 10, steps per second: 94, episode reward: 3.218, mean reward: 0.322 [0.309, 0.437], mean action: 92.300 [77.000, 95.000], mean observation: 3.165 [-0.912, 10.352], loss: 0.911003, mae: 3.654428, mean_q: 4.172692
 11871/100000: episode: 1216, duration: 0.126s, episode steps: 10, steps per second: 80, episode reward: 4.135, mean reward: 0.414 [0.343, 0.490], mean action: 65.400 [13.000, 95.000], mean observation: 3.164 [-1.435, 10.324], loss: 0.814170, mae: 3.655262, mean_q: 4.173026
 11881/100000: episode: 1217, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.101, mean reward: 0.410 [0.356, 0.494], mean action: 63.100 [7.000, 95.000], mean observation: 3.156 [-1.654, 10.310], loss: 0.828205, mae: 3.657289, mean_q: 4.175238
 11891/100000: episode: 1218, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.195, mean reward: 0.419 [0.350, 0.499], mean action: 61.500 [36.000, 98.000], mean observation: 3.166 [-1.114, 10.300], loss: 1.074999, mae: 3.658597, mean_q: 4.175281
 11901/100000: episode: 1219, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.043, mean reward: 0.404 [0.395, 0.427], mean action: 44.000 [7.000, 94.000], mean observation: 3.158 [-1.397, 10.417], loss: 0.817935, mae: 3.658472, mean_q: 4.177595
 11911/100000: episode: 1220, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.374, mean reward: 0.437 [0.426, 0.483], mean action: 51.600 [3.000, 95.000], mean observation: 3.162 [-0.915, 10.340], loss: 0.781309, mae: 3.660292, mean_q: 4.178174
 11921/100000: episode: 1221, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.075, mean reward: 0.408 [0.395, 0.465], mean action: 63.900 [21.000, 97.000], mean observation: 3.158 [-1.237, 10.419], loss: 1.146405, mae: 3.663527, mean_q: 4.181465
 11931/100000: episode: 1222, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.668, mean reward: 0.467 [0.391, 0.491], mean action: 69.300 [8.000, 95.000], mean observation: 3.153 [-0.932, 10.407], loss: 0.795138, mae: 3.663876, mean_q: 4.185530
 11941/100000: episode: 1223, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 5.006, mean reward: 0.501 [0.499, 0.517], mean action: 82.300 [12.000, 95.000], mean observation: 3.129 [-1.307, 10.412], loss: 0.915017, mae: 3.666017, mean_q: 4.189143
 11951/100000: episode: 1224, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.625, mean reward: 0.462 [0.362, 0.515], mean action: 70.100 [4.000, 95.000], mean observation: 3.163 [-2.083, 10.299], loss: 1.066792, mae: 3.667971, mean_q: 4.194954
 11961/100000: episode: 1225, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.470, mean reward: 0.447 [0.443, 0.465], mean action: 79.800 [38.000, 95.000], mean observation: 3.177 [-1.854, 10.223], loss: 0.877523, mae: 3.669136, mean_q: 4.198668
 11971/100000: episode: 1226, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.898, mean reward: 0.390 [0.383, 0.442], mean action: 81.000 [49.000, 101.000], mean observation: 3.159 [-1.694, 10.343], loss: 0.613469, mae: 3.669009, mean_q: 4.198680
 11981/100000: episode: 1227, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.177, mean reward: 0.418 [0.294, 0.450], mean action: 62.100 [0.000, 97.000], mean observation: 3.152 [-1.335, 10.285], loss: 0.965708, mae: 3.672319, mean_q: 4.202950
 11991/100000: episode: 1228, duration: 0.107s, episode steps: 10, steps per second: 94, episode reward: 4.708, mean reward: 0.471 [0.374, 0.517], mean action: 79.500 [14.000, 101.000], mean observation: 3.144 [-0.800, 10.314], loss: 0.868807, mae: 3.672392, mean_q: 4.205462
 12001/100000: episode: 1229, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.825, mean reward: 0.383 [0.324, 0.460], mean action: 68.300 [18.000, 95.000], mean observation: 3.164 [-1.134, 10.313], loss: 1.097759, mae: 3.675361, mean_q: 4.203696
 12011/100000: episode: 1230, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.744, mean reward: 0.474 [0.354, 0.597], mean action: 73.100 [3.000, 95.000], mean observation: 3.161 [-1.174, 10.209], loss: 0.731403, mae: 3.675745, mean_q: 4.202041
 12021/100000: episode: 1231, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 3.753, mean reward: 0.375 [0.363, 0.482], mean action: 71.800 [35.000, 95.000], mean observation: 3.154 [-1.005, 10.253], loss: 0.969508, mae: 3.677533, mean_q: 4.206451
 12031/100000: episode: 1232, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.567, mean reward: 0.457 [0.457, 0.457], mean action: 79.100 [38.000, 95.000], mean observation: 3.161 [-0.938, 10.299], loss: 0.937283, mae: 3.679779, mean_q: 4.213052
 12041/100000: episode: 1233, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 3.740, mean reward: 0.374 [0.321, 0.494], mean action: 79.100 [43.000, 95.000], mean observation: 3.155 [-1.406, 10.284], loss: 0.833683, mae: 3.680323, mean_q: 4.210387
 12051/100000: episode: 1234, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.066, mean reward: 0.407 [0.358, 0.456], mean action: 37.500 [1.000, 95.000], mean observation: 3.167 [-1.428, 10.431], loss: 0.852924, mae: 3.681814, mean_q: 4.210865
 12061/100000: episode: 1235, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.611, mean reward: 0.461 [0.461, 0.461], mean action: 70.900 [6.000, 95.000], mean observation: 3.150 [-1.639, 10.291], loss: 0.767365, mae: 3.683025, mean_q: 4.211562
 12071/100000: episode: 1236, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.503, mean reward: 0.450 [0.426, 0.453], mean action: 75.200 [12.000, 95.000], mean observation: 3.163 [-1.291, 10.283], loss: 1.118025, mae: 3.686223, mean_q: 4.214578
 12081/100000: episode: 1237, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 3.931, mean reward: 0.393 [0.364, 0.425], mean action: 86.500 [37.000, 95.000], mean observation: 3.136 [-0.534, 10.221], loss: 0.924549, mae: 3.686656, mean_q: 4.219807
 12091/100000: episode: 1238, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.305, mean reward: 0.430 [0.382, 0.508], mean action: 78.700 [12.000, 98.000], mean observation: 3.154 [-1.377, 10.440], loss: 0.864072, mae: 3.687951, mean_q: 4.223842
 12101/100000: episode: 1239, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 3.792, mean reward: 0.379 [0.336, 0.394], mean action: 74.500 [23.000, 95.000], mean observation: 3.157 [-1.297, 10.310], loss: 0.885385, mae: 3.688879, mean_q: 4.228206
 12111/100000: episode: 1240, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.704, mean reward: 0.370 [0.315, 0.459], mean action: 72.300 [12.000, 95.000], mean observation: 3.162 [-1.050, 10.206], loss: 0.794432, mae: 3.690147, mean_q: 4.228406
 12121/100000: episode: 1241, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 4.088, mean reward: 0.409 [0.409, 0.409], mean action: 80.200 [32.000, 95.000], mean observation: 3.148 [-2.476, 10.455], loss: 0.992138, mae: 3.692329, mean_q: 4.229234
 12131/100000: episode: 1242, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 4.717, mean reward: 0.472 [0.472, 0.472], mean action: 82.500 [42.000, 95.000], mean observation: 3.158 [-1.775, 10.373], loss: 0.975576, mae: 3.694091, mean_q: 4.229089
 12141/100000: episode: 1243, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.368, mean reward: 0.437 [0.338, 0.484], mean action: 73.500 [24.000, 100.000], mean observation: 3.132 [-1.516, 10.262], loss: 1.053686, mae: 3.696258, mean_q: 4.227383
 12151/100000: episode: 1244, duration: 0.102s, episode steps: 10, steps per second: 98, episode reward: 3.998, mean reward: 0.400 [0.387, 0.419], mean action: 80.700 [12.000, 95.000], mean observation: 3.153 [-2.158, 10.231], loss: 0.850596, mae: 3.695825, mean_q: 4.229153
 12161/100000: episode: 1245, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.673, mean reward: 0.367 [0.358, 0.388], mean action: 82.000 [27.000, 95.000], mean observation: 3.166 [-0.885, 10.256], loss: 0.947232, mae: 3.697484, mean_q: 4.227477
 12171/100000: episode: 1246, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.117, mean reward: 0.412 [0.356, 0.500], mean action: 64.700 [2.000, 95.000], mean observation: 3.155 [-1.501, 10.274], loss: 0.944842, mae: 3.699085, mean_q: 4.225448
 12181/100000: episode: 1247, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.597, mean reward: 0.460 [0.380, 0.540], mean action: 76.500 [20.000, 95.000], mean observation: 3.136 [-1.017, 10.302], loss: 0.757804, mae: 3.698982, mean_q: 4.228631
 12191/100000: episode: 1248, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.120, mean reward: 0.412 [0.357, 0.559], mean action: 61.700 [2.000, 101.000], mean observation: 3.166 [-1.362, 10.370], loss: 0.834746, mae: 3.701397, mean_q: 4.228639
 12201/100000: episode: 1249, duration: 0.126s, episode steps: 10, steps per second: 80, episode reward: 3.833, mean reward: 0.383 [0.349, 0.394], mean action: 78.400 [29.000, 95.000], mean observation: 3.150 [-1.178, 10.367], loss: 0.921161, mae: 3.703277, mean_q: 4.229658
 12211/100000: episode: 1250, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.350, mean reward: 0.435 [0.385, 0.517], mean action: 68.700 [30.000, 95.000], mean observation: 3.156 [-1.577, 10.361], loss: 0.905977, mae: 3.704402, mean_q: 4.227628
 12221/100000: episode: 1251, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.941, mean reward: 0.394 [0.324, 0.429], mean action: 56.600 [1.000, 101.000], mean observation: 3.155 [-1.764, 10.348], loss: 0.994477, mae: 3.706694, mean_q: 4.221475
 12231/100000: episode: 1252, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.074, mean reward: 0.407 [0.307, 0.523], mean action: 78.500 [29.000, 95.000], mean observation: 3.160 [-1.633, 10.251], loss: 0.999283, mae: 3.707574, mean_q: 4.221151
 12241/100000: episode: 1253, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 3.653, mean reward: 0.365 [0.305, 0.555], mean action: 91.300 [56.000, 97.000], mean observation: 3.164 [-0.837, 10.407], loss: 0.907708, mae: 3.708511, mean_q: 4.215806
 12251/100000: episode: 1254, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.120, mean reward: 0.412 [0.380, 0.471], mean action: 73.500 [26.000, 99.000], mean observation: 3.154 [-1.702, 10.236], loss: 0.771784, mae: 3.709483, mean_q: 4.217135
 12261/100000: episode: 1255, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 3.966, mean reward: 0.397 [0.367, 0.459], mean action: 77.600 [1.000, 95.000], mean observation: 3.161 [-0.767, 10.412], loss: 0.846645, mae: 3.711210, mean_q: 4.218318
 12271/100000: episode: 1256, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.034, mean reward: 0.403 [0.351, 0.443], mean action: 55.600 [2.000, 97.000], mean observation: 3.165 [-1.023, 10.450], loss: 0.732043, mae: 3.712344, mean_q: 4.222089
 12281/100000: episode: 1257, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.284, mean reward: 0.428 [0.364, 0.579], mean action: 73.700 [2.000, 95.000], mean observation: 3.152 [-1.337, 10.341], loss: 1.039346, mae: 3.715320, mean_q: 4.226023
 12291/100000: episode: 1258, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.978, mean reward: 0.398 [0.345, 0.459], mean action: 64.000 [15.000, 95.000], mean observation: 3.149 [-1.031, 10.305], loss: 0.821102, mae: 3.715116, mean_q: 4.230286
 12301/100000: episode: 1259, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.774, mean reward: 0.477 [0.319, 0.505], mean action: 79.400 [4.000, 96.000], mean observation: 3.151 [-1.758, 10.255], loss: 0.784868, mae: 3.716486, mean_q: 4.235864
 12311/100000: episode: 1260, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.712, mean reward: 0.371 [0.316, 0.417], mean action: 69.800 [6.000, 95.000], mean observation: 3.164 [-1.158, 10.399], loss: 0.816196, mae: 3.718337, mean_q: 4.237207
 12321/100000: episode: 1261, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.371, mean reward: 0.437 [0.420, 0.554], mean action: 67.200 [19.000, 95.000], mean observation: 3.162 [-1.708, 10.445], loss: 1.013781, mae: 3.720950, mean_q: 4.235452
 12331/100000: episode: 1262, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.948, mean reward: 0.395 [0.339, 0.465], mean action: 44.700 [10.000, 95.000], mean observation: 3.154 [-0.913, 10.374], loss: 0.864494, mae: 3.720788, mean_q: 4.233196
 12337/100000: episode: 1263, duration: 0.111s, episode steps: 6, steps per second: 54, episode reward: 11.946, mean reward: 1.991 [0.319, 10.000], mean action: 19.833 [10.000, 69.000], mean observation: 3.150 [-1.268, 10.315], loss: 0.869365, mae: 3.722320, mean_q: 4.235812
 12347/100000: episode: 1264, duration: 0.215s, episode steps: 10, steps per second: 46, episode reward: 4.233, mean reward: 0.423 [0.343, 0.587], mean action: 23.300 [10.000, 89.000], mean observation: 3.149 [-1.027, 10.234], loss: 0.975057, mae: 3.723741, mean_q: 4.238294
 12357/100000: episode: 1265, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 4.779, mean reward: 0.478 [0.328, 0.553], mean action: 18.900 [0.000, 52.000], mean observation: 3.154 [-1.628, 10.530], loss: 0.930826, mae: 3.724276, mean_q: 4.236940
 12367/100000: episode: 1266, duration: 0.112s, episode steps: 10, steps per second: 90, episode reward: 3.420, mean reward: 0.342 [0.300, 0.419], mean action: 73.000 [10.000, 95.000], mean observation: 3.144 [-1.269, 10.206], loss: 0.777507, mae: 3.724988, mean_q: 4.230350
 12377/100000: episode: 1267, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.251, mean reward: 0.425 [0.413, 0.442], mean action: 57.700 [6.000, 95.000], mean observation: 3.157 [-0.759, 10.307], loss: 0.755951, mae: 3.726137, mean_q: 4.232442
 12387/100000: episode: 1268, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.653, mean reward: 0.365 [0.314, 0.448], mean action: 78.600 [15.000, 96.000], mean observation: 3.161 [-0.991, 10.377], loss: 0.869706, mae: 3.728012, mean_q: 4.233005
 12397/100000: episode: 1269, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 4.072, mean reward: 0.407 [0.330, 0.454], mean action: 48.900 [10.000, 95.000], mean observation: 3.144 [-1.492, 10.383], loss: 0.911315, mae: 3.729709, mean_q: 4.224721
 12407/100000: episode: 1270, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 3.658, mean reward: 0.366 [0.311, 0.413], mean action: 29.600 [10.000, 88.000], mean observation: 3.154 [-1.185, 10.326], loss: 0.914779, mae: 3.730560, mean_q: 4.225760
 12417/100000: episode: 1271, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.229, mean reward: 0.423 [0.292, 0.503], mean action: 23.400 [10.000, 81.000], mean observation: 3.153 [-1.920, 10.362], loss: 1.181754, mae: 3.733464, mean_q: 4.223435
 12427/100000: episode: 1272, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.943, mean reward: 0.394 [0.311, 0.470], mean action: 21.000 [10.000, 90.000], mean observation: 3.161 [-2.036, 10.428], loss: 1.112136, mae: 3.734384, mean_q: 4.221164
 12437/100000: episode: 1273, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.564, mean reward: 0.456 [0.399, 0.490], mean action: 25.700 [1.000, 89.000], mean observation: 3.156 [-1.628, 10.404], loss: 0.923505, mae: 3.734452, mean_q: 4.223886
 12447/100000: episode: 1274, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.856, mean reward: 0.386 [0.255, 0.519], mean action: 45.600 [9.000, 95.000], mean observation: 3.168 [-0.695, 10.428], loss: 1.136922, mae: 3.736861, mean_q: 4.229260
 12457/100000: episode: 1275, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.577, mean reward: 0.358 [0.311, 0.401], mean action: 40.100 [0.000, 86.000], mean observation: 3.157 [-1.390, 10.298], loss: 0.874288, mae: 3.737206, mean_q: 4.232907
 12467/100000: episode: 1276, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.956, mean reward: 0.396 [0.340, 0.465], mean action: 49.800 [5.000, 100.000], mean observation: 3.135 [-1.253, 10.299], loss: 0.990542, mae: 3.739115, mean_q: 4.235018
 12477/100000: episode: 1277, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.142, mean reward: 0.414 [0.346, 0.471], mean action: 29.800 [10.000, 81.000], mean observation: 3.168 [-1.558, 10.311], loss: 0.887532, mae: 3.739700, mean_q: 4.237078
 12487/100000: episode: 1278, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.471, mean reward: 0.447 [0.379, 0.493], mean action: 35.400 [10.000, 83.000], mean observation: 3.167 [-1.224, 10.424], loss: 0.783226, mae: 3.740494, mean_q: 4.240224
 12497/100000: episode: 1279, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.309, mean reward: 0.431 [0.345, 0.521], mean action: 47.900 [3.000, 98.000], mean observation: 3.169 [-1.212, 10.341], loss: 0.847297, mae: 3.742519, mean_q: 4.242494
 12507/100000: episode: 1280, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.008, mean reward: 0.401 [0.335, 0.535], mean action: 32.600 [10.000, 84.000], mean observation: 3.153 [-1.331, 10.298], loss: 0.943606, mae: 3.744166, mean_q: 4.242552
 12517/100000: episode: 1281, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.931, mean reward: 0.393 [0.346, 0.469], mean action: 46.100 [7.000, 101.000], mean observation: 3.154 [-1.612, 10.286], loss: 0.979575, mae: 3.745536, mean_q: 4.240148
 12527/100000: episode: 1282, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.110, mean reward: 0.411 [0.364, 0.527], mean action: 34.200 [4.000, 101.000], mean observation: 3.154 [-1.252, 10.322], loss: 0.920407, mae: 3.747269, mean_q: 4.242352
 12537/100000: episode: 1283, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.170, mean reward: 0.417 [0.359, 0.509], mean action: 27.900 [10.000, 93.000], mean observation: 3.149 [-1.845, 10.359], loss: 0.797013, mae: 3.747822, mean_q: 4.245965
 12547/100000: episode: 1284, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.074, mean reward: 0.407 [0.327, 0.496], mean action: 18.200 [1.000, 51.000], mean observation: 3.160 [-1.399, 10.298], loss: 0.873500, mae: 3.749959, mean_q: 4.248780
 12557/100000: episode: 1285, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.648, mean reward: 0.465 [0.314, 0.656], mean action: 19.800 [1.000, 82.000], mean observation: 3.157 [-1.124, 10.479], loss: 0.826077, mae: 3.750457, mean_q: 4.246096
 12567/100000: episode: 1286, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.273, mean reward: 0.427 [0.340, 0.567], mean action: 25.800 [10.000, 90.000], mean observation: 3.152 [-1.834, 10.401], loss: 0.988233, mae: 3.752480, mean_q: 4.239776
 12577/100000: episode: 1287, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.164, mean reward: 0.416 [0.319, 0.501], mean action: 29.700 [10.000, 95.000], mean observation: 3.149 [-2.438, 10.340], loss: 1.024685, mae: 3.753561, mean_q: 4.236914
 12587/100000: episode: 1288, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.126, mean reward: 0.413 [0.318, 0.479], mean action: 28.800 [10.000, 75.000], mean observation: 3.154 [-1.999, 10.307], loss: 0.797896, mae: 3.754375, mean_q: 4.234639
 12592/100000: episode: 1289, duration: 0.097s, episode steps: 5, steps per second: 51, episode reward: 11.517, mean reward: 2.303 [0.322, 10.000], mean action: 31.200 [10.000, 92.000], mean observation: 3.153 [-1.142, 10.238], loss: 0.948746, mae: 3.756244, mean_q: 4.235795
 12602/100000: episode: 1290, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.012, mean reward: 0.401 [0.320, 0.503], mean action: 40.300 [10.000, 100.000], mean observation: 3.148 [-1.137, 10.324], loss: 0.805327, mae: 3.756851, mean_q: 4.238409
 12612/100000: episode: 1291, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.203, mean reward: 0.420 [0.357, 0.511], mean action: 40.400 [10.000, 98.000], mean observation: 3.157 [-1.483, 10.367], loss: 0.932915, mae: 3.758720, mean_q: 4.242225
 12622/100000: episode: 1292, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.386, mean reward: 0.439 [0.373, 0.504], mean action: 26.300 [10.000, 82.000], mean observation: 3.163 [-2.029, 10.353], loss: 0.856124, mae: 3.759773, mean_q: 4.246138
 12632/100000: episode: 1293, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.084, mean reward: 0.408 [0.288, 0.487], mean action: 24.600 [1.000, 70.000], mean observation: 3.156 [-2.768, 10.423], loss: 0.854254, mae: 3.761538, mean_q: 4.249567
 12642/100000: episode: 1294, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.155, mean reward: 0.415 [0.321, 0.475], mean action: 26.100 [5.000, 82.000], mean observation: 3.153 [-1.168, 10.321], loss: 0.685648, mae: 3.762518, mean_q: 4.249396
 12652/100000: episode: 1295, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.002, mean reward: 0.400 [0.341, 0.490], mean action: 26.800 [4.000, 60.000], mean observation: 3.151 [-1.570, 10.373], loss: 0.947343, mae: 3.764628, mean_q: 4.249536
 12662/100000: episode: 1296, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.844, mean reward: 0.384 [0.303, 0.519], mean action: 25.500 [10.000, 86.000], mean observation: 3.146 [-1.308, 10.405], loss: 0.874756, mae: 3.765824, mean_q: 4.254882
 12672/100000: episode: 1297, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 3.883, mean reward: 0.388 [0.309, 0.436], mean action: 26.900 [8.000, 70.000], mean observation: 3.164 [-1.568, 10.405], loss: 0.947376, mae: 3.767638, mean_q: 4.265597
 12682/100000: episode: 1298, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.029, mean reward: 0.403 [0.331, 0.574], mean action: 35.400 [10.000, 73.000], mean observation: 3.153 [-1.186, 10.319], loss: 0.888530, mae: 3.768190, mean_q: 4.272033
 12687/100000: episode: 1299, duration: 0.094s, episode steps: 5, steps per second: 53, episode reward: 11.545, mean reward: 2.309 [0.291, 10.000], mean action: 39.800 [10.000, 87.000], mean observation: 3.154 [-1.476, 10.291], loss: 0.882706, mae: 3.769031, mean_q: 4.275384
 12697/100000: episode: 1300, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.892, mean reward: 0.489 [0.446, 0.549], mean action: 35.100 [3.000, 85.000], mean observation: 3.154 [-1.649, 10.347], loss: 0.984465, mae: 3.770591, mean_q: 4.280006
 12707/100000: episode: 1301, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.955, mean reward: 0.396 [0.361, 0.438], mean action: 25.500 [3.000, 78.000], mean observation: 3.155 [-0.907, 10.411], loss: 0.907362, mae: 3.771434, mean_q: 4.285534
 12717/100000: episode: 1302, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.041, mean reward: 0.404 [0.313, 0.539], mean action: 24.500 [10.000, 81.000], mean observation: 3.164 [-1.250, 10.434], loss: 0.882687, mae: 3.773104, mean_q: 4.286632
 12727/100000: episode: 1303, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.268, mean reward: 0.427 [0.337, 0.537], mean action: 35.900 [10.000, 100.000], mean observation: 3.151 [-1.677, 10.446], loss: 0.857382, mae: 3.773782, mean_q: 4.288049
 12737/100000: episode: 1304, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.079, mean reward: 0.408 [0.368, 0.465], mean action: 34.900 [3.000, 94.000], mean observation: 3.157 [-1.650, 10.295], loss: 0.881712, mae: 3.775326, mean_q: 4.290590
 12747/100000: episode: 1305, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.524, mean reward: 0.452 [0.332, 0.543], mean action: 22.900 [1.000, 63.000], mean observation: 3.158 [-1.961, 10.423], loss: 0.807993, mae: 3.775216, mean_q: 4.293523
 12757/100000: episode: 1306, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.971, mean reward: 0.397 [0.333, 0.446], mean action: 36.000 [10.000, 101.000], mean observation: 3.154 [-1.286, 10.470], loss: 1.053669, mae: 3.777847, mean_q: 4.297807
 12767/100000: episode: 1307, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.994, mean reward: 0.399 [0.310, 0.504], mean action: 37.800 [1.000, 83.000], mean observation: 3.165 [-2.411, 10.398], loss: 1.170119, mae: 3.779227, mean_q: 4.300235
 12777/100000: episode: 1308, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.763, mean reward: 0.376 [0.335, 0.414], mean action: 27.800 [6.000, 92.000], mean observation: 3.159 [-1.384, 10.215], loss: 0.820134, mae: 3.779534, mean_q: 4.306234
 12787/100000: episode: 1309, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.149, mean reward: 0.415 [0.334, 0.549], mean action: 28.700 [10.000, 79.000], mean observation: 3.148 [-1.112, 10.432], loss: 1.012103, mae: 3.781782, mean_q: 4.311590
 12797/100000: episode: 1310, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.147, mean reward: 0.415 [0.387, 0.533], mean action: 42.600 [10.000, 98.000], mean observation: 3.150 [-1.359, 10.191], loss: 1.010780, mae: 3.782821, mean_q: 4.317393
 12807/100000: episode: 1311, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.073, mean reward: 0.407 [0.343, 0.523], mean action: 25.000 [7.000, 85.000], mean observation: 3.161 [-1.280, 10.421], loss: 0.835639, mae: 3.783725, mean_q: 4.315459
 12817/100000: episode: 1312, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.821, mean reward: 0.382 [0.323, 0.446], mean action: 20.500 [0.000, 78.000], mean observation: 3.159 [-2.002, 10.383], loss: 0.978216, mae: 3.785245, mean_q: 4.308818
 12827/100000: episode: 1313, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.009, mean reward: 0.401 [0.350, 0.520], mean action: 17.700 [1.000, 82.000], mean observation: 3.156 [-1.277, 10.363], loss: 0.862704, mae: 3.785596, mean_q: 4.305703
 12837/100000: episode: 1314, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.380, mean reward: 0.438 [0.382, 0.494], mean action: 40.200 [10.000, 100.000], mean observation: 3.150 [-1.739, 10.385], loss: 0.936203, mae: 3.787017, mean_q: 4.301133
 12847/100000: episode: 1315, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.249, mean reward: 0.425 [0.394, 0.520], mean action: 17.000 [8.000, 69.000], mean observation: 3.165 [-2.171, 10.349], loss: 0.877684, mae: 3.787963, mean_q: 4.295178
 12857/100000: episode: 1316, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.277, mean reward: 0.428 [0.347, 0.495], mean action: 18.600 [6.000, 85.000], mean observation: 3.151 [-1.317, 10.410], loss: 0.781605, mae: 3.789098, mean_q: 4.294989
 12867/100000: episode: 1317, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.885, mean reward: 0.389 [0.304, 0.469], mean action: 24.600 [10.000, 85.000], mean observation: 3.164 [-1.021, 10.313], loss: 1.065946, mae: 3.791841, mean_q: 4.295655
 12877/100000: episode: 1318, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.974, mean reward: 0.397 [0.370, 0.436], mean action: 35.700 [10.000, 83.000], mean observation: 3.154 [-1.496, 10.432], loss: 0.887807, mae: 3.792537, mean_q: 4.298274
 12887/100000: episode: 1319, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.141, mean reward: 0.414 [0.318, 0.541], mean action: 22.300 [10.000, 71.000], mean observation: 3.157 [-1.315, 10.342], loss: 0.790115, mae: 3.793226, mean_q: 4.303250
 12897/100000: episode: 1320, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.102, mean reward: 0.410 [0.359, 0.504], mean action: 19.100 [7.000, 56.000], mean observation: 3.156 [-2.029, 10.223], loss: 0.865653, mae: 3.794554, mean_q: 4.306973
 12907/100000: episode: 1321, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.233, mean reward: 0.423 [0.342, 0.517], mean action: 18.400 [10.000, 42.000], mean observation: 3.158 [-1.709, 10.374], loss: 0.982525, mae: 3.797429, mean_q: 4.306259
 12917/100000: episode: 1322, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.084, mean reward: 0.408 [0.316, 0.486], mean action: 37.000 [4.000, 88.000], mean observation: 3.164 [-1.603, 10.236], loss: 1.126652, mae: 3.799090, mean_q: 4.300393
 12927/100000: episode: 1323, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.193, mean reward: 0.419 [0.370, 0.517], mean action: 18.800 [10.000, 81.000], mean observation: 3.136 [-1.987, 10.382], loss: 0.767758, mae: 3.799072, mean_q: 4.301564
 12937/100000: episode: 1324, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.443, mean reward: 0.444 [0.365, 0.542], mean action: 20.400 [10.000, 77.000], mean observation: 3.160 [-1.338, 10.313], loss: 1.019905, mae: 3.801610, mean_q: 4.305678
 12947/100000: episode: 1325, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.144, mean reward: 0.414 [0.300, 0.499], mean action: 29.900 [10.000, 95.000], mean observation: 3.159 [-1.845, 10.283], loss: 0.832903, mae: 3.802172, mean_q: 4.304939
 12957/100000: episode: 1326, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.036, mean reward: 0.404 [0.365, 0.491], mean action: 21.200 [10.000, 80.000], mean observation: 3.162 [-2.058, 10.319], loss: 1.019355, mae: 3.803841, mean_q: 4.301952
 12967/100000: episode: 1327, duration: 0.231s, episode steps: 10, steps per second: 43, episode reward: 4.183, mean reward: 0.418 [0.334, 0.474], mean action: 22.500 [10.000, 82.000], mean observation: 3.153 [-2.196, 10.261], loss: 1.007805, mae: 3.804391, mean_q: 4.302940
 12977/100000: episode: 1328, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 3.731, mean reward: 0.373 [0.297, 0.436], mean action: 27.500 [10.000, 83.000], mean observation: 3.151 [-1.617, 10.325], loss: 0.837609, mae: 3.805650, mean_q: 4.299270
 12987/100000: episode: 1329, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.002, mean reward: 0.400 [0.298, 0.616], mean action: 27.200 [4.000, 69.000], mean observation: 3.151 [-1.575, 10.194], loss: 0.982942, mae: 3.807318, mean_q: 4.297500
 12997/100000: episode: 1330, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.471, mean reward: 0.347 [0.294, 0.435], mean action: 39.000 [10.000, 95.000], mean observation: 3.164 [-1.530, 10.305], loss: 0.847206, mae: 3.807970, mean_q: 4.294780
 13007/100000: episode: 1331, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.187, mean reward: 0.419 [0.340, 0.507], mean action: 41.400 [10.000, 99.000], mean observation: 3.148 [-1.336, 10.308], loss: 0.873060, mae: 3.810191, mean_q: 4.293106
 13017/100000: episode: 1332, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.348, mean reward: 0.435 [0.430, 0.481], mean action: 40.300 [10.000, 93.000], mean observation: 3.150 [-1.076, 10.346], loss: 0.711402, mae: 3.810612, mean_q: 4.291083
 13027/100000: episode: 1333, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.831, mean reward: 0.383 [0.350, 0.443], mean action: 49.800 [27.000, 96.000], mean observation: 3.158 [-1.057, 10.485], loss: 0.759974, mae: 3.812360, mean_q: 4.292162
 13037/100000: episode: 1334, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.899, mean reward: 0.390 [0.384, 0.416], mean action: 38.500 [10.000, 71.000], mean observation: 3.140 [-1.271, 10.199], loss: 0.930394, mae: 3.814077, mean_q: 4.290143
 13047/100000: episode: 1335, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.584, mean reward: 0.458 [0.356, 0.485], mean action: 40.500 [10.000, 96.000], mean observation: 3.148 [-1.160, 10.411], loss: 0.762755, mae: 3.814986, mean_q: 4.288958
 13057/100000: episode: 1336, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.842, mean reward: 0.384 [0.316, 0.532], mean action: 44.000 [7.000, 85.000], mean observation: 3.155 [-1.909, 10.342], loss: 0.724753, mae: 3.816203, mean_q: 4.290172
 13067/100000: episode: 1337, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.085, mean reward: 0.408 [0.289, 0.573], mean action: 22.800 [3.000, 37.000], mean observation: 3.157 [-1.411, 10.295], loss: 0.910177, mae: 3.819467, mean_q: 4.288987
 13077/100000: episode: 1338, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.096, mean reward: 0.410 [0.392, 0.447], mean action: 37.700 [10.000, 71.000], mean observation: 3.150 [-1.596, 10.337], loss: 0.796763, mae: 3.819816, mean_q: 4.287807
 13087/100000: episode: 1339, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.078, mean reward: 0.408 [0.376, 0.480], mean action: 30.700 [9.000, 54.000], mean observation: 3.161 [-1.441, 10.358], loss: 1.023737, mae: 3.822249, mean_q: 4.289504
 13097/100000: episode: 1340, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.712, mean reward: 0.371 [0.322, 0.447], mean action: 47.200 [9.000, 99.000], mean observation: 3.154 [-1.278, 10.368], loss: 1.163811, mae: 3.824158, mean_q: 4.294540
 13107/100000: episode: 1341, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.113, mean reward: 0.411 [0.343, 0.498], mean action: 37.100 [10.000, 91.000], mean observation: 3.153 [-1.717, 10.357], loss: 0.737367, mae: 3.823071, mean_q: 4.297592
 13114/100000: episode: 1342, duration: 0.163s, episode steps: 7, steps per second: 43, episode reward: 12.677, mean reward: 1.811 [0.404, 10.000], mean action: 18.857 [10.000, 63.000], mean observation: 3.166 [-1.184, 10.316], loss: 0.942731, mae: 3.825582, mean_q: 4.300697
 13124/100000: episode: 1343, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.099, mean reward: 0.410 [0.329, 0.506], mean action: 17.200 [10.000, 82.000], mean observation: 3.156 [-1.545, 10.514], loss: 0.848776, mae: 3.825514, mean_q: 4.301009
 13134/100000: episode: 1344, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.149, mean reward: 0.415 [0.342, 0.508], mean action: 41.100 [10.000, 99.000], mean observation: 3.150 [-1.627, 10.312], loss: 0.938820, mae: 3.827634, mean_q: 4.297989
 13144/100000: episode: 1345, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.228, mean reward: 0.423 [0.380, 0.558], mean action: 36.900 [10.000, 85.000], mean observation: 3.158 [-1.433, 10.376], loss: 1.019727, mae: 3.829192, mean_q: 4.294298
 13154/100000: episode: 1346, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.417, mean reward: 0.442 [0.366, 0.540], mean action: 29.000 [10.000, 87.000], mean observation: 3.149 [-1.352, 10.221], loss: 1.007462, mae: 3.830809, mean_q: 4.293915
 13164/100000: episode: 1347, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.271, mean reward: 0.427 [0.366, 0.527], mean action: 33.300 [4.000, 95.000], mean observation: 3.152 [-1.203, 10.319], loss: 0.845849, mae: 3.831207, mean_q: 4.294466
 13174/100000: episode: 1348, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.135, mean reward: 0.413 [0.326, 0.530], mean action: 16.700 [0.000, 42.000], mean observation: 3.157 [-1.684, 10.321], loss: 0.863852, mae: 3.832814, mean_q: 4.301052
 13184/100000: episode: 1349, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.013, mean reward: 0.401 [0.338, 0.446], mean action: 31.700 [7.000, 101.000], mean observation: 3.149 [-1.585, 10.296], loss: 0.892352, mae: 3.834513, mean_q: 4.306686
 13194/100000: episode: 1350, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.071, mean reward: 0.407 [0.359, 0.473], mean action: 41.000 [10.000, 96.000], mean observation: 3.147 [-1.304, 10.352], loss: 0.871476, mae: 3.835646, mean_q: 4.311683
 13204/100000: episode: 1351, duration: 0.242s, episode steps: 10, steps per second: 41, episode reward: 4.797, mean reward: 0.480 [0.480, 0.480], mean action: 18.800 [10.000, 98.000], mean observation: 3.147 [-1.819, 10.372], loss: 0.874274, mae: 3.837135, mean_q: 4.315097
 13214/100000: episode: 1352, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.128, mean reward: 0.413 [0.334, 0.446], mean action: 21.300 [10.000, 77.000], mean observation: 3.150 [-1.766, 10.380], loss: 1.061130, mae: 3.839130, mean_q: 4.319235
 13224/100000: episode: 1353, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.433, mean reward: 0.443 [0.363, 0.567], mean action: 28.300 [8.000, 93.000], mean observation: 3.169 [-1.664, 10.503], loss: 1.123049, mae: 3.840930, mean_q: 4.320849
 13234/100000: episode: 1354, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.301, mean reward: 0.430 [0.322, 0.543], mean action: 33.700 [10.000, 69.000], mean observation: 3.155 [-1.307, 10.503], loss: 0.911636, mae: 3.841588, mean_q: 4.322551
 13244/100000: episode: 1355, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.362, mean reward: 0.436 [0.348, 0.591], mean action: 26.400 [1.000, 84.000], mean observation: 3.153 [-2.337, 10.396], loss: 0.849798, mae: 3.842257, mean_q: 4.321078
 13254/100000: episode: 1356, duration: 0.227s, episode steps: 10, steps per second: 44, episode reward: 3.888, mean reward: 0.389 [0.327, 0.437], mean action: 26.200 [10.000, 97.000], mean observation: 3.145 [-2.058, 10.307], loss: 0.795160, mae: 3.842495, mean_q: 4.323986
 13264/100000: episode: 1357, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.119, mean reward: 0.412 [0.310, 0.525], mean action: 24.300 [10.000, 69.000], mean observation: 3.158 [-1.943, 10.382], loss: 0.974872, mae: 3.845136, mean_q: 4.328056
 13274/100000: episode: 1358, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.969, mean reward: 0.397 [0.339, 0.525], mean action: 42.400 [10.000, 100.000], mean observation: 3.150 [-1.370, 10.416], loss: 0.803228, mae: 3.845813, mean_q: 4.334960
 13284/100000: episode: 1359, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 3.922, mean reward: 0.392 [0.336, 0.475], mean action: 27.600 [10.000, 58.000], mean observation: 3.170 [-1.364, 10.361], loss: 0.810246, mae: 3.847463, mean_q: 4.340618
 13294/100000: episode: 1360, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.029, mean reward: 0.403 [0.346, 0.462], mean action: 39.400 [3.000, 91.000], mean observation: 3.163 [-1.203, 10.409], loss: 0.748227, mae: 3.848288, mean_q: 4.345157
 13304/100000: episode: 1361, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.454, mean reward: 0.445 [0.390, 0.515], mean action: 24.700 [10.000, 66.000], mean observation: 3.149 [-1.455, 10.179], loss: 0.847117, mae: 3.850118, mean_q: 4.347394
 13314/100000: episode: 1362, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 3.778, mean reward: 0.378 [0.326, 0.427], mean action: 31.200 [10.000, 76.000], mean observation: 3.148 [-1.017, 10.374], loss: 0.787597, mae: 3.851775, mean_q: 4.350747
 13324/100000: episode: 1363, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.040, mean reward: 0.404 [0.344, 0.479], mean action: 25.800 [10.000, 82.000], mean observation: 3.149 [-1.407, 10.301], loss: 0.858312, mae: 3.852995, mean_q: 4.352519
 13334/100000: episode: 1364, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.233, mean reward: 0.423 [0.287, 0.587], mean action: 19.800 [10.000, 51.000], mean observation: 3.157 [-1.256, 10.426], loss: 0.909127, mae: 3.855119, mean_q: 4.353461
 13344/100000: episode: 1365, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.317, mean reward: 0.432 [0.343, 0.547], mean action: 18.900 [10.000, 88.000], mean observation: 3.149 [-1.316, 10.508], loss: 0.891986, mae: 3.856739, mean_q: 4.348584
 13354/100000: episode: 1366, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.834, mean reward: 0.383 [0.381, 0.405], mean action: 36.400 [10.000, 98.000], mean observation: 3.162 [-0.759, 10.305], loss: 0.951869, mae: 3.858555, mean_q: 4.341388
 13364/100000: episode: 1367, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.178, mean reward: 0.418 [0.373, 0.497], mean action: 34.300 [10.000, 93.000], mean observation: 3.152 [-1.568, 10.370], loss: 0.928037, mae: 3.859894, mean_q: 4.339356
 13374/100000: episode: 1368, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.486, mean reward: 0.449 [0.394, 0.500], mean action: 32.900 [10.000, 84.000], mean observation: 3.160 [-1.204, 10.212], loss: 1.091389, mae: 3.861926, mean_q: 4.337708
 13384/100000: episode: 1369, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.903, mean reward: 0.390 [0.369, 0.442], mean action: 48.900 [10.000, 99.000], mean observation: 3.154 [-1.083, 10.337], loss: 0.665379, mae: 3.860025, mean_q: 4.335890
 13394/100000: episode: 1370, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 3.839, mean reward: 0.384 [0.334, 0.456], mean action: 12.300 [8.000, 21.000], mean observation: 3.149 [-1.475, 10.325], loss: 0.926453, mae: 3.862785, mean_q: 4.336336
 13404/100000: episode: 1371, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.350, mean reward: 0.435 [0.332, 0.510], mean action: 30.100 [0.000, 86.000], mean observation: 3.157 [-1.201, 10.269], loss: 1.041092, mae: 3.864480, mean_q: 4.334475
 13414/100000: episode: 1372, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.337, mean reward: 0.434 [0.358, 0.575], mean action: 39.800 [10.000, 85.000], mean observation: 3.166 [-1.433, 10.276], loss: 0.878993, mae: 3.864947, mean_q: 4.333452
 13424/100000: episode: 1373, duration: 0.215s, episode steps: 10, steps per second: 47, episode reward: 3.914, mean reward: 0.391 [0.329, 0.461], mean action: 21.600 [10.000, 88.000], mean observation: 3.156 [-1.172, 10.473], loss: 0.950949, mae: 3.866726, mean_q: 4.333693
 13434/100000: episode: 1374, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.740, mean reward: 0.374 [0.302, 0.459], mean action: 33.700 [10.000, 92.000], mean observation: 3.156 [-1.741, 10.326], loss: 0.768139, mae: 3.867752, mean_q: 4.330698
 13444/100000: episode: 1375, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.206, mean reward: 0.421 [0.325, 0.525], mean action: 36.500 [7.000, 96.000], mean observation: 3.159 [-1.149, 10.368], loss: 0.909749, mae: 3.869348, mean_q: 4.330850
 13454/100000: episode: 1376, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.326, mean reward: 0.433 [0.362, 0.511], mean action: 31.400 [6.000, 94.000], mean observation: 3.155 [-1.377, 10.283], loss: 0.919082, mae: 3.871312, mean_q: 4.332823
 13458/100000: episode: 1377, duration: 0.123s, episode steps: 4, steps per second: 32, episode reward: 11.183, mean reward: 2.796 [0.385, 10.000], mean action: 19.250 [10.000, 42.000], mean observation: 3.146 [-0.588, 10.335], loss: 0.925509, mae: 3.872492, mean_q: 4.332726
 13468/100000: episode: 1378, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.230, mean reward: 0.423 [0.330, 0.515], mean action: 33.000 [10.000, 94.000], mean observation: 3.156 [-1.263, 10.398], loss: 1.083071, mae: 3.873816, mean_q: 4.330503
 13478/100000: episode: 1379, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.983, mean reward: 0.398 [0.343, 0.486], mean action: 27.500 [1.000, 68.000], mean observation: 3.150 [-1.370, 10.413], loss: 1.054860, mae: 3.874313, mean_q: 4.331131
 13488/100000: episode: 1380, duration: 0.232s, episode steps: 10, steps per second: 43, episode reward: 3.611, mean reward: 0.361 [0.313, 0.413], mean action: 12.400 [10.000, 23.000], mean observation: 3.155 [-1.582, 10.290], loss: 1.006572, mae: 3.874960, mean_q: 4.330386
 13498/100000: episode: 1381, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.197, mean reward: 0.420 [0.320, 0.553], mean action: 37.600 [3.000, 82.000], mean observation: 3.159 [-1.340, 10.363], loss: 0.973889, mae: 3.876488, mean_q: 4.327692
 13508/100000: episode: 1382, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 3.853, mean reward: 0.385 [0.291, 0.477], mean action: 25.900 [10.000, 93.000], mean observation: 3.157 [-1.415, 10.385], loss: 0.883895, mae: 3.877193, mean_q: 4.327258
 13518/100000: episode: 1383, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.217, mean reward: 0.422 [0.327, 0.500], mean action: 20.900 [10.000, 95.000], mean observation: 3.156 [-1.928, 10.444], loss: 1.234058, mae: 3.879877, mean_q: 4.327260
 13528/100000: episode: 1384, duration: 0.105s, episode steps: 10, steps per second: 96, episode reward: 4.274, mean reward: 0.427 [0.398, 0.541], mean action: 76.500 [17.000, 96.000], mean observation: 3.174 [-0.984, 10.331], loss: 1.038349, mae: 3.880079, mean_q: 4.327547
 13538/100000: episode: 1385, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 5.252, mean reward: 0.525 [0.422, 0.551], mean action: 87.400 [27.000, 97.000], mean observation: 3.151 [-0.668, 10.301], loss: 0.782230, mae: 3.879591, mean_q: 4.328901
 13548/100000: episode: 1386, duration: 0.099s, episode steps: 10, steps per second: 101, episode reward: 4.009, mean reward: 0.401 [0.332, 0.510], mean action: 82.700 [49.000, 97.000], mean observation: 3.157 [-1.564, 10.357], loss: 0.773984, mae: 3.881025, mean_q: 4.330748
 13558/100000: episode: 1387, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.643, mean reward: 0.464 [0.376, 0.542], mean action: 76.700 [26.000, 95.000], mean observation: 3.159 [-1.190, 10.362], loss: 0.809861, mae: 3.882733, mean_q: 4.330143
 13565/100000: episode: 1388, duration: 0.098s, episode steps: 7, steps per second: 71, episode reward: 12.625, mean reward: 1.804 [0.342, 10.000], mean action: 67.857 [7.000, 95.000], mean observation: 3.159 [-1.181, 10.213], loss: 0.812116, mae: 3.883169, mean_q: 4.330507
 13575/100000: episode: 1389, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.120, mean reward: 0.412 [0.366, 0.476], mean action: 62.000 [3.000, 101.000], mean observation: 3.159 [-2.838, 10.240], loss: 0.731881, mae: 3.884187, mean_q: 4.332891
 13585/100000: episode: 1390, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.982, mean reward: 0.498 [0.394, 0.510], mean action: 70.400 [16.000, 95.000], mean observation: 3.146 [-1.530, 10.303], loss: 0.816413, mae: 3.886300, mean_q: 4.336074
 13595/100000: episode: 1391, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.901, mean reward: 0.390 [0.381, 0.398], mean action: 64.400 [13.000, 100.000], mean observation: 3.158 [-1.748, 10.412], loss: 0.986964, mae: 3.887934, mean_q: 4.337473
 13605/100000: episode: 1392, duration: 0.150s, episode steps: 10, steps per second: 66, episode reward: 3.697, mean reward: 0.370 [0.267, 0.522], mean action: 50.100 [12.000, 94.000], mean observation: 3.155 [-1.640, 10.229], loss: 0.850481, mae: 3.888363, mean_q: 4.339778
 13615/100000: episode: 1393, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.803, mean reward: 0.380 [0.318, 0.466], mean action: 70.700 [16.000, 96.000], mean observation: 3.141 [-0.924, 10.213], loss: 0.687845, mae: 3.889079, mean_q: 4.342640
 13625/100000: episode: 1394, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.927, mean reward: 0.493 [0.441, 0.506], mean action: 66.000 [10.000, 94.000], mean observation: 3.164 [-0.967, 10.251], loss: 0.855652, mae: 3.891238, mean_q: 4.344914
 13635/100000: episode: 1395, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.849, mean reward: 0.385 [0.357, 0.570], mean action: 72.300 [57.000, 94.000], mean observation: 3.157 [-1.104, 10.269], loss: 0.740955, mae: 3.892007, mean_q: 4.348205
 13645/100000: episode: 1396, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.895, mean reward: 0.490 [0.483, 0.509], mean action: 55.100 [19.000, 92.000], mean observation: 3.179 [-0.977, 10.205], loss: 1.087666, mae: 3.895350, mean_q: 4.352379
 13655/100000: episode: 1397, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.238, mean reward: 0.424 [0.289, 0.450], mean action: 50.200 [14.000, 69.000], mean observation: 3.162 [-1.281, 10.311], loss: 0.925237, mae: 3.895322, mean_q: 4.355569
 13665/100000: episode: 1398, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.045, mean reward: 0.404 [0.393, 0.425], mean action: 58.000 [9.000, 100.000], mean observation: 3.150 [-1.354, 10.405], loss: 0.777206, mae: 3.895558, mean_q: 4.355693
 13675/100000: episode: 1399, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.057, mean reward: 0.406 [0.328, 0.491], mean action: 44.900 [1.000, 71.000], mean observation: 3.150 [-1.743, 10.290], loss: 1.057172, mae: 3.897683, mean_q: 4.351187
 13685/100000: episode: 1400, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.315, mean reward: 0.432 [0.333, 0.577], mean action: 45.300 [8.000, 69.000], mean observation: 3.152 [-1.340, 10.229], loss: 0.979301, mae: 3.898520, mean_q: 4.351933
 13695/100000: episode: 1401, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.268, mean reward: 0.427 [0.337, 0.500], mean action: 53.900 [8.000, 96.000], mean observation: 3.153 [-1.335, 10.245], loss: 0.941786, mae: 3.899377, mean_q: 4.354604
 13705/100000: episode: 1402, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.989, mean reward: 0.399 [0.389, 0.400], mean action: 62.300 [11.000, 78.000], mean observation: 3.162 [-1.128, 10.280], loss: 0.891554, mae: 3.900012, mean_q: 4.358210
 13715/100000: episode: 1403, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.692, mean reward: 0.369 [0.319, 0.400], mean action: 47.400 [10.000, 79.000], mean observation: 3.162 [-1.056, 10.322], loss: 0.945229, mae: 3.901012, mean_q: 4.361212
 13725/100000: episode: 1404, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.136, mean reward: 0.414 [0.351, 0.544], mean action: 65.500 [25.000, 88.000], mean observation: 3.164 [-0.909, 10.353], loss: 0.928427, mae: 3.902142, mean_q: 4.357550
 13735/100000: episode: 1405, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.236, mean reward: 0.424 [0.392, 0.522], mean action: 65.000 [20.000, 81.000], mean observation: 3.138 [-1.369, 10.300], loss: 0.869233, mae: 3.903114, mean_q: 4.356425
 13745/100000: episode: 1406, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.620, mean reward: 0.362 [0.337, 0.447], mean action: 64.700 [23.000, 91.000], mean observation: 3.158 [-0.863, 10.427], loss: 1.032097, mae: 3.904928, mean_q: 4.357168
 13755/100000: episode: 1407, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.395, mean reward: 0.439 [0.397, 0.503], mean action: 62.600 [14.000, 91.000], mean observation: 3.149 [-1.110, 10.410], loss: 0.795329, mae: 3.905384, mean_q: 4.359387
 13765/100000: episode: 1408, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.356, mean reward: 0.436 [0.431, 0.464], mean action: 58.600 [26.000, 69.000], mean observation: 3.153 [-1.301, 10.340], loss: 1.016398, mae: 3.907311, mean_q: 4.362711
 13775/100000: episode: 1409, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.942, mean reward: 0.394 [0.343, 0.444], mean action: 60.100 [0.000, 99.000], mean observation: 3.159 [-1.278, 10.303], loss: 0.803299, mae: 3.907145, mean_q: 4.365569
 13785/100000: episode: 1410, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.735, mean reward: 0.374 [0.312, 0.496], mean action: 66.400 [20.000, 84.000], mean observation: 3.167 [-1.340, 10.391], loss: 0.920527, mae: 3.909175, mean_q: 4.368637
 13786/100000: episode: 1411, duration: 0.038s, episode steps: 1, steps per second: 26, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 18.000 [18.000, 18.000], mean observation: 3.172 [-0.695, 10.100], loss: 0.840619, mae: 3.908787, mean_q: 4.370413
 13796/100000: episode: 1412, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.569, mean reward: 0.457 [0.404, 0.473], mean action: 60.400 [20.000, 69.000], mean observation: 3.157 [-1.402, 10.362], loss: 0.987460, mae: 3.910800, mean_q: 4.369988
 13806/100000: episode: 1413, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 5.052, mean reward: 0.505 [0.376, 0.588], mean action: 61.900 [11.000, 99.000], mean observation: 3.150 [-1.423, 10.214], loss: 0.770122, mae: 3.911343, mean_q: 4.366296
 13816/100000: episode: 1414, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.590, mean reward: 0.359 [0.327, 0.416], mean action: 63.600 [21.000, 101.000], mean observation: 3.143 [-1.110, 10.233], loss: 0.778582, mae: 3.913285, mean_q: 4.368941
 13826/100000: episode: 1415, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.203, mean reward: 0.420 [0.321, 0.484], mean action: 58.200 [21.000, 90.000], mean observation: 3.151 [-1.540, 10.259], loss: 0.992922, mae: 3.915569, mean_q: 4.372562
 13836/100000: episode: 1416, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.793, mean reward: 0.379 [0.327, 0.434], mean action: 58.300 [2.000, 70.000], mean observation: 3.152 [-0.887, 10.391], loss: 0.833175, mae: 3.915983, mean_q: 4.376147
 13846/100000: episode: 1417, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.111, mean reward: 0.411 [0.335, 0.471], mean action: 58.500 [3.000, 101.000], mean observation: 3.153 [-0.741, 10.147], loss: 1.283849, mae: 3.919157, mean_q: 4.380035
 13856/100000: episode: 1418, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.205, mean reward: 0.421 [0.394, 0.562], mean action: 64.700 [39.000, 97.000], mean observation: 3.162 [-1.383, 10.342], loss: 0.818759, mae: 3.918256, mean_q: 4.381325
 13866/100000: episode: 1419, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.703, mean reward: 0.370 [0.302, 0.554], mean action: 62.300 [12.000, 99.000], mean observation: 3.162 [-1.240, 10.320], loss: 0.915481, mae: 3.920118, mean_q: 4.378859
 13876/100000: episode: 1420, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.245, mean reward: 0.425 [0.422, 0.425], mean action: 67.100 [30.000, 99.000], mean observation: 3.158 [-1.569, 10.257], loss: 0.799385, mae: 3.920357, mean_q: 4.380084
 13886/100000: episode: 1421, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.680, mean reward: 0.368 [0.294, 0.516], mean action: 50.900 [14.000, 69.000], mean observation: 3.157 [-1.152, 10.403], loss: 1.088439, mae: 3.923103, mean_q: 4.384446
 13896/100000: episode: 1422, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.191, mean reward: 0.419 [0.418, 0.432], mean action: 71.600 [58.000, 88.000], mean observation: 3.144 [-1.250, 10.309], loss: 0.833784, mae: 3.923777, mean_q: 4.389650
 13906/100000: episode: 1423, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.840, mean reward: 0.384 [0.313, 0.447], mean action: 60.400 [24.000, 82.000], mean observation: 3.152 [-1.299, 10.277], loss: 0.874259, mae: 3.925146, mean_q: 4.393470
 13916/100000: episode: 1424, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.095, mean reward: 0.410 [0.377, 0.470], mean action: 58.300 [12.000, 85.000], mean observation: 3.166 [-1.713, 10.331], loss: 0.843453, mae: 3.925733, mean_q: 4.397113
 13926/100000: episode: 1425, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 3.828, mean reward: 0.383 [0.348, 0.426], mean action: 60.800 [29.000, 76.000], mean observation: 3.158 [-1.639, 10.297], loss: 1.037647, mae: 3.927868, mean_q: 4.400928
 13936/100000: episode: 1426, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.697, mean reward: 0.370 [0.285, 0.489], mean action: 50.600 [7.000, 69.000], mean observation: 3.155 [-1.620, 10.334], loss: 0.659273, mae: 3.927134, mean_q: 4.404402
 13946/100000: episode: 1427, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.157, mean reward: 0.416 [0.383, 0.481], mean action: 54.300 [8.000, 76.000], mean observation: 3.167 [-0.892, 10.442], loss: 0.880095, mae: 3.929560, mean_q: 4.407624
 13956/100000: episode: 1428, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.119, mean reward: 0.412 [0.373, 0.507], mean action: 54.700 [12.000, 73.000], mean observation: 3.144 [-1.593, 10.370], loss: 0.791975, mae: 3.930232, mean_q: 4.402994
 13966/100000: episode: 1429, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 4.678, mean reward: 0.468 [0.446, 0.504], mean action: 70.800 [40.000, 99.000], mean observation: 3.149 [-0.826, 10.316], loss: 0.814381, mae: 3.931370, mean_q: 4.397569
 13976/100000: episode: 1430, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.032, mean reward: 0.403 [0.386, 0.471], mean action: 56.700 [30.000, 69.000], mean observation: 3.146 [-1.398, 10.278], loss: 1.166114, mae: 3.933958, mean_q: 4.393282
 13986/100000: episode: 1431, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.593, mean reward: 0.359 [0.302, 0.419], mean action: 65.900 [33.000, 83.000], mean observation: 3.143 [-1.153, 10.283], loss: 0.997576, mae: 3.934016, mean_q: 4.392004
 13996/100000: episode: 1432, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.673, mean reward: 0.367 [0.294, 0.415], mean action: 60.900 [25.000, 86.000], mean observation: 3.158 [-1.417, 10.331], loss: 1.012415, mae: 3.934994, mean_q: 4.393449
 14006/100000: episode: 1433, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.269, mean reward: 0.427 [0.342, 0.548], mean action: 50.500 [3.000, 91.000], mean observation: 3.167 [-1.262, 10.308], loss: 0.942026, mae: 3.935880, mean_q: 4.392070
 14016/100000: episode: 1434, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.567, mean reward: 0.457 [0.420, 0.589], mean action: 66.700 [50.000, 81.000], mean observation: 3.166 [-1.822, 10.346], loss: 1.091635, mae: 3.937047, mean_q: 4.394251
 14026/100000: episode: 1435, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.071, mean reward: 0.407 [0.384, 0.467], mean action: 61.500 [4.000, 69.000], mean observation: 3.154 [-1.248, 10.249], loss: 1.077377, mae: 3.937859, mean_q: 4.399151
 14036/100000: episode: 1436, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.031, mean reward: 0.403 [0.354, 0.459], mean action: 60.800 [13.000, 101.000], mean observation: 3.153 [-0.959, 10.336], loss: 1.109944, mae: 3.939173, mean_q: 4.395538
 14046/100000: episode: 1437, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.399, mean reward: 0.440 [0.305, 0.519], mean action: 35.000 [17.000, 75.000], mean observation: 3.159 [-1.295, 10.402], loss: 1.048613, mae: 3.939241, mean_q: 4.395020
 14056/100000: episode: 1438, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.657, mean reward: 0.466 [0.323, 0.504], mean action: 38.700 [24.000, 75.000], mean observation: 3.163 [-1.141, 10.336], loss: 0.801772, mae: 3.939209, mean_q: 4.396310
 14066/100000: episode: 1439, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 5.289, mean reward: 0.529 [0.415, 0.542], mean action: 38.700 [3.000, 80.000], mean observation: 3.164 [-1.272, 10.316], loss: 0.792086, mae: 3.940536, mean_q: 4.394566
 14076/100000: episode: 1440, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.063, mean reward: 0.406 [0.369, 0.482], mean action: 37.100 [7.000, 78.000], mean observation: 3.162 [-1.458, 10.218], loss: 1.140768, mae: 3.942887, mean_q: 4.388466
 14086/100000: episode: 1441, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.941, mean reward: 0.394 [0.325, 0.497], mean action: 34.000 [1.000, 72.000], mean observation: 3.157 [-1.657, 10.341], loss: 1.013901, mae: 3.943909, mean_q: 4.388104
 14096/100000: episode: 1442, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 5.442, mean reward: 0.544 [0.544, 0.544], mean action: 46.700 [27.000, 83.000], mean observation: 3.153 [-1.207, 10.327], loss: 0.806098, mae: 3.942750, mean_q: 4.389953
 14106/100000: episode: 1443, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.824, mean reward: 0.382 [0.345, 0.427], mean action: 36.900 [26.000, 61.000], mean observation: 3.141 [-1.655, 10.269], loss: 0.818668, mae: 3.944241, mean_q: 4.389691
 14116/100000: episode: 1444, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.195, mean reward: 0.419 [0.349, 0.530], mean action: 45.600 [18.000, 85.000], mean observation: 3.144 [-1.447, 10.192], loss: 0.758123, mae: 3.945126, mean_q: 4.390983
 14126/100000: episode: 1445, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.226, mean reward: 0.423 [0.326, 0.465], mean action: 41.100 [7.000, 96.000], mean observation: 3.153 [-1.931, 10.343], loss: 1.039208, mae: 3.947187, mean_q: 4.388891
 14136/100000: episode: 1446, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.875, mean reward: 0.387 [0.282, 0.490], mean action: 40.400 [8.000, 98.000], mean observation: 3.160 [-1.138, 10.358], loss: 1.116966, mae: 3.948615, mean_q: 4.388943
 14146/100000: episode: 1447, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.223, mean reward: 0.422 [0.327, 0.531], mean action: 33.100 [16.000, 95.000], mean observation: 3.162 [-1.523, 10.309], loss: 0.801271, mae: 3.947442, mean_q: 4.391631
 14156/100000: episode: 1448, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.956, mean reward: 0.396 [0.313, 0.571], mean action: 40.500 [27.000, 76.000], mean observation: 3.147 [-1.223, 10.267], loss: 1.127348, mae: 3.950242, mean_q: 4.395111
 14166/100000: episode: 1449, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.812, mean reward: 0.381 [0.359, 0.485], mean action: 48.100 [27.000, 98.000], mean observation: 3.146 [-1.072, 10.431], loss: 1.000016, mae: 3.950778, mean_q: 4.398210
 14176/100000: episode: 1450, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.990, mean reward: 0.399 [0.345, 0.513], mean action: 30.600 [27.000, 49.000], mean observation: 3.155 [-1.256, 10.320], loss: 0.731876, mae: 3.950407, mean_q: 4.401564
 14186/100000: episode: 1451, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.793, mean reward: 0.379 [0.340, 0.445], mean action: 41.200 [27.000, 91.000], mean observation: 3.160 [-1.011, 10.322], loss: 0.950549, mae: 3.952489, mean_q: 4.405749
 14196/100000: episode: 1452, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.979, mean reward: 0.398 [0.361, 0.488], mean action: 37.100 [3.000, 87.000], mean observation: 3.140 [-2.041, 10.294], loss: 1.050642, mae: 3.953868, mean_q: 4.405576
 14206/100000: episode: 1453, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.032, mean reward: 0.403 [0.364, 0.490], mean action: 39.600 [11.000, 92.000], mean observation: 3.154 [-2.262, 10.316], loss: 0.781526, mae: 3.953854, mean_q: 4.407869
 14216/100000: episode: 1454, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.946, mean reward: 0.395 [0.368, 0.495], mean action: 45.600 [27.000, 85.000], mean observation: 3.149 [-1.183, 10.516], loss: 0.873929, mae: 3.955962, mean_q: 4.411863
 14226/100000: episode: 1455, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.914, mean reward: 0.491 [0.491, 0.491], mean action: 27.200 [17.000, 36.000], mean observation: 3.167 [-2.328, 10.397], loss: 0.972743, mae: 3.957235, mean_q: 4.414950
 14236/100000: episode: 1456, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.673, mean reward: 0.467 [0.456, 0.543], mean action: 39.700 [6.000, 91.000], mean observation: 3.156 [-1.592, 10.254], loss: 0.774023, mae: 3.958167, mean_q: 4.417036
 14246/100000: episode: 1457, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.070, mean reward: 0.407 [0.353, 0.451], mean action: 54.600 [27.000, 99.000], mean observation: 3.160 [-1.307, 10.278], loss: 0.786228, mae: 3.959320, mean_q: 4.418814
 14256/100000: episode: 1458, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.129, mean reward: 0.413 [0.343, 0.560], mean action: 28.500 [6.000, 66.000], mean observation: 3.161 [-1.516, 10.385], loss: 0.911650, mae: 3.961207, mean_q: 4.421382
 14266/100000: episode: 1459, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.323, mean reward: 0.432 [0.321, 0.520], mean action: 30.000 [27.000, 57.000], mean observation: 3.170 [-1.266, 10.265], loss: 1.092346, mae: 3.963195, mean_q: 4.424529
 14276/100000: episode: 1460, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.212, mean reward: 0.421 [0.350, 0.588], mean action: 27.200 [10.000, 81.000], mean observation: 3.166 [-1.189, 10.254], loss: 1.011142, mae: 3.963850, mean_q: 4.426419
 14286/100000: episode: 1461, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.706, mean reward: 0.371 [0.293, 0.455], mean action: 33.500 [23.000, 71.000], mean observation: 3.147 [-1.945, 10.398], loss: 0.942713, mae: 3.964735, mean_q: 4.430167
 14296/100000: episode: 1462, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.046, mean reward: 0.405 [0.391, 0.456], mean action: 45.600 [27.000, 95.000], mean observation: 3.160 [-1.856, 10.291], loss: 0.942222, mae: 3.965243, mean_q: 4.434966
 14306/100000: episode: 1463, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.070, mean reward: 0.407 [0.331, 0.535], mean action: 30.600 [6.000, 60.000], mean observation: 3.157 [-1.855, 10.413], loss: 1.097363, mae: 3.967195, mean_q: 4.433709
 14316/100000: episode: 1464, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.851, mean reward: 0.385 [0.298, 0.471], mean action: 52.900 [19.000, 88.000], mean observation: 3.155 [-1.357, 10.275], loss: 1.035338, mae: 3.967544, mean_q: 4.432523
 14326/100000: episode: 1465, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.106, mean reward: 0.411 [0.354, 0.531], mean action: 44.100 [27.000, 100.000], mean observation: 3.160 [-1.340, 10.311], loss: 1.088560, mae: 3.968985, mean_q: 4.431860
 14336/100000: episode: 1466, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.705, mean reward: 0.471 [0.417, 0.550], mean action: 29.700 [7.000, 74.000], mean observation: 3.158 [-1.580, 10.292], loss: 0.931256, mae: 3.969654, mean_q: 4.436189
 14346/100000: episode: 1467, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.963, mean reward: 0.396 [0.299, 0.528], mean action: 42.300 [4.000, 101.000], mean observation: 3.174 [-0.980, 10.215], loss: 0.956179, mae: 3.970437, mean_q: 4.439471
 14356/100000: episode: 1468, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.130, mean reward: 0.413 [0.337, 0.502], mean action: 50.200 [20.000, 96.000], mean observation: 3.143 [-1.479, 10.331], loss: 0.762446, mae: 3.971328, mean_q: 4.445083
 14366/100000: episode: 1469, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.936, mean reward: 0.394 [0.327, 0.451], mean action: 40.800 [14.000, 98.000], mean observation: 3.162 [-2.105, 10.414], loss: 1.011330, mae: 3.973432, mean_q: 4.448501
 14372/100000: episode: 1470, duration: 0.117s, episode steps: 6, steps per second: 51, episode reward: 11.888, mean reward: 1.981 [0.309, 10.000], mean action: 29.833 [23.000, 48.000], mean observation: 3.150 [-1.202, 10.268], loss: 1.019982, mae: 3.974757, mean_q: 4.447869
 14382/100000: episode: 1471, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.225, mean reward: 0.422 [0.343, 0.465], mean action: 48.600 [0.000, 101.000], mean observation: 3.163 [-1.355, 10.353], loss: 1.034109, mae: 3.975888, mean_q: 4.449677
 14392/100000: episode: 1472, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.929, mean reward: 0.393 [0.343, 0.472], mean action: 35.300 [0.000, 89.000], mean observation: 3.161 [-0.941, 10.334], loss: 1.019649, mae: 3.977221, mean_q: 4.451417
 14402/100000: episode: 1473, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.888, mean reward: 0.389 [0.328, 0.522], mean action: 36.600 [0.000, 99.000], mean observation: 3.172 [-2.224, 10.472], loss: 0.877778, mae: 3.977781, mean_q: 4.449759
 14412/100000: episode: 1474, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.284, mean reward: 0.428 [0.356, 0.502], mean action: 32.100 [13.000, 67.000], mean observation: 3.157 [-1.207, 10.203], loss: 0.849411, mae: 3.978739, mean_q: 4.452626
 14422/100000: episode: 1475, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.265, mean reward: 0.427 [0.355, 0.536], mean action: 41.600 [27.000, 78.000], mean observation: 3.160 [-1.349, 10.266], loss: 0.920063, mae: 3.980384, mean_q: 4.456546
 14432/100000: episode: 1476, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.792, mean reward: 0.379 [0.342, 0.431], mean action: 34.700 [6.000, 89.000], mean observation: 3.148 [-1.586, 10.343], loss: 0.651464, mae: 3.981035, mean_q: 4.459294
 14442/100000: episode: 1477, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.710, mean reward: 0.371 [0.332, 0.461], mean action: 38.900 [23.000, 71.000], mean observation: 3.163 [-1.794, 10.250], loss: 1.093969, mae: 3.983650, mean_q: 4.462786
 14452/100000: episode: 1478, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.402, mean reward: 0.440 [0.362, 0.542], mean action: 37.300 [27.000, 68.000], mean observation: 3.151 [-2.180, 10.485], loss: 1.143449, mae: 3.985896, mean_q: 4.465395
 14462/100000: episode: 1479, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.014, mean reward: 0.401 [0.321, 0.482], mean action: 35.900 [24.000, 98.000], mean observation: 3.175 [-0.952, 10.371], loss: 0.988504, mae: 3.986020, mean_q: 4.464459
 14472/100000: episode: 1480, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.098, mean reward: 0.410 [0.351, 0.468], mean action: 36.600 [14.000, 90.000], mean observation: 3.141 [-1.586, 10.236], loss: 1.211381, mae: 3.987912, mean_q: 4.468513
 14482/100000: episode: 1481, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.474, mean reward: 0.447 [0.376, 0.458], mean action: 46.900 [12.000, 92.000], mean observation: 3.154 [-1.309, 10.397], loss: 0.870623, mae: 3.987068, mean_q: 4.469951
 14492/100000: episode: 1482, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.791, mean reward: 0.379 [0.288, 0.437], mean action: 42.000 [13.000, 83.000], mean observation: 3.152 [-1.062, 10.333], loss: 0.812087, mae: 3.988133, mean_q: 4.470666
 14502/100000: episode: 1483, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.101, mean reward: 0.410 [0.288, 0.515], mean action: 47.100 [11.000, 98.000], mean observation: 3.149 [-1.741, 10.286], loss: 0.830211, mae: 3.989377, mean_q: 4.475904
 14512/100000: episode: 1484, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.165, mean reward: 0.416 [0.322, 0.554], mean action: 41.400 [13.000, 101.000], mean observation: 3.156 [-1.809, 10.273], loss: 0.914873, mae: 3.991259, mean_q: 4.480037
 14522/100000: episode: 1485, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.451, mean reward: 0.445 [0.343, 0.597], mean action: 43.300 [1.000, 82.000], mean observation: 3.158 [-1.349, 10.533], loss: 1.145273, mae: 3.993535, mean_q: 4.484427
 14532/100000: episode: 1486, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.258, mean reward: 0.426 [0.400, 0.524], mean action: 38.100 [13.000, 81.000], mean observation: 3.158 [-1.575, 10.236], loss: 0.977872, mae: 3.993681, mean_q: 4.488999
 14542/100000: episode: 1487, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.973, mean reward: 0.397 [0.384, 0.419], mean action: 55.200 [13.000, 95.000], mean observation: 3.154 [-1.635, 10.246], loss: 0.923691, mae: 3.994689, mean_q: 4.493477
 14552/100000: episode: 1488, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.957, mean reward: 0.396 [0.328, 0.452], mean action: 45.600 [3.000, 101.000], mean observation: 3.154 [-1.128, 10.324], loss: 0.955839, mae: 3.995335, mean_q: 4.490716
 14562/100000: episode: 1489, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.486, mean reward: 0.449 [0.440, 0.477], mean action: 35.000 [12.000, 75.000], mean observation: 3.164 [-1.292, 10.219], loss: 1.092175, mae: 3.997137, mean_q: 4.485162
 14572/100000: episode: 1490, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.265, mean reward: 0.426 [0.402, 0.538], mean action: 38.400 [27.000, 85.000], mean observation: 3.146 [-1.299, 10.308], loss: 0.966828, mae: 3.997108, mean_q: 4.484962
 14582/100000: episode: 1491, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.105, mean reward: 0.411 [0.385, 0.477], mean action: 46.500 [27.000, 97.000], mean observation: 3.162 [-1.829, 10.251], loss: 0.949365, mae: 3.998570, mean_q: 4.489739
 14592/100000: episode: 1492, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.756, mean reward: 0.376 [0.308, 0.434], mean action: 42.200 [27.000, 91.000], mean observation: 3.166 [-1.127, 10.450], loss: 0.951384, mae: 3.999898, mean_q: 4.490667
 14602/100000: episode: 1493, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.956, mean reward: 0.496 [0.483, 0.566], mean action: 38.700 [27.000, 85.000], mean observation: 3.164 [-1.666, 10.301], loss: 1.045113, mae: 4.002018, mean_q: 4.493689
 14612/100000: episode: 1494, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.812, mean reward: 0.481 [0.475, 0.499], mean action: 41.800 [27.000, 93.000], mean observation: 3.147 [-1.922, 10.343], loss: 1.024537, mae: 4.002481, mean_q: 4.492702
 14622/100000: episode: 1495, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.867, mean reward: 0.387 [0.343, 0.432], mean action: 41.900 [27.000, 76.000], mean observation: 3.149 [-1.241, 10.316], loss: 1.129777, mae: 4.004409, mean_q: 4.491361
 14632/100000: episode: 1496, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.279, mean reward: 0.428 [0.398, 0.495], mean action: 29.600 [22.000, 52.000], mean observation: 3.148 [-1.315, 10.484], loss: 1.063522, mae: 4.005759, mean_q: 4.492333
 14642/100000: episode: 1497, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.021, mean reward: 0.402 [0.364, 0.449], mean action: 36.200 [3.000, 97.000], mean observation: 3.167 [-1.480, 10.317], loss: 1.174632, mae: 4.007483, mean_q: 4.489793
 14652/100000: episode: 1498, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.179, mean reward: 0.418 [0.333, 0.523], mean action: 52.200 [27.000, 100.000], mean observation: 3.147 [-2.093, 10.311], loss: 1.167844, mae: 4.007998, mean_q: 4.487977
 14662/100000: episode: 1499, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.038, mean reward: 0.404 [0.387, 0.437], mean action: 47.400 [2.000, 84.000], mean observation: 3.156 [-1.644, 10.448], loss: 1.009471, mae: 4.008252, mean_q: 4.490508
 14672/100000: episode: 1500, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.216, mean reward: 0.422 [0.355, 0.524], mean action: 42.900 [27.000, 100.000], mean observation: 3.147 [-1.677, 10.282], loss: 0.902173, mae: 4.008621, mean_q: 4.487254
 14682/100000: episode: 1501, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.445, mean reward: 0.445 [0.319, 0.538], mean action: 37.000 [12.000, 89.000], mean observation: 3.165 [-1.699, 10.326], loss: 1.048352, mae: 4.010543, mean_q: 4.486374
 14692/100000: episode: 1502, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.156, mean reward: 0.416 [0.359, 0.508], mean action: 33.200 [21.000, 95.000], mean observation: 3.159 [-1.273, 10.395], loss: 0.956638, mae: 4.010330, mean_q: 4.478085
 14702/100000: episode: 1503, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.167, mean reward: 0.417 [0.385, 0.482], mean action: 34.100 [15.000, 68.000], mean observation: 3.170 [-1.525, 10.330], loss: 1.066544, mae: 4.012107, mean_q: 4.468349
 14712/100000: episode: 1504, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.246, mean reward: 0.425 [0.404, 0.479], mean action: 41.600 [24.000, 90.000], mean observation: 3.150 [-1.429, 10.364], loss: 1.100657, mae: 4.012852, mean_q: 4.468775
 14722/100000: episode: 1505, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.940, mean reward: 0.394 [0.319, 0.517], mean action: 35.600 [2.000, 78.000], mean observation: 3.157 [-0.898, 10.395], loss: 0.953312, mae: 4.013129, mean_q: 4.473279
 14732/100000: episode: 1506, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.267, mean reward: 0.427 [0.312, 0.543], mean action: 29.600 [13.000, 80.000], mean observation: 3.162 [-1.457, 10.390], loss: 1.036936, mae: 4.014483, mean_q: 4.478346
 14742/100000: episode: 1507, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.133, mean reward: 0.413 [0.393, 0.452], mean action: 43.200 [2.000, 92.000], mean observation: 3.148 [-1.890, 10.336], loss: 0.990871, mae: 4.015636, mean_q: 4.480875
 14752/100000: episode: 1508, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.961, mean reward: 0.396 [0.360, 0.435], mean action: 35.800 [3.000, 88.000], mean observation: 3.152 [-1.306, 10.313], loss: 1.040918, mae: 4.016176, mean_q: 4.483434
 14762/100000: episode: 1509, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.368, mean reward: 0.437 [0.359, 0.560], mean action: 39.700 [6.000, 101.000], mean observation: 3.153 [-1.142, 10.383], loss: 0.986724, mae: 4.017359, mean_q: 4.488412
 14772/100000: episode: 1510, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.919, mean reward: 0.392 [0.335, 0.403], mean action: 29.900 [2.000, 95.000], mean observation: 3.154 [-1.520, 10.401], loss: 1.048726, mae: 4.018978, mean_q: 4.490972
 14782/100000: episode: 1511, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.182, mean reward: 0.418 [0.314, 0.516], mean action: 50.300 [12.000, 100.000], mean observation: 3.151 [-1.235, 10.404], loss: 1.024951, mae: 4.019483, mean_q: 4.493093
 14792/100000: episode: 1512, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.713, mean reward: 0.471 [0.454, 0.526], mean action: 32.700 [26.000, 80.000], mean observation: 3.160 [-1.855, 10.261], loss: 0.998097, mae: 4.020636, mean_q: 4.497149
 14802/100000: episode: 1513, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.029, mean reward: 0.403 [0.354, 0.476], mean action: 42.700 [4.000, 100.000], mean observation: 3.155 [-1.939, 10.309], loss: 0.830222, mae: 4.021268, mean_q: 4.498502
 14812/100000: episode: 1514, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.209, mean reward: 0.421 [0.299, 0.518], mean action: 31.900 [2.000, 88.000], mean observation: 3.151 [-1.097, 10.548], loss: 1.062851, mae: 4.023406, mean_q: 4.501502
 14822/100000: episode: 1515, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.883, mean reward: 0.388 [0.333, 0.483], mean action: 25.200 [1.000, 76.000], mean observation: 3.143 [-1.402, 10.398], loss: 0.898591, mae: 4.023846, mean_q: 4.500503
 14832/100000: episode: 1516, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.970, mean reward: 0.397 [0.309, 0.482], mean action: 45.700 [27.000, 84.000], mean observation: 3.159 [-0.903, 10.398], loss: 0.820231, mae: 4.024410, mean_q: 4.498027
 14842/100000: episode: 1517, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.988, mean reward: 0.399 [0.340, 0.456], mean action: 43.400 [27.000, 96.000], mean observation: 3.155 [-2.201, 10.174], loss: 0.890988, mae: 4.026486, mean_q: 4.499288
 14852/100000: episode: 1518, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.295, mean reward: 0.430 [0.311, 0.491], mean action: 27.200 [4.000, 64.000], mean observation: 3.153 [-1.514, 10.336], loss: 0.912743, mae: 4.028206, mean_q: 4.502681
 14862/100000: episode: 1519, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.023, mean reward: 0.402 [0.361, 0.451], mean action: 39.400 [6.000, 99.000], mean observation: 3.157 [-1.692, 10.264], loss: 0.869372, mae: 4.028976, mean_q: 4.502461
 14872/100000: episode: 1520, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.715, mean reward: 0.371 [0.323, 0.413], mean action: 49.500 [27.000, 92.000], mean observation: 3.153 [-2.115, 10.370], loss: 0.881863, mae: 4.030381, mean_q: 4.503713
 14882/100000: episode: 1521, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.605, mean reward: 0.361 [0.344, 0.414], mean action: 40.600 [27.000, 101.000], mean observation: 3.163 [-1.556, 10.372], loss: 1.032631, mae: 4.032744, mean_q: 4.508948
 14892/100000: episode: 1522, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.558, mean reward: 0.456 [0.399, 0.597], mean action: 48.500 [10.000, 101.000], mean observation: 3.146 [-1.231, 10.301], loss: 0.927271, mae: 4.033615, mean_q: 4.513808
 14902/100000: episode: 1523, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.994, mean reward: 0.399 [0.347, 0.557], mean action: 30.800 [3.000, 88.000], mean observation: 3.148 [-1.712, 10.392], loss: 1.005728, mae: 4.035203, mean_q: 4.519145
 14912/100000: episode: 1524, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.745, mean reward: 0.474 [0.448, 0.534], mean action: 53.600 [26.000, 100.000], mean observation: 3.169 [-1.143, 10.216], loss: 0.991914, mae: 4.036290, mean_q: 4.524488
 14922/100000: episode: 1525, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.537, mean reward: 0.454 [0.321, 0.529], mean action: 38.500 [7.000, 74.000], mean observation: 3.157 [-1.452, 10.311], loss: 0.982977, mae: 4.037038, mean_q: 4.529649
 14932/100000: episode: 1526, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.960, mean reward: 0.396 [0.342, 0.470], mean action: 44.000 [25.000, 82.000], mean observation: 3.151 [-1.134, 10.270], loss: 0.744729, mae: 4.037164, mean_q: 4.532892
 14942/100000: episode: 1527, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.194, mean reward: 0.419 [0.352, 0.549], mean action: 40.800 [17.000, 79.000], mean observation: 3.168 [-1.506, 10.495], loss: 0.812275, mae: 4.039048, mean_q: 4.530125
 14952/100000: episode: 1528, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.034, mean reward: 0.403 [0.340, 0.537], mean action: 35.400 [5.000, 71.000], mean observation: 3.164 [-1.313, 10.324], loss: 0.968213, mae: 4.040808, mean_q: 4.520660
 14962/100000: episode: 1529, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.960, mean reward: 0.396 [0.396, 0.396], mean action: 38.300 [0.000, 75.000], mean observation: 3.151 [-1.130, 10.290], loss: 0.905751, mae: 4.041915, mean_q: 4.517638
 14972/100000: episode: 1530, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.507, mean reward: 0.351 [0.317, 0.465], mean action: 39.200 [27.000, 77.000], mean observation: 3.155 [-1.174, 10.260], loss: 1.135268, mae: 4.044209, mean_q: 4.519196
 14982/100000: episode: 1531, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.497, mean reward: 0.450 [0.381, 0.536], mean action: 35.800 [5.000, 82.000], mean observation: 3.164 [-0.996, 10.278], loss: 0.887206, mae: 4.044389, mean_q: 4.517310
 14992/100000: episode: 1532, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.914, mean reward: 0.391 [0.323, 0.437], mean action: 37.300 [9.000, 100.000], mean observation: 3.144 [-1.370, 10.470], loss: 0.991901, mae: 4.045817, mean_q: 4.516917
 15002/100000: episode: 1533, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.478, mean reward: 0.448 [0.398, 0.523], mean action: 35.900 [22.000, 82.000], mean observation: 3.152 [-1.407, 10.421], loss: 0.900640, mae: 4.047006, mean_q: 4.514715
 15012/100000: episode: 1534, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.593, mean reward: 0.359 [0.332, 0.528], mean action: 65.600 [27.000, 97.000], mean observation: 3.142 [-1.052, 10.436], loss: 1.236251, mae: 4.049142, mean_q: 4.509773
 15022/100000: episode: 1535, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.909, mean reward: 0.391 [0.358, 0.444], mean action: 42.900 [27.000, 71.000], mean observation: 3.161 [-1.410, 10.364], loss: 0.880178, mae: 4.048563, mean_q: 4.507720
 15032/100000: episode: 1536, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.111, mean reward: 0.411 [0.300, 0.574], mean action: 43.200 [27.000, 100.000], mean observation: 3.156 [-1.399, 10.290], loss: 0.854536, mae: 4.049576, mean_q: 4.507382
 15042/100000: episode: 1537, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.107, mean reward: 0.411 [0.293, 0.598], mean action: 35.400 [1.000, 80.000], mean observation: 3.156 [-1.716, 10.574], loss: 0.941763, mae: 4.051116, mean_q: 4.507457
 15052/100000: episode: 1538, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.957, mean reward: 0.396 [0.385, 0.415], mean action: 31.600 [15.000, 77.000], mean observation: 3.161 [-1.483, 10.421], loss: 0.933479, mae: 4.052025, mean_q: 4.508282
 15062/100000: episode: 1539, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.135, mean reward: 0.413 [0.296, 0.462], mean action: 33.300 [0.000, 81.000], mean observation: 3.164 [-1.571, 10.411], loss: 0.799367, mae: 4.052930, mean_q: 4.511045
 15072/100000: episode: 1540, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.198, mean reward: 0.420 [0.343, 0.499], mean action: 36.900 [15.000, 92.000], mean observation: 3.163 [-1.139, 10.308], loss: 0.918463, mae: 4.054178, mean_q: 4.508237
 15082/100000: episode: 1541, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.591, mean reward: 0.459 [0.427, 0.543], mean action: 28.900 [5.000, 50.000], mean observation: 3.167 [-1.419, 10.369], loss: 1.008395, mae: 4.056101, mean_q: 4.508414
 15092/100000: episode: 1542, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.477, mean reward: 0.348 [0.272, 0.466], mean action: 47.400 [27.000, 77.000], mean observation: 3.156 [-0.905, 10.256], loss: 0.809485, mae: 4.055558, mean_q: 4.512461
 15102/100000: episode: 1543, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.791, mean reward: 0.379 [0.349, 0.452], mean action: 38.300 [19.000, 100.000], mean observation: 3.146 [-1.440, 10.328], loss: 0.823090, mae: 4.056989, mean_q: 4.517757
 15112/100000: episode: 1544, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.895, mean reward: 0.390 [0.269, 0.505], mean action: 39.800 [27.000, 75.000], mean observation: 3.161 [-1.633, 10.399], loss: 0.916448, mae: 4.058426, mean_q: 4.520308
 15122/100000: episode: 1545, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.602, mean reward: 0.360 [0.333, 0.431], mean action: 37.400 [7.000, 89.000], mean observation: 3.165 [-1.543, 10.428], loss: 0.807555, mae: 4.059518, mean_q: 4.524624
 15132/100000: episode: 1546, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.365, mean reward: 0.436 [0.362, 0.550], mean action: 44.000 [23.000, 100.000], mean observation: 3.150 [-1.782, 10.403], loss: 0.939955, mae: 4.061036, mean_q: 4.528841
 15142/100000: episode: 1547, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.123, mean reward: 0.412 [0.317, 0.492], mean action: 44.300 [27.000, 93.000], mean observation: 3.155 [-0.700, 10.321], loss: 1.042311, mae: 4.063767, mean_q: 4.530921
 15152/100000: episode: 1548, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.007, mean reward: 0.401 [0.361, 0.443], mean action: 34.000 [8.000, 63.000], mean observation: 3.161 [-1.330, 10.158], loss: 0.905455, mae: 4.063957, mean_q: 4.526568
 15162/100000: episode: 1549, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.053, mean reward: 0.405 [0.302, 0.516], mean action: 33.500 [14.000, 63.000], mean observation: 3.164 [-1.360, 10.301], loss: 1.157487, mae: 4.066106, mean_q: 4.523332
 15172/100000: episode: 1550, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.015, mean reward: 0.401 [0.327, 0.486], mean action: 36.100 [26.000, 74.000], mean observation: 3.158 [-1.236, 10.354], loss: 0.925508, mae: 4.065723, mean_q: 4.520677
 15182/100000: episode: 1551, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.277, mean reward: 0.428 [0.380, 0.471], mean action: 38.600 [0.000, 88.000], mean observation: 3.159 [-1.496, 10.418], loss: 0.851935, mae: 4.066813, mean_q: 4.523463
 15192/100000: episode: 1552, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.970, mean reward: 0.497 [0.497, 0.497], mean action: 37.400 [27.000, 86.000], mean observation: 3.151 [-1.372, 10.237], loss: 0.804525, mae: 4.067668, mean_q: 4.527764
 15193/100000: episode: 1553, duration: 0.049s, episode steps: 1, steps per second: 20, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 27.000 [27.000, 27.000], mean observation: 3.155 [-0.974, 10.196], loss: 1.303159, mae: 4.070797, mean_q: 4.529340
 15203/100000: episode: 1554, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.281, mean reward: 0.428 [0.381, 0.443], mean action: 60.300 [17.000, 101.000], mean observation: 3.157 [-1.926, 10.294], loss: 1.055999, mae: 4.070783, mean_q: 4.528663
 15213/100000: episode: 1555, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.790, mean reward: 0.379 [0.330, 0.498], mean action: 37.300 [13.000, 88.000], mean observation: 3.155 [-1.380, 10.340], loss: 0.894916, mae: 4.070772, mean_q: 4.524498
 15223/100000: episode: 1556, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.826, mean reward: 0.383 [0.289, 0.482], mean action: 23.500 [8.000, 30.000], mean observation: 3.160 [-1.351, 10.434], loss: 0.708076, mae: 4.071098, mean_q: 4.522631
 15233/100000: episode: 1557, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.232, mean reward: 0.423 [0.412, 0.493], mean action: 41.000 [27.000, 95.000], mean observation: 3.147 [-1.370, 10.259], loss: 0.918333, mae: 4.072995, mean_q: 4.525207
 15243/100000: episode: 1558, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.095, mean reward: 0.410 [0.333, 0.528], mean action: 34.900 [3.000, 93.000], mean observation: 3.146 [-1.307, 10.399], loss: 1.065520, mae: 4.074920, mean_q: 4.525906
 15253/100000: episode: 1559, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.037, mean reward: 0.404 [0.311, 0.524], mean action: 42.100 [23.000, 99.000], mean observation: 3.149 [-2.322, 10.251], loss: 0.963966, mae: 4.074962, mean_q: 4.523779
 15263/100000: episode: 1560, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.111, mean reward: 0.411 [0.340, 0.488], mean action: 44.600 [1.000, 89.000], mean observation: 3.156 [-1.252, 10.362], loss: 0.895841, mae: 4.075465, mean_q: 4.523166
 15273/100000: episode: 1561, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.255, mean reward: 0.426 [0.354, 0.571], mean action: 61.800 [30.000, 97.000], mean observation: 3.156 [-1.152, 10.332], loss: 0.944212, mae: 4.077289, mean_q: 4.525492
 15283/100000: episode: 1562, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.838, mean reward: 0.384 [0.324, 0.514], mean action: 54.300 [16.000, 101.000], mean observation: 3.156 [-1.110, 10.405], loss: 1.078436, mae: 4.079106, mean_q: 4.527732
 15293/100000: episode: 1563, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.101, mean reward: 0.410 [0.330, 0.522], mean action: 41.100 [6.000, 53.000], mean observation: 3.148 [-1.306, 10.307], loss: 1.147063, mae: 4.080136, mean_q: 4.529284
 15303/100000: episode: 1564, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.919, mean reward: 0.392 [0.318, 0.448], mean action: 52.800 [17.000, 90.000], mean observation: 3.150 [-1.387, 10.199], loss: 0.938728, mae: 4.080378, mean_q: 4.530982
 15313/100000: episode: 1565, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.054, mean reward: 0.405 [0.342, 0.475], mean action: 47.300 [25.000, 69.000], mean observation: 3.158 [-1.584, 10.338], loss: 0.878526, mae: 4.081278, mean_q: 4.532780
 15323/100000: episode: 1566, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.971, mean reward: 0.397 [0.315, 0.452], mean action: 55.500 [32.000, 99.000], mean observation: 3.152 [-1.285, 10.365], loss: 1.158602, mae: 4.083460, mean_q: 4.534349
 15333/100000: episode: 1567, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.172, mean reward: 0.417 [0.363, 0.466], mean action: 53.900 [30.000, 83.000], mean observation: 3.164 [-1.098, 10.270], loss: 1.033060, mae: 4.083778, mean_q: 4.537871
 15343/100000: episode: 1568, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.875, mean reward: 0.387 [0.304, 0.443], mean action: 49.600 [6.000, 88.000], mean observation: 3.152 [-0.995, 10.357], loss: 0.887603, mae: 4.084714, mean_q: 4.544616
 15353/100000: episode: 1569, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.395, mean reward: 0.439 [0.419, 0.448], mean action: 48.300 [27.000, 64.000], mean observation: 3.150 [-1.020, 10.307], loss: 0.759072, mae: 4.085223, mean_q: 4.543490
 15363/100000: episode: 1570, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.871, mean reward: 0.387 [0.346, 0.503], mean action: 59.100 [50.000, 89.000], mean observation: 3.157 [-2.208, 10.336], loss: 0.915000, mae: 4.086649, mean_q: 4.543413
 15373/100000: episode: 1571, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.165, mean reward: 0.417 [0.321, 0.466], mean action: 49.400 [7.000, 83.000], mean observation: 3.151 [-1.332, 10.335], loss: 0.916650, mae: 4.088135, mean_q: 4.546069
 15380/100000: episode: 1572, duration: 0.132s, episode steps: 7, steps per second: 53, episode reward: 12.723, mean reward: 1.818 [0.435, 10.000], mean action: 41.429 [6.000, 57.000], mean observation: 3.152 [-1.654, 10.368], loss: 1.163736, mae: 4.090579, mean_q: 4.548849
 15390/100000: episode: 1573, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.217, mean reward: 0.422 [0.401, 0.519], mean action: 49.800 [28.000, 84.000], mean observation: 3.153 [-1.518, 10.306], loss: 0.967617, mae: 4.090313, mean_q: 4.551181
 15400/100000: episode: 1574, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.695, mean reward: 0.469 [0.468, 0.483], mean action: 48.400 [3.000, 82.000], mean observation: 3.164 [-1.828, 10.361], loss: 0.746928, mae: 4.090587, mean_q: 4.551943
 15410/100000: episode: 1575, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.463, mean reward: 0.446 [0.367, 0.495], mean action: 54.500 [5.000, 99.000], mean observation: 3.146 [-1.508, 10.248], loss: 1.187744, mae: 4.093732, mean_q: 4.550021
 15420/100000: episode: 1576, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.732, mean reward: 0.373 [0.328, 0.460], mean action: 53.100 [35.000, 73.000], mean observation: 3.172 [-1.666, 10.304], loss: 0.864810, mae: 4.093177, mean_q: 4.549891
 15430/100000: episode: 1577, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 5.152, mean reward: 0.515 [0.450, 0.572], mean action: 40.600 [3.000, 61.000], mean observation: 3.144 [-1.570, 10.319], loss: 1.169678, mae: 4.095961, mean_q: 4.548439
 15440/100000: episode: 1578, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.996, mean reward: 0.400 [0.392, 0.459], mean action: 52.300 [37.000, 83.000], mean observation: 3.159 [-1.059, 10.221], loss: 0.883170, mae: 4.095495, mean_q: 4.546256
 15450/100000: episode: 1579, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.669, mean reward: 0.367 [0.300, 0.458], mean action: 53.300 [44.000, 95.000], mean observation: 3.157 [-1.484, 10.383], loss: 1.002930, mae: 4.097291, mean_q: 4.546012
 15460/100000: episode: 1580, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.059, mean reward: 0.406 [0.327, 0.553], mean action: 43.000 [4.000, 57.000], mean observation: 3.150 [-1.252, 10.475], loss: 1.000178, mae: 4.098990, mean_q: 4.551157
 15470/100000: episode: 1581, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.947, mean reward: 0.395 [0.295, 0.499], mean action: 53.000 [18.000, 93.000], mean observation: 3.151 [-0.916, 10.269], loss: 0.858685, mae: 4.099295, mean_q: 4.553463
 15478/100000: episode: 1582, duration: 0.135s, episode steps: 8, steps per second: 59, episode reward: 12.884, mean reward: 1.611 [0.395, 10.000], mean action: 42.625 [10.000, 73.000], mean observation: 3.157 [-1.117, 10.281], loss: 1.109513, mae: 4.101527, mean_q: 4.554807
 15488/100000: episode: 1583, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.524, mean reward: 0.452 [0.391, 0.509], mean action: 51.200 [1.000, 72.000], mean observation: 3.164 [-1.297, 10.491], loss: 1.062298, mae: 4.102038, mean_q: 4.554316
 15498/100000: episode: 1584, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.326, mean reward: 0.433 [0.354, 0.521], mean action: 50.900 [17.000, 83.000], mean observation: 3.160 [-1.578, 10.333], loss: 0.945385, mae: 4.102499, mean_q: 4.551393
 15508/100000: episode: 1585, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.123, mean reward: 0.412 [0.340, 0.521], mean action: 47.900 [0.000, 95.000], mean observation: 3.157 [-1.497, 10.395], loss: 0.877767, mae: 4.103106, mean_q: 4.552720
 15518/100000: episode: 1586, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.406, mean reward: 0.441 [0.375, 0.493], mean action: 47.400 [22.000, 97.000], mean observation: 3.164 [-2.240, 10.310], loss: 0.909755, mae: 4.103980, mean_q: 4.554799
 15528/100000: episode: 1587, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.451, mean reward: 0.445 [0.396, 0.586], mean action: 55.100 [15.000, 96.000], mean observation: 3.156 [-1.247, 10.304], loss: 0.994403, mae: 4.105565, mean_q: 4.553335
 15538/100000: episode: 1588, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.021, mean reward: 0.402 [0.351, 0.495], mean action: 41.900 [1.000, 89.000], mean observation: 3.154 [-0.831, 10.295], loss: 0.959742, mae: 4.106576, mean_q: 4.553679
 15548/100000: episode: 1589, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.868, mean reward: 0.487 [0.409, 0.521], mean action: 50.700 [12.000, 91.000], mean observation: 3.155 [-1.275, 10.276], loss: 0.905523, mae: 4.107364, mean_q: 4.552949
 15558/100000: episode: 1590, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.720, mean reward: 0.372 [0.345, 0.412], mean action: 46.700 [6.000, 62.000], mean observation: 3.157 [-1.672, 10.223], loss: 0.990399, mae: 4.108975, mean_q: 4.554591
 15568/100000: episode: 1591, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.033, mean reward: 0.403 [0.338, 0.450], mean action: 46.100 [21.000, 61.000], mean observation: 3.146 [-2.108, 10.371], loss: 1.068152, mae: 4.110438, mean_q: 4.553185
 15578/100000: episode: 1592, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.038, mean reward: 0.404 [0.300, 0.525], mean action: 52.600 [11.000, 85.000], mean observation: 3.153 [-2.115, 10.220], loss: 0.847521, mae: 4.110255, mean_q: 4.552703
 15588/100000: episode: 1593, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.651, mean reward: 0.365 [0.277, 0.435], mean action: 53.400 [7.000, 81.000], mean observation: 3.165 [-1.306, 10.308], loss: 1.094566, mae: 4.112093, mean_q: 4.552271
 15598/100000: episode: 1594, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.912, mean reward: 0.491 [0.343, 0.547], mean action: 48.400 [19.000, 97.000], mean observation: 3.168 [-1.080, 10.355], loss: 1.109627, mae: 4.113441, mean_q: 4.555116
 15608/100000: episode: 1595, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.205, mean reward: 0.321 [0.268, 0.381], mean action: 62.200 [41.000, 101.000], mean observation: 3.154 [-1.502, 10.346], loss: 1.206579, mae: 4.115346, mean_q: 4.558045
 15618/100000: episode: 1596, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.902, mean reward: 0.390 [0.303, 0.489], mean action: 41.800 [1.000, 68.000], mean observation: 3.158 [-1.223, 10.337], loss: 0.817677, mae: 4.114174, mean_q: 4.559977
 15628/100000: episode: 1597, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.074, mean reward: 0.407 [0.353, 0.477], mean action: 39.000 [1.000, 56.000], mean observation: 3.151 [-1.465, 10.374], loss: 1.069991, mae: 4.116758, mean_q: 4.562995
 15638/100000: episode: 1598, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.970, mean reward: 0.397 [0.355, 0.561], mean action: 52.000 [31.000, 94.000], mean observation: 3.160 [-1.297, 10.393], loss: 1.097856, mae: 4.117679, mean_q: 4.564883
 15648/100000: episode: 1599, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.398, mean reward: 0.440 [0.434, 0.463], mean action: 51.900 [13.000, 99.000], mean observation: 3.172 [-0.726, 10.310], loss: 1.079375, mae: 4.118213, mean_q: 4.562239
 15658/100000: episode: 1600, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.431, mean reward: 0.443 [0.368, 0.555], mean action: 51.600 [7.000, 94.000], mean observation: 3.154 [-1.877, 10.284], loss: 0.930304, mae: 4.118566, mean_q: 4.561274
 15668/100000: episode: 1601, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.642, mean reward: 0.464 [0.379, 0.483], mean action: 43.400 [17.000, 77.000], mean observation: 3.158 [-1.724, 10.237], loss: 1.024294, mae: 4.120484, mean_q: 4.563078
 15678/100000: episode: 1602, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.081, mean reward: 0.408 [0.376, 0.488], mean action: 59.900 [50.000, 100.000], mean observation: 3.154 [-1.769, 10.411], loss: 1.103989, mae: 4.121634, mean_q: 4.565763
 15688/100000: episode: 1603, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.107, mean reward: 0.411 [0.390, 0.434], mean action: 37.900 [0.000, 79.000], mean observation: 3.156 [-1.507, 10.285], loss: 0.934333, mae: 4.122002, mean_q: 4.569196
 15698/100000: episode: 1604, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.312, mean reward: 0.431 [0.385, 0.574], mean action: 50.700 [6.000, 99.000], mean observation: 3.166 [-1.047, 10.295], loss: 1.093474, mae: 4.123816, mean_q: 4.570611
 15708/100000: episode: 1605, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.725, mean reward: 0.472 [0.434, 0.519], mean action: 48.800 [16.000, 91.000], mean observation: 3.165 [-1.917, 10.368], loss: 1.074368, mae: 4.124991, mean_q: 4.570105
 15718/100000: episode: 1606, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.975, mean reward: 0.397 [0.288, 0.514], mean action: 39.900 [17.000, 59.000], mean observation: 3.154 [-1.579, 10.339], loss: 1.116301, mae: 4.125921, mean_q: 4.571652
 15728/100000: episode: 1607, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.922, mean reward: 0.492 [0.492, 0.494], mean action: 66.900 [50.000, 101.000], mean observation: 3.150 [-0.922, 10.336], loss: 1.114807, mae: 4.127032, mean_q: 4.572453
 15738/100000: episode: 1608, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.052, mean reward: 0.405 [0.356, 0.455], mean action: 58.100 [31.000, 84.000], mean observation: 3.155 [-1.200, 10.260], loss: 1.068316, mae: 4.127379, mean_q: 4.571970
 15748/100000: episode: 1609, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.254, mean reward: 0.425 [0.416, 0.434], mean action: 50.500 [22.000, 86.000], mean observation: 3.166 [-1.310, 10.390], loss: 0.816382, mae: 4.127735, mean_q: 4.575004
 15758/100000: episode: 1610, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.016, mean reward: 0.402 [0.376, 0.496], mean action: 60.200 [50.000, 80.000], mean observation: 3.155 [-1.327, 10.302], loss: 0.942253, mae: 4.129804, mean_q: 4.580995
 15768/100000: episode: 1611, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.413, mean reward: 0.441 [0.309, 0.456], mean action: 46.300 [7.000, 68.000], mean observation: 3.147 [-1.078, 10.357], loss: 0.782011, mae: 4.129863, mean_q: 4.585219
 15778/100000: episode: 1612, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.343, mean reward: 0.434 [0.324, 0.579], mean action: 48.100 [15.000, 90.000], mean observation: 3.157 [-1.554, 10.399], loss: 1.043700, mae: 4.132243, mean_q: 4.588045
 15788/100000: episode: 1613, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.307, mean reward: 0.431 [0.423, 0.498], mean action: 45.000 [11.000, 97.000], mean observation: 3.162 [-1.015, 10.383], loss: 0.965857, mae: 4.132663, mean_q: 4.590098
 15798/100000: episode: 1614, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.721, mean reward: 0.372 [0.361, 0.430], mean action: 64.900 [49.000, 91.000], mean observation: 3.154 [-1.301, 10.305], loss: 0.883942, mae: 4.133592, mean_q: 4.597277
 15808/100000: episode: 1615, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.426, mean reward: 0.443 [0.393, 0.486], mean action: 49.300 [9.000, 80.000], mean observation: 3.171 [-1.543, 10.314], loss: 0.938143, mae: 4.135282, mean_q: 4.604785
 15818/100000: episode: 1616, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.116, mean reward: 0.412 [0.300, 0.542], mean action: 50.500 [9.000, 98.000], mean observation: 3.181 [-1.720, 10.188], loss: 1.253798, mae: 4.138450, mean_q: 4.607078
 15828/100000: episode: 1617, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.383, mean reward: 0.438 [0.365, 0.476], mean action: 50.800 [5.000, 95.000], mean observation: 3.153 [-1.326, 10.308], loss: 0.758839, mae: 4.137650, mean_q: 4.610229
 15838/100000: episode: 1618, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.200, mean reward: 0.420 [0.372, 0.522], mean action: 37.400 [3.000, 67.000], mean observation: 3.156 [-1.392, 10.246], loss: 1.116522, mae: 4.140475, mean_q: 4.612120
 15848/100000: episode: 1619, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.236, mean reward: 0.424 [0.417, 0.459], mean action: 51.600 [49.000, 68.000], mean observation: 3.161 [-0.950, 10.262], loss: 0.964952, mae: 4.140977, mean_q: 4.607155
 15858/100000: episode: 1620, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.147, mean reward: 0.415 [0.364, 0.485], mean action: 50.600 [8.000, 101.000], mean observation: 3.158 [-0.754, 10.383], loss: 0.894031, mae: 4.141824, mean_q: 4.603500
 15868/100000: episode: 1621, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.880, mean reward: 0.388 [0.351, 0.491], mean action: 54.500 [27.000, 88.000], mean observation: 3.166 [-0.982, 10.423], loss: 1.019454, mae: 4.143451, mean_q: 4.604346
 15878/100000: episode: 1622, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.572, mean reward: 0.457 [0.456, 0.464], mean action: 57.000 [28.000, 86.000], mean observation: 3.153 [-1.282, 10.286], loss: 1.169769, mae: 4.144892, mean_q: 4.602843
 15888/100000: episode: 1623, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.071, mean reward: 0.407 [0.356, 0.508], mean action: 38.800 [12.000, 52.000], mean observation: 3.161 [-1.120, 10.246], loss: 1.188567, mae: 4.146221, mean_q: 4.601456
 15898/100000: episode: 1624, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.049, mean reward: 0.405 [0.303, 0.485], mean action: 43.700 [0.000, 89.000], mean observation: 3.152 [-1.156, 10.332], loss: 0.778638, mae: 4.144463, mean_q: 4.595260
 15908/100000: episode: 1625, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.115, mean reward: 0.412 [0.395, 0.438], mean action: 44.700 [7.000, 64.000], mean observation: 3.176 [-1.477, 10.248], loss: 1.041698, mae: 4.146646, mean_q: 4.594696
 15918/100000: episode: 1626, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.014, mean reward: 0.401 [0.385, 0.488], mean action: 57.600 [12.000, 93.000], mean observation: 3.153 [-1.391, 10.294], loss: 1.004592, mae: 4.147757, mean_q: 4.598252
 15928/100000: episode: 1627, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.349, mean reward: 0.435 [0.413, 0.470], mean action: 60.500 [50.000, 82.000], mean observation: 3.159 [-2.220, 10.302], loss: 0.872512, mae: 4.148606, mean_q: 4.602456
 15938/100000: episode: 1628, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.645, mean reward: 0.465 [0.340, 0.536], mean action: 42.000 [14.000, 50.000], mean observation: 3.140 [-1.274, 10.231], loss: 1.205390, mae: 4.150886, mean_q: 4.606052
 15948/100000: episode: 1629, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.205, mean reward: 0.420 [0.399, 0.493], mean action: 66.200 [50.000, 101.000], mean observation: 3.166 [-1.093, 10.439], loss: 1.085361, mae: 4.151244, mean_q: 4.607323
 15958/100000: episode: 1630, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.874, mean reward: 0.387 [0.352, 0.454], mean action: 51.000 [1.000, 100.000], mean observation: 3.168 [-1.423, 10.292], loss: 0.784936, mae: 4.151227, mean_q: 4.607463
 15968/100000: episode: 1631, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.828, mean reward: 0.383 [0.285, 0.460], mean action: 46.500 [15.000, 50.000], mean observation: 3.153 [-1.458, 10.224], loss: 1.007100, mae: 4.153040, mean_q: 4.613409
 15978/100000: episode: 1632, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.014, mean reward: 0.401 [0.354, 0.501], mean action: 55.700 [23.000, 100.000], mean observation: 3.158 [-1.581, 10.282], loss: 1.017704, mae: 4.154709, mean_q: 4.615377
 15988/100000: episode: 1633, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.897, mean reward: 0.390 [0.382, 0.442], mean action: 55.000 [50.000, 85.000], mean observation: 3.154 [-1.222, 10.412], loss: 1.137799, mae: 4.155529, mean_q: 4.615686
 15998/100000: episode: 1634, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.330, mean reward: 0.433 [0.379, 0.471], mean action: 43.100 [13.000, 86.000], mean observation: 3.163 [-1.446, 10.326], loss: 1.116782, mae: 4.157461, mean_q: 4.615683
 16008/100000: episode: 1635, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.003, mean reward: 0.400 [0.315, 0.548], mean action: 50.400 [1.000, 101.000], mean observation: 3.156 [-1.419, 10.406], loss: 0.944671, mae: 4.156817, mean_q: 4.618398
 16018/100000: episode: 1636, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.805, mean reward: 0.481 [0.391, 0.565], mean action: 41.900 [4.000, 50.000], mean observation: 3.156 [-1.558, 10.377], loss: 1.150995, mae: 4.159264, mean_q: 4.627059
 16028/100000: episode: 1637, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.588, mean reward: 0.459 [0.431, 0.536], mean action: 60.200 [44.000, 96.000], mean observation: 3.155 [-0.785, 10.282], loss: 1.061623, mae: 4.159976, mean_q: 4.634081
 16038/100000: episode: 1638, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.641, mean reward: 0.364 [0.294, 0.446], mean action: 51.000 [20.000, 90.000], mean observation: 3.151 [-1.662, 10.338], loss: 1.062419, mae: 4.160908, mean_q: 4.635919
 16048/100000: episode: 1639, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.923, mean reward: 0.392 [0.367, 0.482], mean action: 54.600 [23.000, 99.000], mean observation: 3.148 [-1.024, 10.385], loss: 0.883323, mae: 4.161400, mean_q: 4.640653
 16058/100000: episode: 1640, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.851, mean reward: 0.385 [0.308, 0.452], mean action: 54.300 [17.000, 89.000], mean observation: 3.165 [-1.493, 10.466], loss: 0.940418, mae: 4.163041, mean_q: 4.646274
 16068/100000: episode: 1641, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.417, mean reward: 0.442 [0.395, 0.535], mean action: 51.600 [17.000, 92.000], mean observation: 3.154 [-1.138, 10.480], loss: 0.941838, mae: 4.164143, mean_q: 4.650679
 16078/100000: episode: 1642, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.799, mean reward: 0.380 [0.312, 0.548], mean action: 33.500 [5.000, 50.000], mean observation: 3.152 [-1.154, 10.405], loss: 1.038403, mae: 4.166109, mean_q: 4.653839
 16088/100000: episode: 1643, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.940, mean reward: 0.394 [0.310, 0.492], mean action: 58.100 [38.000, 97.000], mean observation: 3.159 [-0.859, 10.264], loss: 1.122510, mae: 4.167670, mean_q: 4.658214
 16098/100000: episode: 1644, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.304, mean reward: 0.430 [0.333, 0.519], mean action: 48.900 [11.000, 88.000], mean observation: 3.162 [-1.084, 10.280], loss: 0.993405, mae: 4.168612, mean_q: 4.660088
 16108/100000: episode: 1645, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.986, mean reward: 0.399 [0.377, 0.445], mean action: 44.800 [0.000, 90.000], mean observation: 3.166 [-1.621, 10.251], loss: 0.789860, mae: 4.168563, mean_q: 4.661526
 16118/100000: episode: 1646, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.870, mean reward: 0.387 [0.329, 0.405], mean action: 52.200 [22.000, 100.000], mean observation: 3.153 [-1.454, 10.380], loss: 0.969396, mae: 4.170748, mean_q: 4.663022
 16128/100000: episode: 1647, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.647, mean reward: 0.365 [0.333, 0.438], mean action: 61.100 [32.000, 83.000], mean observation: 3.150 [-1.692, 10.250], loss: 0.971385, mae: 4.171961, mean_q: 4.660712
 16138/100000: episode: 1648, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.911, mean reward: 0.391 [0.361, 0.423], mean action: 53.200 [6.000, 89.000], mean observation: 3.149 [-1.609, 10.415], loss: 1.145756, mae: 4.173584, mean_q: 4.659263
 16148/100000: episode: 1649, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.858, mean reward: 0.386 [0.296, 0.483], mean action: 44.800 [3.000, 76.000], mean observation: 3.159 [-1.553, 10.384], loss: 0.690648, mae: 4.172507, mean_q: 4.653481
 16158/100000: episode: 1650, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.381, mean reward: 0.438 [0.363, 0.460], mean action: 43.000 [2.000, 85.000], mean observation: 3.154 [-0.986, 10.349], loss: 1.169646, mae: 4.175305, mean_q: 4.651770
 16168/100000: episode: 1651, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.783, mean reward: 0.378 [0.322, 0.448], mean action: 25.200 [7.000, 50.000], mean observation: 3.148 [-1.615, 10.425], loss: 0.985995, mae: 4.175763, mean_q: 4.649235
 16178/100000: episode: 1652, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.889, mean reward: 0.489 [0.489, 0.489], mean action: 53.800 [26.000, 91.000], mean observation: 3.156 [-1.077, 10.398], loss: 1.021129, mae: 4.176837, mean_q: 4.650361
 16188/100000: episode: 1653, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.979, mean reward: 0.398 [0.365, 0.503], mean action: 57.900 [39.000, 93.000], mean observation: 3.155 [-1.823, 10.331], loss: 0.939276, mae: 4.178056, mean_q: 4.653190
 16198/100000: episode: 1654, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.992, mean reward: 0.399 [0.308, 0.565], mean action: 50.100 [18.000, 98.000], mean observation: 3.142 [-1.120, 10.182], loss: 1.043188, mae: 4.179428, mean_q: 4.654087
 16200/100000: episode: 1655, duration: 0.054s, episode steps: 2, steps per second: 37, episode reward: 10.389, mean reward: 5.194 [0.389, 10.000], mean action: 48.500 [47.000, 50.000], mean observation: 3.131 [-1.178, 10.100], loss: 1.420053, mae: 4.181708, mean_q: 4.655079
 16210/100000: episode: 1656, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.292, mean reward: 0.429 [0.332, 0.565], mean action: 46.600 [18.000, 50.000], mean observation: 3.165 [-1.457, 10.306], loss: 0.965811, mae: 4.180578, mean_q: 4.656740
 16220/100000: episode: 1657, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.604, mean reward: 0.360 [0.339, 0.512], mean action: 60.800 [37.000, 100.000], mean observation: 3.148 [-1.349, 10.407], loss: 0.916604, mae: 4.181466, mean_q: 4.656716
 16230/100000: episode: 1658, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.902, mean reward: 0.390 [0.337, 0.474], mean action: 51.300 [10.000, 85.000], mean observation: 3.161 [-1.248, 10.460], loss: 1.092842, mae: 4.183198, mean_q: 4.653438
 16235/100000: episode: 1659, duration: 0.109s, episode steps: 5, steps per second: 46, episode reward: 11.605, mean reward: 2.321 [0.395, 10.000], mean action: 29.400 [2.000, 57.000], mean observation: 3.150 [-1.351, 10.143], loss: 1.168040, mae: 4.184335, mean_q: 4.650784
 16245/100000: episode: 1660, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.106, mean reward: 0.411 [0.327, 0.561], mean action: 46.900 [7.000, 92.000], mean observation: 3.162 [-1.393, 10.339], loss: 1.102477, mae: 4.184777, mean_q: 4.646941
 16255/100000: episode: 1661, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 4.384, mean reward: 0.438 [0.406, 0.510], mean action: 61.200 [28.000, 101.000], mean observation: 3.152 [-1.449, 10.428], loss: 1.162882, mae: 4.185833, mean_q: 4.643031
 16265/100000: episode: 1662, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.954, mean reward: 0.395 [0.361, 0.426], mean action: 50.000 [19.000, 76.000], mean observation: 3.154 [-1.397, 10.427], loss: 1.011602, mae: 4.185781, mean_q: 4.644597
 16275/100000: episode: 1663, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.864, mean reward: 0.386 [0.350, 0.532], mean action: 60.800 [35.000, 87.000], mean observation: 3.149 [-1.262, 10.294], loss: 1.143671, mae: 4.186894, mean_q: 4.645391
 16285/100000: episode: 1664, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.609, mean reward: 0.461 [0.458, 0.482], mean action: 55.500 [8.000, 94.000], mean observation: 3.157 [-1.714, 10.166], loss: 1.407912, mae: 4.189436, mean_q: 4.641529
 16295/100000: episode: 1665, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.145, mean reward: 0.415 [0.356, 0.484], mean action: 44.000 [6.000, 60.000], mean observation: 3.158 [-2.139, 10.308], loss: 0.892281, mae: 4.188560, mean_q: 4.635351
 16305/100000: episode: 1666, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.084, mean reward: 0.408 [0.368, 0.483], mean action: 52.200 [3.000, 73.000], mean observation: 3.160 [-1.232, 10.200], loss: 0.964719, mae: 4.189871, mean_q: 4.633653
 16315/100000: episode: 1667, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.422, mean reward: 0.442 [0.421, 0.455], mean action: 51.100 [11.000, 100.000], mean observation: 3.148 [-1.237, 10.382], loss: 0.988409, mae: 4.190951, mean_q: 4.632639
 16325/100000: episode: 1668, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.959, mean reward: 0.396 [0.301, 0.540], mean action: 51.600 [27.000, 77.000], mean observation: 3.157 [-1.551, 10.175], loss: 0.922469, mae: 4.191855, mean_q: 4.634115
 16335/100000: episode: 1669, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.055, mean reward: 0.406 [0.346, 0.495], mean action: 42.800 [11.000, 64.000], mean observation: 3.163 [-1.327, 10.221], loss: 1.093524, mae: 4.193829, mean_q: 4.638923
 16345/100000: episode: 1670, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.661, mean reward: 0.366 [0.290, 0.518], mean action: 59.400 [28.000, 97.000], mean observation: 3.160 [-0.960, 10.360], loss: 1.130356, mae: 4.194916, mean_q: 4.639940
 16355/100000: episode: 1671, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.810, mean reward: 0.481 [0.481, 0.483], mean action: 54.600 [38.000, 89.000], mean observation: 3.166 [-1.563, 10.392], loss: 0.840443, mae: 4.194433, mean_q: 4.637876
 16365/100000: episode: 1672, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.936, mean reward: 0.394 [0.332, 0.471], mean action: 50.500 [25.000, 100.000], mean observation: 3.173 [-1.530, 10.300], loss: 0.889940, mae: 4.195538, mean_q: 4.638220
 16375/100000: episode: 1673, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.684, mean reward: 0.368 [0.307, 0.498], mean action: 43.800 [28.000, 50.000], mean observation: 3.165 [-2.502, 10.325], loss: 1.183763, mae: 4.197886, mean_q: 4.639224
 16385/100000: episode: 1674, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 5.440, mean reward: 0.544 [0.543, 0.552], mean action: 68.100 [42.000, 101.000], mean observation: 3.163 [-1.229, 10.267], loss: 0.980692, mae: 4.197668, mean_q: 4.637871
 16395/100000: episode: 1675, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.192, mean reward: 0.419 [0.401, 0.467], mean action: 49.900 [19.000, 87.000], mean observation: 3.168 [-1.132, 10.165], loss: 0.895982, mae: 4.198138, mean_q: 4.636378
 16405/100000: episode: 1676, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.317, mean reward: 0.432 [0.350, 0.511], mean action: 54.700 [22.000, 93.000], mean observation: 3.144 [-1.719, 10.359], loss: 1.070292, mae: 4.200027, mean_q: 4.636552
 16415/100000: episode: 1677, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.053, mean reward: 0.405 [0.373, 0.417], mean action: 50.500 [6.000, 99.000], mean observation: 3.150 [-1.495, 10.336], loss: 0.945061, mae: 4.200164, mean_q: 4.638929
 16425/100000: episode: 1678, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.226, mean reward: 0.423 [0.407, 0.458], mean action: 65.600 [50.000, 100.000], mean observation: 3.159 [-1.035, 10.363], loss: 0.845930, mae: 4.201334, mean_q: 4.643169
 16435/100000: episode: 1679, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.487, mean reward: 0.449 [0.322, 0.573], mean action: 46.100 [1.000, 98.000], mean observation: 3.148 [-1.260, 10.348], loss: 0.952204, mae: 4.202773, mean_q: 4.646473
 16445/100000: episode: 1680, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.192, mean reward: 0.419 [0.297, 0.560], mean action: 52.000 [9.000, 90.000], mean observation: 3.165 [-1.924, 10.265], loss: 0.947266, mae: 4.203513, mean_q: 4.647917
 16455/100000: episode: 1681, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 3.655, mean reward: 0.366 [0.321, 0.431], mean action: 55.400 [41.000, 101.000], mean observation: 3.154 [-1.734, 10.266], loss: 1.051043, mae: 4.204577, mean_q: 4.648987
 16465/100000: episode: 1682, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.228, mean reward: 0.423 [0.390, 0.488], mean action: 59.600 [33.000, 93.000], mean observation: 3.166 [-1.370, 10.232], loss: 0.920323, mae: 4.205345, mean_q: 4.648650
 16475/100000: episode: 1683, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.016, mean reward: 0.402 [0.389, 0.438], mean action: 55.000 [46.000, 100.000], mean observation: 3.146 [-1.103, 10.308], loss: 0.781908, mae: 4.205335, mean_q: 4.650671
 16485/100000: episode: 1684, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.208, mean reward: 0.421 [0.328, 0.491], mean action: 46.900 [28.000, 98.000], mean observation: 3.146 [-1.391, 10.408], loss: 1.122967, mae: 4.208577, mean_q: 4.654762
 16495/100000: episode: 1685, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.092, mean reward: 0.409 [0.326, 0.515], mean action: 47.600 [19.000, 90.000], mean observation: 3.149 [-1.446, 10.226], loss: 1.031950, mae: 4.208640, mean_q: 4.656876
 16505/100000: episode: 1686, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.386, mean reward: 0.439 [0.394, 0.571], mean action: 52.000 [42.000, 81.000], mean observation: 3.153 [-1.357, 10.406], loss: 1.003780, mae: 4.208869, mean_q: 4.658055
 16515/100000: episode: 1687, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.874, mean reward: 0.387 [0.352, 0.498], mean action: 53.000 [2.000, 101.000], mean observation: 3.157 [-1.817, 10.286], loss: 0.835587, mae: 4.209989, mean_q: 4.655780
 16525/100000: episode: 1688, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.867, mean reward: 0.387 [0.376, 0.428], mean action: 62.200 [4.000, 101.000], mean observation: 3.157 [-0.467, 10.417], loss: 0.958035, mae: 4.211644, mean_q: 4.653863
 16535/100000: episode: 1689, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.979, mean reward: 0.398 [0.306, 0.461], mean action: 52.300 [34.000, 73.000], mean observation: 3.153 [-1.029, 10.272], loss: 0.961637, mae: 4.213397, mean_q: 4.654289
 16545/100000: episode: 1690, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.218, mean reward: 0.422 [0.350, 0.576], mean action: 40.600 [0.000, 83.000], mean observation: 3.162 [-2.000, 10.306], loss: 0.940985, mae: 4.214645, mean_q: 4.655321
 16555/100000: episode: 1691, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.015, mean reward: 0.402 [0.346, 0.444], mean action: 56.300 [30.000, 94.000], mean observation: 3.157 [-1.336, 10.303], loss: 1.337565, mae: 4.216546, mean_q: 4.656686
 16565/100000: episode: 1692, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.934, mean reward: 0.393 [0.351, 0.428], mean action: 36.400 [6.000, 50.000], mean observation: 3.146 [-1.173, 10.272], loss: 0.962951, mae: 4.216313, mean_q: 4.659518
 16567/100000: episode: 1693, duration: 0.053s, episode steps: 2, steps per second: 38, episode reward: 10.415, mean reward: 5.207 [0.415, 10.000], mean action: 30.500 [11.000, 50.000], mean observation: 3.162 [-1.227, 10.349], loss: 1.044191, mae: 4.217002, mean_q: 4.659763
 16576/100000: episode: 1694, duration: 0.149s, episode steps: 9, steps per second: 61, episode reward: 13.523, mean reward: 1.503 [0.387, 10.000], mean action: 52.111 [22.000, 99.000], mean observation: 3.151 [-1.095, 10.308], loss: 1.199287, mae: 4.218741, mean_q: 4.659983
 16586/100000: episode: 1695, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.415, mean reward: 0.442 [0.442, 0.442], mean action: 41.500 [5.000, 60.000], mean observation: 3.148 [-1.234, 10.266], loss: 1.073154, mae: 4.218547, mean_q: 4.660123
 16596/100000: episode: 1696, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.252, mean reward: 0.425 [0.424, 0.439], mean action: 55.200 [50.000, 94.000], mean observation: 3.180 [-1.017, 10.317], loss: 0.949832, mae: 4.218672, mean_q: 4.660395
 16606/100000: episode: 1697, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.976, mean reward: 0.398 [0.307, 0.527], mean action: 51.300 [23.000, 71.000], mean observation: 3.156 [-2.125, 10.421], loss: 1.081724, mae: 4.220332, mean_q: 4.663175
 16616/100000: episode: 1698, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.857, mean reward: 0.386 [0.349, 0.462], mean action: 59.500 [26.000, 97.000], mean observation: 3.144 [-0.976, 10.420], loss: 0.830109, mae: 4.220394, mean_q: 4.666615
 16626/100000: episode: 1699, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.452, mean reward: 0.445 [0.361, 0.559], mean action: 57.500 [26.000, 94.000], mean observation: 3.167 [-1.162, 10.435], loss: 0.885429, mae: 4.221031, mean_q: 4.665708
 16636/100000: episode: 1700, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.929, mean reward: 0.393 [0.339, 0.476], mean action: 48.500 [24.000, 81.000], mean observation: 3.150 [-1.143, 10.273], loss: 1.247446, mae: 4.224317, mean_q: 4.665599
 16646/100000: episode: 1701, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.998, mean reward: 0.400 [0.313, 0.490], mean action: 51.000 [15.000, 98.000], mean observation: 3.140 [-1.293, 10.298], loss: 0.928500, mae: 4.223845, mean_q: 4.660695
 16656/100000: episode: 1702, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.356, mean reward: 0.436 [0.354, 0.529], mean action: 47.400 [5.000, 88.000], mean observation: 3.175 [-1.263, 10.347], loss: 0.881077, mae: 4.224514, mean_q: 4.658463
 16666/100000: episode: 1703, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.154, mean reward: 0.415 [0.314, 0.462], mean action: 49.800 [32.000, 89.000], mean observation: 3.164 [-1.190, 10.249], loss: 1.086069, mae: 4.226132, mean_q: 4.660389
 16676/100000: episode: 1704, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.964, mean reward: 0.396 [0.372, 0.453], mean action: 57.800 [33.000, 94.000], mean observation: 3.159 [-1.078, 10.326], loss: 1.048555, mae: 4.227753, mean_q: 4.659379
 16686/100000: episode: 1705, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.845, mean reward: 0.384 [0.353, 0.451], mean action: 48.800 [13.000, 97.000], mean observation: 3.154 [-1.654, 10.333], loss: 1.010401, mae: 4.228186, mean_q: 4.662330
 16694/100000: episode: 1706, duration: 0.139s, episode steps: 8, steps per second: 58, episode reward: 12.807, mean reward: 1.601 [0.362, 10.000], mean action: 39.875 [0.000, 89.000], mean observation: 3.162 [-1.311, 10.680], loss: 1.078113, mae: 4.229427, mean_q: 4.666403
 16704/100000: episode: 1707, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.246, mean reward: 0.425 [0.352, 0.590], mean action: 47.300 [15.000, 71.000], mean observation: 3.168 [-1.486, 10.353], loss: 0.975279, mae: 4.229798, mean_q: 4.671698
 16714/100000: episode: 1708, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.096, mean reward: 0.410 [0.307, 0.478], mean action: 39.800 [5.000, 64.000], mean observation: 3.159 [-1.059, 10.379], loss: 0.845185, mae: 4.230662, mean_q: 4.675430
 16724/100000: episode: 1709, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.292, mean reward: 0.429 [0.412, 0.494], mean action: 42.500 [2.000, 50.000], mean observation: 3.149 [-1.317, 10.316], loss: 0.898484, mae: 4.232411, mean_q: 4.678288
 16734/100000: episode: 1710, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.222, mean reward: 0.422 [0.360, 0.474], mean action: 49.900 [1.000, 97.000], mean observation: 3.157 [-1.928, 10.272], loss: 0.991544, mae: 4.233921, mean_q: 4.679327
 16744/100000: episode: 1711, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.765, mean reward: 0.377 [0.322, 0.514], mean action: 46.100 [20.000, 98.000], mean observation: 3.151 [-2.565, 10.387], loss: 1.229563, mae: 4.235943, mean_q: 4.680048
 16754/100000: episode: 1712, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.116, mean reward: 0.412 [0.352, 0.507], mean action: 52.300 [50.000, 73.000], mean observation: 3.162 [-1.691, 10.424], loss: 1.201709, mae: 4.236150, mean_q: 4.679727
 16764/100000: episode: 1713, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.375, mean reward: 0.437 [0.389, 0.520], mean action: 52.000 [5.000, 88.000], mean observation: 3.151 [-1.889, 10.273], loss: 0.912787, mae: 4.235113, mean_q: 4.678414
 16774/100000: episode: 1714, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.748, mean reward: 0.475 [0.471, 0.511], mean action: 53.300 [50.000, 83.000], mean observation: 3.162 [-1.681, 10.295], loss: 1.200902, mae: 4.237359, mean_q: 4.680105
 16784/100000: episode: 1715, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.283, mean reward: 0.428 [0.380, 0.538], mean action: 50.600 [0.000, 91.000], mean observation: 3.150 [-1.722, 10.230], loss: 1.074433, mae: 4.237996, mean_q: 4.679841
 16794/100000: episode: 1716, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.122, mean reward: 0.412 [0.387, 0.480], mean action: 50.000 [31.000, 87.000], mean observation: 3.147 [-1.033, 10.333], loss: 1.113328, mae: 4.239093, mean_q: 4.681231
 16804/100000: episode: 1717, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.355, mean reward: 0.435 [0.392, 0.453], mean action: 60.600 [19.000, 100.000], mean observation: 3.148 [-1.618, 10.312], loss: 0.991353, mae: 4.239375, mean_q: 4.678907
 16814/100000: episode: 1718, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.691, mean reward: 0.369 [0.317, 0.425], mean action: 39.400 [0.000, 50.000], mean observation: 3.150 [-1.472, 10.347], loss: 0.960507, mae: 4.239913, mean_q: 4.677046
 16824/100000: episode: 1719, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.171, mean reward: 0.417 [0.308, 0.584], mean action: 44.100 [12.000, 50.000], mean observation: 3.161 [-1.096, 10.318], loss: 1.108428, mae: 4.241717, mean_q: 4.677156
 16834/100000: episode: 1720, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 3.977, mean reward: 0.398 [0.337, 0.514], mean action: 36.500 [5.000, 73.000], mean observation: 3.152 [-2.172, 10.359], loss: 0.983760, mae: 4.242002, mean_q: 4.679117
 16844/100000: episode: 1721, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.244, mean reward: 0.424 [0.362, 0.470], mean action: 46.500 [23.000, 79.000], mean observation: 3.161 [-1.608, 10.432], loss: 0.977645, mae: 4.242852, mean_q: 4.680022
 16854/100000: episode: 1722, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.508, mean reward: 0.451 [0.393, 0.511], mean action: 43.700 [8.000, 77.000], mean observation: 3.164 [-1.297, 10.346], loss: 0.988917, mae: 4.243551, mean_q: 4.677204
 16864/100000: episode: 1723, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.186, mean reward: 0.419 [0.386, 0.488], mean action: 44.800 [9.000, 78.000], mean observation: 3.168 [-1.175, 10.364], loss: 0.937605, mae: 4.244157, mean_q: 4.678594
 16874/100000: episode: 1724, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.183, mean reward: 0.418 [0.407, 0.519], mean action: 43.800 [3.000, 60.000], mean observation: 3.151 [-2.004, 10.234], loss: 0.907261, mae: 4.245731, mean_q: 4.683728
 16884/100000: episode: 1725, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.439, mean reward: 0.444 [0.430, 0.510], mean action: 53.300 [10.000, 93.000], mean observation: 3.152 [-1.299, 10.283], loss: 1.056798, mae: 4.247561, mean_q: 4.687528
 16894/100000: episode: 1726, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.962, mean reward: 0.396 [0.370, 0.425], mean action: 42.900 [1.000, 98.000], mean observation: 3.151 [-0.991, 10.238], loss: 0.906253, mae: 4.247379, mean_q: 4.691749
 16904/100000: episode: 1727, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.764, mean reward: 0.376 [0.279, 0.487], mean action: 48.700 [15.000, 95.000], mean observation: 3.143 [-2.122, 10.254], loss: 0.986018, mae: 4.248712, mean_q: 4.696438
 16914/100000: episode: 1728, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.960, mean reward: 0.396 [0.374, 0.446], mean action: 58.400 [10.000, 97.000], mean observation: 3.165 [-1.521, 10.287], loss: 0.798804, mae: 4.249745, mean_q: 4.702013
 16924/100000: episode: 1729, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.488, mean reward: 0.349 [0.318, 0.408], mean action: 58.200 [7.000, 100.000], mean observation: 3.159 [-0.985, 10.358], loss: 1.058323, mae: 4.251929, mean_q: 4.708032
 16934/100000: episode: 1730, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 13.843, mean reward: 1.384 [0.380, 10.000], mean action: 42.100 [10.000, 94.000], mean observation: 3.163 [-1.933, 10.420], loss: 0.934512, mae: 4.252719, mean_q: 4.710230
 16944/100000: episode: 1731, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.128, mean reward: 0.413 [0.351, 0.494], mean action: 51.000 [17.000, 90.000], mean observation: 3.144 [-2.541, 10.253], loss: 1.325160, mae: 4.255584, mean_q: 4.709730
 16954/100000: episode: 1732, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.650, mean reward: 0.365 [0.324, 0.439], mean action: 53.900 [12.000, 97.000], mean observation: 3.151 [-0.900, 10.351], loss: 1.042687, mae: 4.254474, mean_q: 4.708201
 16964/100000: episode: 1733, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.061, mean reward: 0.406 [0.369, 0.455], mean action: 50.000 [3.000, 101.000], mean observation: 3.166 [-1.332, 10.358], loss: 1.265110, mae: 4.256124, mean_q: 4.707007
 16974/100000: episode: 1734, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.101, mean reward: 0.410 [0.346, 0.478], mean action: 42.800 [4.000, 50.000], mean observation: 3.153 [-1.972, 10.337], loss: 1.160139, mae: 4.256459, mean_q: 4.705638
 16984/100000: episode: 1735, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.396, mean reward: 0.440 [0.383, 0.470], mean action: 52.400 [3.000, 99.000], mean observation: 3.136 [-1.777, 10.231], loss: 0.893975, mae: 4.255891, mean_q: 4.708397
 16990/100000: episode: 1736, duration: 0.086s, episode steps: 6, steps per second: 69, episode reward: 11.991, mean reward: 1.999 [0.307, 10.000], mean action: 56.500 [33.000, 95.000], mean observation: 3.153 [-1.525, 10.290], loss: 1.199684, mae: 4.257940, mean_q: 4.709086
 17000/100000: episode: 1737, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.954, mean reward: 0.495 [0.495, 0.496], mean action: 74.600 [50.000, 101.000], mean observation: 3.172 [-1.371, 10.282], loss: 0.946975, mae: 4.257913, mean_q: 4.710509
 17010/100000: episode: 1738, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.130, mean reward: 0.413 [0.384, 0.446], mean action: 58.200 [50.000, 81.000], mean observation: 3.166 [-1.386, 10.371], loss: 0.997678, mae: 4.258901, mean_q: 4.713457
 17020/100000: episode: 1739, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.919, mean reward: 0.392 [0.331, 0.481], mean action: 46.900 [15.000, 54.000], mean observation: 3.158 [-1.055, 10.512], loss: 1.294325, mae: 4.261464, mean_q: 4.711962
 17030/100000: episode: 1740, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.429, mean reward: 0.443 [0.390, 0.505], mean action: 48.100 [10.000, 87.000], mean observation: 3.158 [-1.486, 10.315], loss: 1.105373, mae: 4.260924, mean_q: 4.713253
 17040/100000: episode: 1741, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 3.766, mean reward: 0.377 [0.294, 0.458], mean action: 52.700 [14.000, 91.000], mean observation: 3.158 [-2.303, 10.504], loss: 0.887605, mae: 4.260972, mean_q: 4.716115
 17050/100000: episode: 1742, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.521, mean reward: 0.452 [0.446, 0.505], mean action: 45.000 [10.000, 65.000], mean observation: 3.155 [-1.139, 10.564], loss: 0.872542, mae: 4.262564, mean_q: 4.712977
 17060/100000: episode: 1743, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.215, mean reward: 0.422 [0.325, 0.479], mean action: 49.700 [11.000, 88.000], mean observation: 3.157 [-1.623, 10.316], loss: 1.022481, mae: 4.264460, mean_q: 4.709890
 17070/100000: episode: 1744, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.766, mean reward: 0.377 [0.346, 0.462], mean action: 47.800 [37.000, 53.000], mean observation: 3.154 [-1.246, 10.342], loss: 1.034885, mae: 4.265647, mean_q: 4.708359
 17080/100000: episode: 1745, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.474, mean reward: 0.447 [0.381, 0.479], mean action: 55.200 [9.000, 101.000], mean observation: 3.152 [-1.277, 10.439], loss: 1.172326, mae: 4.267713, mean_q: 4.705855
 17090/100000: episode: 1746, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.404, mean reward: 0.440 [0.337, 0.492], mean action: 56.900 [4.000, 96.000], mean observation: 3.177 [-1.706, 10.407], loss: 1.144190, mae: 4.268031, mean_q: 4.701372
 17100/100000: episode: 1747, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.727, mean reward: 0.373 [0.327, 0.449], mean action: 51.100 [8.000, 89.000], mean observation: 3.160 [-1.493, 10.307], loss: 0.964625, mae: 4.267951, mean_q: 4.697964
 17110/100000: episode: 1748, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.317, mean reward: 0.432 [0.398, 0.501], mean action: 48.400 [6.000, 68.000], mean observation: 3.145 [-1.277, 10.232], loss: 1.054749, mae: 4.269518, mean_q: 4.695123
 17120/100000: episode: 1749, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.884, mean reward: 0.388 [0.344, 0.511], mean action: 49.400 [35.000, 73.000], mean observation: 3.183 [-1.661, 10.258], loss: 0.894529, mae: 4.269746, mean_q: 4.693521
 17130/100000: episode: 1750, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.410, mean reward: 0.441 [0.423, 0.505], mean action: 58.600 [50.000, 99.000], mean observation: 3.151 [-2.430, 10.302], loss: 0.998016, mae: 4.271590, mean_q: 4.693977
 17140/100000: episode: 1751, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.097, mean reward: 0.410 [0.335, 0.537], mean action: 39.300 [6.000, 50.000], mean observation: 3.166 [-1.256, 10.423], loss: 0.919371, mae: 4.272232, mean_q: 4.692544
 17144/100000: episode: 1752, duration: 0.066s, episode steps: 4, steps per second: 61, episode reward: 11.315, mean reward: 2.829 [0.376, 10.000], mean action: 62.250 [30.000, 96.000], mean observation: 3.157 [-2.399, 10.174], loss: 1.130475, mae: 4.273552, mean_q: 4.693225
 17154/100000: episode: 1753, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.095, mean reward: 0.410 [0.350, 0.505], mean action: 48.700 [3.000, 95.000], mean observation: 3.154 [-2.164, 10.371], loss: 1.068895, mae: 4.274131, mean_q: 4.695072
 17164/100000: episode: 1754, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.048, mean reward: 0.405 [0.359, 0.480], mean action: 50.100 [28.000, 62.000], mean observation: 3.162 [-1.105, 10.330], loss: 1.225465, mae: 4.275440, mean_q: 4.696532
 17174/100000: episode: 1755, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.247, mean reward: 0.425 [0.296, 0.510], mean action: 48.000 [5.000, 99.000], mean observation: 3.151 [-1.564, 10.390], loss: 0.967985, mae: 4.275193, mean_q: 4.694920
 17184/100000: episode: 1756, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 5.425, mean reward: 0.542 [0.542, 0.542], mean action: 52.400 [23.000, 90.000], mean observation: 3.133 [-1.273, 10.368], loss: 1.131123, mae: 4.276031, mean_q: 4.696203
 17194/100000: episode: 1757, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.204, mean reward: 0.420 [0.389, 0.596], mean action: 51.200 [15.000, 97.000], mean observation: 3.148 [-1.194, 10.257], loss: 1.151835, mae: 4.276982, mean_q: 4.699952
 17204/100000: episode: 1758, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.995, mean reward: 0.400 [0.325, 0.476], mean action: 51.500 [33.000, 77.000], mean observation: 3.174 [-1.253, 10.272], loss: 0.820025, mae: 4.276597, mean_q: 4.700780
 17214/100000: episode: 1759, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.118, mean reward: 0.412 [0.390, 0.473], mean action: 47.000 [35.000, 50.000], mean observation: 3.156 [-1.585, 10.301], loss: 0.876997, mae: 4.277545, mean_q: 4.702414
 17224/100000: episode: 1760, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.955, mean reward: 0.395 [0.341, 0.434], mean action: 51.100 [26.000, 86.000], mean observation: 3.154 [-1.134, 10.358], loss: 0.673071, mae: 4.277796, mean_q: 4.703993
 17234/100000: episode: 1761, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.993, mean reward: 0.399 [0.290, 0.552], mean action: 49.000 [13.000, 77.000], mean observation: 3.158 [-2.092, 10.257], loss: 1.035077, mae: 4.280654, mean_q: 4.706644
 17244/100000: episode: 1762, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.428, mean reward: 0.443 [0.359, 0.515], mean action: 52.600 [0.000, 97.000], mean observation: 3.162 [-2.684, 10.269], loss: 1.207923, mae: 4.282465, mean_q: 4.706937
 17254/100000: episode: 1763, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.127, mean reward: 0.413 [0.366, 0.444], mean action: 53.200 [22.000, 85.000], mean observation: 3.150 [-1.004, 10.233], loss: 0.967490, mae: 4.282053, mean_q: 4.707888
 17264/100000: episode: 1764, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.168, mean reward: 0.417 [0.387, 0.469], mean action: 52.900 [1.000, 98.000], mean observation: 3.153 [-1.618, 10.279], loss: 0.993861, mae: 4.283085, mean_q: 4.711671
 17274/100000: episode: 1765, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.879, mean reward: 0.388 [0.306, 0.452], mean action: 46.200 [15.000, 90.000], mean observation: 3.163 [-1.493, 10.357], loss: 0.959299, mae: 4.284037, mean_q: 4.717053
 17284/100000: episode: 1766, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.003, mean reward: 0.400 [0.355, 0.518], mean action: 47.000 [17.000, 53.000], mean observation: 3.145 [-0.967, 10.386], loss: 1.099845, mae: 4.285491, mean_q: 4.720606
 17294/100000: episode: 1767, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.910, mean reward: 0.491 [0.392, 0.530], mean action: 57.800 [48.000, 100.000], mean observation: 3.147 [-1.086, 10.232], loss: 0.961914, mae: 4.285945, mean_q: 4.723934
 17304/100000: episode: 1768, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.266, mean reward: 0.427 [0.333, 0.529], mean action: 47.900 [17.000, 81.000], mean observation: 3.156 [-1.086, 10.383], loss: 0.837118, mae: 4.286220, mean_q: 4.722581
 17314/100000: episode: 1769, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.088, mean reward: 0.409 [0.371, 0.510], mean action: 50.100 [13.000, 75.000], mean observation: 3.159 [-1.438, 10.291], loss: 1.051542, mae: 4.288348, mean_q: 4.721568
 17324/100000: episode: 1770, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.193, mean reward: 0.419 [0.383, 0.503], mean action: 45.300 [1.000, 99.000], mean observation: 3.142 [-1.983, 10.192], loss: 0.895012, mae: 4.288835, mean_q: 4.715428
 17334/100000: episode: 1771, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.166, mean reward: 0.417 [0.391, 0.461], mean action: 51.400 [29.000, 79.000], mean observation: 3.167 [-0.941, 10.541], loss: 0.888898, mae: 4.290468, mean_q: 4.713132
 17344/100000: episode: 1772, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.972, mean reward: 0.397 [0.346, 0.475], mean action: 43.400 [4.000, 80.000], mean observation: 3.151 [-1.435, 10.233], loss: 1.062871, mae: 4.292085, mean_q: 4.709300
 17354/100000: episode: 1773, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.233, mean reward: 0.423 [0.364, 0.527], mean action: 49.800 [15.000, 98.000], mean observation: 3.147 [-1.896, 10.363], loss: 1.059958, mae: 4.293120, mean_q: 4.707694
 17364/100000: episode: 1774, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.239, mean reward: 0.424 [0.306, 0.500], mean action: 43.400 [7.000, 84.000], mean observation: 3.169 [-1.504, 10.342], loss: 0.932610, mae: 4.293369, mean_q: 4.707222
 17374/100000: episode: 1775, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.468, mean reward: 0.447 [0.383, 0.468], mean action: 56.900 [23.000, 96.000], mean observation: 3.164 [-1.316, 10.293], loss: 1.171108, mae: 4.295077, mean_q: 4.708076
 17384/100000: episode: 1776, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.740, mean reward: 0.374 [0.345, 0.491], mean action: 72.600 [70.000, 96.000], mean observation: 3.163 [-1.454, 10.315], loss: 1.062998, mae: 4.295693, mean_q: 4.709209
 17394/100000: episode: 1777, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.791, mean reward: 0.379 [0.364, 0.514], mean action: 50.300 [5.000, 70.000], mean observation: 3.140 [-2.020, 10.268], loss: 0.924196, mae: 4.295819, mean_q: 4.711162
 17404/100000: episode: 1778, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.977, mean reward: 0.398 [0.303, 0.543], mean action: 49.500 [1.000, 70.000], mean observation: 3.148 [-0.938, 10.213], loss: 1.045857, mae: 4.297517, mean_q: 4.712014
 17414/100000: episode: 1779, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.144, mean reward: 0.414 [0.350, 0.470], mean action: 59.800 [11.000, 94.000], mean observation: 3.168 [-1.385, 10.310], loss: 1.181921, mae: 4.298839, mean_q: 4.712283
 17424/100000: episode: 1780, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.645, mean reward: 0.365 [0.313, 0.443], mean action: 55.200 [12.000, 92.000], mean observation: 3.164 [-2.001, 10.322], loss: 1.210662, mae: 4.299327, mean_q: 4.712801
 17434/100000: episode: 1781, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.891, mean reward: 0.389 [0.351, 0.499], mean action: 68.500 [13.000, 101.000], mean observation: 3.151 [-1.628, 10.256], loss: 1.064911, mae: 4.299590, mean_q: 4.713853
 17444/100000: episode: 1782, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.272, mean reward: 0.427 [0.365, 0.497], mean action: 56.500 [6.000, 70.000], mean observation: 3.146 [-1.139, 10.320], loss: 0.960436, mae: 4.299620, mean_q: 4.716013
 17454/100000: episode: 1783, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.100, mean reward: 0.410 [0.380, 0.479], mean action: 68.400 [42.000, 93.000], mean observation: 3.139 [-1.173, 10.316], loss: 0.909772, mae: 4.300602, mean_q: 4.718220
 17464/100000: episode: 1784, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.966, mean reward: 0.397 [0.332, 0.513], mean action: 63.100 [18.000, 70.000], mean observation: 3.162 [-0.909, 10.386], loss: 1.099131, mae: 4.302556, mean_q: 4.716741
 17474/100000: episode: 1785, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.644, mean reward: 0.464 [0.464, 0.464], mean action: 60.500 [19.000, 70.000], mean observation: 3.168 [-1.696, 10.264], loss: 1.402064, mae: 4.304616, mean_q: 4.712857
 17484/100000: episode: 1786, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.999, mean reward: 0.400 [0.351, 0.475], mean action: 58.400 [5.000, 84.000], mean observation: 3.150 [-2.145, 10.301], loss: 0.985362, mae: 4.303874, mean_q: 4.713770
 17487/100000: episode: 1787, duration: 0.062s, episode steps: 3, steps per second: 49, episode reward: 10.822, mean reward: 3.607 [0.403, 10.000], mean action: 52.333 [17.000, 70.000], mean observation: 3.163 [-1.285, 10.167], loss: 0.803922, mae: 4.303410, mean_q: 4.715153
 17497/100000: episode: 1788, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.009, mean reward: 0.401 [0.370, 0.463], mean action: 59.300 [4.000, 97.000], mean observation: 3.159 [-1.303, 10.327], loss: 1.087905, mae: 4.305542, mean_q: 4.716530
 17507/100000: episode: 1789, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.541, mean reward: 0.354 [0.335, 0.467], mean action: 68.300 [44.000, 79.000], mean observation: 3.158 [-1.162, 10.299], loss: 1.020506, mae: 4.306285, mean_q: 4.719818
 17517/100000: episode: 1790, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.174, mean reward: 0.417 [0.384, 0.433], mean action: 63.100 [12.000, 95.000], mean observation: 3.163 [-1.553, 10.297], loss: 0.973443, mae: 4.306516, mean_q: 4.722135
 17527/100000: episode: 1791, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.722, mean reward: 0.372 [0.318, 0.519], mean action: 59.800 [8.000, 99.000], mean observation: 3.159 [-1.292, 10.281], loss: 0.824892, mae: 4.306878, mean_q: 4.717655
 17537/100000: episode: 1792, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.063, mean reward: 0.406 [0.376, 0.476], mean action: 53.500 [10.000, 70.000], mean observation: 3.167 [-1.483, 10.288], loss: 0.898027, mae: 4.307985, mean_q: 4.714573
 17547/100000: episode: 1793, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.311, mean reward: 0.431 [0.325, 0.493], mean action: 24.100 [2.000, 101.000], mean observation: 3.147 [-1.942, 10.381], loss: 1.068196, mae: 4.309303, mean_q: 4.715682
 17557/100000: episode: 1794, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.049, mean reward: 0.405 [0.348, 0.555], mean action: 28.400 [10.000, 89.000], mean observation: 3.157 [-1.303, 10.260], loss: 0.914842, mae: 4.309474, mean_q: 4.719373
 17567/100000: episode: 1795, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.967, mean reward: 0.397 [0.332, 0.507], mean action: 29.000 [10.000, 70.000], mean observation: 3.161 [-1.145, 10.341], loss: 1.031530, mae: 4.311507, mean_q: 4.720248
 17577/100000: episode: 1796, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.148, mean reward: 0.415 [0.403, 0.435], mean action: 68.500 [42.000, 86.000], mean observation: 3.161 [-0.935, 10.330], loss: 1.057800, mae: 4.311977, mean_q: 4.723501
 17587/100000: episode: 1797, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.649, mean reward: 0.365 [0.291, 0.460], mean action: 60.800 [20.000, 92.000], mean observation: 3.151 [-2.224, 10.325], loss: 1.017103, mae: 4.312413, mean_q: 4.727245
 17597/100000: episode: 1798, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.373, mean reward: 0.437 [0.320, 0.499], mean action: 57.800 [6.000, 97.000], mean observation: 3.170 [-1.895, 10.303], loss: 1.326799, mae: 4.314239, mean_q: 4.730786
 17607/100000: episode: 1799, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.389, mean reward: 0.439 [0.437, 0.454], mean action: 73.100 [70.000, 93.000], mean observation: 3.172 [-0.957, 10.469], loss: 0.855649, mae: 4.312961, mean_q: 4.735347
 17617/100000: episode: 1800, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.563, mean reward: 0.456 [0.456, 0.456], mean action: 68.300 [46.000, 98.000], mean observation: 3.140 [-1.087, 10.310], loss: 1.226512, mae: 4.314879, mean_q: 4.739087
 17627/100000: episode: 1801, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.994, mean reward: 0.399 [0.326, 0.465], mean action: 52.900 [12.000, 70.000], mean observation: 3.161 [-1.401, 10.258], loss: 1.317697, mae: 4.316215, mean_q: 4.741966
 17637/100000: episode: 1802, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 5.049, mean reward: 0.505 [0.469, 0.588], mean action: 56.400 [1.000, 70.000], mean observation: 3.163 [-1.145, 10.257], loss: 0.886712, mae: 4.314973, mean_q: 4.739539
 17647/100000: episode: 1803, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.686, mean reward: 0.369 [0.304, 0.428], mean action: 70.200 [47.000, 101.000], mean observation: 3.147 [-0.958, 10.260], loss: 1.123521, mae: 4.316403, mean_q: 4.738145
 17657/100000: episode: 1804, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.959, mean reward: 0.396 [0.326, 0.486], mean action: 57.200 [0.000, 81.000], mean observation: 3.155 [-1.223, 10.473], loss: 0.872157, mae: 4.316168, mean_q: 4.741437
 17667/100000: episode: 1805, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 5.284, mean reward: 0.528 [0.454, 0.560], mean action: 50.300 [5.000, 70.000], mean observation: 3.159 [-1.388, 10.446], loss: 1.139293, mae: 4.318452, mean_q: 4.744577
 17677/100000: episode: 1806, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.768, mean reward: 0.377 [0.285, 0.440], mean action: 70.700 [4.000, 100.000], mean observation: 3.159 [-1.109, 10.323], loss: 1.076138, mae: 4.318841, mean_q: 4.747077
 17687/100000: episode: 1807, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.318, mean reward: 0.432 [0.402, 0.458], mean action: 60.900 [20.000, 100.000], mean observation: 3.167 [-1.654, 10.206], loss: 1.113699, mae: 4.319945, mean_q: 4.747395
 17697/100000: episode: 1808, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.772, mean reward: 0.377 [0.312, 0.554], mean action: 52.400 [0.000, 96.000], mean observation: 3.160 [-2.019, 10.579], loss: 1.060352, mae: 4.320811, mean_q: 4.739690
 17707/100000: episode: 1809, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.969, mean reward: 0.397 [0.383, 0.464], mean action: 60.100 [8.000, 70.000], mean observation: 3.159 [-1.908, 10.327], loss: 1.011510, mae: 4.321912, mean_q: 4.734765
 17717/100000: episode: 1810, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.118, mean reward: 0.412 [0.374, 0.466], mean action: 53.800 [4.000, 92.000], mean observation: 3.161 [-1.535, 10.294], loss: 1.201996, mae: 4.323100, mean_q: 4.728391
 17727/100000: episode: 1811, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.190, mean reward: 0.419 [0.286, 0.523], mean action: 43.000 [10.000, 70.000], mean observation: 3.157 [-1.778, 10.267], loss: 0.917675, mae: 4.322549, mean_q: 4.727811
 17737/100000: episode: 1812, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.757, mean reward: 0.376 [0.358, 0.405], mean action: 59.200 [11.000, 89.000], mean observation: 3.153 [-1.342, 10.395], loss: 0.994445, mae: 4.323945, mean_q: 4.730788
 17747/100000: episode: 1813, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.335, mean reward: 0.434 [0.375, 0.447], mean action: 63.900 [9.000, 70.000], mean observation: 3.147 [-1.862, 10.251], loss: 1.065068, mae: 4.325004, mean_q: 4.730304
 17757/100000: episode: 1814, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.633, mean reward: 0.363 [0.266, 0.485], mean action: 62.900 [30.000, 70.000], mean observation: 3.166 [-1.375, 10.384], loss: 1.170869, mae: 4.326334, mean_q: 4.726857
 17767/100000: episode: 1815, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.393, mean reward: 0.439 [0.422, 0.527], mean action: 54.200 [17.000, 70.000], mean observation: 3.161 [-1.417, 10.412], loss: 1.033679, mae: 4.326006, mean_q: 4.727927
 17777/100000: episode: 1816, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.808, mean reward: 0.381 [0.331, 0.444], mean action: 60.500 [14.000, 70.000], mean observation: 3.156 [-0.806, 10.315], loss: 1.098847, mae: 4.327466, mean_q: 4.729922
 17787/100000: episode: 1817, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.815, mean reward: 0.481 [0.365, 0.579], mean action: 58.400 [19.000, 87.000], mean observation: 3.156 [-2.011, 10.397], loss: 1.362438, mae: 4.329285, mean_q: 4.726741
 17797/100000: episode: 1818, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.668, mean reward: 0.467 [0.467, 0.467], mean action: 70.700 [34.000, 100.000], mean observation: 3.167 [-1.268, 10.365], loss: 0.989269, mae: 4.328478, mean_q: 4.723150
 17807/100000: episode: 1819, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.041, mean reward: 0.404 [0.368, 0.459], mean action: 65.300 [40.000, 95.000], mean observation: 3.164 [-1.079, 10.320], loss: 1.039243, mae: 4.329209, mean_q: 4.725744
 17817/100000: episode: 1820, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.815, mean reward: 0.382 [0.291, 0.418], mean action: 49.300 [6.000, 88.000], mean observation: 3.154 [-2.212, 10.252], loss: 1.074398, mae: 4.330397, mean_q: 4.730597
 17827/100000: episode: 1821, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.124, mean reward: 0.412 [0.346, 0.513], mean action: 57.300 [13.000, 90.000], mean observation: 3.155 [-1.264, 10.395], loss: 0.648228, mae: 4.329381, mean_q: 4.736228
 17837/100000: episode: 1822, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.004, mean reward: 0.400 [0.359, 0.469], mean action: 59.900 [7.000, 99.000], mean observation: 3.154 [-1.784, 10.215], loss: 0.792440, mae: 4.331150, mean_q: 4.742970
 17847/100000: episode: 1823, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.606, mean reward: 0.461 [0.261, 0.560], mean action: 51.400 [2.000, 70.000], mean observation: 3.171 [-0.988, 10.261], loss: 1.177837, mae: 4.333983, mean_q: 4.749410
 17857/100000: episode: 1824, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.452, mean reward: 0.445 [0.339, 0.500], mean action: 61.500 [5.000, 100.000], mean observation: 3.155 [-1.290, 10.404], loss: 0.865965, mae: 4.333713, mean_q: 4.753487
 17867/100000: episode: 1825, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.243, mean reward: 0.424 [0.366, 0.533], mean action: 62.500 [12.000, 86.000], mean observation: 3.152 [-1.284, 10.254], loss: 1.145815, mae: 4.335504, mean_q: 4.753156
 17875/100000: episode: 1826, duration: 0.119s, episode steps: 8, steps per second: 67, episode reward: 12.741, mean reward: 1.593 [0.381, 10.000], mean action: 69.125 [30.000, 91.000], mean observation: 3.146 [-1.844, 10.376], loss: 1.278247, mae: 4.336473, mean_q: 4.744740
 17885/100000: episode: 1827, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.179, mean reward: 0.418 [0.388, 0.465], mean action: 42.900 [4.000, 87.000], mean observation: 3.156 [-2.295, 10.203], loss: 1.302736, mae: 4.337036, mean_q: 4.737842
 17895/100000: episode: 1828, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.889, mean reward: 0.389 [0.287, 0.485], mean action: 66.300 [38.000, 98.000], mean observation: 3.153 [-1.555, 10.427], loss: 1.096219, mae: 4.336296, mean_q: 4.737526
 17905/100000: episode: 1829, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.889, mean reward: 0.389 [0.326, 0.468], mean action: 64.700 [33.000, 96.000], mean observation: 3.159 [-1.346, 10.349], loss: 0.755234, mae: 4.335738, mean_q: 4.736571
 17915/100000: episode: 1830, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.126, mean reward: 0.413 [0.380, 0.491], mean action: 60.000 [11.000, 101.000], mean observation: 3.156 [-1.467, 10.449], loss: 1.040652, mae: 4.337907, mean_q: 4.738154
 17925/100000: episode: 1831, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.795, mean reward: 0.380 [0.300, 0.478], mean action: 45.500 [3.000, 83.000], mean observation: 3.166 [-1.428, 10.408], loss: 1.111769, mae: 4.339495, mean_q: 4.739726
 17935/100000: episode: 1832, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.063, mean reward: 0.406 [0.366, 0.496], mean action: 60.200 [41.000, 93.000], mean observation: 3.161 [-1.014, 10.284], loss: 0.926943, mae: 4.339393, mean_q: 4.742002
 17945/100000: episode: 1833, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.838, mean reward: 0.384 [0.366, 0.421], mean action: 54.800 [1.000, 99.000], mean observation: 3.161 [-1.252, 10.305], loss: 1.071943, mae: 4.340371, mean_q: 4.745448
 17955/100000: episode: 1834, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.654, mean reward: 0.465 [0.460, 0.467], mean action: 59.500 [22.000, 95.000], mean observation: 3.152 [-1.797, 10.247], loss: 0.979657, mae: 4.340462, mean_q: 4.742795
 17965/100000: episode: 1835, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.063, mean reward: 0.406 [0.356, 0.556], mean action: 50.300 [23.000, 98.000], mean observation: 3.156 [-1.114, 10.192], loss: 0.964468, mae: 4.341428, mean_q: 4.741208
 17975/100000: episode: 1836, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.034, mean reward: 0.403 [0.370, 0.499], mean action: 49.300 [12.000, 100.000], mean observation: 3.147 [-1.131, 10.311], loss: 0.786517, mae: 4.341499, mean_q: 4.743026
 17985/100000: episode: 1837, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.277, mean reward: 0.428 [0.353, 0.487], mean action: 47.900 [3.000, 99.000], mean observation: 3.145 [-1.578, 10.427], loss: 1.074978, mae: 4.343644, mean_q: 4.744073
 17995/100000: episode: 1838, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.992, mean reward: 0.399 [0.379, 0.493], mean action: 61.300 [24.000, 94.000], mean observation: 3.149 [-1.867, 10.262], loss: 1.163560, mae: 4.345417, mean_q: 4.740919
 18005/100000: episode: 1839, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.790, mean reward: 0.479 [0.479, 0.479], mean action: 49.600 [25.000, 74.000], mean observation: 3.157 [-1.015, 10.308], loss: 0.979953, mae: 4.345461, mean_q: 4.740861
 18015/100000: episode: 1840, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.760, mean reward: 0.376 [0.314, 0.443], mean action: 46.100 [6.000, 95.000], mean observation: 3.166 [-0.896, 10.441], loss: 0.968300, mae: 4.346147, mean_q: 4.741941
 18025/100000: episode: 1841, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.198, mean reward: 0.420 [0.385, 0.585], mean action: 49.000 [20.000, 67.000], mean observation: 3.169 [-1.778, 10.366], loss: 1.126450, mae: 4.347950, mean_q: 4.743151
 18035/100000: episode: 1842, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.364, mean reward: 0.436 [0.429, 0.437], mean action: 46.600 [12.000, 80.000], mean observation: 3.163 [-1.198, 10.393], loss: 0.991012, mae: 4.348328, mean_q: 4.744170
 18045/100000: episode: 1843, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.035, mean reward: 0.404 [0.385, 0.446], mean action: 58.700 [40.000, 70.000], mean observation: 3.154 [-0.945, 10.223], loss: 1.028329, mae: 4.348567, mean_q: 4.742637
 18047/100000: episode: 1844, duration: 0.041s, episode steps: 2, steps per second: 48, episode reward: 10.343, mean reward: 5.172 [0.343, 10.000], mean action: 60.000 [60.000, 60.000], mean observation: 3.154 [-1.019, 10.220], loss: 0.982665, mae: 4.349995, mean_q: 4.743356
 18057/100000: episode: 1845, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.685, mean reward: 0.469 [0.445, 0.556], mean action: 66.500 [45.000, 98.000], mean observation: 3.157 [-1.083, 10.278], loss: 0.911420, mae: 4.349020, mean_q: 4.743676
 18067/100000: episode: 1846, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.553, mean reward: 0.455 [0.443, 0.539], mean action: 62.000 [41.000, 90.000], mean observation: 3.162 [-1.441, 10.415], loss: 0.924219, mae: 4.350253, mean_q: 4.744345
 18077/100000: episode: 1847, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.889, mean reward: 0.389 [0.305, 0.499], mean action: 57.300 [14.000, 88.000], mean observation: 3.158 [-1.866, 10.296], loss: 1.062005, mae: 4.351533, mean_q: 4.746818
 18087/100000: episode: 1848, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.323, mean reward: 0.432 [0.353, 0.566], mean action: 58.600 [6.000, 95.000], mean observation: 3.155 [-1.577, 10.325], loss: 1.045651, mae: 4.353008, mean_q: 4.749949
 18097/100000: episode: 1849, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.802, mean reward: 0.380 [0.344, 0.420], mean action: 54.200 [30.000, 96.000], mean observation: 3.169 [-1.284, 10.453], loss: 1.097283, mae: 4.353747, mean_q: 4.752091
 18107/100000: episode: 1850, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.620, mean reward: 0.462 [0.355, 0.582], mean action: 39.000 [7.000, 56.000], mean observation: 3.148 [-1.596, 10.302], loss: 1.233915, mae: 4.355098, mean_q: 4.754640
 18117/100000: episode: 1851, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 13.992, mean reward: 1.399 [0.389, 10.000], mean action: 53.100 [7.000, 86.000], mean observation: 3.157 [-1.500, 10.508], loss: 1.013880, mae: 4.355251, mean_q: 4.757677
 18127/100000: episode: 1852, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.245, mean reward: 0.425 [0.417, 0.478], mean action: 58.500 [55.000, 90.000], mean observation: 3.173 [-0.835, 10.278], loss: 1.124539, mae: 4.356910, mean_q: 4.760308
 18137/100000: episode: 1853, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.381, mean reward: 0.438 [0.418, 0.484], mean action: 56.000 [38.000, 77.000], mean observation: 3.153 [-1.503, 10.286], loss: 0.792525, mae: 4.356023, mean_q: 4.753858
 18147/100000: episode: 1854, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.993, mean reward: 0.399 [0.357, 0.472], mean action: 64.600 [36.000, 93.000], mean observation: 3.152 [-1.311, 10.472], loss: 0.988638, mae: 4.357810, mean_q: 4.751727
 18157/100000: episode: 1855, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.407, mean reward: 0.441 [0.433, 0.469], mean action: 55.500 [11.000, 100.000], mean observation: 3.157 [-1.520, 10.401], loss: 1.130053, mae: 4.359191, mean_q: 4.753154
 18167/100000: episode: 1856, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.875, mean reward: 0.387 [0.353, 0.471], mean action: 48.500 [17.000, 82.000], mean observation: 3.160 [-0.959, 10.294], loss: 1.029157, mae: 4.359796, mean_q: 4.756580
 18177/100000: episode: 1857, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.479, mean reward: 0.448 [0.426, 0.450], mean action: 48.300 [0.000, 90.000], mean observation: 3.168 [-1.719, 10.389], loss: 0.975595, mae: 4.360590, mean_q: 4.757573
 18187/100000: episode: 1858, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.113, mean reward: 0.411 [0.318, 0.488], mean action: 45.300 [13.000, 60.000], mean observation: 3.156 [-1.561, 10.325], loss: 0.875306, mae: 4.361165, mean_q: 4.757077
 18197/100000: episode: 1859, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.134, mean reward: 0.413 [0.347, 0.538], mean action: 40.900 [5.000, 55.000], mean observation: 3.163 [-1.345, 10.605], loss: 1.107788, mae: 4.362584, mean_q: 4.759206
 18207/100000: episode: 1860, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.293, mean reward: 0.429 [0.366, 0.498], mean action: 48.300 [17.000, 55.000], mean observation: 3.159 [-1.453, 10.222], loss: 1.255155, mae: 4.363769, mean_q: 4.758618
 18217/100000: episode: 1861, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.068, mean reward: 0.407 [0.362, 0.484], mean action: 49.800 [2.000, 96.000], mean observation: 3.150 [-1.191, 10.280], loss: 1.266517, mae: 4.364635, mean_q: 4.755571
 18227/100000: episode: 1862, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.259, mean reward: 0.426 [0.419, 0.469], mean action: 58.700 [50.000, 97.000], mean observation: 3.146 [-1.522, 10.433], loss: 1.113934, mae: 4.364653, mean_q: 4.754142
 18237/100000: episode: 1863, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.234, mean reward: 0.423 [0.387, 0.487], mean action: 40.400 [2.000, 73.000], mean observation: 3.156 [-1.363, 10.338], loss: 1.082785, mae: 4.364909, mean_q: 4.751578
 18247/100000: episode: 1864, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.116, mean reward: 0.412 [0.394, 0.467], mean action: 49.900 [8.000, 57.000], mean observation: 3.166 [-1.101, 10.368], loss: 0.779928, mae: 4.364192, mean_q: 4.752826
 18257/100000: episode: 1865, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.126, mean reward: 0.413 [0.354, 0.459], mean action: 56.000 [18.000, 94.000], mean observation: 3.151 [-1.113, 10.353], loss: 1.018692, mae: 4.366158, mean_q: 4.754481
 18267/100000: episode: 1866, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.950, mean reward: 0.395 [0.369, 0.448], mean action: 40.100 [6.000, 60.000], mean observation: 3.147 [-1.042, 10.383], loss: 1.029852, mae: 4.367314, mean_q: 4.757272
 18277/100000: episode: 1867, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.563, mean reward: 0.456 [0.450, 0.483], mean action: 60.100 [55.000, 80.000], mean observation: 3.157 [-1.641, 10.275], loss: 1.091922, mae: 4.369013, mean_q: 4.757926
 18287/100000: episode: 1868, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 5.005, mean reward: 0.501 [0.501, 0.501], mean action: 61.600 [47.000, 78.000], mean observation: 3.151 [-1.025, 10.369], loss: 0.951731, mae: 4.368354, mean_q: 4.757491
 18297/100000: episode: 1869, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.938, mean reward: 0.394 [0.352, 0.481], mean action: 51.800 [22.000, 81.000], mean observation: 3.144 [-1.016, 10.366], loss: 1.097985, mae: 4.370149, mean_q: 4.759105
 18307/100000: episode: 1870, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.446, mean reward: 0.445 [0.345, 0.530], mean action: 50.200 [2.000, 95.000], mean observation: 3.166 [-2.090, 10.398], loss: 0.854779, mae: 4.370130, mean_q: 4.763103
 18317/100000: episode: 1871, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.887, mean reward: 0.389 [0.351, 0.440], mean action: 42.500 [5.000, 74.000], mean observation: 3.151 [-1.103, 10.297], loss: 1.073982, mae: 4.371253, mean_q: 4.767006
 18327/100000: episode: 1872, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.441, mean reward: 0.444 [0.363, 0.581], mean action: 46.800 [20.000, 86.000], mean observation: 3.153 [-1.175, 10.435], loss: 0.993511, mae: 4.372074, mean_q: 4.767453
 18337/100000: episode: 1873, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.136, mean reward: 0.414 [0.369, 0.595], mean action: 66.700 [31.000, 89.000], mean observation: 3.158 [-0.897, 10.285], loss: 1.186978, mae: 4.372954, mean_q: 4.766378
 18347/100000: episode: 1874, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 3.527, mean reward: 0.353 [0.294, 0.443], mean action: 59.400 [15.000, 99.000], mean observation: 3.165 [-1.768, 10.305], loss: 0.893688, mae: 4.372260, mean_q: 4.769277
 18357/100000: episode: 1875, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.961, mean reward: 0.396 [0.380, 0.461], mean action: 58.800 [25.000, 90.000], mean observation: 3.156 [-1.894, 10.292], loss: 1.481751, mae: 4.375526, mean_q: 4.773243
 18367/100000: episode: 1876, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.553, mean reward: 0.355 [0.280, 0.464], mean action: 54.200 [10.000, 84.000], mean observation: 3.153 [-0.867, 10.228], loss: 1.112666, mae: 4.374636, mean_q: 4.776152
 18377/100000: episode: 1877, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.277, mean reward: 0.428 [0.333, 0.532], mean action: 41.300 [16.000, 55.000], mean observation: 3.145 [-1.107, 10.329], loss: 1.246123, mae: 4.375680, mean_q: 4.779685
 18387/100000: episode: 1878, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.412, mean reward: 0.441 [0.384, 0.535], mean action: 56.200 [18.000, 94.000], mean observation: 3.157 [-1.000, 10.364], loss: 0.973376, mae: 4.375290, mean_q: 4.784466
 18397/100000: episode: 1879, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.928, mean reward: 0.393 [0.353, 0.485], mean action: 53.600 [12.000, 84.000], mean observation: 3.147 [-1.206, 10.345], loss: 1.224293, mae: 4.377200, mean_q: 4.785233
 18407/100000: episode: 1880, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.202, mean reward: 0.420 [0.286, 0.554], mean action: 62.700 [25.000, 95.000], mean observation: 3.161 [-1.333, 10.301], loss: 0.973488, mae: 4.376887, mean_q: 4.786632
 18417/100000: episode: 1881, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.034, mean reward: 0.403 [0.367, 0.532], mean action: 50.400 [0.000, 84.000], mean observation: 3.157 [-1.421, 10.237], loss: 1.039305, mae: 4.378128, mean_q: 4.789163
 18427/100000: episode: 1882, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.303, mean reward: 0.430 [0.348, 0.567], mean action: 37.500 [2.000, 75.000], mean observation: 3.159 [-1.174, 10.210], loss: 0.936930, mae: 4.378603, mean_q: 4.791902
 18437/100000: episode: 1883, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.891, mean reward: 0.489 [0.410, 0.531], mean action: 46.000 [4.000, 73.000], mean observation: 3.156 [-1.114, 10.283], loss: 0.752737, mae: 4.378928, mean_q: 4.793833
 18447/100000: episode: 1884, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.733, mean reward: 0.373 [0.314, 0.458], mean action: 47.000 [8.000, 67.000], mean observation: 3.152 [-1.387, 10.264], loss: 1.483842, mae: 4.382980, mean_q: 4.793019
 18457/100000: episode: 1885, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.547, mean reward: 0.355 [0.304, 0.415], mean action: 57.400 [25.000, 69.000], mean observation: 3.168 [-0.951, 10.275], loss: 0.908953, mae: 4.380956, mean_q: 4.794173
 18467/100000: episode: 1886, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.928, mean reward: 0.493 [0.478, 0.537], mean action: 64.000 [2.000, 100.000], mean observation: 3.146 [-2.454, 10.266], loss: 0.880290, mae: 4.381721, mean_q: 4.796843
 18473/100000: episode: 1887, duration: 0.084s, episode steps: 6, steps per second: 72, episode reward: 12.109, mean reward: 2.018 [0.369, 10.000], mean action: 66.500 [60.000, 99.000], mean observation: 3.158 [-1.207, 10.253], loss: 1.087771, mae: 4.383700, mean_q: 4.799347
 18483/100000: episode: 1888, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.010, mean reward: 0.401 [0.383, 0.499], mean action: 57.700 [1.000, 88.000], mean observation: 3.154 [-1.784, 10.273], loss: 0.966751, mae: 4.383945, mean_q: 4.801372
 18493/100000: episode: 1889, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.038, mean reward: 0.404 [0.325, 0.476], mean action: 55.700 [3.000, 101.000], mean observation: 3.159 [-1.194, 10.218], loss: 1.024643, mae: 4.385015, mean_q: 4.804421
 18503/100000: episode: 1890, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.704, mean reward: 0.470 [0.369, 0.545], mean action: 49.100 [1.000, 95.000], mean observation: 3.162 [-0.865, 10.330], loss: 0.978926, mae: 4.385963, mean_q: 4.807709
 18513/100000: episode: 1891, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.863, mean reward: 0.386 [0.351, 0.452], mean action: 64.900 [29.000, 99.000], mean observation: 3.159 [-1.431, 10.353], loss: 1.103805, mae: 4.387146, mean_q: 4.807046
 18523/100000: episode: 1892, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.880, mean reward: 0.388 [0.317, 0.454], mean action: 52.500 [8.000, 88.000], mean observation: 3.164 [-1.179, 10.270], loss: 1.036356, mae: 4.387460, mean_q: 4.803975
 18533/100000: episode: 1893, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.847, mean reward: 0.385 [0.300, 0.532], mean action: 50.700 [4.000, 85.000], mean observation: 3.145 [-1.310, 10.311], loss: 1.125217, mae: 4.388955, mean_q: 4.807603
 18543/100000: episode: 1894, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.137, mean reward: 0.414 [0.350, 0.482], mean action: 52.300 [12.000, 86.000], mean observation: 3.150 [-1.570, 10.312], loss: 1.071334, mae: 4.389626, mean_q: 4.803700
 18553/100000: episode: 1895, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.874, mean reward: 0.487 [0.471, 0.513], mean action: 65.500 [22.000, 80.000], mean observation: 3.177 [-2.322, 10.426], loss: 0.929088, mae: 4.390151, mean_q: 4.802833
 18563/100000: episode: 1896, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.540, mean reward: 0.454 [0.396, 0.512], mean action: 67.600 [17.000, 80.000], mean observation: 3.120 [-1.196, 10.422], loss: 1.279592, mae: 4.392470, mean_q: 4.798636
 18573/100000: episode: 1897, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.660, mean reward: 0.466 [0.383, 0.588], mean action: 58.500 [4.000, 80.000], mean observation: 3.149 [-1.622, 10.327], loss: 0.904754, mae: 4.391552, mean_q: 4.797077
 18583/100000: episode: 1898, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.004, mean reward: 0.400 [0.326, 0.481], mean action: 52.600 [31.000, 60.000], mean observation: 3.162 [-1.346, 10.348], loss: 1.139418, mae: 4.393843, mean_q: 4.798698
 18593/100000: episode: 1899, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.676, mean reward: 0.468 [0.397, 0.490], mean action: 59.500 [4.000, 80.000], mean observation: 3.166 [-0.904, 10.295], loss: 1.197585, mae: 4.394686, mean_q: 4.798398
 18603/100000: episode: 1900, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.885, mean reward: 0.389 [0.320, 0.437], mean action: 70.800 [39.000, 96.000], mean observation: 3.138 [-1.007, 10.295], loss: 1.198864, mae: 4.395535, mean_q: 4.794082
 18613/100000: episode: 1901, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 4.350, mean reward: 0.435 [0.369, 0.509], mean action: 70.500 [4.000, 100.000], mean observation: 3.152 [-1.204, 10.327], loss: 1.024565, mae: 4.394918, mean_q: 4.794138
 18623/100000: episode: 1902, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.821, mean reward: 0.382 [0.343, 0.529], mean action: 72.400 [37.000, 89.000], mean observation: 3.143 [-1.558, 10.311], loss: 0.986590, mae: 4.395784, mean_q: 4.796909
 18633/100000: episode: 1903, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.020, mean reward: 0.402 [0.335, 0.543], mean action: 75.500 [0.000, 98.000], mean observation: 3.147 [-0.817, 10.378], loss: 0.885024, mae: 4.396548, mean_q: 4.799676
 18643/100000: episode: 1904, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.989, mean reward: 0.499 [0.336, 0.582], mean action: 65.900 [13.000, 80.000], mean observation: 3.154 [-1.251, 10.276], loss: 1.176745, mae: 4.399535, mean_q: 4.801844
 18653/100000: episode: 1905, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.264, mean reward: 0.426 [0.395, 0.542], mean action: 49.700 [1.000, 90.000], mean observation: 3.163 [-1.326, 10.275], loss: 1.112497, mae: 4.399339, mean_q: 4.803813
 18663/100000: episode: 1906, duration: 0.126s, episode steps: 10, steps per second: 80, episode reward: 4.734, mean reward: 0.473 [0.473, 0.473], mean action: 71.900 [45.000, 91.000], mean observation: 3.164 [-1.364, 10.310], loss: 0.889556, mae: 4.399232, mean_q: 4.803891
 18673/100000: episode: 1907, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.882, mean reward: 0.388 [0.333, 0.423], mean action: 57.800 [18.000, 89.000], mean observation: 3.162 [-1.036, 10.399], loss: 1.277148, mae: 4.401613, mean_q: 4.802557
 18683/100000: episode: 1908, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.467, mean reward: 0.447 [0.373, 0.509], mean action: 61.900 [17.000, 100.000], mean observation: 3.147 [-2.125, 10.328], loss: 0.985183, mae: 4.401346, mean_q: 4.806022
 18693/100000: episode: 1909, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.970, mean reward: 0.397 [0.347, 0.445], mean action: 54.300 [20.000, 98.000], mean observation: 3.147 [-1.263, 10.224], loss: 1.367303, mae: 4.403736, mean_q: 4.808189
 18703/100000: episode: 1910, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.995, mean reward: 0.399 [0.336, 0.489], mean action: 52.600 [8.000, 80.000], mean observation: 3.153 [-1.504, 10.200], loss: 1.046778, mae: 4.402684, mean_q: 4.806405
 18713/100000: episode: 1911, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 4.429, mean reward: 0.443 [0.398, 0.464], mean action: 71.100 [24.000, 99.000], mean observation: 3.160 [-1.056, 10.279], loss: 1.118916, mae: 4.404045, mean_q: 4.807025
 18723/100000: episode: 1912, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.673, mean reward: 0.467 [0.467, 0.467], mean action: 76.900 [56.000, 94.000], mean observation: 3.139 [-1.642, 10.374], loss: 1.039362, mae: 4.404621, mean_q: 4.808766
 18733/100000: episode: 1913, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.082, mean reward: 0.408 [0.366, 0.516], mean action: 49.100 [5.000, 95.000], mean observation: 3.152 [-1.347, 10.360], loss: 0.966303, mae: 4.405084, mean_q: 4.804755
 18743/100000: episode: 1914, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.487, mean reward: 0.449 [0.416, 0.539], mean action: 52.500 [2.000, 91.000], mean observation: 3.161 [-1.272, 10.406], loss: 1.074398, mae: 4.406306, mean_q: 4.805705
 18753/100000: episode: 1915, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.692, mean reward: 0.469 [0.351, 0.482], mean action: 58.700 [9.000, 100.000], mean observation: 3.140 [-1.389, 10.236], loss: 0.957280, mae: 4.407322, mean_q: 4.809235
 18763/100000: episode: 1916, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.646, mean reward: 0.365 [0.346, 0.467], mean action: 52.000 [31.000, 81.000], mean observation: 3.162 [-0.881, 10.299], loss: 1.089628, mae: 4.408629, mean_q: 4.813072
 18773/100000: episode: 1917, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.269, mean reward: 0.427 [0.408, 0.463], mean action: 48.100 [19.000, 81.000], mean observation: 3.157 [-1.024, 10.307], loss: 1.030116, mae: 4.409694, mean_q: 4.818328
 18783/100000: episode: 1918, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.983, mean reward: 0.398 [0.289, 0.541], mean action: 47.000 [3.000, 101.000], mean observation: 3.166 [-1.112, 10.247], loss: 1.067014, mae: 4.410378, mean_q: 4.822292
 18793/100000: episode: 1919, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.772, mean reward: 0.377 [0.312, 0.447], mean action: 57.100 [4.000, 95.000], mean observation: 3.158 [-1.138, 10.241], loss: 1.307569, mae: 4.412081, mean_q: 4.825989
 18803/100000: episode: 1920, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.192, mean reward: 0.419 [0.331, 0.466], mean action: 46.000 [8.000, 92.000], mean observation: 3.146 [-1.152, 10.230], loss: 1.213892, mae: 4.412305, mean_q: 4.825593
 18813/100000: episode: 1921, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.006, mean reward: 0.401 [0.335, 0.475], mean action: 52.200 [1.000, 81.000], mean observation: 3.149 [-1.601, 10.307], loss: 1.056524, mae: 4.411980, mean_q: 4.813693
 18823/100000: episode: 1922, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.764, mean reward: 0.376 [0.312, 0.421], mean action: 52.900 [15.000, 87.000], mean observation: 3.154 [-1.557, 10.319], loss: 1.025873, mae: 4.412779, mean_q: 4.807843
 18833/100000: episode: 1923, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.656, mean reward: 0.366 [0.315, 0.463], mean action: 50.500 [21.000, 73.000], mean observation: 3.157 [-1.033, 10.256], loss: 1.009652, mae: 4.413588, mean_q: 4.806327
 18843/100000: episode: 1924, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.832, mean reward: 0.483 [0.483, 0.483], mean action: 60.500 [55.000, 91.000], mean observation: 3.150 [-1.431, 10.325], loss: 1.059099, mae: 4.414483, mean_q: 4.808162
 18853/100000: episode: 1925, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.155, mean reward: 0.416 [0.320, 0.495], mean action: 40.500 [13.000, 67.000], mean observation: 3.163 [-1.121, 10.483], loss: 0.827517, mae: 4.414304, mean_q: 4.808553
 18863/100000: episode: 1926, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.729, mean reward: 0.373 [0.317, 0.547], mean action: 62.800 [31.000, 95.000], mean observation: 3.154 [-1.800, 10.364], loss: 1.190795, mae: 4.416608, mean_q: 4.809538
 18873/100000: episode: 1927, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.799, mean reward: 0.380 [0.360, 0.420], mean action: 53.600 [35.000, 71.000], mean observation: 3.163 [-1.370, 10.325], loss: 1.351865, mae: 4.417787, mean_q: 4.811388
 18883/100000: episode: 1928, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.143, mean reward: 0.414 [0.376, 0.459], mean action: 57.800 [1.000, 99.000], mean observation: 3.170 [-1.339, 10.348], loss: 1.241711, mae: 4.418215, mean_q: 4.812212
 18893/100000: episode: 1929, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.362, mean reward: 0.436 [0.392, 0.530], mean action: 36.400 [1.000, 78.000], mean observation: 3.157 [-1.196, 10.257], loss: 1.057076, mae: 4.417921, mean_q: 4.811113
 18903/100000: episode: 1930, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.052, mean reward: 0.405 [0.375, 0.454], mean action: 59.600 [13.000, 98.000], mean observation: 3.164 [-0.952, 10.407], loss: 0.828831, mae: 4.417792, mean_q: 4.809018
 18913/100000: episode: 1931, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.368, mean reward: 0.437 [0.341, 0.460], mean action: 69.000 [28.000, 100.000], mean observation: 3.159 [-1.065, 10.278], loss: 1.007920, mae: 4.419366, mean_q: 4.811775
 18923/100000: episode: 1932, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.811, mean reward: 0.381 [0.341, 0.409], mean action: 68.100 [4.000, 89.000], mean observation: 3.164 [-0.775, 10.356], loss: 0.965090, mae: 4.419932, mean_q: 4.815820
 18933/100000: episode: 1933, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 4.695, mean reward: 0.470 [0.393, 0.478], mean action: 68.500 [3.000, 98.000], mean observation: 3.167 [-1.301, 10.358], loss: 1.019766, mae: 4.421700, mean_q: 4.820987
 18943/100000: episode: 1934, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.764, mean reward: 0.376 [0.316, 0.420], mean action: 70.100 [18.000, 93.000], mean observation: 3.153 [-1.194, 10.272], loss: 1.280237, mae: 4.423628, mean_q: 4.825673
 18953/100000: episode: 1935, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.867, mean reward: 0.387 [0.301, 0.439], mean action: 50.400 [0.000, 80.000], mean observation: 3.159 [-1.961, 10.337], loss: 0.897901, mae: 4.422770, mean_q: 4.829741
 18963/100000: episode: 1936, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.970, mean reward: 0.397 [0.366, 0.459], mean action: 63.200 [9.000, 80.000], mean observation: 3.151 [-1.185, 10.387], loss: 1.215533, mae: 4.424907, mean_q: 4.829003
 18973/100000: episode: 1937, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.979, mean reward: 0.398 [0.319, 0.475], mean action: 38.300 [0.000, 68.000], mean observation: 3.151 [-1.465, 10.300], loss: 1.302697, mae: 4.426005, mean_q: 4.825336
 18983/100000: episode: 1938, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.060, mean reward: 0.406 [0.304, 0.516], mean action: 50.900 [15.000, 74.000], mean observation: 3.157 [-1.394, 10.335], loss: 1.343064, mae: 4.426836, mean_q: 4.830153
 18993/100000: episode: 1939, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.251, mean reward: 0.425 [0.293, 0.586], mean action: 46.600 [3.000, 88.000], mean observation: 3.164 [-1.596, 10.284], loss: 1.271665, mae: 4.426675, mean_q: 4.833107
 19003/100000: episode: 1940, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.240, mean reward: 0.424 [0.360, 0.488], mean action: 43.300 [0.000, 87.000], mean observation: 3.159 [-1.457, 10.270], loss: 1.018720, mae: 4.426094, mean_q: 4.832189
 19013/100000: episode: 1941, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.383, mean reward: 0.438 [0.322, 0.504], mean action: 54.200 [45.000, 57.000], mean observation: 3.141 [-1.470, 10.251], loss: 0.726679, mae: 4.425174, mean_q: 4.834895
 19023/100000: episode: 1942, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.110, mean reward: 0.411 [0.335, 0.542], mean action: 51.800 [15.000, 91.000], mean observation: 3.139 [-1.617, 10.172], loss: 0.967435, mae: 4.427312, mean_q: 4.838068
 19033/100000: episode: 1943, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.525, mean reward: 0.352 [0.302, 0.386], mean action: 56.200 [48.000, 72.000], mean observation: 3.154 [-1.307, 10.277], loss: 1.105161, mae: 4.428089, mean_q: 4.841613
 19043/100000: episode: 1944, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.897, mean reward: 0.390 [0.286, 0.456], mean action: 47.200 [0.000, 83.000], mean observation: 3.144 [-1.414, 10.245], loss: 1.119070, mae: 4.429358, mean_q: 4.843752
 19053/100000: episode: 1945, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.320, mean reward: 0.432 [0.410, 0.533], mean action: 58.200 [28.000, 78.000], mean observation: 3.178 [-1.621, 10.342], loss: 1.149755, mae: 4.430816, mean_q: 4.842156
 19063/100000: episode: 1946, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.915, mean reward: 0.392 [0.359, 0.432], mean action: 59.400 [30.000, 94.000], mean observation: 3.158 [-1.509, 10.283], loss: 0.969514, mae: 4.430747, mean_q: 4.837300
 19073/100000: episode: 1947, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.747, mean reward: 0.375 [0.323, 0.426], mean action: 54.600 [2.000, 81.000], mean observation: 3.163 [-1.170, 10.336], loss: 0.902684, mae: 4.431262, mean_q: 4.838609
 19083/100000: episode: 1948, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.058, mean reward: 0.406 [0.378, 0.472], mean action: 56.300 [55.000, 68.000], mean observation: 3.150 [-2.091, 10.323], loss: 1.447488, mae: 4.434338, mean_q: 4.834977
 19093/100000: episode: 1949, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.519, mean reward: 0.452 [0.391, 0.497], mean action: 51.100 [15.000, 97.000], mean observation: 3.156 [-1.071, 10.402], loss: 1.151047, mae: 4.433792, mean_q: 4.824717
 19103/100000: episode: 1950, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.633, mean reward: 0.363 [0.331, 0.436], mean action: 56.600 [10.000, 78.000], mean observation: 3.153 [-1.028, 10.273], loss: 0.874376, mae: 4.433286, mean_q: 4.820617
 19113/100000: episode: 1951, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.646, mean reward: 0.465 [0.323, 0.597], mean action: 54.300 [8.000, 85.000], mean observation: 3.162 [-1.494, 10.403], loss: 1.063525, mae: 4.435215, mean_q: 4.822030
 19123/100000: episode: 1952, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.191, mean reward: 0.419 [0.361, 0.521], mean action: 52.800 [11.000, 86.000], mean observation: 3.155 [-1.141, 10.330], loss: 1.100426, mae: 4.436381, mean_q: 4.819886
 19133/100000: episode: 1953, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.602, mean reward: 0.460 [0.382, 0.520], mean action: 50.300 [16.000, 71.000], mean observation: 3.162 [-1.548, 10.213], loss: 0.979346, mae: 4.436612, mean_q: 4.815982
 19143/100000: episode: 1954, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.150, mean reward: 0.415 [0.300, 0.561], mean action: 45.900 [6.000, 90.000], mean observation: 3.160 [-1.921, 10.286], loss: 1.148279, mae: 4.438071, mean_q: 4.815363
 19153/100000: episode: 1955, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.094, mean reward: 0.409 [0.338, 0.482], mean action: 45.900 [11.000, 64.000], mean observation: 3.160 [-1.560, 10.328], loss: 1.010125, mae: 4.437892, mean_q: 4.813584
 19163/100000: episode: 1956, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.221, mean reward: 0.422 [0.343, 0.498], mean action: 41.100 [3.000, 95.000], mean observation: 3.173 [-1.863, 10.301], loss: 0.938433, mae: 4.438327, mean_q: 4.814873
 19173/100000: episode: 1957, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.086, mean reward: 0.409 [0.336, 0.517], mean action: 45.900 [0.000, 55.000], mean observation: 3.166 [-1.492, 10.357], loss: 0.981476, mae: 4.439303, mean_q: 4.817786
 19183/100000: episode: 1958, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.609, mean reward: 0.361 [0.285, 0.451], mean action: 50.800 [3.000, 82.000], mean observation: 3.160 [-1.147, 10.303], loss: 1.250718, mae: 4.441087, mean_q: 4.820352
 19193/100000: episode: 1959, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.257, mean reward: 0.426 [0.339, 0.482], mean action: 46.200 [7.000, 77.000], mean observation: 3.155 [-1.162, 10.457], loss: 1.121957, mae: 4.441117, mean_q: 4.816377
 19203/100000: episode: 1960, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.519, mean reward: 0.352 [0.333, 0.394], mean action: 61.900 [23.000, 100.000], mean observation: 3.150 [-0.998, 10.235], loss: 1.315088, mae: 4.442478, mean_q: 4.813832
 19213/100000: episode: 1961, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.232, mean reward: 0.423 [0.352, 0.521], mean action: 48.100 [1.000, 69.000], mean observation: 3.152 [-1.965, 10.240], loss: 1.149165, mae: 4.442241, mean_q: 4.812742
 19223/100000: episode: 1962, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.251, mean reward: 0.425 [0.386, 0.573], mean action: 49.800 [6.000, 76.000], mean observation: 3.163 [-1.503, 10.338], loss: 0.852351, mae: 4.441719, mean_q: 4.814034
 19224/100000: episode: 1963, duration: 0.032s, episode steps: 1, steps per second: 32, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 21.000 [21.000, 21.000], mean observation: 3.135 [-0.801, 10.100], loss: 1.283131, mae: 4.443474, mean_q: 4.815669
 19234/100000: episode: 1964, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.936, mean reward: 0.394 [0.317, 0.531], mean action: 63.400 [41.000, 99.000], mean observation: 3.152 [-1.615, 10.325], loss: 1.255761, mae: 4.443634, mean_q: 4.815686
 19244/100000: episode: 1965, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.956, mean reward: 0.396 [0.322, 0.432], mean action: 56.200 [3.000, 96.000], mean observation: 3.157 [-1.614, 10.340], loss: 1.324070, mae: 4.444172, mean_q: 4.816737
 19254/100000: episode: 1966, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.801, mean reward: 0.380 [0.366, 0.440], mean action: 60.500 [0.000, 98.000], mean observation: 3.158 [-1.644, 10.374], loss: 1.210818, mae: 4.444194, mean_q: 4.818626
 19264/100000: episode: 1967, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.681, mean reward: 0.368 [0.325, 0.438], mean action: 65.000 [2.000, 95.000], mean observation: 3.135 [-1.251, 10.268], loss: 1.204454, mae: 4.444482, mean_q: 4.818923
 19274/100000: episode: 1968, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.596, mean reward: 0.460 [0.369, 0.535], mean action: 62.000 [9.000, 93.000], mean observation: 3.159 [-1.595, 10.249], loss: 1.189349, mae: 4.445535, mean_q: 4.819062
 19284/100000: episode: 1969, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 3.447, mean reward: 0.345 [0.341, 0.374], mean action: 82.300 [80.000, 91.000], mean observation: 3.159 [-1.431, 10.360], loss: 1.090789, mae: 4.445695, mean_q: 4.821622
 19294/100000: episode: 1970, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.032, mean reward: 0.403 [0.374, 0.490], mean action: 56.200 [4.000, 90.000], mean observation: 3.144 [-1.650, 10.270], loss: 0.977977, mae: 4.445720, mean_q: 4.823836
 19304/100000: episode: 1971, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.101, mean reward: 0.410 [0.378, 0.467], mean action: 61.600 [24.000, 80.000], mean observation: 3.159 [-1.294, 10.293], loss: 1.062865, mae: 4.446612, mean_q: 4.823151
 19314/100000: episode: 1972, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.149, mean reward: 0.415 [0.413, 0.423], mean action: 85.400 [80.000, 101.000], mean observation: 3.156 [-1.088, 10.325], loss: 1.014328, mae: 4.447431, mean_q: 4.821320
 19324/100000: episode: 1973, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.466, mean reward: 0.447 [0.295, 0.544], mean action: 73.700 [40.000, 92.000], mean observation: 3.157 [-1.392, 10.382], loss: 1.221011, mae: 4.448803, mean_q: 4.822124
 19334/100000: episode: 1974, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.817, mean reward: 0.382 [0.312, 0.551], mean action: 56.000 [3.000, 80.000], mean observation: 3.156 [-1.414, 10.359], loss: 1.201058, mae: 4.449217, mean_q: 4.822392
 19344/100000: episode: 1975, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.223, mean reward: 0.422 [0.335, 0.543], mean action: 62.700 [44.000, 91.000], mean observation: 3.147 [-1.362, 10.360], loss: 0.828447, mae: 4.448485, mean_q: 4.819636
 19354/100000: episode: 1976, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.012, mean reward: 0.401 [0.334, 0.507], mean action: 28.700 [1.000, 78.000], mean observation: 3.172 [-1.257, 10.305], loss: 0.962485, mae: 4.449452, mean_q: 4.817278
 19364/100000: episode: 1977, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.053, mean reward: 0.405 [0.347, 0.579], mean action: 58.600 [12.000, 98.000], mean observation: 3.165 [-1.274, 10.310], loss: 1.102056, mae: 4.450801, mean_q: 4.818493
 19374/100000: episode: 1978, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.221, mean reward: 0.422 [0.336, 0.563], mean action: 50.500 [1.000, 96.000], mean observation: 3.162 [-1.065, 10.351], loss: 1.036580, mae: 4.451386, mean_q: 4.820833
 19384/100000: episode: 1979, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.391, mean reward: 0.439 [0.373, 0.535], mean action: 50.400 [1.000, 80.000], mean observation: 3.142 [-1.388, 10.263], loss: 1.097552, mae: 4.452410, mean_q: 4.823603
 19394/100000: episode: 1980, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.040, mean reward: 0.404 [0.377, 0.480], mean action: 69.100 [2.000, 89.000], mean observation: 3.162 [-1.266, 10.319], loss: 1.089043, mae: 4.452621, mean_q: 4.826818
 19404/100000: episode: 1981, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.438, mean reward: 0.444 [0.351, 0.555], mean action: 55.300 [6.000, 93.000], mean observation: 3.157 [-2.197, 10.426], loss: 0.924725, mae: 4.452256, mean_q: 4.825916
 19414/100000: episode: 1982, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.816, mean reward: 0.382 [0.343, 0.470], mean action: 61.600 [20.000, 97.000], mean observation: 3.160 [-1.540, 10.400], loss: 1.124578, mae: 4.453887, mean_q: 4.826707
 19424/100000: episode: 1983, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 4.315, mean reward: 0.432 [0.368, 0.490], mean action: 78.000 [61.000, 93.000], mean observation: 3.165 [-1.233, 10.315], loss: 0.996078, mae: 4.453779, mean_q: 4.829025
 19434/100000: episode: 1984, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.680, mean reward: 0.468 [0.391, 0.530], mean action: 49.300 [11.000, 96.000], mean observation: 3.156 [-0.995, 10.188], loss: 1.019609, mae: 4.454345, mean_q: 4.829784
 19444/100000: episode: 1985, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.497, mean reward: 0.450 [0.319, 0.540], mean action: 27.900 [11.000, 89.000], mean observation: 3.163 [-1.520, 10.620], loss: 1.138195, mae: 4.455812, mean_q: 4.830914
 19454/100000: episode: 1986, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.093, mean reward: 0.409 [0.347, 0.527], mean action: 27.500 [11.000, 63.000], mean observation: 3.159 [-1.309, 10.298], loss: 1.146963, mae: 4.456625, mean_q: 4.832391
 19464/100000: episode: 1987, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.138, mean reward: 0.414 [0.310, 0.506], mean action: 30.600 [11.000, 90.000], mean observation: 3.157 [-2.255, 10.315], loss: 1.038592, mae: 4.456808, mean_q: 4.833712
 19474/100000: episode: 1988, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 3.637, mean reward: 0.364 [0.284, 0.429], mean action: 26.500 [10.000, 88.000], mean observation: 3.157 [-0.989, 10.307], loss: 1.185736, mae: 4.458048, mean_q: 4.836432
 19484/100000: episode: 1989, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.224, mean reward: 0.422 [0.358, 0.486], mean action: 29.400 [0.000, 91.000], mean observation: 3.153 [-2.251, 10.402], loss: 1.034940, mae: 4.458061, mean_q: 4.839374
 19494/100000: episode: 1990, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.132, mean reward: 0.413 [0.332, 0.510], mean action: 14.800 [11.000, 48.000], mean observation: 3.144 [-1.610, 10.552], loss: 1.262378, mae: 4.459538, mean_q: 4.841809
 19504/100000: episode: 1991, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.914, mean reward: 0.391 [0.330, 0.448], mean action: 23.100 [7.000, 79.000], mean observation: 3.163 [-1.644, 10.383], loss: 1.216005, mae: 4.459714, mean_q: 4.844606
 19514/100000: episode: 1992, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 3.596, mean reward: 0.360 [0.277, 0.430], mean action: 32.700 [11.000, 72.000], mean observation: 3.163 [-0.933, 10.343], loss: 0.935236, mae: 4.459363, mean_q: 4.847592
 19524/100000: episode: 1993, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.769, mean reward: 0.377 [0.319, 0.464], mean action: 31.900 [3.000, 99.000], mean observation: 3.166 [-1.215, 10.373], loss: 1.083844, mae: 4.461113, mean_q: 4.848859
 19534/100000: episode: 1994, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.580, mean reward: 0.458 [0.378, 0.500], mean action: 43.100 [10.000, 80.000], mean observation: 3.155 [-1.356, 10.304], loss: 1.003407, mae: 4.461208, mean_q: 4.844868
 19544/100000: episode: 1995, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 4.168, mean reward: 0.417 [0.400, 0.485], mean action: 75.700 [55.000, 100.000], mean observation: 3.149 [-1.463, 10.320], loss: 1.174452, mae: 4.463099, mean_q: 4.845037
 19554/100000: episode: 1996, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.207, mean reward: 0.421 [0.421, 0.421], mean action: 68.000 [31.000, 85.000], mean observation: 3.167 [-1.531, 10.370], loss: 0.929112, mae: 4.462349, mean_q: 4.847809
 19564/100000: episode: 1997, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.567, mean reward: 0.457 [0.457, 0.457], mean action: 78.400 [52.000, 91.000], mean observation: 3.156 [-1.085, 10.322], loss: 1.044448, mae: 4.463979, mean_q: 4.851108
 19574/100000: episode: 1998, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.961, mean reward: 0.496 [0.496, 0.496], mean action: 78.900 [70.000, 85.000], mean observation: 3.154 [-0.986, 10.275], loss: 1.090290, mae: 4.464540, mean_q: 4.854187
 19584/100000: episode: 1999, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.313, mean reward: 0.431 [0.345, 0.527], mean action: 71.300 [46.000, 81.000], mean observation: 3.157 [-1.060, 10.383], loss: 1.014293, mae: 4.464979, mean_q: 4.857120
 19594/100000: episode: 2000, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.458, mean reward: 0.446 [0.339, 0.521], mean action: 66.000 [1.000, 87.000], mean observation: 3.173 [-1.904, 10.537], loss: 1.159193, mae: 4.466232, mean_q: 4.858256
 19604/100000: episode: 2001, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.245, mean reward: 0.424 [0.419, 0.478], mean action: 60.300 [24.000, 80.000], mean observation: 3.155 [-1.328, 10.272], loss: 0.850813, mae: 4.465530, mean_q: 4.857215
 19614/100000: episode: 2002, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.089, mean reward: 0.409 [0.291, 0.492], mean action: 61.100 [7.000, 90.000], mean observation: 3.156 [-1.064, 10.384], loss: 0.981852, mae: 4.466424, mean_q: 4.860507
 19624/100000: episode: 2003, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.962, mean reward: 0.396 [0.332, 0.475], mean action: 49.700 [7.000, 80.000], mean observation: 3.155 [-1.273, 10.295], loss: 1.041414, mae: 4.467156, mean_q: 4.864854
 19634/100000: episode: 2004, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.089, mean reward: 0.409 [0.350, 0.527], mean action: 62.700 [15.000, 80.000], mean observation: 3.151 [-0.897, 10.307], loss: 0.898769, mae: 4.467570, mean_q: 4.868942
 19644/100000: episode: 2005, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.675, mean reward: 0.467 [0.395, 0.523], mean action: 63.700 [6.000, 80.000], mean observation: 3.161 [-1.797, 10.286], loss: 0.858371, mae: 4.468102, mean_q: 4.872637
 19654/100000: episode: 2006, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.268, mean reward: 0.427 [0.346, 0.474], mean action: 68.200 [10.000, 99.000], mean observation: 3.161 [-1.276, 10.295], loss: 1.007810, mae: 4.469265, mean_q: 4.875209
 19664/100000: episode: 2007, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.868, mean reward: 0.387 [0.374, 0.459], mean action: 69.300 [21.000, 80.000], mean observation: 3.158 [-1.053, 10.192], loss: 1.330004, mae: 4.470859, mean_q: 4.873477
 19674/100000: episode: 2008, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.474, mean reward: 0.447 [0.435, 0.492], mean action: 68.700 [23.000, 84.000], mean observation: 3.162 [-1.565, 10.487], loss: 0.975583, mae: 4.470160, mean_q: 4.874506
 19684/100000: episode: 2009, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 5.154, mean reward: 0.515 [0.418, 0.567], mean action: 68.700 [11.000, 86.000], mean observation: 3.170 [-1.775, 10.382], loss: 0.849899, mae: 4.470195, mean_q: 4.875188
 19694/100000: episode: 2010, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.686, mean reward: 0.369 [0.337, 0.430], mean action: 58.700 [2.000, 85.000], mean observation: 3.154 [-1.705, 10.431], loss: 1.094928, mae: 4.471744, mean_q: 4.876354
 19704/100000: episode: 2011, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.280, mean reward: 0.428 [0.374, 0.491], mean action: 70.700 [0.000, 89.000], mean observation: 3.148 [-1.906, 10.215], loss: 1.346395, mae: 4.473812, mean_q: 4.876505
 19714/100000: episode: 2012, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.241, mean reward: 0.424 [0.300, 0.503], mean action: 64.300 [11.000, 84.000], mean observation: 3.156 [-1.899, 10.342], loss: 0.921463, mae: 4.472499, mean_q: 4.877700
 19724/100000: episode: 2013, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.801, mean reward: 0.380 [0.361, 0.454], mean action: 54.500 [9.000, 80.000], mean observation: 3.155 [-0.888, 10.377], loss: 1.360417, mae: 4.474805, mean_q: 4.880454
 19734/100000: episode: 2014, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.645, mean reward: 0.365 [0.300, 0.522], mean action: 69.800 [3.000, 80.000], mean observation: 3.166 [-1.689, 10.366], loss: 0.826221, mae: 4.473123, mean_q: 4.879908
 19744/100000: episode: 2015, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.176, mean reward: 0.418 [0.328, 0.472], mean action: 70.000 [6.000, 86.000], mean observation: 3.167 [-1.307, 10.317], loss: 0.867890, mae: 4.474086, mean_q: 4.881513
 19754/100000: episode: 2016, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.925, mean reward: 0.393 [0.349, 0.480], mean action: 68.800 [3.000, 91.000], mean observation: 3.153 [-1.273, 10.362], loss: 1.392109, mae: 4.477303, mean_q: 4.884564
 19764/100000: episode: 2017, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.810, mean reward: 0.381 [0.330, 0.464], mean action: 70.400 [16.000, 80.000], mean observation: 3.147 [-0.878, 10.310], loss: 1.124534, mae: 4.476842, mean_q: 4.888416
 19774/100000: episode: 2018, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.810, mean reward: 0.381 [0.334, 0.501], mean action: 54.900 [16.000, 80.000], mean observation: 3.162 [-1.281, 10.242], loss: 1.114676, mae: 4.477179, mean_q: 4.892657
 19784/100000: episode: 2019, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.305, mean reward: 0.431 [0.355, 0.585], mean action: 57.900 [4.000, 80.000], mean observation: 3.151 [-1.406, 10.349], loss: 1.230109, mae: 4.478583, mean_q: 4.896562
 19794/100000: episode: 2020, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 3.834, mean reward: 0.383 [0.347, 0.459], mean action: 58.600 [1.000, 80.000], mean observation: 3.164 [-1.164, 10.304], loss: 1.268173, mae: 4.479030, mean_q: 4.900091
 19804/100000: episode: 2021, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 5.420, mean reward: 0.542 [0.542, 0.542], mean action: 64.900 [22.000, 98.000], mean observation: 3.157 [-1.450, 10.421], loss: 1.005425, mae: 4.478882, mean_q: 4.900341
 19814/100000: episode: 2022, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.888, mean reward: 0.389 [0.358, 0.467], mean action: 72.000 [43.000, 91.000], mean observation: 3.169 [-1.515, 10.297], loss: 1.026836, mae: 4.479681, mean_q: 4.897719
 19824/100000: episode: 2023, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.589, mean reward: 0.359 [0.339, 0.455], mean action: 75.000 [43.000, 80.000], mean observation: 3.148 [-0.625, 10.229], loss: 1.156830, mae: 4.481379, mean_q: 4.895410
 19834/100000: episode: 2024, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.161, mean reward: 0.416 [0.398, 0.455], mean action: 65.900 [13.000, 80.000], mean observation: 3.155 [-1.143, 10.294], loss: 1.190957, mae: 4.482188, mean_q: 4.897495
 19844/100000: episode: 2025, duration: 0.116s, episode steps: 10, steps per second: 87, episode reward: 4.284, mean reward: 0.428 [0.368, 0.457], mean action: 66.000 [13.000, 84.000], mean observation: 3.164 [-1.207, 10.323], loss: 1.169340, mae: 4.482930, mean_q: 4.900538
 19854/100000: episode: 2026, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.262, mean reward: 0.426 [0.321, 0.457], mean action: 62.300 [7.000, 89.000], mean observation: 3.159 [-1.642, 10.335], loss: 1.178319, mae: 4.483363, mean_q: 4.901019
 19864/100000: episode: 2027, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 3.762, mean reward: 0.376 [0.374, 0.400], mean action: 78.800 [55.000, 101.000], mean observation: 3.172 [-1.659, 10.296], loss: 0.943056, mae: 4.483432, mean_q: 4.899403
 19874/100000: episode: 2028, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.883, mean reward: 0.488 [0.432, 0.533], mean action: 62.900 [4.000, 101.000], mean observation: 3.157 [-0.972, 10.273], loss: 1.155717, mae: 4.485269, mean_q: 4.897887
 19884/100000: episode: 2029, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.149, mean reward: 0.415 [0.384, 0.505], mean action: 63.000 [3.000, 86.000], mean observation: 3.163 [-1.768, 10.254], loss: 1.142203, mae: 4.486453, mean_q: 4.896032
 19894/100000: episode: 2030, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.865, mean reward: 0.386 [0.324, 0.441], mean action: 56.400 [1.000, 101.000], mean observation: 3.153 [-1.208, 10.346], loss: 1.118757, mae: 4.487325, mean_q: 4.889861
 19904/100000: episode: 2031, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.914, mean reward: 0.391 [0.373, 0.450], mean action: 66.500 [18.000, 85.000], mean observation: 3.156 [-1.885, 10.336], loss: 0.866925, mae: 4.487471, mean_q: 4.881621
 19914/100000: episode: 2032, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 3.512, mean reward: 0.351 [0.331, 0.377], mean action: 62.300 [12.000, 84.000], mean observation: 3.159 [-1.353, 10.428], loss: 0.971308, mae: 4.488902, mean_q: 4.880721
 19924/100000: episode: 2033, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.220, mean reward: 0.422 [0.407, 0.484], mean action: 72.800 [39.000, 93.000], mean observation: 3.158 [-1.966, 10.375], loss: 1.181182, mae: 4.490514, mean_q: 4.883203
 19934/100000: episode: 2034, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.847, mean reward: 0.385 [0.345, 0.490], mean action: 61.600 [20.000, 80.000], mean observation: 3.154 [-0.727, 10.290], loss: 1.050078, mae: 4.491407, mean_q: 4.883246
 19944/100000: episode: 2035, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 4.406, mean reward: 0.441 [0.416, 0.489], mean action: 71.300 [29.000, 92.000], mean observation: 3.146 [-1.555, 10.424], loss: 0.967765, mae: 4.491975, mean_q: 4.881322
 19954/100000: episode: 2036, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.315, mean reward: 0.431 [0.393, 0.443], mean action: 71.500 [14.000, 95.000], mean observation: 3.146 [-1.051, 10.270], loss: 1.396382, mae: 4.494719, mean_q: 4.882715
 19964/100000: episode: 2037, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.012, mean reward: 0.401 [0.358, 0.449], mean action: 58.500 [18.000, 84.000], mean observation: 3.157 [-1.483, 10.396], loss: 1.187714, mae: 4.494432, mean_q: 4.883044
 19974/100000: episode: 2038, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 4.017, mean reward: 0.402 [0.386, 0.438], mean action: 74.500 [35.000, 99.000], mean observation: 3.136 [-2.323, 10.242], loss: 1.156747, mae: 4.494651, mean_q: 4.883589
 19984/100000: episode: 2039, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 4.604, mean reward: 0.460 [0.420, 0.504], mean action: 76.500 [24.000, 95.000], mean observation: 3.168 [-0.761, 10.366], loss: 1.026389, mae: 4.495097, mean_q: 4.886395
 19994/100000: episode: 2040, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.234, mean reward: 0.423 [0.411, 0.518], mean action: 59.000 [8.000, 94.000], mean observation: 3.157 [-2.125, 10.302], loss: 1.078846, mae: 4.496165, mean_q: 4.889173
 20004/100000: episode: 2041, duration: 0.113s, episode steps: 10, steps per second: 88, episode reward: 3.443, mean reward: 0.344 [0.326, 0.400], mean action: 73.700 [40.000, 94.000], mean observation: 3.139 [-1.465, 10.361], loss: 1.099901, mae: 4.496672, mean_q: 4.892141
 20014/100000: episode: 2042, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.111, mean reward: 0.411 [0.360, 0.499], mean action: 60.100 [3.000, 100.000], mean observation: 3.150 [-1.634, 10.354], loss: 1.197615, mae: 4.498302, mean_q: 4.893471
 20024/100000: episode: 2043, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.509, mean reward: 0.351 [0.334, 0.391], mean action: 63.400 [0.000, 96.000], mean observation: 3.152 [-1.298, 10.386], loss: 1.139960, mae: 4.498345, mean_q: 4.890817
 20034/100000: episode: 2044, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.205, mean reward: 0.420 [0.392, 0.484], mean action: 60.900 [7.000, 80.000], mean observation: 3.162 [-1.294, 10.317], loss: 1.087809, mae: 4.499241, mean_q: 4.890272
 20044/100000: episode: 2045, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.128, mean reward: 0.413 [0.354, 0.470], mean action: 63.200 [16.000, 100.000], mean observation: 3.154 [-1.720, 10.296], loss: 1.125571, mae: 4.499784, mean_q: 4.890461
 20054/100000: episode: 2046, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.631, mean reward: 0.463 [0.366, 0.498], mean action: 54.700 [5.000, 80.000], mean observation: 3.164 [-1.592, 10.411], loss: 1.259373, mae: 4.500629, mean_q: 4.888902
 20064/100000: episode: 2047, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.112, mean reward: 0.411 [0.302, 0.484], mean action: 66.700 [18.000, 80.000], mean observation: 3.157 [-1.460, 10.215], loss: 1.077169, mae: 4.501016, mean_q: 4.889499
 20074/100000: episode: 2048, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.628, mean reward: 0.463 [0.409, 0.548], mean action: 79.900 [46.000, 97.000], mean observation: 3.153 [-1.860, 10.289], loss: 0.906845, mae: 4.500821, mean_q: 4.887782
 20084/100000: episode: 2049, duration: 0.121s, episode steps: 10, steps per second: 82, episode reward: 3.527, mean reward: 0.353 [0.276, 0.424], mean action: 73.400 [20.000, 100.000], mean observation: 3.156 [-1.021, 10.350], loss: 0.925864, mae: 4.501858, mean_q: 4.889784
 20094/100000: episode: 2050, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.306, mean reward: 0.431 [0.332, 0.485], mean action: 59.600 [3.000, 95.000], mean observation: 3.154 [-1.885, 10.319], loss: 0.864892, mae: 4.502501, mean_q: 4.886478
 20104/100000: episode: 2051, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 4.244, mean reward: 0.424 [0.424, 0.424], mean action: 76.200 [35.000, 100.000], mean observation: 3.171 [-0.681, 10.260], loss: 0.994668, mae: 4.504172, mean_q: 4.881860
 20114/100000: episode: 2052, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.162, mean reward: 0.416 [0.373, 0.477], mean action: 74.900 [28.000, 100.000], mean observation: 3.154 [-1.204, 10.354], loss: 0.926682, mae: 4.504577, mean_q: 4.875783
 20124/100000: episode: 2053, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.110, mean reward: 0.411 [0.358, 0.471], mean action: 64.500 [7.000, 91.000], mean observation: 3.155 [-0.873, 10.430], loss: 1.139896, mae: 4.506295, mean_q: 4.871691
 20134/100000: episode: 2054, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.027, mean reward: 0.403 [0.276, 0.474], mean action: 48.000 [1.000, 80.000], mean observation: 3.152 [-0.984, 10.260], loss: 1.142148, mae: 4.506997, mean_q: 4.867181
 20144/100000: episode: 2055, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 4.328, mean reward: 0.433 [0.410, 0.495], mean action: 72.200 [6.000, 93.000], mean observation: 3.159 [-1.078, 10.370], loss: 1.087074, mae: 4.507262, mean_q: 4.866930
 20154/100000: episode: 2056, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.103, mean reward: 0.410 [0.407, 0.423], mean action: 79.400 [46.000, 99.000], mean observation: 3.166 [-1.001, 10.337], loss: 1.206613, mae: 4.507989, mean_q: 4.869653
 20164/100000: episode: 2057, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.167, mean reward: 0.417 [0.361, 0.480], mean action: 64.700 [13.000, 80.000], mean observation: 3.155 [-1.309, 10.315], loss: 0.969562, mae: 4.507613, mean_q: 4.873361
 20174/100000: episode: 2058, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.912, mean reward: 0.391 [0.298, 0.513], mean action: 62.100 [4.000, 99.000], mean observation: 3.161 [-1.247, 10.327], loss: 1.178986, mae: 4.508623, mean_q: 4.873053
 20184/100000: episode: 2059, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.881, mean reward: 0.388 [0.349, 0.475], mean action: 44.900 [2.000, 80.000], mean observation: 3.167 [-2.081, 10.361], loss: 0.774436, mae: 4.508091, mean_q: 4.871473
 20194/100000: episode: 2060, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.721, mean reward: 0.472 [0.418, 0.579], mean action: 65.900 [2.000, 99.000], mean observation: 3.152 [-1.428, 10.309], loss: 0.854782, mae: 4.509159, mean_q: 4.871411
 20204/100000: episode: 2061, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.779, mean reward: 0.378 [0.346, 0.427], mean action: 63.800 [7.000, 88.000], mean observation: 3.158 [-0.599, 10.417], loss: 1.168412, mae: 4.511155, mean_q: 4.871954
 20214/100000: episode: 2062, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 3.857, mean reward: 0.386 [0.360, 0.456], mean action: 65.800 [7.000, 86.000], mean observation: 3.150 [-0.850, 10.492], loss: 1.094614, mae: 4.511319, mean_q: 4.873572
 20224/100000: episode: 2063, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.813, mean reward: 0.381 [0.348, 0.481], mean action: 67.200 [12.000, 82.000], mean observation: 3.157 [-0.885, 10.298], loss: 1.104897, mae: 4.512212, mean_q: 4.874994
 20234/100000: episode: 2064, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.580, mean reward: 0.458 [0.457, 0.464], mean action: 70.800 [27.000, 100.000], mean observation: 3.151 [-0.798, 10.362], loss: 0.976004, mae: 4.512364, mean_q: 4.873688
 20244/100000: episode: 2065, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.934, mean reward: 0.393 [0.327, 0.511], mean action: 60.500 [9.000, 80.000], mean observation: 3.159 [-1.398, 10.325], loss: 0.947160, mae: 4.512757, mean_q: 4.875544
 20254/100000: episode: 2066, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.810, mean reward: 0.381 [0.318, 0.427], mean action: 63.500 [1.000, 80.000], mean observation: 3.152 [-1.389, 10.273], loss: 0.841456, mae: 4.513208, mean_q: 4.876379
 20264/100000: episode: 2067, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 5.187, mean reward: 0.519 [0.519, 0.519], mean action: 75.300 [31.000, 99.000], mean observation: 3.153 [-0.964, 10.214], loss: 1.090400, mae: 4.514697, mean_q: 4.879424
 20274/100000: episode: 2068, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.665, mean reward: 0.367 [0.291, 0.529], mean action: 69.200 [4.000, 97.000], mean observation: 3.152 [-1.008, 10.402], loss: 1.073385, mae: 4.515850, mean_q: 4.881549
 20284/100000: episode: 2069, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.387, mean reward: 0.439 [0.439, 0.439], mean action: 68.200 [13.000, 80.000], mean observation: 3.137 [-1.261, 10.316], loss: 1.045387, mae: 4.516351, mean_q: 4.882054
 20294/100000: episode: 2070, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.845, mean reward: 0.384 [0.327, 0.459], mean action: 66.600 [7.000, 88.000], mean observation: 3.150 [-1.357, 10.460], loss: 0.791779, mae: 4.516062, mean_q: 4.883899
 20304/100000: episode: 2071, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.979, mean reward: 0.398 [0.348, 0.497], mean action: 56.200 [0.000, 80.000], mean observation: 3.164 [-1.272, 10.271], loss: 0.834503, mae: 4.517396, mean_q: 4.885960
 20314/100000: episode: 2072, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.390, mean reward: 0.439 [0.332, 0.509], mean action: 54.700 [7.000, 80.000], mean observation: 3.163 [-1.162, 10.234], loss: 0.913887, mae: 4.518806, mean_q: 4.888375
 20324/100000: episode: 2073, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 3.815, mean reward: 0.382 [0.369, 0.465], mean action: 78.900 [50.000, 97.000], mean observation: 3.154 [-1.947, 10.299], loss: 1.259381, mae: 4.520852, mean_q: 4.886571
 20334/100000: episode: 2074, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.301, mean reward: 0.430 [0.405, 0.553], mean action: 65.700 [10.000, 85.000], mean observation: 3.146 [-1.289, 10.251], loss: 1.283940, mae: 4.521684, mean_q: 4.881793
 20344/100000: episode: 2075, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.288, mean reward: 0.429 [0.348, 0.531], mean action: 59.600 [5.000, 100.000], mean observation: 3.158 [-1.096, 10.269], loss: 0.827059, mae: 4.520185, mean_q: 4.881230
 20354/100000: episode: 2076, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 3.915, mean reward: 0.391 [0.346, 0.430], mean action: 77.900 [59.000, 80.000], mean observation: 3.171 [-0.862, 10.269], loss: 0.930360, mae: 4.522126, mean_q: 4.880044
 20364/100000: episode: 2077, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.065, mean reward: 0.406 [0.402, 0.427], mean action: 76.900 [49.000, 80.000], mean observation: 3.168 [-0.877, 10.360], loss: 1.146896, mae: 4.524018, mean_q: 4.882730
 20374/100000: episode: 2078, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.417, mean reward: 0.442 [0.306, 0.492], mean action: 79.300 [48.000, 96.000], mean observation: 3.163 [-1.170, 10.318], loss: 0.772470, mae: 4.523032, mean_q: 4.882827
 20384/100000: episode: 2079, duration: 0.117s, episode steps: 10, steps per second: 86, episode reward: 4.205, mean reward: 0.420 [0.384, 0.495], mean action: 65.300 [0.000, 94.000], mean observation: 3.147 [-1.230, 10.483], loss: 0.850061, mae: 4.524508, mean_q: 4.883729
 20394/100000: episode: 2080, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 3.773, mean reward: 0.377 [0.345, 0.448], mean action: 61.300 [0.000, 80.000], mean observation: 3.154 [-1.443, 10.285], loss: 1.106848, mae: 4.525954, mean_q: 4.886575
 20404/100000: episode: 2081, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 4.051, mean reward: 0.405 [0.328, 0.479], mean action: 76.200 [38.000, 97.000], mean observation: 3.166 [-1.008, 10.324], loss: 1.069604, mae: 4.526460, mean_q: 4.887686
 20414/100000: episode: 2082, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 4.545, mean reward: 0.455 [0.420, 0.537], mean action: 83.400 [65.000, 100.000], mean observation: 3.165 [-0.970, 10.295], loss: 0.974083, mae: 4.527169, mean_q: 4.890069
 20424/100000: episode: 2083, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.060, mean reward: 0.406 [0.402, 0.441], mean action: 66.500 [9.000, 80.000], mean observation: 3.144 [-1.661, 10.351], loss: 1.417570, mae: 4.529445, mean_q: 4.890776
 20434/100000: episode: 2084, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.086, mean reward: 0.409 [0.387, 0.460], mean action: 74.400 [1.000, 100.000], mean observation: 3.154 [-0.959, 10.348], loss: 0.907392, mae: 4.527774, mean_q: 4.888653
 20444/100000: episode: 2085, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.104, mean reward: 0.410 [0.319, 0.463], mean action: 63.400 [12.000, 91.000], mean observation: 3.160 [-0.878, 10.259], loss: 1.075332, mae: 4.529336, mean_q: 4.887635
 20454/100000: episode: 2086, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.439, mean reward: 0.444 [0.418, 0.461], mean action: 66.300 [4.000, 97.000], mean observation: 3.152 [-1.492, 10.248], loss: 1.160210, mae: 4.530362, mean_q: 4.889132
 20464/100000: episode: 2087, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.031, mean reward: 0.403 [0.354, 0.507], mean action: 61.300 [1.000, 93.000], mean observation: 3.164 [-0.987, 10.408], loss: 1.133165, mae: 4.530855, mean_q: 4.891126
 20474/100000: episode: 2088, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.853, mean reward: 0.385 [0.349, 0.413], mean action: 63.100 [20.000, 100.000], mean observation: 3.169 [-1.693, 10.359], loss: 1.002592, mae: 4.530601, mean_q: 4.894574
 20481/100000: episode: 2089, duration: 0.106s, episode steps: 7, steps per second: 66, episode reward: 12.493, mean reward: 1.785 [0.384, 10.000], mean action: 58.286 [0.000, 93.000], mean observation: 3.177 [-1.122, 10.428], loss: 1.031833, mae: 4.531528, mean_q: 4.896464
 20491/100000: episode: 2090, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.952, mean reward: 0.395 [0.307, 0.515], mean action: 47.900 [4.000, 80.000], mean observation: 3.167 [-1.368, 10.407], loss: 1.023928, mae: 4.531925, mean_q: 4.897226
 20501/100000: episode: 2091, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.678, mean reward: 0.368 [0.357, 0.394], mean action: 72.500 [38.000, 92.000], mean observation: 3.147 [-1.028, 10.278], loss: 0.962542, mae: 4.532410, mean_q: 4.897498
 20511/100000: episode: 2092, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.749, mean reward: 0.375 [0.319, 0.588], mean action: 73.600 [22.000, 96.000], mean observation: 3.165 [-1.363, 10.288], loss: 0.974828, mae: 4.533337, mean_q: 4.896763
 20521/100000: episode: 2093, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.522, mean reward: 0.352 [0.339, 0.413], mean action: 73.200 [12.000, 80.000], mean observation: 3.158 [-0.980, 10.300], loss: 1.377540, mae: 4.535274, mean_q: 4.897994
 20531/100000: episode: 2094, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.833, mean reward: 0.383 [0.367, 0.429], mean action: 70.100 [37.000, 85.000], mean observation: 3.133 [-1.080, 10.196], loss: 1.294545, mae: 4.535422, mean_q: 4.898558
 20541/100000: episode: 2095, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.265, mean reward: 0.426 [0.389, 0.518], mean action: 66.500 [16.000, 91.000], mean observation: 3.165 [-1.407, 10.305], loss: 1.223784, mae: 4.535850, mean_q: 4.895637
 20551/100000: episode: 2096, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.893, mean reward: 0.389 [0.333, 0.428], mean action: 69.700 [16.000, 91.000], mean observation: 3.145 [-1.193, 10.338], loss: 0.850970, mae: 4.534669, mean_q: 4.895552
 20561/100000: episode: 2097, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.896, mean reward: 0.390 [0.353, 0.400], mean action: 58.600 [0.000, 99.000], mean observation: 3.147 [-0.841, 10.230], loss: 1.070088, mae: 4.536560, mean_q: 4.898625
 20571/100000: episode: 2098, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.625, mean reward: 0.462 [0.462, 0.469], mean action: 80.000 [80.000, 80.000], mean observation: 3.171 [-1.425, 10.285], loss: 0.943014, mae: 4.536518, mean_q: 4.900951
 20581/100000: episode: 2099, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.335, mean reward: 0.434 [0.427, 0.495], mean action: 64.000 [7.000, 94.000], mean observation: 3.171 [-1.114, 10.392], loss: 1.100438, mae: 4.537997, mean_q: 4.900584
 20591/100000: episode: 2100, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.531, mean reward: 0.453 [0.422, 0.501], mean action: 57.200 [6.000, 80.000], mean observation: 3.150 [-1.712, 10.228], loss: 1.083477, mae: 4.538416, mean_q: 4.895036
 20601/100000: episode: 2101, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.764, mean reward: 0.376 [0.351, 0.441], mean action: 56.300 [1.000, 101.000], mean observation: 3.156 [-1.617, 10.372], loss: 0.839424, mae: 4.538302, mean_q: 4.892141
 20611/100000: episode: 2102, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.841, mean reward: 0.484 [0.352, 0.560], mean action: 60.600 [4.000, 96.000], mean observation: 3.148 [-1.424, 10.318], loss: 1.014766, mae: 4.540024, mean_q: 4.893535
 20621/100000: episode: 2103, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.921, mean reward: 0.392 [0.315, 0.454], mean action: 60.600 [0.000, 88.000], mean observation: 3.148 [-1.540, 10.365], loss: 0.998669, mae: 4.540831, mean_q: 4.895645
 20631/100000: episode: 2104, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.557, mean reward: 0.356 [0.332, 0.420], mean action: 72.400 [14.000, 80.000], mean observation: 3.142 [-1.506, 10.254], loss: 1.061626, mae: 4.541869, mean_q: 4.895610
 20641/100000: episode: 2105, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.795, mean reward: 0.379 [0.318, 0.529], mean action: 48.100 [0.000, 80.000], mean observation: 3.164 [-1.988, 10.291], loss: 1.001676, mae: 4.542562, mean_q: 4.897480
 20651/100000: episode: 2106, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.380, mean reward: 0.438 [0.402, 0.467], mean action: 70.700 [8.000, 98.000], mean observation: 3.160 [-1.462, 10.174], loss: 0.895835, mae: 4.542849, mean_q: 4.895642
 20661/100000: episode: 2107, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.248, mean reward: 0.425 [0.325, 0.512], mean action: 62.500 [35.000, 99.000], mean observation: 3.150 [-1.129, 10.297], loss: 1.059724, mae: 4.544271, mean_q: 4.895731
 20671/100000: episode: 2108, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.284, mean reward: 0.428 [0.312, 0.535], mean action: 55.100 [9.000, 99.000], mean observation: 3.159 [-1.806, 10.167], loss: 1.025297, mae: 4.545159, mean_q: 4.897168
 20681/100000: episode: 2109, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.131, mean reward: 0.413 [0.354, 0.490], mean action: 43.000 [34.000, 44.000], mean observation: 3.150 [-1.745, 10.256], loss: 1.188347, mae: 4.546052, mean_q: 4.898352
 20691/100000: episode: 2110, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.452, mean reward: 0.445 [0.415, 0.494], mean action: 42.100 [2.000, 92.000], mean observation: 3.149 [-1.508, 10.391], loss: 0.754905, mae: 4.544867, mean_q: 4.898999
 20701/100000: episode: 2111, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.837, mean reward: 0.384 [0.326, 0.461], mean action: 56.600 [5.000, 92.000], mean observation: 3.167 [-1.463, 10.348], loss: 1.165110, mae: 4.547475, mean_q: 4.900723
 20703/100000: episode: 2112, duration: 0.064s, episode steps: 2, steps per second: 31, episode reward: 10.393, mean reward: 5.197 [0.393, 10.000], mean action: 25.000 [6.000, 44.000], mean observation: 3.145 [-0.607, 10.181], loss: 1.152892, mae: 4.546959, mean_q: 4.902269
 20713/100000: episode: 2113, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.405, mean reward: 0.440 [0.437, 0.462], mean action: 40.500 [9.000, 49.000], mean observation: 3.145 [-1.794, 10.281], loss: 1.174273, mae: 4.547678, mean_q: 4.904539
 20723/100000: episode: 2114, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.854, mean reward: 0.385 [0.358, 0.482], mean action: 50.200 [32.000, 88.000], mean observation: 3.158 [-1.172, 10.276], loss: 1.133973, mae: 4.548336, mean_q: 4.908607
 20733/100000: episode: 2115, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.873, mean reward: 0.387 [0.315, 0.442], mean action: 49.000 [44.000, 77.000], mean observation: 3.147 [-1.110, 10.229], loss: 1.190591, mae: 4.549488, mean_q: 4.911644
 20743/100000: episode: 2116, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.628, mean reward: 0.363 [0.320, 0.434], mean action: 45.500 [11.000, 89.000], mean observation: 3.171 [-1.138, 10.266], loss: 1.405629, mae: 4.550835, mean_q: 4.914025
 20753/100000: episode: 2117, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.995, mean reward: 0.399 [0.341, 0.496], mean action: 46.700 [18.000, 99.000], mean observation: 3.165 [-1.312, 10.294], loss: 1.196110, mae: 4.550128, mean_q: 4.915631
 20763/100000: episode: 2118, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.086, mean reward: 0.409 [0.387, 0.495], mean action: 58.500 [44.000, 88.000], mean observation: 3.153 [-1.124, 10.331], loss: 0.848557, mae: 4.548963, mean_q: 4.918378
 20773/100000: episode: 2119, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.629, mean reward: 0.463 [0.462, 0.468], mean action: 47.600 [24.000, 88.000], mean observation: 3.143 [-1.089, 10.434], loss: 1.085768, mae: 4.551114, mean_q: 4.921426
 20783/100000: episode: 2120, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 13.366, mean reward: 1.337 [0.349, 10.000], mean action: 39.000 [9.000, 67.000], mean observation: 3.162 [-0.959, 10.353], loss: 1.040593, mae: 4.551671, mean_q: 4.922759
 20793/100000: episode: 2121, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.117, mean reward: 0.412 [0.383, 0.458], mean action: 47.200 [38.000, 61.000], mean observation: 3.157 [-1.186, 10.334], loss: 1.020391, mae: 4.551969, mean_q: 4.924392
 20803/100000: episode: 2122, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.901, mean reward: 0.390 [0.326, 0.451], mean action: 54.900 [0.000, 95.000], mean observation: 3.158 [-1.462, 10.346], loss: 0.975285, mae: 4.552782, mean_q: 4.928346
 20813/100000: episode: 2123, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 5.032, mean reward: 0.503 [0.432, 0.574], mean action: 46.400 [12.000, 75.000], mean observation: 3.156 [-1.400, 10.259], loss: 1.440096, mae: 4.555242, mean_q: 4.932471
 20823/100000: episode: 2124, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.965, mean reward: 0.396 [0.347, 0.515], mean action: 47.500 [29.000, 89.000], mean observation: 3.151 [-1.501, 10.271], loss: 1.195602, mae: 4.554698, mean_q: 4.935519
 20833/100000: episode: 2125, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.721, mean reward: 0.372 [0.288, 0.438], mean action: 48.000 [29.000, 80.000], mean observation: 3.150 [-1.035, 10.265], loss: 0.934790, mae: 4.554250, mean_q: 4.938081
 20843/100000: episode: 2126, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.161, mean reward: 0.416 [0.323, 0.567], mean action: 47.400 [39.000, 80.000], mean observation: 3.158 [-1.792, 10.347], loss: 1.071323, mae: 4.555360, mean_q: 4.940710
 20853/100000: episode: 2127, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.690, mean reward: 0.469 [0.465, 0.506], mean action: 52.100 [43.000, 99.000], mean observation: 3.163 [-1.831, 10.336], loss: 0.920533, mae: 4.555406, mean_q: 4.942805
 20863/100000: episode: 2128, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.401, mean reward: 0.440 [0.414, 0.589], mean action: 53.000 [44.000, 97.000], mean observation: 3.166 [-0.903, 10.223], loss: 1.098050, mae: 4.556842, mean_q: 4.945898
 20873/100000: episode: 2129, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.423, mean reward: 0.442 [0.422, 0.550], mean action: 49.200 [37.000, 76.000], mean observation: 3.170 [-1.573, 10.307], loss: 1.416336, mae: 4.558718, mean_q: 4.943419
 20883/100000: episode: 2130, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.885, mean reward: 0.388 [0.339, 0.520], mean action: 45.100 [0.000, 99.000], mean observation: 3.150 [-1.216, 10.411], loss: 1.237265, mae: 4.558549, mean_q: 4.941549
 20893/100000: episode: 2131, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.993, mean reward: 0.399 [0.344, 0.466], mean action: 53.500 [18.000, 87.000], mean observation: 3.153 [-1.145, 10.320], loss: 1.273340, mae: 4.559033, mean_q: 4.943099
 20903/100000: episode: 2132, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.323, mean reward: 0.432 [0.374, 0.515], mean action: 40.700 [2.000, 65.000], mean observation: 3.173 [-1.798, 10.229], loss: 1.155530, mae: 4.559124, mean_q: 4.946098
 20913/100000: episode: 2133, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.999, mean reward: 0.400 [0.325, 0.465], mean action: 54.100 [13.000, 98.000], mean observation: 3.154 [-1.301, 10.365], loss: 1.199455, mae: 4.559623, mean_q: 4.950400
 20923/100000: episode: 2134, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.962, mean reward: 0.396 [0.377, 0.435], mean action: 32.700 [4.000, 44.000], mean observation: 3.165 [-1.693, 10.281], loss: 1.150487, mae: 4.560365, mean_q: 4.952815
 20933/100000: episode: 2135, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.935, mean reward: 0.394 [0.356, 0.480], mean action: 46.200 [6.000, 100.000], mean observation: 3.156 [-1.565, 10.336], loss: 0.968847, mae: 4.560087, mean_q: 4.954979
 20943/100000: episode: 2136, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.012, mean reward: 0.401 [0.371, 0.464], mean action: 57.000 [44.000, 98.000], mean observation: 3.156 [-1.209, 10.308], loss: 1.186722, mae: 4.561840, mean_q: 4.958424
 20944/100000: episode: 2137, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 44.000 [44.000, 44.000], mean observation: 3.135 [-1.171, 10.159], loss: 1.403431, mae: 4.562997, mean_q: 4.960834
 20954/100000: episode: 2138, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.868, mean reward: 0.387 [0.354, 0.427], mean action: 42.700 [3.000, 83.000], mean observation: 3.159 [-1.127, 10.253], loss: 1.006879, mae: 4.561981, mean_q: 4.963328
 20964/100000: episode: 2139, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.710, mean reward: 0.371 [0.317, 0.501], mean action: 48.200 [39.000, 69.000], mean observation: 3.155 [-1.474, 10.305], loss: 0.809211, mae: 4.562263, mean_q: 4.967939
 20974/100000: episode: 2140, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.378, mean reward: 0.438 [0.395, 0.568], mean action: 45.500 [2.000, 95.000], mean observation: 3.152 [-1.809, 10.242], loss: 1.172036, mae: 4.564621, mean_q: 4.972699
 20984/100000: episode: 2141, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.141, mean reward: 0.414 [0.409, 0.456], mean action: 52.500 [35.000, 93.000], mean observation: 3.159 [-1.649, 10.363], loss: 1.263726, mae: 4.565394, mean_q: 4.973092
 20994/100000: episode: 2142, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.560, mean reward: 0.456 [0.453, 0.481], mean action: 53.300 [17.000, 93.000], mean observation: 3.167 [-1.330, 10.279], loss: 1.058036, mae: 4.565282, mean_q: 4.972721
 21004/100000: episode: 2143, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.212, mean reward: 0.421 [0.397, 0.479], mean action: 56.600 [31.000, 92.000], mean observation: 3.152 [-2.473, 10.198], loss: 0.930288, mae: 4.565681, mean_q: 4.974814
 21014/100000: episode: 2144, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.173, mean reward: 0.417 [0.354, 0.498], mean action: 48.900 [44.000, 93.000], mean observation: 3.170 [-2.169, 10.302], loss: 1.086741, mae: 4.566949, mean_q: 4.977810
 21024/100000: episode: 2145, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.761, mean reward: 0.376 [0.311, 0.485], mean action: 54.800 [3.000, 95.000], mean observation: 3.147 [-1.549, 10.339], loss: 1.150446, mae: 4.568117, mean_q: 4.979900
 21034/100000: episode: 2146, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.482, mean reward: 0.448 [0.440, 0.469], mean action: 56.200 [30.000, 98.000], mean observation: 3.159 [-1.259, 10.213], loss: 1.411794, mae: 4.569772, mean_q: 4.976667
 21044/100000: episode: 2147, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.446, mean reward: 0.445 [0.362, 0.523], mean action: 43.200 [10.000, 85.000], mean observation: 3.163 [-1.346, 10.377], loss: 0.980218, mae: 4.568427, mean_q: 4.977359
 21054/100000: episode: 2148, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.596, mean reward: 0.360 [0.330, 0.434], mean action: 63.100 [38.000, 100.000], mean observation: 3.174 [-1.413, 10.257], loss: 1.007022, mae: 4.569123, mean_q: 4.980218
 21064/100000: episode: 2149, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.122, mean reward: 0.412 [0.395, 0.448], mean action: 42.000 [33.000, 57.000], mean observation: 3.159 [-1.004, 10.334], loss: 1.019361, mae: 4.570119, mean_q: 4.982782
 21074/100000: episode: 2150, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.298, mean reward: 0.430 [0.362, 0.465], mean action: 41.600 [5.000, 97.000], mean observation: 3.168 [-1.538, 10.331], loss: 1.298071, mae: 4.572315, mean_q: 4.987552
 21084/100000: episode: 2151, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.637, mean reward: 0.464 [0.389, 0.503], mean action: 27.700 [4.000, 60.000], mean observation: 3.163 [-1.992, 10.415], loss: 1.226915, mae: 4.572552, mean_q: 4.992522
 21094/100000: episode: 2152, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.835, mean reward: 0.384 [0.334, 0.410], mean action: 49.400 [0.000, 95.000], mean observation: 3.162 [-1.088, 10.205], loss: 1.001426, mae: 4.572143, mean_q: 4.995849
 21104/100000: episode: 2153, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.993, mean reward: 0.399 [0.353, 0.512], mean action: 36.400 [9.000, 76.000], mean observation: 3.152 [-1.743, 10.427], loss: 1.122815, mae: 4.573293, mean_q: 4.995801
 21114/100000: episode: 2154, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.526, mean reward: 0.453 [0.453, 0.453], mean action: 46.800 [24.000, 73.000], mean observation: 3.150 [-1.038, 10.413], loss: 1.253308, mae: 4.574517, mean_q: 4.993850
 21124/100000: episode: 2155, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.254, mean reward: 0.425 [0.374, 0.467], mean action: 47.100 [15.000, 95.000], mean observation: 3.153 [-1.861, 10.416], loss: 0.997786, mae: 4.574230, mean_q: 4.991331
 21134/100000: episode: 2156, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.453, mean reward: 0.445 [0.351, 0.519], mean action: 61.400 [20.000, 96.000], mean observation: 3.153 [-1.603, 10.459], loss: 1.001366, mae: 4.575177, mean_q: 4.993218
 21144/100000: episode: 2157, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.352, mean reward: 0.435 [0.380, 0.547], mean action: 51.900 [0.000, 100.000], mean observation: 3.160 [-1.307, 10.328], loss: 1.200797, mae: 4.576447, mean_q: 4.995734
 21154/100000: episode: 2158, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.365, mean reward: 0.437 [0.426, 0.476], mean action: 43.400 [3.000, 69.000], mean observation: 3.154 [-1.803, 10.316], loss: 1.146855, mae: 4.577259, mean_q: 4.993137
 21164/100000: episode: 2159, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.032, mean reward: 0.403 [0.278, 0.488], mean action: 47.200 [25.000, 82.000], mean observation: 3.158 [-1.578, 10.307], loss: 1.330569, mae: 4.578469, mean_q: 4.988661
 21174/100000: episode: 2160, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.807, mean reward: 0.381 [0.354, 0.439], mean action: 55.500 [22.000, 93.000], mean observation: 3.166 [-0.807, 10.301], loss: 1.267817, mae: 4.578510, mean_q: 4.984242
 21182/100000: episode: 2161, duration: 0.147s, episode steps: 8, steps per second: 54, episode reward: 12.667, mean reward: 1.583 [0.342, 10.000], mean action: 44.875 [26.000, 86.000], mean observation: 3.162 [-1.689, 10.357], loss: 1.160155, mae: 4.578593, mean_q: 4.982678
 21192/100000: episode: 2162, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.883, mean reward: 0.388 [0.289, 0.469], mean action: 49.400 [37.000, 78.000], mean observation: 3.158 [-1.731, 10.388], loss: 1.155704, mae: 4.579360, mean_q: 4.985001
 21202/100000: episode: 2163, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.542, mean reward: 0.454 [0.394, 0.479], mean action: 41.400 [8.000, 53.000], mean observation: 3.152 [-1.889, 10.276], loss: 1.239148, mae: 4.580229, mean_q: 4.988711
 21212/100000: episode: 2164, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.113, mean reward: 0.411 [0.305, 0.537], mean action: 66.300 [41.000, 98.000], mean observation: 3.161 [-1.529, 10.352], loss: 1.297977, mae: 4.581315, mean_q: 4.989743
 21222/100000: episode: 2165, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.296, mean reward: 0.430 [0.415, 0.502], mean action: 46.400 [18.000, 86.000], mean observation: 3.159 [-1.840, 10.345], loss: 1.084284, mae: 4.581394, mean_q: 4.988801
 21232/100000: episode: 2166, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.808, mean reward: 0.381 [0.338, 0.482], mean action: 44.400 [14.000, 76.000], mean observation: 3.170 [-0.752, 10.375], loss: 1.053180, mae: 4.581945, mean_q: 4.985013
 21242/100000: episode: 2167, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.692, mean reward: 0.369 [0.337, 0.396], mean action: 48.700 [31.000, 101.000], mean observation: 3.147 [-1.639, 10.254], loss: 1.083842, mae: 4.582955, mean_q: 4.985978
 21252/100000: episode: 2168, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.966, mean reward: 0.397 [0.340, 0.545], mean action: 49.200 [12.000, 97.000], mean observation: 3.157 [-1.292, 10.304], loss: 0.969174, mae: 4.583477, mean_q: 4.989192
 21262/100000: episode: 2169, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.032, mean reward: 0.403 [0.341, 0.474], mean action: 41.800 [3.000, 83.000], mean observation: 3.151 [-1.991, 10.292], loss: 0.974002, mae: 4.584414, mean_q: 4.990153
 21272/100000: episode: 2170, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.089, mean reward: 0.409 [0.381, 0.481], mean action: 57.700 [44.000, 82.000], mean observation: 3.155 [-1.255, 10.358], loss: 0.998970, mae: 4.585755, mean_q: 4.989375
 21282/100000: episode: 2171, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.865, mean reward: 0.386 [0.363, 0.411], mean action: 54.900 [22.000, 94.000], mean observation: 3.149 [-1.773, 10.231], loss: 1.466745, mae: 4.588836, mean_q: 4.992290
 21292/100000: episode: 2172, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.187, mean reward: 0.419 [0.349, 0.552], mean action: 53.300 [0.000, 94.000], mean observation: 3.156 [-1.542, 10.310], loss: 1.130465, mae: 4.588109, mean_q: 4.995536
 21302/100000: episode: 2173, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.704, mean reward: 0.370 [0.327, 0.435], mean action: 57.600 [4.000, 94.000], mean observation: 3.162 [-1.400, 10.228], loss: 1.011580, mae: 4.589138, mean_q: 4.997851
 21312/100000: episode: 2174, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.995, mean reward: 0.399 [0.377, 0.506], mean action: 49.400 [24.000, 95.000], mean observation: 3.159 [-1.292, 10.269], loss: 1.459625, mae: 4.591146, mean_q: 4.997321
 21322/100000: episode: 2175, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.900, mean reward: 0.390 [0.362, 0.439], mean action: 52.600 [42.000, 100.000], mean observation: 3.170 [-1.787, 10.430], loss: 1.235515, mae: 4.590871, mean_q: 4.994981
 21332/100000: episode: 2176, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.537, mean reward: 0.354 [0.314, 0.436], mean action: 48.900 [4.000, 91.000], mean observation: 3.166 [-1.007, 10.293], loss: 1.296661, mae: 4.591644, mean_q: 4.996856
 21342/100000: episode: 2177, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.982, mean reward: 0.398 [0.309, 0.537], mean action: 43.800 [11.000, 77.000], mean observation: 3.154 [-1.433, 10.454], loss: 1.017656, mae: 4.591457, mean_q: 5.000369
 21352/100000: episode: 2178, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.870, mean reward: 0.387 [0.302, 0.568], mean action: 39.100 [2.000, 83.000], mean observation: 3.151 [-1.153, 10.294], loss: 1.392037, mae: 4.593768, mean_q: 5.004872
 21362/100000: episode: 2179, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.950, mean reward: 0.395 [0.293, 0.470], mean action: 54.100 [10.000, 93.000], mean observation: 3.154 [-1.480, 10.260], loss: 1.375208, mae: 4.594285, mean_q: 5.007069
 21372/100000: episode: 2180, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.156, mean reward: 0.416 [0.317, 0.479], mean action: 36.000 [7.000, 65.000], mean observation: 3.170 [-1.337, 10.325], loss: 1.002011, mae: 4.593399, mean_q: 4.999017
 21382/100000: episode: 2181, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.284, mean reward: 0.428 [0.425, 0.462], mean action: 47.300 [35.000, 68.000], mean observation: 3.147 [-1.396, 10.333], loss: 1.211778, mae: 4.594609, mean_q: 4.990808
 21392/100000: episode: 2182, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.248, mean reward: 0.425 [0.394, 0.461], mean action: 50.300 [0.000, 98.000], mean observation: 3.165 [-0.995, 10.226], loss: 1.168122, mae: 4.594714, mean_q: 4.985199
 21402/100000: episode: 2183, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.952, mean reward: 0.395 [0.395, 0.395], mean action: 50.700 [43.000, 81.000], mean observation: 3.139 [-2.062, 10.322], loss: 0.908611, mae: 4.594940, mean_q: 4.983914
 21412/100000: episode: 2184, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.103, mean reward: 0.410 [0.393, 0.448], mean action: 57.000 [44.000, 89.000], mean observation: 3.169 [-0.799, 10.455], loss: 1.078270, mae: 4.596036, mean_q: 4.984170
 21422/100000: episode: 2185, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.463, mean reward: 0.446 [0.328, 0.489], mean action: 41.700 [6.000, 70.000], mean observation: 3.153 [-2.080, 10.321], loss: 1.065169, mae: 4.596415, mean_q: 4.983628
 21432/100000: episode: 2186, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.900, mean reward: 0.390 [0.332, 0.468], mean action: 44.200 [6.000, 76.000], mean observation: 3.150 [-0.826, 10.320], loss: 1.090384, mae: 4.597496, mean_q: 4.985087
 21442/100000: episode: 2187, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 3.850, mean reward: 0.385 [0.368, 0.420], mean action: 53.400 [10.000, 100.000], mean observation: 3.164 [-1.399, 10.268], loss: 1.194093, mae: 4.598309, mean_q: 4.986039
 21452/100000: episode: 2188, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.475, mean reward: 0.447 [0.446, 0.461], mean action: 49.600 [14.000, 90.000], mean observation: 3.176 [-1.392, 10.215], loss: 1.200278, mae: 4.599126, mean_q: 4.987948
 21462/100000: episode: 2189, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.967, mean reward: 0.397 [0.339, 0.514], mean action: 43.600 [6.000, 91.000], mean observation: 3.145 [-1.524, 10.326], loss: 1.064672, mae: 4.599651, mean_q: 4.986797
 21472/100000: episode: 2190, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.422, mean reward: 0.442 [0.421, 0.486], mean action: 39.300 [16.000, 72.000], mean observation: 3.145 [-0.782, 10.421], loss: 1.231576, mae: 4.601061, mean_q: 4.983298
 21482/100000: episode: 2191, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.724, mean reward: 0.372 [0.282, 0.431], mean action: 69.000 [41.000, 93.000], mean observation: 3.164 [-1.175, 10.341], loss: 1.375574, mae: 4.602640, mean_q: 4.982556
 21492/100000: episode: 2192, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.967, mean reward: 0.497 [0.492, 0.541], mean action: 37.600 [17.000, 53.000], mean observation: 3.146 [-1.076, 10.388], loss: 1.218614, mae: 4.601991, mean_q: 4.983300
 21502/100000: episode: 2193, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.258, mean reward: 0.426 [0.426, 0.426], mean action: 46.200 [35.000, 74.000], mean observation: 3.156 [-1.343, 10.333], loss: 1.360132, mae: 4.602750, mean_q: 4.983140
 21512/100000: episode: 2194, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.793, mean reward: 0.379 [0.365, 0.433], mean action: 53.200 [20.000, 99.000], mean observation: 3.162 [-1.036, 10.336], loss: 1.120967, mae: 4.602518, mean_q: 4.986251
 21522/100000: episode: 2195, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.241, mean reward: 0.424 [0.338, 0.473], mean action: 36.200 [5.000, 71.000], mean observation: 3.149 [-0.960, 10.424], loss: 1.108195, mae: 4.603174, mean_q: 4.987239
 21532/100000: episode: 2196, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.237, mean reward: 0.424 [0.424, 0.424], mean action: 52.100 [44.000, 88.000], mean observation: 3.158 [-1.476, 10.296], loss: 1.372490, mae: 4.605103, mean_q: 4.988913
 21542/100000: episode: 2197, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.942, mean reward: 0.394 [0.341, 0.458], mean action: 45.000 [3.000, 94.000], mean observation: 3.155 [-1.455, 10.325], loss: 1.039878, mae: 4.604468, mean_q: 4.994706
 21552/100000: episode: 2198, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.021, mean reward: 0.402 [0.369, 0.512], mean action: 43.500 [2.000, 81.000], mean observation: 3.158 [-1.143, 10.295], loss: 1.271925, mae: 4.605715, mean_q: 4.999880
 21562/100000: episode: 2199, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.188, mean reward: 0.419 [0.321, 0.515], mean action: 34.700 [1.000, 77.000], mean observation: 3.155 [-1.431, 10.284], loss: 1.102937, mae: 4.605548, mean_q: 4.995255
 21572/100000: episode: 2200, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.822, mean reward: 0.382 [0.329, 0.427], mean action: 52.400 [8.000, 100.000], mean observation: 3.170 [-1.141, 10.425], loss: 1.106697, mae: 4.606025, mean_q: 4.989947
 21582/100000: episode: 2201, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.470, mean reward: 0.447 [0.447, 0.447], mean action: 54.700 [15.000, 100.000], mean observation: 3.154 [-1.562, 10.390], loss: 0.965829, mae: 4.606058, mean_q: 4.989654
 21592/100000: episode: 2202, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.790, mean reward: 0.379 [0.329, 0.444], mean action: 43.100 [5.000, 90.000], mean observation: 3.173 [-1.080, 10.331], loss: 1.130668, mae: 4.607835, mean_q: 4.987864
 21602/100000: episode: 2203, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.207, mean reward: 0.421 [0.405, 0.504], mean action: 57.500 [25.000, 91.000], mean observation: 3.165 [-1.445, 10.316], loss: 1.183051, mae: 4.608821, mean_q: 4.986691
 21612/100000: episode: 2204, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.217, mean reward: 0.422 [0.369, 0.505], mean action: 46.300 [9.000, 101.000], mean observation: 3.152 [-1.313, 10.150], loss: 1.223643, mae: 4.609982, mean_q: 4.986882
 21622/100000: episode: 2205, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.784, mean reward: 0.378 [0.305, 0.470], mean action: 43.300 [5.000, 75.000], mean observation: 3.149 [-1.426, 10.377], loss: 0.718934, mae: 4.607921, mean_q: 4.988429
 21632/100000: episode: 2206, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.719, mean reward: 0.472 [0.372, 0.507], mean action: 45.100 [12.000, 96.000], mean observation: 3.149 [-1.304, 10.329], loss: 1.208133, mae: 4.611129, mean_q: 4.990974
 21642/100000: episode: 2207, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.773, mean reward: 0.377 [0.363, 0.426], mean action: 41.700 [7.000, 81.000], mean observation: 3.162 [-1.451, 10.329], loss: 1.076564, mae: 4.611262, mean_q: 4.987710
 21652/100000: episode: 2208, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.495, mean reward: 0.450 [0.342, 0.527], mean action: 41.800 [2.000, 63.000], mean observation: 3.155 [-0.953, 10.331], loss: 1.050086, mae: 4.611446, mean_q: 4.982165
 21662/100000: episode: 2209, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.148, mean reward: 0.415 [0.338, 0.534], mean action: 38.800 [2.000, 72.000], mean observation: 3.161 [-1.329, 10.423], loss: 1.157106, mae: 4.612779, mean_q: 4.979303
 21672/100000: episode: 2210, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.334, mean reward: 0.433 [0.375, 0.528], mean action: 40.500 [8.000, 84.000], mean observation: 3.174 [-1.259, 10.252], loss: 1.133086, mae: 4.613247, mean_q: 4.979100
 21682/100000: episode: 2211, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.193, mean reward: 0.419 [0.371, 0.534], mean action: 48.600 [15.000, 82.000], mean observation: 3.156 [-1.887, 10.307], loss: 1.319895, mae: 4.614951, mean_q: 4.983571
 21692/100000: episode: 2212, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.317, mean reward: 0.432 [0.421, 0.448], mean action: 36.500 [2.000, 66.000], mean observation: 3.159 [-1.889, 10.344], loss: 1.247625, mae: 4.615052, mean_q: 4.987259
 21702/100000: episode: 2213, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.747, mean reward: 0.475 [0.345, 0.511], mean action: 48.400 [6.000, 99.000], mean observation: 3.156 [-0.808, 10.327], loss: 1.241460, mae: 4.615730, mean_q: 4.992204
 21712/100000: episode: 2214, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.025, mean reward: 0.402 [0.351, 0.522], mean action: 44.600 [20.000, 60.000], mean observation: 3.155 [-1.196, 10.386], loss: 1.432343, mae: 4.616882, mean_q: 4.984656
 21722/100000: episode: 2215, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.543, mean reward: 0.454 [0.434, 0.500], mean action: 53.600 [8.000, 100.000], mean observation: 3.159 [-0.917, 10.400], loss: 1.664771, mae: 4.618223, mean_q: 4.974357
 21732/100000: episode: 2216, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.609, mean reward: 0.461 [0.394, 0.563], mean action: 45.000 [1.000, 94.000], mean observation: 3.161 [-1.235, 10.318], loss: 0.768586, mae: 4.614919, mean_q: 4.965078
 21742/100000: episode: 2217, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.817, mean reward: 0.382 [0.340, 0.433], mean action: 63.200 [26.000, 88.000], mean observation: 3.150 [-1.080, 10.286], loss: 1.354719, mae: 4.617887, mean_q: 4.963133
 21752/100000: episode: 2218, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.773, mean reward: 0.377 [0.308, 0.443], mean action: 45.300 [9.000, 92.000], mean observation: 3.157 [-1.644, 10.353], loss: 1.311275, mae: 4.618315, mean_q: 4.962447
 21762/100000: episode: 2219, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.203, mean reward: 0.420 [0.349, 0.531], mean action: 44.900 [6.000, 81.000], mean observation: 3.151 [-1.343, 10.345], loss: 1.519287, mae: 4.619540, mean_q: 4.961078
 21772/100000: episode: 2220, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.793, mean reward: 0.379 [0.331, 0.482], mean action: 50.500 [3.000, 98.000], mean observation: 3.166 [-1.303, 10.357], loss: 0.865272, mae: 4.617220, mean_q: 4.958092
 21782/100000: episode: 2221, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.396, mean reward: 0.440 [0.352, 0.570], mean action: 39.600 [4.000, 93.000], mean observation: 3.155 [-2.004, 10.244], loss: 1.010431, mae: 4.618427, mean_q: 4.957325
 21792/100000: episode: 2222, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.451, mean reward: 0.445 [0.382, 0.521], mean action: 45.100 [0.000, 96.000], mean observation: 3.181 [-1.291, 10.342], loss: 1.378979, mae: 4.620242, mean_q: 4.956532
 21793/100000: episode: 2223, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 44.000 [44.000, 44.000], mean observation: 3.144 [-0.820, 10.142], loss: 1.093325, mae: 4.620206, mean_q: 4.957476
 21803/100000: episode: 2224, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.346, mean reward: 0.435 [0.358, 0.475], mean action: 47.600 [0.000, 90.000], mean observation: 3.158 [-1.550, 10.316], loss: 1.292100, mae: 4.620184, mean_q: 4.958463
 21813/100000: episode: 2225, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.770, mean reward: 0.377 [0.309, 0.476], mean action: 43.500 [5.000, 78.000], mean observation: 3.155 [-1.170, 10.320], loss: 0.961700, mae: 4.619592, mean_q: 4.959043
 21823/100000: episode: 2226, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.808, mean reward: 0.381 [0.325, 0.517], mean action: 41.400 [9.000, 99.000], mean observation: 3.145 [-1.853, 10.322], loss: 1.104897, mae: 4.620371, mean_q: 4.960366
 21833/100000: episode: 2227, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.840, mean reward: 0.384 [0.368, 0.442], mean action: 48.300 [11.000, 99.000], mean observation: 3.155 [-0.912, 10.356], loss: 1.196856, mae: 4.621498, mean_q: 4.955818
 21843/100000: episode: 2228, duration: 0.230s, episode steps: 10, steps per second: 43, episode reward: 3.997, mean reward: 0.400 [0.292, 0.515], mean action: 25.000 [10.000, 73.000], mean observation: 3.158 [-1.221, 10.360], loss: 0.953960, mae: 4.620738, mean_q: 4.953833
 21853/100000: episode: 2229, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.037, mean reward: 0.404 [0.329, 0.461], mean action: 26.700 [10.000, 87.000], mean observation: 3.156 [-1.926, 10.399], loss: 1.145919, mae: 4.622513, mean_q: 4.956220
 21863/100000: episode: 2230, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.844, mean reward: 0.384 [0.323, 0.547], mean action: 32.200 [11.000, 85.000], mean observation: 3.148 [-2.062, 10.251], loss: 0.926917, mae: 4.621939, mean_q: 4.958394
 21873/100000: episode: 2231, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.659, mean reward: 0.366 [0.288, 0.410], mean action: 42.300 [11.000, 101.000], mean observation: 3.168 [-1.158, 10.321], loss: 1.142940, mae: 4.623106, mean_q: 4.959859
 21883/100000: episode: 2232, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.412, mean reward: 0.441 [0.441, 0.442], mean action: 45.700 [11.000, 95.000], mean observation: 3.158 [-1.556, 10.291], loss: 1.127098, mae: 4.623840, mean_q: 4.957526
 21893/100000: episode: 2233, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.948, mean reward: 0.495 [0.432, 0.609], mean action: 29.800 [11.000, 72.000], mean observation: 3.161 [-1.255, 10.586], loss: 1.148247, mae: 4.624658, mean_q: 4.956734
 21903/100000: episode: 2234, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.760, mean reward: 0.376 [0.332, 0.447], mean action: 22.600 [6.000, 91.000], mean observation: 3.150 [-1.224, 10.400], loss: 0.883114, mae: 4.624219, mean_q: 4.957339
 21913/100000: episode: 2235, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 3.743, mean reward: 0.374 [0.324, 0.434], mean action: 32.700 [11.000, 100.000], mean observation: 3.159 [-1.825, 10.253], loss: 1.192866, mae: 4.625833, mean_q: 4.956784
 21914/100000: episode: 2236, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 91.000 [91.000, 91.000], mean observation: 3.183 [-0.253, 10.409], loss: 0.732730, mae: 4.624031, mean_q: 4.954445
 21924/100000: episode: 2237, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.010, mean reward: 0.401 [0.345, 0.466], mean action: 31.400 [10.000, 77.000], mean observation: 3.153 [-0.932, 10.394], loss: 1.195949, mae: 4.626626, mean_q: 4.953980
 21934/100000: episode: 2238, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 3.979, mean reward: 0.398 [0.342, 0.549], mean action: 27.300 [10.000, 100.000], mean observation: 3.147 [-1.433, 10.487], loss: 0.944742, mae: 4.626080, mean_q: 4.953565
 21944/100000: episode: 2239, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.097, mean reward: 0.410 [0.341, 0.498], mean action: 11.300 [10.000, 15.000], mean observation: 3.152 [-1.238, 10.385], loss: 1.219492, mae: 4.627927, mean_q: 4.952593
 21954/100000: episode: 2240, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.824, mean reward: 0.382 [0.326, 0.480], mean action: 31.200 [10.000, 101.000], mean observation: 3.155 [-1.023, 10.291], loss: 0.948942, mae: 4.627461, mean_q: 4.952747
 21964/100000: episode: 2241, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.443, mean reward: 0.444 [0.366, 0.525], mean action: 26.900 [10.000, 88.000], mean observation: 3.158 [-1.259, 10.369], loss: 1.071935, mae: 4.628402, mean_q: 4.951195
 21974/100000: episode: 2242, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 4.396, mean reward: 0.440 [0.306, 0.523], mean action: 22.100 [6.000, 86.000], mean observation: 3.153 [-1.265, 10.335], loss: 1.177914, mae: 4.629552, mean_q: 4.948491
 21984/100000: episode: 2243, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.959, mean reward: 0.396 [0.347, 0.435], mean action: 28.700 [9.000, 78.000], mean observation: 3.158 [-1.460, 10.470], loss: 1.146045, mae: 4.629906, mean_q: 4.946839
 21994/100000: episode: 2244, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.930, mean reward: 0.393 [0.336, 0.510], mean action: 31.600 [10.000, 78.000], mean observation: 3.153 [-1.873, 10.305], loss: 1.049850, mae: 4.629920, mean_q: 4.948472
 22004/100000: episode: 2245, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.933, mean reward: 0.393 [0.333, 0.430], mean action: 35.200 [4.000, 97.000], mean observation: 3.148 [-1.964, 10.303], loss: 1.213169, mae: 4.631277, mean_q: 4.949233
 22014/100000: episode: 2246, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.782, mean reward: 0.378 [0.354, 0.441], mean action: 28.200 [5.000, 82.000], mean observation: 3.154 [-2.122, 10.288], loss: 1.176480, mae: 4.631339, mean_q: 4.951368
 22024/100000: episode: 2247, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.949, mean reward: 0.395 [0.325, 0.557], mean action: 37.500 [10.000, 98.000], mean observation: 3.149 [-1.277, 10.634], loss: 1.025476, mae: 4.630973, mean_q: 4.953888
 22034/100000: episode: 2248, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.669, mean reward: 0.367 [0.322, 0.431], mean action: 56.800 [10.000, 101.000], mean observation: 3.152 [-1.211, 10.382], loss: 1.138265, mae: 4.632003, mean_q: 4.956097
 22044/100000: episode: 2249, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 3.935, mean reward: 0.393 [0.349, 0.447], mean action: 30.200 [10.000, 90.000], mean observation: 3.160 [-1.088, 10.277], loss: 0.648708, mae: 4.630265, mean_q: 4.955988
 22054/100000: episode: 2250, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.522, mean reward: 0.452 [0.442, 0.485], mean action: 25.900 [6.000, 100.000], mean observation: 3.158 [-1.680, 10.301], loss: 0.799171, mae: 4.631654, mean_q: 4.957303
 22064/100000: episode: 2251, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.988, mean reward: 0.399 [0.338, 0.486], mean action: 21.600 [10.000, 93.000], mean observation: 3.158 [-0.979, 10.353], loss: 1.368906, mae: 4.635079, mean_q: 4.957010
 22074/100000: episode: 2252, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.783, mean reward: 0.378 [0.304, 0.508], mean action: 43.500 [1.000, 95.000], mean observation: 3.160 [-1.509, 10.222], loss: 1.584071, mae: 4.636724, mean_q: 4.951554
 22084/100000: episode: 2253, duration: 0.246s, episode steps: 10, steps per second: 41, episode reward: 3.950, mean reward: 0.395 [0.309, 0.585], mean action: 25.000 [10.000, 68.000], mean observation: 3.161 [-1.401, 10.195], loss: 1.149359, mae: 4.635386, mean_q: 4.947635
 22094/100000: episode: 2254, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.400, mean reward: 0.440 [0.307, 0.516], mean action: 31.600 [10.000, 76.000], mean observation: 3.159 [-1.038, 10.303], loss: 1.097213, mae: 4.635427, mean_q: 4.947781
 22104/100000: episode: 2255, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.101, mean reward: 0.410 [0.323, 0.538], mean action: 32.400 [10.000, 91.000], mean observation: 3.146 [-1.710, 10.336], loss: 1.347106, mae: 4.637031, mean_q: 4.949452
 22114/100000: episode: 2256, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.245, mean reward: 0.424 [0.367, 0.526], mean action: 46.700 [10.000, 97.000], mean observation: 3.172 [-1.662, 10.412], loss: 1.241094, mae: 4.636090, mean_q: 4.951099
 22124/100000: episode: 2257, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.999, mean reward: 0.400 [0.296, 0.444], mean action: 55.600 [10.000, 92.000], mean observation: 3.153 [-1.844, 10.505], loss: 1.018571, mae: 4.635788, mean_q: 4.948029
 22134/100000: episode: 2258, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.524, mean reward: 0.452 [0.400, 0.502], mean action: 31.500 [10.000, 77.000], mean observation: 3.153 [-1.522, 10.389], loss: 1.223107, mae: 4.636691, mean_q: 4.947944
 22144/100000: episode: 2259, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.059, mean reward: 0.406 [0.344, 0.499], mean action: 32.500 [11.000, 85.000], mean observation: 3.161 [-1.729, 10.256], loss: 1.089619, mae: 4.636430, mean_q: 4.949785
 22154/100000: episode: 2260, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.397, mean reward: 0.440 [0.351, 0.556], mean action: 26.800 [0.000, 101.000], mean observation: 3.159 [-1.293, 10.519], loss: 1.347138, mae: 4.637837, mean_q: 4.951657
 22164/100000: episode: 2261, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.981, mean reward: 0.398 [0.318, 0.492], mean action: 15.700 [11.000, 43.000], mean observation: 3.150 [-1.311, 10.327], loss: 1.117127, mae: 4.637475, mean_q: 4.951686
 22174/100000: episode: 2262, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.333, mean reward: 0.433 [0.387, 0.473], mean action: 35.800 [11.000, 92.000], mean observation: 3.153 [-1.082, 10.325], loss: 1.471302, mae: 4.639457, mean_q: 4.949960
 22184/100000: episode: 2263, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.320, mean reward: 0.432 [0.344, 0.511], mean action: 31.000 [9.000, 91.000], mean observation: 3.152 [-1.081, 10.298], loss: 1.195503, mae: 4.638211, mean_q: 4.950556
 22194/100000: episode: 2264, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.901, mean reward: 0.390 [0.304, 0.462], mean action: 35.900 [11.000, 101.000], mean observation: 3.163 [-1.803, 10.357], loss: 1.242389, mae: 4.638523, mean_q: 4.951683
 22204/100000: episode: 2265, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.391, mean reward: 0.439 [0.355, 0.523], mean action: 31.100 [9.000, 93.000], mean observation: 3.159 [-0.950, 10.273], loss: 1.126399, mae: 4.638213, mean_q: 4.953312
 22214/100000: episode: 2266, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.118, mean reward: 0.412 [0.352, 0.508], mean action: 34.600 [5.000, 93.000], mean observation: 3.157 [-1.462, 10.563], loss: 0.945664, mae: 4.637869, mean_q: 4.955802
 22224/100000: episode: 2267, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.904, mean reward: 0.390 [0.292, 0.461], mean action: 38.600 [11.000, 91.000], mean observation: 3.159 [-1.014, 10.392], loss: 1.122963, mae: 4.638498, mean_q: 4.957180
 22234/100000: episode: 2268, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 4.071, mean reward: 0.407 [0.342, 0.483], mean action: 19.100 [11.000, 54.000], mean observation: 3.158 [-1.503, 10.447], loss: 1.014992, mae: 4.639128, mean_q: 4.956180
 22244/100000: episode: 2269, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.463, mean reward: 0.446 [0.310, 0.565], mean action: 26.200 [2.000, 101.000], mean observation: 3.151 [-1.466, 10.267], loss: 0.841379, mae: 4.638898, mean_q: 4.957490
 22254/100000: episode: 2270, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.031, mean reward: 0.403 [0.346, 0.438], mean action: 52.100 [1.000, 97.000], mean observation: 3.152 [-1.123, 10.279], loss: 1.135688, mae: 4.640879, mean_q: 4.959151
 22264/100000: episode: 2271, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.044, mean reward: 0.404 [0.344, 0.521], mean action: 30.600 [11.000, 92.000], mean observation: 3.149 [-1.721, 10.279], loss: 1.307196, mae: 4.641548, mean_q: 4.960310
 22274/100000: episode: 2272, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.265, mean reward: 0.426 [0.342, 0.541], mean action: 32.000 [11.000, 101.000], mean observation: 3.150 [-0.904, 10.264], loss: 0.983835, mae: 4.640291, mean_q: 4.961753
 22284/100000: episode: 2273, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.884, mean reward: 0.388 [0.338, 0.426], mean action: 46.900 [11.000, 96.000], mean observation: 3.151 [-2.055, 10.445], loss: 1.365382, mae: 4.642657, mean_q: 4.963791
 22294/100000: episode: 2274, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.372, mean reward: 0.437 [0.386, 0.501], mean action: 24.400 [0.000, 57.000], mean observation: 3.165 [-1.499, 10.154], loss: 1.131631, mae: 4.641848, mean_q: 4.965760
 22304/100000: episode: 2275, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.305, mean reward: 0.430 [0.378, 0.512], mean action: 32.800 [11.000, 87.000], mean observation: 3.157 [-1.423, 10.242], loss: 1.453977, mae: 4.643641, mean_q: 4.967322
 22305/100000: episode: 2276, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 11.000 [11.000, 11.000], mean observation: 3.141 [-1.832, 10.100], loss: 1.045257, mae: 4.641303, mean_q: 4.966467
 22315/100000: episode: 2277, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.158, mean reward: 0.416 [0.337, 0.515], mean action: 21.300 [2.000, 80.000], mean observation: 3.159 [-1.477, 10.374], loss: 0.936432, mae: 4.641535, mean_q: 4.965931
 22325/100000: episode: 2278, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.523, mean reward: 0.452 [0.428, 0.528], mean action: 32.000 [11.000, 97.000], mean observation: 3.156 [-1.885, 10.495], loss: 1.048287, mae: 4.642288, mean_q: 4.966917
 22335/100000: episode: 2279, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.914, mean reward: 0.391 [0.330, 0.461], mean action: 22.900 [11.000, 59.000], mean observation: 3.159 [-0.990, 10.419], loss: 1.161596, mae: 4.643419, mean_q: 4.969235
 22345/100000: episode: 2280, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.311, mean reward: 0.431 [0.274, 0.511], mean action: 30.900 [11.000, 100.000], mean observation: 3.154 [-1.262, 10.320], loss: 0.980719, mae: 4.643188, mean_q: 4.968170
 22355/100000: episode: 2281, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.468, mean reward: 0.347 [0.265, 0.416], mean action: 50.200 [11.000, 99.000], mean observation: 3.159 [-1.169, 10.265], loss: 0.811665, mae: 4.643134, mean_q: 4.966050
 22365/100000: episode: 2282, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.991, mean reward: 0.399 [0.287, 0.451], mean action: 19.200 [6.000, 56.000], mean observation: 3.145 [-1.488, 10.495], loss: 1.076933, mae: 4.645319, mean_q: 4.965549
 22375/100000: episode: 2283, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.369, mean reward: 0.437 [0.331, 0.502], mean action: 23.800 [11.000, 67.000], mean observation: 3.157 [-1.332, 10.357], loss: 0.746856, mae: 4.644464, mean_q: 4.967363
 22385/100000: episode: 2284, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.106, mean reward: 0.411 [0.342, 0.482], mean action: 14.500 [11.000, 35.000], mean observation: 3.161 [-1.899, 10.315], loss: 1.502653, mae: 4.648297, mean_q: 4.969897
 22395/100000: episode: 2285, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.253, mean reward: 0.425 [0.346, 0.546], mean action: 33.100 [7.000, 98.000], mean observation: 3.159 [-1.856, 10.212], loss: 1.620706, mae: 4.648788, mean_q: 4.972533
 22405/100000: episode: 2286, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.078, mean reward: 0.408 [0.394, 0.434], mean action: 39.800 [11.000, 101.000], mean observation: 3.168 [-1.677, 10.286], loss: 1.094276, mae: 4.646865, mean_q: 4.975676
 22415/100000: episode: 2287, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.655, mean reward: 0.466 [0.411, 0.596], mean action: 42.000 [2.000, 99.000], mean observation: 3.148 [-1.199, 10.546], loss: 1.264913, mae: 4.648170, mean_q: 4.979293
 22425/100000: episode: 2288, duration: 0.217s, episode steps: 10, steps per second: 46, episode reward: 3.969, mean reward: 0.397 [0.330, 0.466], mean action: 16.300 [0.000, 75.000], mean observation: 3.157 [-1.126, 10.387], loss: 1.234295, mae: 4.648443, mean_q: 4.983022
 22435/100000: episode: 2289, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.916, mean reward: 0.392 [0.321, 0.477], mean action: 24.400 [11.000, 51.000], mean observation: 3.153 [-1.234, 10.268], loss: 0.845058, mae: 4.647279, mean_q: 4.986592
 22445/100000: episode: 2290, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.339, mean reward: 0.434 [0.382, 0.472], mean action: 33.000 [11.000, 90.000], mean observation: 3.158 [-1.279, 10.242], loss: 1.312741, mae: 4.650098, mean_q: 4.988856
 22455/100000: episode: 2291, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 4.157, mean reward: 0.416 [0.319, 0.502], mean action: 24.400 [6.000, 94.000], mean observation: 3.160 [-1.540, 10.360], loss: 1.207352, mae: 4.650183, mean_q: 4.987695
 22465/100000: episode: 2292, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.084, mean reward: 0.408 [0.314, 0.497], mean action: 40.600 [11.000, 87.000], mean observation: 3.148 [-1.632, 10.260], loss: 1.043560, mae: 4.649821, mean_q: 4.989270
 22475/100000: episode: 2293, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 3.963, mean reward: 0.396 [0.318, 0.502], mean action: 27.300 [9.000, 94.000], mean observation: 3.154 [-1.069, 10.265], loss: 1.299953, mae: 4.651496, mean_q: 4.992095
 22485/100000: episode: 2294, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.045, mean reward: 0.404 [0.359, 0.516], mean action: 29.400 [10.000, 68.000], mean observation: 3.144 [-2.312, 10.250], loss: 1.219952, mae: 4.651451, mean_q: 4.990893
 22495/100000: episode: 2295, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.533, mean reward: 0.453 [0.338, 0.500], mean action: 53.300 [11.000, 101.000], mean observation: 3.160 [-1.297, 10.405], loss: 1.033897, mae: 4.650892, mean_q: 4.990039
 22505/100000: episode: 2296, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.913, mean reward: 0.391 [0.358, 0.504], mean action: 35.200 [4.000, 91.000], mean observation: 3.159 [-1.550, 10.448], loss: 0.998589, mae: 4.650910, mean_q: 4.987697
 22515/100000: episode: 2297, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 3.952, mean reward: 0.395 [0.338, 0.476], mean action: 19.400 [1.000, 53.000], mean observation: 3.157 [-1.313, 10.326], loss: 1.111521, mae: 4.652118, mean_q: 4.987004
 22525/100000: episode: 2298, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.429, mean reward: 0.443 [0.357, 0.516], mean action: 36.900 [11.000, 88.000], mean observation: 3.159 [-1.106, 10.571], loss: 0.986455, mae: 4.652205, mean_q: 4.989461
 22535/100000: episode: 2299, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 3.827, mean reward: 0.383 [0.265, 0.573], mean action: 21.400 [11.000, 93.000], mean observation: 3.160 [-1.845, 10.177], loss: 1.371449, mae: 4.653602, mean_q: 4.988607
 22545/100000: episode: 2300, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.626, mean reward: 0.363 [0.295, 0.443], mean action: 22.400 [6.000, 74.000], mean observation: 3.143 [-1.543, 10.196], loss: 0.926216, mae: 4.652278, mean_q: 4.988332
 22555/100000: episode: 2301, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.288, mean reward: 0.429 [0.366, 0.553], mean action: 32.900 [6.000, 101.000], mean observation: 3.160 [-2.096, 10.381], loss: 1.027559, mae: 4.652930, mean_q: 4.991156
 22565/100000: episode: 2302, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.296, mean reward: 0.430 [0.311, 0.547], mean action: 28.600 [2.000, 72.000], mean observation: 3.148 [-1.472, 10.307], loss: 1.339285, mae: 4.654762, mean_q: 4.995089
 22575/100000: episode: 2303, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.986, mean reward: 0.399 [0.322, 0.445], mean action: 20.500 [7.000, 84.000], mean observation: 3.163 [-1.381, 10.272], loss: 1.208684, mae: 4.654440, mean_q: 4.997851
 22585/100000: episode: 2304, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.816, mean reward: 0.382 [0.287, 0.470], mean action: 35.900 [11.000, 83.000], mean observation: 3.159 [-2.817, 10.457], loss: 1.396265, mae: 4.655948, mean_q: 4.998422
 22595/100000: episode: 2305, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.250, mean reward: 0.425 [0.354, 0.513], mean action: 40.800 [11.000, 85.000], mean observation: 3.154 [-1.555, 10.287], loss: 1.188056, mae: 4.654993, mean_q: 4.998578
 22605/100000: episode: 2306, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.045, mean reward: 0.405 [0.361, 0.493], mean action: 26.300 [5.000, 68.000], mean observation: 3.164 [-1.681, 10.413], loss: 1.122087, mae: 4.654774, mean_q: 5.001702
 22611/100000: episode: 2307, duration: 0.117s, episode steps: 6, steps per second: 51, episode reward: 11.797, mean reward: 1.966 [0.321, 10.000], mean action: 31.500 [7.000, 100.000], mean observation: 3.167 [-1.197, 10.410], loss: 0.625455, mae: 4.653005, mean_q: 5.005333
 22621/100000: episode: 2308, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.237, mean reward: 0.424 [0.334, 0.500], mean action: 21.400 [5.000, 61.000], mean observation: 3.164 [-1.587, 10.303], loss: 1.413971, mae: 4.656877, mean_q: 5.008510
 22631/100000: episode: 2309, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.138, mean reward: 0.414 [0.370, 0.454], mean action: 41.600 [11.000, 95.000], mean observation: 3.163 [-1.209, 10.339], loss: 1.292879, mae: 4.656984, mean_q: 5.007651
 22641/100000: episode: 2310, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 4.172, mean reward: 0.417 [0.381, 0.461], mean action: 20.500 [1.000, 72.000], mean observation: 3.163 [-1.292, 10.334], loss: 0.969018, mae: 4.655936, mean_q: 5.010106
 22651/100000: episode: 2311, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.031, mean reward: 0.403 [0.341, 0.448], mean action: 30.000 [11.000, 86.000], mean observation: 3.161 [-1.157, 10.353], loss: 1.265101, mae: 4.657506, mean_q: 5.011674
 22661/100000: episode: 2312, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.091, mean reward: 0.409 [0.293, 0.509], mean action: 19.400 [11.000, 56.000], mean observation: 3.157 [-1.417, 10.478], loss: 1.325188, mae: 4.658563, mean_q: 5.007005
 22671/100000: episode: 2313, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.569, mean reward: 0.457 [0.356, 0.628], mean action: 51.400 [3.000, 98.000], mean observation: 3.156 [-2.014, 10.395], loss: 1.089905, mae: 4.657766, mean_q: 5.007658
 22681/100000: episode: 2314, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.927, mean reward: 0.393 [0.326, 0.449], mean action: 27.300 [11.000, 80.000], mean observation: 3.157 [-1.239, 10.342], loss: 1.209180, mae: 4.658347, mean_q: 5.011894
 22691/100000: episode: 2315, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.038, mean reward: 0.404 [0.358, 0.554], mean action: 35.100 [11.000, 94.000], mean observation: 3.154 [-1.016, 10.383], loss: 0.901340, mae: 4.657554, mean_q: 5.012073
 22701/100000: episode: 2316, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.310, mean reward: 0.431 [0.320, 0.539], mean action: 26.000 [8.000, 99.000], mean observation: 3.160 [-1.829, 10.414], loss: 1.041947, mae: 4.658942, mean_q: 5.011662
 22711/100000: episode: 2317, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 4.815, mean reward: 0.481 [0.403, 0.572], mean action: 34.200 [2.000, 100.000], mean observation: 3.180 [-1.822, 10.481], loss: 1.239554, mae: 4.660765, mean_q: 5.012486
 22721/100000: episode: 2318, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.067, mean reward: 0.407 [0.321, 0.476], mean action: 40.500 [7.000, 100.000], mean observation: 3.165 [-1.284, 10.251], loss: 1.177956, mae: 4.661189, mean_q: 5.014600
 22731/100000: episode: 2319, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.698, mean reward: 0.470 [0.418, 0.562], mean action: 34.900 [11.000, 87.000], mean observation: 3.163 [-1.565, 10.270], loss: 1.228588, mae: 4.662081, mean_q: 5.010364
 22741/100000: episode: 2320, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.350, mean reward: 0.435 [0.392, 0.475], mean action: 31.600 [3.000, 79.000], mean observation: 3.171 [-1.398, 10.563], loss: 1.276990, mae: 4.662780, mean_q: 5.008835
 22751/100000: episode: 2321, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.360, mean reward: 0.436 [0.398, 0.533], mean action: 30.500 [11.000, 92.000], mean observation: 3.162 [-1.513, 10.364], loss: 1.036716, mae: 4.662169, mean_q: 5.001343
 22761/100000: episode: 2322, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.901, mean reward: 0.390 [0.324, 0.484], mean action: 28.800 [11.000, 95.000], mean observation: 3.159 [-1.632, 10.382], loss: 1.179328, mae: 4.663445, mean_q: 4.995229
 22771/100000: episode: 2323, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.844, mean reward: 0.384 [0.343, 0.430], mean action: 35.400 [11.000, 89.000], mean observation: 3.167 [-1.340, 10.391], loss: 1.226097, mae: 4.664124, mean_q: 4.995816
 22781/100000: episode: 2324, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.398, mean reward: 0.440 [0.381, 0.487], mean action: 34.200 [5.000, 94.000], mean observation: 3.151 [-2.231, 10.328], loss: 1.001155, mae: 4.663764, mean_q: 4.998037
 22791/100000: episode: 2325, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.216, mean reward: 0.422 [0.331, 0.560], mean action: 39.300 [6.000, 101.000], mean observation: 3.147 [-1.655, 10.269], loss: 1.296090, mae: 4.665359, mean_q: 4.999573
 22801/100000: episode: 2326, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.300, mean reward: 0.430 [0.299, 0.565], mean action: 28.700 [11.000, 98.000], mean observation: 3.148 [-2.478, 10.394], loss: 1.257906, mae: 4.665620, mean_q: 4.999240
 22811/100000: episode: 2327, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.424, mean reward: 0.442 [0.410, 0.534], mean action: 41.400 [2.000, 92.000], mean observation: 3.152 [-1.415, 10.362], loss: 1.126961, mae: 4.666099, mean_q: 4.999028
 22821/100000: episode: 2328, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.133, mean reward: 0.413 [0.295, 0.479], mean action: 31.800 [9.000, 86.000], mean observation: 3.148 [-1.285, 10.190], loss: 1.118146, mae: 4.666666, mean_q: 4.997241
 22831/100000: episode: 2329, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.369, mean reward: 0.437 [0.357, 0.529], mean action: 35.800 [7.000, 86.000], mean observation: 3.158 [-1.164, 10.330], loss: 1.305691, mae: 4.668212, mean_q: 4.995885
 22837/100000: episode: 2330, duration: 0.105s, episode steps: 6, steps per second: 57, episode reward: 12.647, mean reward: 2.108 [0.415, 10.000], mean action: 41.333 [11.000, 91.000], mean observation: 3.159 [-1.923, 10.505], loss: 1.154269, mae: 4.668269, mean_q: 4.993407
 22847/100000: episode: 2331, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.938, mean reward: 0.394 [0.326, 0.575], mean action: 53.800 [0.000, 90.000], mean observation: 3.161 [-1.528, 10.454], loss: 1.004895, mae: 4.667758, mean_q: 4.993985
 22857/100000: episode: 2332, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.063, mean reward: 0.406 [0.337, 0.444], mean action: 24.400 [9.000, 93.000], mean observation: 3.163 [-1.787, 10.381], loss: 1.078790, mae: 4.668698, mean_q: 4.996067
 22867/100000: episode: 2333, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.666, mean reward: 0.467 [0.324, 0.569], mean action: 37.600 [2.000, 96.000], mean observation: 3.147 [-1.283, 10.359], loss: 1.018241, mae: 4.669509, mean_q: 4.997130
 22877/100000: episode: 2334, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.051, mean reward: 0.405 [0.346, 0.485], mean action: 25.100 [11.000, 101.000], mean observation: 3.156 [-1.925, 10.276], loss: 1.064446, mae: 4.670468, mean_q: 4.995826
 22887/100000: episode: 2335, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.502, mean reward: 0.450 [0.423, 0.533], mean action: 44.800 [1.000, 94.000], mean observation: 3.169 [-1.712, 10.579], loss: 1.061670, mae: 4.671298, mean_q: 4.995133
 22897/100000: episode: 2336, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.152, mean reward: 0.415 [0.313, 0.575], mean action: 30.700 [11.000, 95.000], mean observation: 3.156 [-1.332, 10.301], loss: 1.122242, mae: 4.672259, mean_q: 4.993406
 22907/100000: episode: 2337, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.581, mean reward: 0.458 [0.458, 0.458], mean action: 46.600 [11.000, 90.000], mean observation: 3.150 [-1.449, 10.241], loss: 0.979468, mae: 4.672051, mean_q: 4.992079
 22917/100000: episode: 2338, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.956, mean reward: 0.396 [0.354, 0.458], mean action: 25.400 [11.000, 69.000], mean observation: 3.158 [-1.115, 10.307], loss: 1.113463, mae: 4.672925, mean_q: 4.989329
 22927/100000: episode: 2339, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.343, mean reward: 0.434 [0.392, 0.489], mean action: 41.000 [11.000, 96.000], mean observation: 3.159 [-1.687, 10.438], loss: 1.371161, mae: 4.674357, mean_q: 4.987908
 22937/100000: episode: 2340, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.209, mean reward: 0.421 [0.323, 0.535], mean action: 29.000 [11.000, 77.000], mean observation: 3.165 [-2.111, 10.342], loss: 1.018637, mae: 4.673089, mean_q: 4.984196
 22947/100000: episode: 2341, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.957, mean reward: 0.396 [0.335, 0.465], mean action: 36.500 [11.000, 96.000], mean observation: 3.151 [-1.079, 10.278], loss: 1.056806, mae: 4.674028, mean_q: 4.982221
 22957/100000: episode: 2342, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.088, mean reward: 0.409 [0.328, 0.497], mean action: 25.100 [7.000, 69.000], mean observation: 3.160 [-1.073, 10.407], loss: 0.955599, mae: 4.674262, mean_q: 4.982516
 22967/100000: episode: 2343, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.054, mean reward: 0.405 [0.381, 0.468], mean action: 24.700 [7.000, 94.000], mean observation: 3.166 [-1.828, 10.438], loss: 1.292393, mae: 4.676187, mean_q: 4.984898
 22977/100000: episode: 2344, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.297, mean reward: 0.430 [0.376, 0.530], mean action: 24.100 [8.000, 75.000], mean observation: 3.162 [-1.710, 10.308], loss: 1.231873, mae: 4.676111, mean_q: 4.986399
 22987/100000: episode: 2345, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.166, mean reward: 0.417 [0.328, 0.543], mean action: 29.600 [7.000, 95.000], mean observation: 3.157 [-1.495, 10.344], loss: 1.273921, mae: 4.676615, mean_q: 4.985105
 22997/100000: episode: 2346, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.314, mean reward: 0.431 [0.335, 0.498], mean action: 39.400 [9.000, 86.000], mean observation: 3.153 [-1.131, 10.411], loss: 1.018482, mae: 4.676172, mean_q: 4.987276
 23007/100000: episode: 2347, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.244, mean reward: 0.424 [0.373, 0.527], mean action: 30.800 [11.000, 73.000], mean observation: 3.154 [-1.562, 10.308], loss: 1.117473, mae: 4.677158, mean_q: 4.988935
 23017/100000: episode: 2348, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.069, mean reward: 0.407 [0.354, 0.486], mean action: 18.500 [3.000, 58.000], mean observation: 3.169 [-1.277, 10.288], loss: 0.827456, mae: 4.676540, mean_q: 4.992303
 23027/100000: episode: 2349, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.808, mean reward: 0.381 [0.305, 0.426], mean action: 31.900 [8.000, 99.000], mean observation: 3.160 [-1.436, 10.248], loss: 1.279924, mae: 4.678736, mean_q: 4.996447
 23037/100000: episode: 2350, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.326, mean reward: 0.433 [0.319, 0.553], mean action: 22.700 [11.000, 61.000], mean observation: 3.149 [-1.218, 10.427], loss: 0.855309, mae: 4.677331, mean_q: 4.999692
 23047/100000: episode: 2351, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.386, mean reward: 0.439 [0.319, 0.564], mean action: 30.900 [11.000, 96.000], mean observation: 3.152 [-1.062, 10.263], loss: 1.264184, mae: 4.679502, mean_q: 5.002791
 23057/100000: episode: 2352, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.126, mean reward: 0.413 [0.297, 0.508], mean action: 44.800 [11.000, 99.000], mean observation: 3.171 [-1.466, 10.423], loss: 1.253800, mae: 4.679669, mean_q: 5.005219
 23067/100000: episode: 2353, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.988, mean reward: 0.399 [0.382, 0.468], mean action: 56.900 [11.000, 95.000], mean observation: 3.152 [-0.906, 10.376], loss: 1.294850, mae: 4.680075, mean_q: 5.003327
 23075/100000: episode: 2354, duration: 0.136s, episode steps: 8, steps per second: 59, episode reward: 13.132, mean reward: 1.641 [0.343, 10.000], mean action: 58.375 [11.000, 90.000], mean observation: 3.159 [-1.067, 10.574], loss: 1.124943, mae: 4.679661, mean_q: 5.001585
 23085/100000: episode: 2355, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.289, mean reward: 0.429 [0.357, 0.537], mean action: 30.100 [2.000, 101.000], mean observation: 3.150 [-0.955, 10.223], loss: 0.891192, mae: 4.678855, mean_q: 5.000415
 23095/100000: episode: 2356, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.049, mean reward: 0.405 [0.364, 0.461], mean action: 35.700 [11.000, 79.000], mean observation: 3.162 [-1.441, 10.420], loss: 1.007748, mae: 4.680221, mean_q: 5.002392
 23105/100000: episode: 2357, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.018, mean reward: 0.402 [0.339, 0.490], mean action: 40.900 [11.000, 98.000], mean observation: 3.152 [-2.155, 10.313], loss: 1.263932, mae: 4.681808, mean_q: 5.005688
 23115/100000: episode: 2358, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 3.957, mean reward: 0.396 [0.318, 0.481], mean action: 16.100 [11.000, 35.000], mean observation: 3.156 [-1.847, 10.214], loss: 1.143433, mae: 4.681241, mean_q: 5.008349
 23125/100000: episode: 2359, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.901, mean reward: 0.390 [0.309, 0.464], mean action: 24.700 [11.000, 84.000], mean observation: 3.156 [-1.452, 10.379], loss: 1.173988, mae: 4.681868, mean_q: 5.003558
 23135/100000: episode: 2360, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.055, mean reward: 0.405 [0.333, 0.557], mean action: 22.600 [2.000, 72.000], mean observation: 3.163 [-1.634, 10.231], loss: 1.198328, mae: 4.682187, mean_q: 4.999715
 23145/100000: episode: 2361, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 3.641, mean reward: 0.364 [0.323, 0.445], mean action: 24.900 [11.000, 84.000], mean observation: 3.160 [-0.747, 10.240], loss: 1.073004, mae: 4.682121, mean_q: 4.992585
 23155/100000: episode: 2362, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.079, mean reward: 0.408 [0.285, 0.582], mean action: 26.100 [11.000, 93.000], mean observation: 3.148 [-1.274, 10.319], loss: 1.318407, mae: 4.683397, mean_q: 4.986669
 23165/100000: episode: 2363, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.149, mean reward: 0.415 [0.319, 0.465], mean action: 22.000 [11.000, 72.000], mean observation: 3.166 [-1.782, 10.555], loss: 1.017714, mae: 4.682614, mean_q: 4.986650
 23175/100000: episode: 2364, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.885, mean reward: 0.389 [0.319, 0.428], mean action: 41.200 [5.000, 99.000], mean observation: 3.143 [-1.486, 10.286], loss: 1.070670, mae: 4.683292, mean_q: 4.988574
 23185/100000: episode: 2365, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.215, mean reward: 0.421 [0.357, 0.509], mean action: 31.600 [11.000, 88.000], mean observation: 3.155 [-1.662, 10.400], loss: 1.381089, mae: 4.685197, mean_q: 4.988721
 23195/100000: episode: 2366, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.835, mean reward: 0.384 [0.324, 0.546], mean action: 27.000 [11.000, 80.000], mean observation: 3.158 [-1.373, 10.354], loss: 1.191371, mae: 4.684847, mean_q: 4.986856
 23205/100000: episode: 2367, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.409, mean reward: 0.441 [0.348, 0.570], mean action: 30.800 [11.000, 83.000], mean observation: 3.156 [-1.278, 10.208], loss: 1.139317, mae: 4.685254, mean_q: 4.987137
 23215/100000: episode: 2368, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.985, mean reward: 0.399 [0.297, 0.522], mean action: 39.600 [11.000, 78.000], mean observation: 3.162 [-1.558, 10.354], loss: 1.135589, mae: 4.685552, mean_q: 4.989818
 23225/100000: episode: 2369, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.215, mean reward: 0.421 [0.326, 0.568], mean action: 25.900 [1.000, 88.000], mean observation: 3.159 [-1.471, 10.283], loss: 1.232902, mae: 4.686061, mean_q: 4.992978
 23235/100000: episode: 2370, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.735, mean reward: 0.374 [0.300, 0.454], mean action: 29.700 [10.000, 81.000], mean observation: 3.155 [-2.032, 10.502], loss: 1.013389, mae: 4.686095, mean_q: 4.994062
 23245/100000: episode: 2371, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.290, mean reward: 0.429 [0.316, 0.489], mean action: 45.400 [11.000, 100.000], mean observation: 3.151 [-2.099, 10.264], loss: 1.065131, mae: 4.686813, mean_q: 4.993102
 23255/100000: episode: 2372, duration: 0.223s, episode steps: 10, steps per second: 45, episode reward: 4.341, mean reward: 0.434 [0.406, 0.495], mean action: 19.600 [8.000, 49.000], mean observation: 3.160 [-1.284, 10.323], loss: 0.923863, mae: 4.686851, mean_q: 4.995540
 23265/100000: episode: 2373, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.089, mean reward: 0.409 [0.354, 0.528], mean action: 24.700 [0.000, 97.000], mean observation: 3.146 [-1.696, 10.205], loss: 1.341078, mae: 4.689095, mean_q: 4.998784
 23275/100000: episode: 2374, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.940, mean reward: 0.394 [0.302, 0.486], mean action: 22.100 [11.000, 53.000], mean observation: 3.157 [-1.319, 10.542], loss: 1.115159, mae: 4.688448, mean_q: 5.000545
 23285/100000: episode: 2375, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.091, mean reward: 0.409 [0.389, 0.461], mean action: 35.000 [11.000, 100.000], mean observation: 3.154 [-1.455, 10.336], loss: 1.048764, mae: 4.689191, mean_q: 4.999148
 23295/100000: episode: 2376, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.340, mean reward: 0.434 [0.283, 0.511], mean action: 47.400 [11.000, 101.000], mean observation: 3.157 [-1.575, 10.264], loss: 1.069968, mae: 4.689586, mean_q: 4.992277
 23305/100000: episode: 2377, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.299, mean reward: 0.430 [0.380, 0.476], mean action: 39.400 [11.000, 89.000], mean observation: 3.147 [-1.075, 10.320], loss: 1.304540, mae: 4.690921, mean_q: 4.988873
 23315/100000: episode: 2378, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.611, mean reward: 0.461 [0.409, 0.687], mean action: 17.200 [11.000, 53.000], mean observation: 3.151 [-1.343, 10.338], loss: 1.034597, mae: 4.690391, mean_q: 4.989547
 23325/100000: episode: 2379, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.083, mean reward: 0.408 [0.332, 0.487], mean action: 37.800 [11.000, 100.000], mean observation: 3.146 [-0.970, 10.347], loss: 1.147511, mae: 4.691550, mean_q: 4.990111
 23335/100000: episode: 2380, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.903, mean reward: 0.390 [0.331, 0.480], mean action: 31.100 [11.000, 70.000], mean observation: 3.157 [-1.324, 10.330], loss: 1.265880, mae: 4.692183, mean_q: 4.989015
 23345/100000: episode: 2381, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.846, mean reward: 0.385 [0.343, 0.432], mean action: 31.900 [11.000, 98.000], mean observation: 3.138 [-1.301, 10.442], loss: 0.974697, mae: 4.691062, mean_q: 4.982821
 23352/100000: episode: 2382, duration: 0.109s, episode steps: 7, steps per second: 64, episode reward: 12.426, mean reward: 1.775 [0.350, 10.000], mean action: 53.286 [12.000, 85.000], mean observation: 3.152 [-1.740, 10.332], loss: 0.799571, mae: 4.691045, mean_q: 4.980616
 23362/100000: episode: 2383, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.771, mean reward: 0.377 [0.348, 0.409], mean action: 66.100 [17.000, 99.000], mean observation: 3.143 [-1.736, 10.240], loss: 1.139296, mae: 4.693032, mean_q: 4.982965
 23368/100000: episode: 2384, duration: 0.121s, episode steps: 6, steps per second: 50, episode reward: 11.648, mean reward: 1.941 [0.275, 10.000], mean action: 46.833 [6.000, 69.000], mean observation: 3.146 [-0.523, 10.264], loss: 0.789493, mae: 4.692260, mean_q: 4.984686
 23378/100000: episode: 2385, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.181, mean reward: 0.418 [0.376, 0.479], mean action: 64.200 [6.000, 98.000], mean observation: 3.152 [-1.282, 10.405], loss: 1.058355, mae: 4.693481, mean_q: 4.986589
 23388/100000: episode: 2386, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.483, mean reward: 0.448 [0.435, 0.557], mean action: 56.700 [15.000, 77.000], mean observation: 3.150 [-1.358, 10.421], loss: 1.233343, mae: 4.694707, mean_q: 4.989172
 23398/100000: episode: 2387, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.984, mean reward: 0.398 [0.332, 0.501], mean action: 47.700 [14.000, 93.000], mean observation: 3.167 [-1.780, 10.409], loss: 1.145073, mae: 4.694593, mean_q: 4.991448
 23408/100000: episode: 2388, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.217, mean reward: 0.422 [0.393, 0.440], mean action: 63.100 [22.000, 84.000], mean observation: 3.141 [-1.436, 10.311], loss: 1.212358, mae: 4.695249, mean_q: 4.993546
 23418/100000: episode: 2389, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.508, mean reward: 0.351 [0.326, 0.420], mean action: 65.400 [22.000, 95.000], mean observation: 3.145 [-0.669, 10.283], loss: 1.086486, mae: 4.695125, mean_q: 4.993723
 23419/100000: episode: 2390, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 69.000 [69.000, 69.000], mean observation: 3.156 [-1.150, 10.100], loss: 0.952862, mae: 4.695122, mean_q: 4.993187
 23429/100000: episode: 2391, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 3.927, mean reward: 0.393 [0.307, 0.498], mean action: 66.800 [8.000, 101.000], mean observation: 3.168 [-1.837, 10.261], loss: 0.968765, mae: 4.695187, mean_q: 4.993674
 23439/100000: episode: 2392, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.916, mean reward: 0.392 [0.380, 0.422], mean action: 67.800 [35.000, 98.000], mean observation: 3.148 [-1.452, 10.316], loss: 0.834680, mae: 4.695146, mean_q: 4.995904
 23449/100000: episode: 2393, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.146, mean reward: 0.415 [0.382, 0.530], mean action: 50.900 [9.000, 90.000], mean observation: 3.157 [-1.921, 10.290], loss: 1.125921, mae: 4.697195, mean_q: 4.999278
 23459/100000: episode: 2394, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.926, mean reward: 0.393 [0.372, 0.434], mean action: 61.100 [32.000, 99.000], mean observation: 3.163 [-1.259, 10.308], loss: 1.301185, mae: 4.698333, mean_q: 5.002614
 23469/100000: episode: 2395, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.906, mean reward: 0.491 [0.491, 0.491], mean action: 67.800 [48.000, 93.000], mean observation: 3.169 [-1.586, 10.362], loss: 1.324204, mae: 4.699119, mean_q: 5.005893
 23479/100000: episode: 2396, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.664, mean reward: 0.366 [0.351, 0.412], mean action: 60.400 [17.000, 92.000], mean observation: 3.151 [-1.082, 10.294], loss: 1.259326, mae: 4.698929, mean_q: 5.008680
 23489/100000: episode: 2397, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 5.002, mean reward: 0.500 [0.500, 0.500], mean action: 76.800 [69.000, 100.000], mean observation: 3.181 [-1.181, 10.353], loss: 1.250090, mae: 4.699239, mean_q: 5.006826
 23499/100000: episode: 2398, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.336, mean reward: 0.434 [0.348, 0.493], mean action: 51.500 [0.000, 81.000], mean observation: 3.162 [-1.215, 10.234], loss: 1.155838, mae: 4.699042, mean_q: 4.998337
 23509/100000: episode: 2399, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.189, mean reward: 0.419 [0.383, 0.504], mean action: 62.700 [7.000, 93.000], mean observation: 3.156 [-2.000, 10.272], loss: 1.026794, mae: 4.698968, mean_q: 4.992908
 23519/100000: episode: 2400, duration: 0.232s, episode steps: 10, steps per second: 43, episode reward: 4.457, mean reward: 0.446 [0.322, 0.593], mean action: 3.700 [0.000, 37.000], mean observation: 3.151 [-2.089, 10.498], loss: 0.873552, mae: 4.699069, mean_q: 4.991878
 23522/100000: episode: 2401, duration: 0.085s, episode steps: 3, steps per second: 35, episode reward: 10.808, mean reward: 3.603 [0.344, 10.000], mean action: 29.000 [0.000, 61.000], mean observation: 3.148 [-1.317, 10.100], loss: 1.268902, mae: 4.701200, mean_q: 4.992703
 23532/100000: episode: 2402, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.881, mean reward: 0.388 [0.310, 0.490], mean action: 29.500 [0.000, 99.000], mean observation: 3.155 [-1.779, 10.479], loss: 1.382319, mae: 4.701680, mean_q: 4.994236
 23542/100000: episode: 2403, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.298, mean reward: 0.430 [0.355, 0.542], mean action: 19.100 [0.000, 74.000], mean observation: 3.145 [-0.975, 10.404], loss: 1.276464, mae: 4.701530, mean_q: 4.994695
 23552/100000: episode: 2404, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.784, mean reward: 0.378 [0.322, 0.430], mean action: 57.500 [34.000, 101.000], mean observation: 3.164 [-1.200, 10.359], loss: 0.881103, mae: 4.700069, mean_q: 4.997433
 23559/100000: episode: 2405, duration: 0.154s, episode steps: 7, steps per second: 46, episode reward: 12.481, mean reward: 1.783 [0.353, 10.000], mean action: 40.143 [1.000, 80.000], mean observation: 3.153 [-1.650, 10.551], loss: 0.893794, mae: 4.700492, mean_q: 5.000318
 23569/100000: episode: 2406, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.541, mean reward: 0.454 [0.435, 0.486], mean action: 25.900 [6.000, 43.000], mean observation: 3.155 [-2.116, 10.345], loss: 1.028666, mae: 4.701694, mean_q: 5.001720
 23579/100000: episode: 2407, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.402, mean reward: 0.440 [0.330, 0.551], mean action: 41.300 [4.000, 74.000], mean observation: 3.164 [-1.602, 10.389], loss: 1.001864, mae: 4.701944, mean_q: 5.005421
 23589/100000: episode: 2408, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.568, mean reward: 0.457 [0.290, 0.569], mean action: 45.300 [5.000, 93.000], mean observation: 3.168 [-1.302, 10.304], loss: 0.894011, mae: 4.702175, mean_q: 5.014874
 23599/100000: episode: 2409, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.404, mean reward: 0.440 [0.432, 0.501], mean action: 49.400 [34.000, 98.000], mean observation: 3.151 [-2.109, 10.133], loss: 0.971126, mae: 4.703027, mean_q: 5.020251
 23609/100000: episode: 2410, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.293, mean reward: 0.429 [0.348, 0.527], mean action: 53.100 [6.000, 95.000], mean observation: 3.147 [-2.024, 10.346], loss: 1.206982, mae: 4.704858, mean_q: 5.022704
 23619/100000: episode: 2411, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.166, mean reward: 0.417 [0.341, 0.469], mean action: 37.900 [6.000, 87.000], mean observation: 3.156 [-1.971, 10.376], loss: 1.160602, mae: 4.705230, mean_q: 5.023787
 23629/100000: episode: 2412, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.986, mean reward: 0.399 [0.347, 0.502], mean action: 36.600 [23.000, 48.000], mean observation: 3.160 [-1.209, 10.216], loss: 0.915072, mae: 4.704740, mean_q: 5.024472
 23639/100000: episode: 2413, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.504, mean reward: 0.450 [0.437, 0.510], mean action: 37.700 [7.000, 80.000], mean observation: 3.160 [-1.409, 10.308], loss: 1.324543, mae: 4.707132, mean_q: 5.020965
 23649/100000: episode: 2414, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.069, mean reward: 0.407 [0.350, 0.521], mean action: 47.200 [14.000, 86.000], mean observation: 3.155 [-1.550, 10.305], loss: 0.898578, mae: 4.705722, mean_q: 5.017938
 23659/100000: episode: 2415, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.965, mean reward: 0.397 [0.373, 0.464], mean action: 51.300 [36.000, 99.000], mean observation: 3.143 [-2.051, 10.312], loss: 1.011819, mae: 4.706792, mean_q: 5.018330
 23668/100000: episode: 2416, duration: 0.145s, episode steps: 9, steps per second: 62, episode reward: 13.209, mean reward: 1.468 [0.346, 10.000], mean action: 48.222 [36.000, 87.000], mean observation: 3.155 [-1.805, 10.320], loss: 1.232216, mae: 4.708444, mean_q: 5.019593
 23678/100000: episode: 2417, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.837, mean reward: 0.384 [0.316, 0.496], mean action: 32.400 [9.000, 43.000], mean observation: 3.155 [-1.194, 10.279], loss: 1.198837, mae: 4.708511, mean_q: 5.022069
 23688/100000: episode: 2418, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.279, mean reward: 0.428 [0.428, 0.428], mean action: 50.000 [36.000, 81.000], mean observation: 3.146 [-1.193, 10.285], loss: 1.239859, mae: 4.709103, mean_q: 5.024104
 23698/100000: episode: 2419, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.424, mean reward: 0.442 [0.364, 0.520], mean action: 40.300 [19.000, 81.000], mean observation: 3.168 [-1.075, 10.343], loss: 1.386729, mae: 4.709756, mean_q: 5.025593
 23708/100000: episode: 2420, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.242, mean reward: 0.424 [0.407, 0.532], mean action: 45.300 [20.000, 92.000], mean observation: 3.152 [-1.557, 10.305], loss: 0.941907, mae: 4.707822, mean_q: 5.027603
 23718/100000: episode: 2421, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.153, mean reward: 0.415 [0.363, 0.468], mean action: 35.900 [2.000, 100.000], mean observation: 3.161 [-1.858, 10.366], loss: 1.173458, mae: 4.709271, mean_q: 5.029120
 23728/100000: episode: 2422, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.846, mean reward: 0.385 [0.307, 0.456], mean action: 37.700 [22.000, 79.000], mean observation: 3.154 [-0.951, 10.496], loss: 1.214935, mae: 4.709465, mean_q: 5.026815
 23738/100000: episode: 2423, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.821, mean reward: 0.482 [0.387, 0.509], mean action: 53.800 [3.000, 98.000], mean observation: 3.164 [-0.887, 10.355], loss: 1.186366, mae: 4.709384, mean_q: 5.025508
 23748/100000: episode: 2424, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 3.885, mean reward: 0.389 [0.356, 0.493], mean action: 31.900 [1.000, 69.000], mean observation: 3.145 [-1.302, 10.402], loss: 1.044983, mae: 4.709600, mean_q: 5.026023
 23758/100000: episode: 2425, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 3.848, mean reward: 0.385 [0.331, 0.494], mean action: 31.700 [6.000, 46.000], mean observation: 3.158 [-2.051, 10.233], loss: 1.098370, mae: 4.710156, mean_q: 5.026554
 23768/100000: episode: 2426, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.042, mean reward: 0.404 [0.315, 0.519], mean action: 35.600 [4.000, 74.000], mean observation: 3.155 [-1.516, 10.191], loss: 1.253848, mae: 4.711089, mean_q: 5.027518
 23778/100000: episode: 2427, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.627, mean reward: 0.463 [0.439, 0.586], mean action: 47.900 [20.000, 100.000], mean observation: 3.163 [-1.818, 10.292], loss: 1.314177, mae: 4.711574, mean_q: 5.029404
 23788/100000: episode: 2428, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.959, mean reward: 0.396 [0.299, 0.514], mean action: 36.300 [17.000, 69.000], mean observation: 3.151 [-1.770, 10.196], loss: 1.072358, mae: 4.710807, mean_q: 5.031548
 23798/100000: episode: 2429, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.166, mean reward: 0.417 [0.339, 0.520], mean action: 30.000 [6.000, 87.000], mean observation: 3.150 [-1.092, 10.506], loss: 0.991502, mae: 4.710754, mean_q: 5.027706
 23808/100000: episode: 2430, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.025, mean reward: 0.403 [0.366, 0.464], mean action: 47.900 [0.000, 93.000], mean observation: 3.153 [-1.439, 10.247], loss: 1.489182, mae: 4.713265, mean_q: 5.026646
 23818/100000: episode: 2431, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.898, mean reward: 0.490 [0.490, 0.490], mean action: 43.800 [36.000, 78.000], mean observation: 3.161 [-1.031, 10.325], loss: 1.008189, mae: 4.711545, mean_q: 5.027829
 23828/100000: episode: 2432, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.990, mean reward: 0.399 [0.333, 0.454], mean action: 44.100 [22.000, 93.000], mean observation: 3.142 [-1.293, 10.417], loss: 1.152901, mae: 4.712708, mean_q: 5.026463
 23838/100000: episode: 2433, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.094, mean reward: 0.409 [0.372, 0.454], mean action: 48.300 [27.000, 85.000], mean observation: 3.152 [-1.279, 10.455], loss: 1.372366, mae: 4.713941, mean_q: 5.022696
 23848/100000: episode: 2434, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.482, mean reward: 0.448 [0.443, 0.493], mean action: 44.600 [26.000, 84.000], mean observation: 3.149 [-1.221, 10.439], loss: 1.164852, mae: 4.713398, mean_q: 5.016851
 23858/100000: episode: 2435, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.483, mean reward: 0.448 [0.448, 0.448], mean action: 47.700 [8.000, 97.000], mean observation: 3.144 [-1.208, 10.286], loss: 1.271623, mae: 4.714101, mean_q: 5.013123
 23868/100000: episode: 2436, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.208, mean reward: 0.421 [0.331, 0.497], mean action: 29.000 [0.000, 77.000], mean observation: 3.163 [-1.608, 10.340], loss: 1.271936, mae: 4.714541, mean_q: 5.011173
 23878/100000: episode: 2437, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.476, mean reward: 0.448 [0.309, 0.522], mean action: 26.800 [0.000, 91.000], mean observation: 3.162 [-1.488, 10.365], loss: 1.174806, mae: 4.714392, mean_q: 5.012762
 23888/100000: episode: 2438, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.951, mean reward: 0.395 [0.357, 0.474], mean action: 34.000 [0.000, 79.000], mean observation: 3.160 [-1.677, 10.324], loss: 1.032788, mae: 4.714506, mean_q: 5.014871
 23898/100000: episode: 2439, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.728, mean reward: 0.373 [0.267, 0.476], mean action: 50.200 [17.000, 69.000], mean observation: 3.151 [-1.359, 10.247], loss: 1.309669, mae: 4.716029, mean_q: 5.017936
 23908/100000: episode: 2440, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.672, mean reward: 0.367 [0.325, 0.449], mean action: 58.800 [30.000, 73.000], mean observation: 3.149 [-1.840, 10.242], loss: 0.950637, mae: 4.714941, mean_q: 5.020392
 23918/100000: episode: 2441, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.969, mean reward: 0.397 [0.327, 0.503], mean action: 62.700 [5.000, 71.000], mean observation: 3.162 [-1.647, 10.258], loss: 1.485923, mae: 4.717607, mean_q: 5.021461
 23928/100000: episode: 2442, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 5.033, mean reward: 0.503 [0.495, 0.543], mean action: 57.900 [13.000, 81.000], mean observation: 3.149 [-0.970, 10.217], loss: 0.999539, mae: 4.715493, mean_q: 5.020809
 23938/100000: episode: 2443, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.320, mean reward: 0.432 [0.392, 0.529], mean action: 63.500 [0.000, 99.000], mean observation: 3.160 [-1.111, 10.283], loss: 1.264743, mae: 4.717231, mean_q: 5.022812
 23948/100000: episode: 2444, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.513, mean reward: 0.351 [0.341, 0.384], mean action: 65.800 [38.000, 95.000], mean observation: 3.159 [-1.446, 10.314], loss: 1.148077, mae: 4.716836, mean_q: 5.022345
 23958/100000: episode: 2445, duration: 0.153s, episode steps: 10, steps per second: 66, episode reward: 3.934, mean reward: 0.393 [0.343, 0.470], mean action: 54.400 [22.000, 83.000], mean observation: 3.158 [-1.283, 10.260], loss: 1.201163, mae: 4.717455, mean_q: 5.022059
 23968/100000: episode: 2446, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.377, mean reward: 0.438 [0.328, 0.518], mean action: 48.300 [0.000, 93.000], mean observation: 3.158 [-1.809, 10.293], loss: 1.651530, mae: 4.719345, mean_q: 5.021892
 23978/100000: episode: 2447, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.050, mean reward: 0.405 [0.360, 0.534], mean action: 45.500 [0.000, 79.000], mean observation: 3.165 [-1.317, 10.236], loss: 1.245156, mae: 4.718137, mean_q: 5.021685
 23988/100000: episode: 2448, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 5.479, mean reward: 0.548 [0.399, 0.587], mean action: 61.300 [3.000, 101.000], mean observation: 3.152 [-1.379, 10.280], loss: 0.879022, mae: 4.716963, mean_q: 5.022273
 23998/100000: episode: 2449, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.665, mean reward: 0.367 [0.364, 0.367], mean action: 62.200 [20.000, 76.000], mean observation: 3.154 [-1.002, 10.212], loss: 1.319644, mae: 4.719333, mean_q: 5.023444
 24008/100000: episode: 2450, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.897, mean reward: 0.390 [0.275, 0.445], mean action: 24.500 [0.000, 92.000], mean observation: 3.164 [-1.540, 10.395], loss: 1.406021, mae: 4.720325, mean_q: 5.022927
 24018/100000: episode: 2451, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.114, mean reward: 0.411 [0.370, 0.463], mean action: 23.800 [0.000, 93.000], mean observation: 3.164 [-1.051, 10.316], loss: 1.043744, mae: 4.719337, mean_q: 5.019818
 24028/100000: episode: 2452, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 4.071, mean reward: 0.407 [0.310, 0.508], mean action: 6.000 [0.000, 60.000], mean observation: 3.157 [-1.553, 10.439], loss: 1.207890, mae: 4.720570, mean_q: 5.019576
 24038/100000: episode: 2453, duration: 0.131s, episode steps: 10, steps per second: 77, episode reward: 4.593, mean reward: 0.459 [0.453, 0.516], mean action: 72.200 [54.000, 101.000], mean observation: 3.157 [-0.833, 10.325], loss: 1.102563, mae: 4.720235, mean_q: 5.020495
 24048/100000: episode: 2454, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.747, mean reward: 0.375 [0.307, 0.470], mean action: 37.900 [4.000, 69.000], mean observation: 3.158 [-1.194, 10.334], loss: 1.285182, mae: 4.721344, mean_q: 5.022544
 24058/100000: episode: 2455, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.896, mean reward: 0.390 [0.332, 0.452], mean action: 55.600 [10.000, 97.000], mean observation: 3.168 [-1.211, 10.296], loss: 1.275949, mae: 4.721461, mean_q: 5.023742
 24068/100000: episode: 2456, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.051, mean reward: 0.405 [0.349, 0.483], mean action: 54.000 [6.000, 69.000], mean observation: 3.159 [-1.298, 10.250], loss: 1.104844, mae: 4.720831, mean_q: 5.020000
 24078/100000: episode: 2457, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.140, mean reward: 0.414 [0.393, 0.419], mean action: 72.300 [26.000, 101.000], mean observation: 3.157 [-0.810, 10.287], loss: 1.151507, mae: 4.721222, mean_q: 5.019260
 24088/100000: episode: 2458, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.017, mean reward: 0.402 [0.331, 0.490], mean action: 11.000 [0.000, 71.000], mean observation: 3.162 [-1.430, 10.492], loss: 1.290032, mae: 4.721942, mean_q: 5.020422
 24098/100000: episode: 2459, duration: 0.221s, episode steps: 10, steps per second: 45, episode reward: 4.206, mean reward: 0.421 [0.346, 0.566], mean action: 14.500 [0.000, 81.000], mean observation: 3.167 [-1.234, 10.204], loss: 1.137675, mae: 4.721835, mean_q: 5.023812
 24108/100000: episode: 2460, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 3.817, mean reward: 0.382 [0.324, 0.506], mean action: 24.500 [0.000, 63.000], mean observation: 3.155 [-1.357, 10.333], loss: 1.355253, mae: 4.723042, mean_q: 5.026440
 24118/100000: episode: 2461, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.185, mean reward: 0.418 [0.330, 0.493], mean action: 40.700 [0.000, 99.000], mean observation: 3.160 [-1.080, 10.277], loss: 1.125144, mae: 4.722358, mean_q: 5.028412
 24128/100000: episode: 2462, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.534, mean reward: 0.353 [0.309, 0.486], mean action: 32.300 [0.000, 101.000], mean observation: 3.161 [-1.324, 10.471], loss: 1.353268, mae: 4.723345, mean_q: 5.027250
 24138/100000: episode: 2463, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.891, mean reward: 0.389 [0.293, 0.510], mean action: 20.500 [0.000, 52.000], mean observation: 3.155 [-1.451, 10.271], loss: 1.262132, mae: 4.723310, mean_q: 5.020730
 24148/100000: episode: 2464, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.111, mean reward: 0.411 [0.382, 0.458], mean action: 32.200 [8.000, 101.000], mean observation: 3.145 [-1.632, 10.239], loss: 1.080094, mae: 4.722375, mean_q: 5.021823
 24158/100000: episode: 2465, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.882, mean reward: 0.388 [0.333, 0.500], mean action: 40.600 [8.000, 83.000], mean observation: 3.151 [-1.720, 10.387], loss: 1.208976, mae: 4.723687, mean_q: 5.022620
 24168/100000: episode: 2466, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.194, mean reward: 0.419 [0.336, 0.530], mean action: 32.400 [2.000, 99.000], mean observation: 3.159 [-2.166, 10.237], loss: 1.039981, mae: 4.723354, mean_q: 5.023213
 24178/100000: episode: 2467, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.055, mean reward: 0.405 [0.335, 0.529], mean action: 27.400 [5.000, 95.000], mean observation: 3.153 [-1.450, 10.339], loss: 1.133858, mae: 4.724181, mean_q: 5.023747
 24188/100000: episode: 2468, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.241, mean reward: 0.424 [0.308, 0.546], mean action: 24.500 [1.000, 66.000], mean observation: 3.154 [-1.144, 10.237], loss: 1.134826, mae: 4.724532, mean_q: 5.027757
 24198/100000: episode: 2469, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.271, mean reward: 0.427 [0.334, 0.501], mean action: 20.300 [8.000, 68.000], mean observation: 3.166 [-1.411, 10.377], loss: 1.233877, mae: 4.725654, mean_q: 5.034614
 24208/100000: episode: 2470, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.409, mean reward: 0.441 [0.329, 0.522], mean action: 23.900 [8.000, 85.000], mean observation: 3.154 [-1.507, 10.370], loss: 1.110335, mae: 4.725234, mean_q: 5.038023
 24218/100000: episode: 2471, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.175, mean reward: 0.418 [0.346, 0.524], mean action: 38.400 [8.000, 88.000], mean observation: 3.166 [-1.285, 10.254], loss: 1.096621, mae: 4.725488, mean_q: 5.040532
 24228/100000: episode: 2472, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.842, mean reward: 0.384 [0.318, 0.454], mean action: 36.600 [8.000, 99.000], mean observation: 3.155 [-0.997, 10.251], loss: 1.357007, mae: 4.726566, mean_q: 5.039575
 24238/100000: episode: 2473, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.074, mean reward: 0.407 [0.385, 0.431], mean action: 43.800 [8.000, 95.000], mean observation: 3.157 [-0.958, 10.285], loss: 1.345958, mae: 4.726861, mean_q: 5.034641
 24248/100000: episode: 2474, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.177, mean reward: 0.418 [0.414, 0.436], mean action: 56.800 [8.000, 85.000], mean observation: 3.159 [-1.714, 10.223], loss: 1.138824, mae: 4.726026, mean_q: 5.032399
 24258/100000: episode: 2475, duration: 0.131s, episode steps: 10, steps per second: 77, episode reward: 4.257, mean reward: 0.426 [0.384, 0.459], mean action: 64.400 [4.000, 90.000], mean observation: 3.156 [-1.158, 10.253], loss: 1.442758, mae: 4.727691, mean_q: 5.035404
 24268/100000: episode: 2476, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.528, mean reward: 0.453 [0.409, 0.526], mean action: 57.800 [0.000, 100.000], mean observation: 3.156 [-1.522, 10.386], loss: 1.099108, mae: 4.726324, mean_q: 5.038271
 24278/100000: episode: 2477, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.721, mean reward: 0.472 [0.429, 0.537], mean action: 64.700 [3.000, 101.000], mean observation: 3.166 [-1.776, 10.373], loss: 1.169801, mae: 4.727159, mean_q: 5.038717
 24288/100000: episode: 2478, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.657, mean reward: 0.366 [0.320, 0.478], mean action: 66.500 [48.000, 93.000], mean observation: 3.162 [-1.863, 10.266], loss: 1.236388, mae: 4.727902, mean_q: 5.042044
 24298/100000: episode: 2479, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.146, mean reward: 0.415 [0.399, 0.431], mean action: 65.100 [16.000, 96.000], mean observation: 3.142 [-1.249, 10.244], loss: 1.557196, mae: 4.729284, mean_q: 5.043169
 24308/100000: episode: 2480, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.868, mean reward: 0.387 [0.341, 0.499], mean action: 58.300 [2.000, 91.000], mean observation: 3.159 [-1.134, 10.269], loss: 1.292587, mae: 4.728112, mean_q: 5.041701
 24318/100000: episode: 2481, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.064, mean reward: 0.406 [0.388, 0.438], mean action: 52.300 [0.000, 98.000], mean observation: 3.151 [-1.530, 10.236], loss: 1.019464, mae: 4.727302, mean_q: 5.042348
 24328/100000: episode: 2482, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.190, mean reward: 0.419 [0.366, 0.543], mean action: 45.900 [8.000, 69.000], mean observation: 3.162 [-0.981, 10.322], loss: 1.216359, mae: 4.728592, mean_q: 5.041443
 24338/100000: episode: 2483, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 3.762, mean reward: 0.376 [0.312, 0.446], mean action: 33.000 [8.000, 97.000], mean observation: 3.145 [-0.966, 10.266], loss: 1.172851, mae: 4.728842, mean_q: 5.042967
 24348/100000: episode: 2484, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.993, mean reward: 0.399 [0.319, 0.583], mean action: 49.400 [8.000, 90.000], mean observation: 3.168 [-1.044, 10.336], loss: 0.980274, mae: 4.728247, mean_q: 5.039568
 24358/100000: episode: 2485, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.213, mean reward: 0.421 [0.390, 0.453], mean action: 50.600 [5.000, 92.000], mean observation: 3.144 [-1.585, 10.343], loss: 1.238400, mae: 4.729403, mean_q: 5.038545
 24368/100000: episode: 2486, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.384, mean reward: 0.438 [0.335, 0.503], mean action: 32.400 [4.000, 101.000], mean observation: 3.157 [-1.674, 10.356], loss: 1.052962, mae: 4.729066, mean_q: 5.038707
 24378/100000: episode: 2487, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 4.172, mean reward: 0.417 [0.315, 0.512], mean action: 20.400 [1.000, 92.000], mean observation: 3.149 [-1.177, 10.312], loss: 1.104543, mae: 4.730022, mean_q: 5.039809
 24388/100000: episode: 2488, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.879, mean reward: 0.388 [0.346, 0.433], mean action: 32.900 [5.000, 88.000], mean observation: 3.151 [-1.185, 10.352], loss: 1.377459, mae: 4.731244, mean_q: 5.039221
 24398/100000: episode: 2489, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.181, mean reward: 0.418 [0.355, 0.560], mean action: 29.500 [5.000, 96.000], mean observation: 3.146 [-1.337, 10.352], loss: 1.077323, mae: 4.730756, mean_q: 5.035783
 24408/100000: episode: 2490, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.188, mean reward: 0.419 [0.336, 0.478], mean action: 41.600 [8.000, 101.000], mean observation: 3.158 [-1.080, 10.352], loss: 1.065581, mae: 4.731154, mean_q: 5.035913
 24418/100000: episode: 2491, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.909, mean reward: 0.391 [0.292, 0.464], mean action: 42.000 [8.000, 95.000], mean observation: 3.155 [-0.977, 10.378], loss: 1.390565, mae: 4.732923, mean_q: 5.033531
 24428/100000: episode: 2492, duration: 0.225s, episode steps: 10, steps per second: 45, episode reward: 3.959, mean reward: 0.396 [0.363, 0.502], mean action: 12.200 [8.000, 30.000], mean observation: 3.153 [-1.824, 10.446], loss: 0.889123, mae: 4.731336, mean_q: 5.033263
 24438/100000: episode: 2493, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.014, mean reward: 0.401 [0.328, 0.527], mean action: 31.600 [8.000, 70.000], mean observation: 3.150 [-0.959, 10.399], loss: 0.999001, mae: 4.732307, mean_q: 5.034761
 24448/100000: episode: 2494, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.250, mean reward: 0.425 [0.318, 0.564], mean action: 22.000 [8.000, 87.000], mean observation: 3.162 [-1.255, 10.335], loss: 1.142588, mae: 4.733386, mean_q: 5.037833
 24458/100000: episode: 2495, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.058, mean reward: 0.406 [0.326, 0.454], mean action: 19.100 [1.000, 75.000], mean observation: 3.155 [-0.966, 10.311], loss: 1.021900, mae: 4.733535, mean_q: 5.040601
 24468/100000: episode: 2496, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.258, mean reward: 0.426 [0.336, 0.506], mean action: 35.200 [8.000, 85.000], mean observation: 3.160 [-1.142, 10.531], loss: 1.550533, mae: 4.736184, mean_q: 5.044172
 24478/100000: episode: 2497, duration: 0.221s, episode steps: 10, steps per second: 45, episode reward: 4.153, mean reward: 0.415 [0.346, 0.530], mean action: 14.700 [2.000, 84.000], mean observation: 3.153 [-1.687, 10.299], loss: 1.113949, mae: 4.734933, mean_q: 5.045800
 24488/100000: episode: 2498, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.951, mean reward: 0.395 [0.335, 0.463], mean action: 34.200 [8.000, 82.000], mean observation: 3.151 [-1.625, 10.388], loss: 1.358208, mae: 4.736249, mean_q: 5.038272
 24498/100000: episode: 2499, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 3.924, mean reward: 0.392 [0.339, 0.512], mean action: 13.100 [5.000, 34.000], mean observation: 3.154 [-1.317, 10.516], loss: 0.949862, mae: 4.735097, mean_q: 5.036230
 24508/100000: episode: 2500, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.371, mean reward: 0.437 [0.387, 0.510], mean action: 28.400 [8.000, 89.000], mean observation: 3.151 [-1.790, 10.422], loss: 1.151935, mae: 4.735940, mean_q: 5.036527
 24518/100000: episode: 2501, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.347, mean reward: 0.435 [0.354, 0.492], mean action: 24.100 [8.000, 74.000], mean observation: 3.167 [-1.627, 10.319], loss: 1.123666, mae: 4.736348, mean_q: 5.037748
 24528/100000: episode: 2502, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.130, mean reward: 0.413 [0.327, 0.507], mean action: 23.000 [8.000, 54.000], mean observation: 3.164 [-1.618, 10.388], loss: 1.239952, mae: 4.736970, mean_q: 5.038042
 24538/100000: episode: 2503, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.144, mean reward: 0.414 [0.335, 0.481], mean action: 22.800 [1.000, 82.000], mean observation: 3.160 [-1.912, 10.277], loss: 1.239042, mae: 4.737176, mean_q: 5.035905
 24548/100000: episode: 2504, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.656, mean reward: 0.366 [0.247, 0.461], mean action: 39.400 [8.000, 95.000], mean observation: 3.168 [-1.456, 10.526], loss: 1.244024, mae: 4.737458, mean_q: 5.038029
 24558/100000: episode: 2505, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.245, mean reward: 0.424 [0.324, 0.506], mean action: 24.400 [0.000, 86.000], mean observation: 3.151 [-1.486, 10.265], loss: 1.052732, mae: 4.737185, mean_q: 5.040450
 24568/100000: episode: 2506, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.491, mean reward: 0.449 [0.305, 0.506], mean action: 32.000 [8.000, 89.000], mean observation: 3.159 [-1.715, 10.563], loss: 1.012549, mae: 4.737456, mean_q: 5.042064
 24578/100000: episode: 2507, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.136, mean reward: 0.414 [0.344, 0.520], mean action: 29.800 [8.000, 99.000], mean observation: 3.158 [-1.083, 10.363], loss: 1.012241, mae: 4.738060, mean_q: 5.043684
 24588/100000: episode: 2508, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.329, mean reward: 0.433 [0.360, 0.498], mean action: 33.500 [8.000, 96.000], mean observation: 3.149 [-1.547, 10.262], loss: 1.252847, mae: 4.739565, mean_q: 5.044855
 24598/100000: episode: 2509, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.170, mean reward: 0.417 [0.297, 0.579], mean action: 39.400 [3.000, 99.000], mean observation: 3.160 [-1.091, 10.230], loss: 1.047136, mae: 4.739308, mean_q: 5.046233
 24608/100000: episode: 2510, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.239, mean reward: 0.424 [0.326, 0.476], mean action: 33.100 [8.000, 88.000], mean observation: 3.160 [-1.383, 10.320], loss: 1.055939, mae: 4.739618, mean_q: 5.045587
 24618/100000: episode: 2511, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.736, mean reward: 0.374 [0.318, 0.535], mean action: 39.000 [8.000, 90.000], mean observation: 3.159 [-1.405, 10.245], loss: 1.199053, mae: 4.740539, mean_q: 5.042174
 24628/100000: episode: 2512, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.261, mean reward: 0.426 [0.367, 0.518], mean action: 32.300 [8.000, 100.000], mean observation: 3.151 [-1.796, 10.265], loss: 1.099724, mae: 4.740149, mean_q: 5.038435
 24638/100000: episode: 2513, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 4.219, mean reward: 0.422 [0.342, 0.514], mean action: 9.500 [8.000, 23.000], mean observation: 3.146 [-1.389, 10.376], loss: 1.040087, mae: 4.740832, mean_q: 5.036611
 24648/100000: episode: 2514, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.893, mean reward: 0.389 [0.332, 0.435], mean action: 42.100 [6.000, 101.000], mean observation: 3.152 [-1.467, 10.294], loss: 1.094931, mae: 4.741028, mean_q: 5.034997
 24658/100000: episode: 2515, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.660, mean reward: 0.366 [0.316, 0.473], mean action: 37.700 [8.000, 92.000], mean observation: 3.156 [-2.257, 10.369], loss: 1.115289, mae: 4.741800, mean_q: 5.037264
 24668/100000: episode: 2516, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.006, mean reward: 0.401 [0.356, 0.493], mean action: 34.000 [8.000, 74.000], mean observation: 3.161 [-1.425, 10.330], loss: 1.050768, mae: 4.741817, mean_q: 5.040682
 24678/100000: episode: 2517, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.030, mean reward: 0.403 [0.382, 0.495], mean action: 22.000 [8.000, 78.000], mean observation: 3.156 [-1.820, 10.258], loss: 1.051232, mae: 4.742131, mean_q: 5.042081
 24688/100000: episode: 2518, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.843, mean reward: 0.384 [0.294, 0.507], mean action: 32.700 [8.000, 97.000], mean observation: 3.152 [-1.066, 10.363], loss: 1.225765, mae: 4.742794, mean_q: 5.036529
 24698/100000: episode: 2519, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.053, mean reward: 0.405 [0.337, 0.488], mean action: 45.100 [6.000, 96.000], mean observation: 3.154 [-1.069, 10.356], loss: 1.096901, mae: 4.743040, mean_q: 5.035045
 24708/100000: episode: 2520, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.093, mean reward: 0.409 [0.321, 0.459], mean action: 22.500 [8.000, 51.000], mean observation: 3.159 [-2.095, 10.260], loss: 1.091695, mae: 4.743614, mean_q: 5.036086
 24718/100000: episode: 2521, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.204, mean reward: 0.420 [0.355, 0.556], mean action: 29.000 [8.000, 88.000], mean observation: 3.152 [-1.099, 10.289], loss: 1.207690, mae: 4.744150, mean_q: 5.037071
 24728/100000: episode: 2522, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.022, mean reward: 0.402 [0.340, 0.523], mean action: 27.200 [3.000, 94.000], mean observation: 3.157 [-1.446, 10.341], loss: 1.207636, mae: 4.744696, mean_q: 5.038558
 24738/100000: episode: 2523, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.176, mean reward: 0.418 [0.342, 0.504], mean action: 32.400 [5.000, 85.000], mean observation: 3.138 [-1.502, 10.314], loss: 1.130076, mae: 4.744241, mean_q: 5.036658
 24748/100000: episode: 2524, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.300, mean reward: 0.430 [0.322, 0.506], mean action: 33.000 [8.000, 90.000], mean observation: 3.154 [-1.624, 10.273], loss: 1.230176, mae: 4.744824, mean_q: 5.037037
 24758/100000: episode: 2525, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.010, mean reward: 0.401 [0.338, 0.449], mean action: 33.800 [7.000, 100.000], mean observation: 3.153 [-2.451, 10.395], loss: 1.252765, mae: 4.745445, mean_q: 5.035097
 24768/100000: episode: 2526, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.091, mean reward: 0.409 [0.375, 0.560], mean action: 13.000 [5.000, 37.000], mean observation: 3.149 [-1.837, 10.297], loss: 1.080855, mae: 4.745086, mean_q: 5.033283
 24778/100000: episode: 2527, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 3.936, mean reward: 0.394 [0.297, 0.526], mean action: 14.800 [8.000, 49.000], mean observation: 3.169 [-1.416, 10.317], loss: 1.072240, mae: 4.745326, mean_q: 5.034632
 24788/100000: episode: 2528, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.407, mean reward: 0.441 [0.367, 0.538], mean action: 16.400 [8.000, 69.000], mean observation: 3.154 [-1.730, 10.313], loss: 0.945336, mae: 4.745481, mean_q: 5.037803
 24798/100000: episode: 2529, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.148, mean reward: 0.415 [0.349, 0.485], mean action: 38.400 [7.000, 96.000], mean observation: 3.160 [-1.458, 10.312], loss: 1.205423, mae: 4.746763, mean_q: 5.040945
 24808/100000: episode: 2530, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.634, mean reward: 0.463 [0.346, 0.513], mean action: 43.300 [8.000, 94.000], mean observation: 3.161 [-1.640, 10.306], loss: 1.245665, mae: 4.746850, mean_q: 5.041351
 24818/100000: episode: 2531, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.456, mean reward: 0.446 [0.343, 0.551], mean action: 26.000 [8.000, 61.000], mean observation: 3.157 [-1.562, 10.228], loss: 1.283320, mae: 4.747119, mean_q: 5.043228
 24828/100000: episode: 2532, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.841, mean reward: 0.384 [0.303, 0.461], mean action: 23.100 [0.000, 90.000], mean observation: 3.147 [-1.400, 10.366], loss: 0.969929, mae: 4.746127, mean_q: 5.043259
 24838/100000: episode: 2533, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.066, mean reward: 0.407 [0.359, 0.451], mean action: 24.700 [0.000, 94.000], mean observation: 3.151 [-2.397, 10.291], loss: 1.563001, mae: 4.749237, mean_q: 5.045825
 24848/100000: episode: 2534, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.640, mean reward: 0.364 [0.309, 0.423], mean action: 13.500 [0.000, 97.000], mean observation: 3.150 [-1.356, 10.259], loss: 0.963830, mae: 4.746955, mean_q: 5.047891
 24858/100000: episode: 2535, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 3.893, mean reward: 0.389 [0.340, 0.462], mean action: 11.900 [0.000, 82.000], mean observation: 3.154 [-1.222, 10.353], loss: 1.355383, mae: 4.748819, mean_q: 5.049790
 24868/100000: episode: 2536, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.412, mean reward: 0.441 [0.360, 0.537], mean action: 19.600 [0.000, 98.000], mean observation: 3.165 [-1.606, 10.397], loss: 1.084372, mae: 4.748012, mean_q: 5.052049
 24878/100000: episode: 2537, duration: 0.223s, episode steps: 10, steps per second: 45, episode reward: 4.081, mean reward: 0.408 [0.322, 0.475], mean action: 17.200 [0.000, 88.000], mean observation: 3.162 [-1.524, 10.204], loss: 0.997902, mae: 4.748087, mean_q: 5.054334
 24888/100000: episode: 2538, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.251, mean reward: 0.425 [0.359, 0.539], mean action: 24.800 [0.000, 78.000], mean observation: 3.154 [-1.618, 10.425], loss: 1.156826, mae: 4.749339, mean_q: 5.055810
 24898/100000: episode: 2539, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 3.732, mean reward: 0.373 [0.296, 0.548], mean action: 14.400 [0.000, 57.000], mean observation: 3.155 [-1.922, 10.227], loss: 1.234379, mae: 4.749599, mean_q: 5.056867
 24908/100000: episode: 2540, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.268, mean reward: 0.427 [0.360, 0.537], mean action: 57.900 [0.000, 98.000], mean observation: 3.150 [-1.240, 10.294], loss: 1.028442, mae: 4.748968, mean_q: 5.058137
 24918/100000: episode: 2541, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.938, mean reward: 0.394 [0.367, 0.433], mean action: 37.300 [0.000, 101.000], mean observation: 3.159 [-1.332, 10.363], loss: 1.136671, mae: 4.749816, mean_q: 5.059201
 24928/100000: episode: 2542, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.296, mean reward: 0.430 [0.352, 0.453], mean action: 38.500 [0.000, 85.000], mean observation: 3.157 [-2.001, 10.542], loss: 1.175292, mae: 4.750240, mean_q: 5.060819
 24931/100000: episode: 2543, duration: 0.083s, episode steps: 3, steps per second: 36, episode reward: 10.703, mean reward: 3.568 [0.272, 10.000], mean action: 19.667 [0.000, 59.000], mean observation: 3.160 [-1.021, 10.449], loss: 0.865195, mae: 4.749269, mean_q: 5.061776
 24941/100000: episode: 2544, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.939, mean reward: 0.394 [0.365, 0.477], mean action: 29.600 [0.000, 89.000], mean observation: 3.153 [-1.135, 10.471], loss: 1.124525, mae: 4.750316, mean_q: 5.062794
 24951/100000: episode: 2545, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.089, mean reward: 0.409 [0.342, 0.461], mean action: 36.100 [0.000, 91.000], mean observation: 3.159 [-1.788, 10.348], loss: 1.175549, mae: 4.751041, mean_q: 5.064691
 24961/100000: episode: 2546, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.023, mean reward: 0.402 [0.329, 0.468], mean action: 16.400 [0.000, 59.000], mean observation: 3.150 [-1.164, 10.399], loss: 1.248883, mae: 4.751870, mean_q: 5.066247
 24971/100000: episode: 2547, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.114, mean reward: 0.411 [0.348, 0.519], mean action: 14.500 [0.000, 87.000], mean observation: 3.150 [-2.067, 10.375], loss: 1.023093, mae: 4.751239, mean_q: 5.066763
 24980/100000: episode: 2548, duration: 0.187s, episode steps: 9, steps per second: 48, episode reward: 13.635, mean reward: 1.515 [0.367, 10.000], mean action: 20.111 [0.000, 94.000], mean observation: 3.162 [-1.498, 10.505], loss: 1.272717, mae: 4.752719, mean_q: 5.063485
 24990/100000: episode: 2549, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 4.179, mean reward: 0.418 [0.323, 0.568], mean action: 20.300 [0.000, 91.000], mean observation: 3.153 [-1.396, 10.256], loss: 1.110394, mae: 4.752444, mean_q: 5.064136
 24997/100000: episode: 2550, duration: 0.131s, episode steps: 7, steps per second: 53, episode reward: 12.389, mean reward: 1.770 [0.347, 10.000], mean action: 37.286 [0.000, 96.000], mean observation: 3.175 [-1.093, 10.258], loss: 1.080113, mae: 4.752880, mean_q: 5.065322
 25007/100000: episode: 2551, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.502, mean reward: 0.450 [0.370, 0.599], mean action: 25.000 [0.000, 101.000], mean observation: 3.166 [-1.900, 10.567], loss: 1.240921, mae: 4.753695, mean_q: 5.062119
 25017/100000: episode: 2552, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.878, mean reward: 0.488 [0.422, 0.549], mean action: 63.300 [0.000, 100.000], mean observation: 3.147 [-1.281, 10.177], loss: 0.980622, mae: 4.753192, mean_q: 5.060376
 25027/100000: episode: 2553, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.018, mean reward: 0.402 [0.350, 0.479], mean action: 10.800 [0.000, 49.000], mean observation: 3.157 [-1.702, 10.296], loss: 1.204428, mae: 4.754415, mean_q: 5.061055
 25037/100000: episode: 2554, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.705, mean reward: 0.371 [0.327, 0.438], mean action: 21.700 [0.000, 59.000], mean observation: 3.158 [-1.282, 10.332], loss: 1.042815, mae: 4.754364, mean_q: 5.060998
 25047/100000: episode: 2555, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 3.934, mean reward: 0.393 [0.341, 0.453], mean action: 25.700 [0.000, 86.000], mean observation: 3.150 [-2.332, 10.311], loss: 0.923085, mae: 4.754394, mean_q: 5.059698
 25053/100000: episode: 2556, duration: 0.111s, episode steps: 6, steps per second: 54, episode reward: 11.905, mean reward: 1.984 [0.339, 10.000], mean action: 33.833 [0.000, 89.000], mean observation: 3.153 [-1.210, 10.511], loss: 1.107097, mae: 4.755809, mean_q: 5.061024
 25063/100000: episode: 2557, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.040, mean reward: 0.404 [0.332, 0.494], mean action: 35.400 [0.000, 94.000], mean observation: 3.156 [-1.405, 10.320], loss: 0.981321, mae: 4.755842, mean_q: 5.063465
 25073/100000: episode: 2558, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.358, mean reward: 0.436 [0.405, 0.516], mean action: 26.100 [0.000, 81.000], mean observation: 3.150 [-1.509, 10.250], loss: 1.109418, mae: 4.757033, mean_q: 5.065297
 25083/100000: episode: 2559, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.102, mean reward: 0.410 [0.327, 0.523], mean action: 36.600 [0.000, 71.000], mean observation: 3.157 [-1.322, 10.284], loss: 1.087842, mae: 4.757726, mean_q: 5.063185
 25084/100000: episode: 2560, duration: 0.023s, episode steps: 1, steps per second: 44, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 43.000 [43.000, 43.000], mean observation: 3.151 [-0.928, 10.453], loss: 1.754032, mae: 4.760013, mean_q: 5.062916
 25094/100000: episode: 2561, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.046, mean reward: 0.405 [0.336, 0.561], mean action: 19.500 [0.000, 70.000], mean observation: 3.153 [-1.885, 10.407], loss: 1.320090, mae: 4.759328, mean_q: 5.058608
 25104/100000: episode: 2562, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 4.330, mean reward: 0.433 [0.353, 0.506], mean action: 33.200 [0.000, 92.000], mean observation: 3.160 [-1.395, 10.499], loss: 0.944465, mae: 4.758183, mean_q: 5.054780
 25114/100000: episode: 2563, duration: 0.268s, episode steps: 10, steps per second: 37, episode reward: 4.299, mean reward: 0.430 [0.373, 0.480], mean action: 12.600 [0.000, 89.000], mean observation: 3.155 [-1.933, 10.288], loss: 1.131422, mae: 4.759672, mean_q: 5.055040
 25124/100000: episode: 2564, duration: 0.215s, episode steps: 10, steps per second: 46, episode reward: 3.958, mean reward: 0.396 [0.316, 0.495], mean action: 18.800 [0.000, 73.000], mean observation: 3.158 [-1.745, 10.336], loss: 1.163372, mae: 4.760118, mean_q: 5.056626
 25134/100000: episode: 2565, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.087, mean reward: 0.409 [0.347, 0.570], mean action: 22.800 [0.000, 84.000], mean observation: 3.160 [-1.275, 10.392], loss: 1.138316, mae: 4.760556, mean_q: 5.059354
 25144/100000: episode: 2566, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.443, mean reward: 0.444 [0.393, 0.523], mean action: 24.900 [0.000, 74.000], mean observation: 3.153 [-1.507, 10.265], loss: 1.178356, mae: 4.760964, mean_q: 5.063176
 25154/100000: episode: 2567, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.810, mean reward: 0.381 [0.354, 0.427], mean action: 34.500 [0.000, 76.000], mean observation: 3.150 [-1.530, 10.189], loss: 0.836972, mae: 4.760356, mean_q: 5.066630
 25164/100000: episode: 2568, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.133, mean reward: 0.413 [0.359, 0.528], mean action: 14.700 [0.000, 52.000], mean observation: 3.159 [-1.323, 10.609], loss: 1.381745, mae: 4.763119, mean_q: 5.068684
 25174/100000: episode: 2569, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.222, mean reward: 0.422 [0.351, 0.510], mean action: 17.800 [0.000, 89.000], mean observation: 3.150 [-1.615, 10.275], loss: 1.196614, mae: 4.762703, mean_q: 5.066738
 25184/100000: episode: 2570, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.744, mean reward: 0.474 [0.397, 0.570], mean action: 22.300 [0.000, 91.000], mean observation: 3.164 [-1.279, 10.296], loss: 1.070766, mae: 4.762505, mean_q: 5.066937
 25194/100000: episode: 2571, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.016, mean reward: 0.402 [0.297, 0.523], mean action: 12.000 [0.000, 52.000], mean observation: 3.157 [-1.216, 10.488], loss: 1.234631, mae: 4.763690, mean_q: 5.069863
 25200/100000: episode: 2572, duration: 0.146s, episode steps: 6, steps per second: 41, episode reward: 12.134, mean reward: 2.022 [0.374, 10.000], mean action: 6.833 [0.000, 22.000], mean observation: 3.161 [-1.739, 10.481], loss: 1.009421, mae: 4.763234, mean_q: 5.072549
 25210/100000: episode: 2573, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.035, mean reward: 0.403 [0.295, 0.478], mean action: 24.600 [0.000, 92.000], mean observation: 3.159 [-1.802, 10.375], loss: 1.297269, mae: 4.764589, mean_q: 5.074385
 25220/100000: episode: 2574, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 4.015, mean reward: 0.402 [0.324, 0.534], mean action: 12.700 [0.000, 94.000], mean observation: 3.159 [-1.416, 10.519], loss: 1.133321, mae: 4.764434, mean_q: 5.076368
 25230/100000: episode: 2575, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.755, mean reward: 0.376 [0.309, 0.460], mean action: 48.600 [0.000, 94.000], mean observation: 3.152 [-1.245, 10.318], loss: 1.220289, mae: 4.765471, mean_q: 5.078800
 25240/100000: episode: 2576, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 3.887, mean reward: 0.389 [0.321, 0.498], mean action: 21.400 [0.000, 67.000], mean observation: 3.150 [-1.092, 10.342], loss: 0.889479, mae: 4.764988, mean_q: 5.082680
 25250/100000: episode: 2577, duration: 0.226s, episode steps: 10, steps per second: 44, episode reward: 4.056, mean reward: 0.406 [0.339, 0.498], mean action: 16.400 [0.000, 52.000], mean observation: 3.155 [-1.522, 10.373], loss: 1.037288, mae: 4.766292, mean_q: 5.083350
 25260/100000: episode: 2578, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.773, mean reward: 0.377 [0.300, 0.573], mean action: 36.500 [0.000, 86.000], mean observation: 3.146 [-1.504, 10.356], loss: 1.078641, mae: 4.767301, mean_q: 5.082936
 25270/100000: episode: 2579, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.246, mean reward: 0.425 [0.377, 0.464], mean action: 24.500 [0.000, 97.000], mean observation: 3.157 [-1.187, 10.508], loss: 1.101266, mae: 4.767960, mean_q: 5.079511
 25280/100000: episode: 2580, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.934, mean reward: 0.393 [0.329, 0.431], mean action: 61.000 [20.000, 78.000], mean observation: 3.157 [-0.741, 10.297], loss: 1.174537, mae: 4.768588, mean_q: 5.074661
 25290/100000: episode: 2581, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.711, mean reward: 0.371 [0.334, 0.500], mean action: 68.800 [52.000, 83.000], mean observation: 3.151 [-2.503, 10.500], loss: 1.308033, mae: 4.769015, mean_q: 5.074388
 25300/100000: episode: 2582, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.389, mean reward: 0.339 [0.301, 0.383], mean action: 60.100 [17.000, 69.000], mean observation: 3.170 [-0.930, 10.310], loss: 1.027163, mae: 4.768106, mean_q: 5.076490
 25310/100000: episode: 2583, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.126, mean reward: 0.413 [0.345, 0.480], mean action: 71.800 [37.000, 100.000], mean observation: 3.154 [-0.843, 10.222], loss: 0.966573, mae: 4.767507, mean_q: 5.078612
 25320/100000: episode: 2584, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 5.089, mean reward: 0.509 [0.509, 0.509], mean action: 61.300 [38.000, 91.000], mean observation: 3.158 [-1.317, 10.360], loss: 1.408569, mae: 4.769396, mean_q: 5.077701
 25330/100000: episode: 2585, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.284, mean reward: 0.428 [0.367, 0.536], mean action: 67.400 [11.000, 101.000], mean observation: 3.150 [-1.420, 10.358], loss: 1.319091, mae: 4.769103, mean_q: 5.075221
 25340/100000: episode: 2586, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.365, mean reward: 0.436 [0.372, 0.542], mean action: 28.900 [0.000, 92.000], mean observation: 3.155 [-1.955, 10.403], loss: 1.204811, mae: 4.768702, mean_q: 5.074392
 25349/100000: episode: 2587, duration: 0.194s, episode steps: 9, steps per second: 46, episode reward: 13.572, mean reward: 1.508 [0.408, 10.000], mean action: 11.889 [0.000, 74.000], mean observation: 3.162 [-1.440, 10.278], loss: 1.165680, mae: 4.768563, mean_q: 5.075449
 25359/100000: episode: 2588, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.242, mean reward: 0.424 [0.379, 0.495], mean action: 18.100 [0.000, 91.000], mean observation: 3.162 [-1.417, 10.263], loss: 1.293951, mae: 4.769739, mean_q: 5.074677
 25369/100000: episode: 2589, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.163, mean reward: 0.416 [0.369, 0.463], mean action: 13.500 [0.000, 56.000], mean observation: 3.152 [-1.743, 10.290], loss: 0.843422, mae: 4.768325, mean_q: 5.076285
 25379/100000: episode: 2590, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.276, mean reward: 0.428 [0.295, 0.507], mean action: 35.100 [0.000, 87.000], mean observation: 3.154 [-1.492, 10.312], loss: 1.084127, mae: 4.769895, mean_q: 5.079112
 25389/100000: episode: 2591, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.629, mean reward: 0.463 [0.385, 0.566], mean action: 20.000 [0.000, 62.000], mean observation: 3.149 [-1.848, 10.273], loss: 1.154152, mae: 4.770728, mean_q: 5.082138
 25399/100000: episode: 2592, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 4.087, mean reward: 0.409 [0.313, 0.465], mean action: 14.200 [0.000, 93.000], mean observation: 3.158 [-1.307, 10.445], loss: 1.534253, mae: 4.772798, mean_q: 5.083805
 25409/100000: episode: 2593, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.277, mean reward: 0.428 [0.346, 0.642], mean action: 10.100 [0.000, 67.000], mean observation: 3.155 [-1.629, 10.282], loss: 1.368332, mae: 4.772170, mean_q: 5.083234
 25419/100000: episode: 2594, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.128, mean reward: 0.413 [0.337, 0.562], mean action: 38.300 [0.000, 94.000], mean observation: 3.158 [-1.698, 10.267], loss: 0.904236, mae: 4.770340, mean_q: 5.082836
 25429/100000: episode: 2595, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.703, mean reward: 0.370 [0.311, 0.478], mean action: 33.100 [0.000, 90.000], mean observation: 3.154 [-1.647, 10.311], loss: 1.017552, mae: 4.771444, mean_q: 5.081792
 25439/100000: episode: 2596, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.014, mean reward: 0.401 [0.307, 0.487], mean action: 34.100 [0.000, 98.000], mean observation: 3.155 [-0.995, 10.225], loss: 1.272210, mae: 4.772687, mean_q: 5.081303
 25449/100000: episode: 2597, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.980, mean reward: 0.398 [0.320, 0.525], mean action: 27.800 [0.000, 76.000], mean observation: 3.169 [-1.335, 10.404], loss: 1.255355, mae: 4.772670, mean_q: 5.079571
 25459/100000: episode: 2598, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 4.068, mean reward: 0.407 [0.330, 0.475], mean action: 9.700 [0.000, 40.000], mean observation: 3.160 [-1.256, 10.364], loss: 1.151070, mae: 4.772520, mean_q: 5.080079
 25469/100000: episode: 2599, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.421, mean reward: 0.442 [0.403, 0.487], mean action: 45.300 [0.000, 98.000], mean observation: 3.162 [-1.425, 10.251], loss: 0.975656, mae: 4.772561, mean_q: 5.077889
 25478/100000: episode: 2600, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 13.008, mean reward: 1.445 [0.295, 10.000], mean action: 31.667 [0.000, 75.000], mean observation: 3.156 [-1.267, 10.451], loss: 1.112491, mae: 4.773489, mean_q: 5.079131
 25488/100000: episode: 2601, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.970, mean reward: 0.397 [0.340, 0.464], mean action: 8.800 [0.000, 73.000], mean observation: 3.153 [-1.496, 10.149], loss: 1.093118, mae: 4.774011, mean_q: 5.082359
 25498/100000: episode: 2602, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.788, mean reward: 0.479 [0.391, 0.533], mean action: 38.800 [0.000, 100.000], mean observation: 3.163 [-1.660, 10.320], loss: 1.162295, mae: 4.774805, mean_q: 5.085604
 25508/100000: episode: 2603, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.236, mean reward: 0.424 [0.350, 0.514], mean action: 10.600 [0.000, 47.000], mean observation: 3.151 [-1.486, 10.172], loss: 1.198202, mae: 4.775264, mean_q: 5.088580
 25518/100000: episode: 2604, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.127, mean reward: 0.413 [0.356, 0.498], mean action: 32.400 [0.000, 98.000], mean observation: 3.143 [-1.516, 10.223], loss: 0.815606, mae: 4.774366, mean_q: 5.092626
 25528/100000: episode: 2605, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.852, mean reward: 0.385 [0.321, 0.505], mean action: 51.800 [0.000, 101.000], mean observation: 3.144 [-1.605, 10.361], loss: 1.302199, mae: 4.776372, mean_q: 5.094745
 25538/100000: episode: 2606, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.990, mean reward: 0.399 [0.331, 0.545], mean action: 34.600 [0.000, 94.000], mean observation: 3.151 [-1.475, 10.330], loss: 1.493656, mae: 4.777296, mean_q: 5.093487
 25548/100000: episode: 2607, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.969, mean reward: 0.397 [0.348, 0.450], mean action: 53.200 [0.000, 100.000], mean observation: 3.141 [-1.714, 10.272], loss: 0.934617, mae: 4.775565, mean_q: 5.087749
 25557/100000: episode: 2608, duration: 0.144s, episode steps: 9, steps per second: 62, episode reward: 13.612, mean reward: 1.512 [0.331, 10.000], mean action: 43.333 [0.000, 101.000], mean observation: 3.150 [-0.833, 10.356], loss: 1.385088, mae: 4.777483, mean_q: 5.084120
 25567/100000: episode: 2609, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 3.921, mean reward: 0.392 [0.272, 0.452], mean action: 22.400 [0.000, 68.000], mean observation: 3.153 [-1.671, 10.456], loss: 1.114946, mae: 4.776783, mean_q: 5.084610
 25577/100000: episode: 2610, duration: 0.226s, episode steps: 10, steps per second: 44, episode reward: 3.844, mean reward: 0.384 [0.316, 0.545], mean action: 13.100 [0.000, 101.000], mean observation: 3.153 [-1.715, 10.224], loss: 1.279254, mae: 4.777301, mean_q: 5.086855
 25587/100000: episode: 2611, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.025, mean reward: 0.402 [0.332, 0.452], mean action: 24.800 [0.000, 92.000], mean observation: 3.151 [-1.611, 10.276], loss: 1.149612, mae: 4.777070, mean_q: 5.088227
 25597/100000: episode: 2612, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.565, mean reward: 0.457 [0.350, 0.579], mean action: 31.800 [0.000, 99.000], mean observation: 3.159 [-2.272, 10.375], loss: 1.005565, mae: 4.777199, mean_q: 5.090331
 25607/100000: episode: 2613, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.087, mean reward: 0.409 [0.315, 0.520], mean action: 14.800 [0.000, 100.000], mean observation: 3.148 [-1.754, 10.276], loss: 1.028915, mae: 4.777521, mean_q: 5.086586
 25617/100000: episode: 2614, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.897, mean reward: 0.390 [0.372, 0.427], mean action: 54.700 [18.000, 69.000], mean observation: 3.154 [-1.194, 10.292], loss: 1.230309, mae: 4.778959, mean_q: 5.082667
 25627/100000: episode: 2615, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.147, mean reward: 0.415 [0.323, 0.510], mean action: 59.700 [1.000, 91.000], mean observation: 3.166 [-0.931, 10.263], loss: 1.102188, mae: 4.778938, mean_q: 5.086676
 25637/100000: episode: 2616, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.855, mean reward: 0.385 [0.312, 0.446], mean action: 41.400 [4.000, 69.000], mean observation: 3.149 [-1.790, 10.150], loss: 1.199443, mae: 4.779472, mean_q: 5.089824
 25647/100000: episode: 2617, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.865, mean reward: 0.386 [0.348, 0.464], mean action: 51.100 [16.000, 69.000], mean observation: 3.154 [-1.014, 10.277], loss: 1.261026, mae: 4.779679, mean_q: 5.088532
 25657/100000: episode: 2618, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.247, mean reward: 0.425 [0.316, 0.571], mean action: 50.400 [0.000, 92.000], mean observation: 3.157 [-1.219, 10.306], loss: 1.273644, mae: 4.780354, mean_q: 5.087653
 25667/100000: episode: 2619, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.929, mean reward: 0.393 [0.348, 0.566], mean action: 57.200 [20.000, 87.000], mean observation: 3.149 [-1.386, 10.325], loss: 1.125437, mae: 4.780004, mean_q: 5.086380
 25677/100000: episode: 2620, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.739, mean reward: 0.374 [0.329, 0.504], mean action: 55.500 [12.000, 99.000], mean observation: 3.161 [-1.463, 10.292], loss: 1.177940, mae: 4.780361, mean_q: 5.085595
 25678/100000: episode: 2621, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 66.000 [66.000, 66.000], mean observation: 3.181 [-0.960, 10.713], loss: 1.369731, mae: 4.781128, mean_q: 5.086091
 25688/100000: episode: 2622, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.671, mean reward: 0.367 [0.296, 0.484], mean action: 44.600 [0.000, 88.000], mean observation: 3.148 [-1.970, 10.386], loss: 0.969499, mae: 4.780072, mean_q: 5.085103
 25698/100000: episode: 2623, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.018, mean reward: 0.402 [0.354, 0.539], mean action: 18.700 [0.000, 72.000], mean observation: 3.156 [-1.570, 10.463], loss: 1.015316, mae: 4.780863, mean_q: 5.087960
 25708/100000: episode: 2624, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 3.964, mean reward: 0.396 [0.333, 0.536], mean action: 22.000 [0.000, 84.000], mean observation: 3.157 [-1.213, 10.260], loss: 1.080845, mae: 4.781846, mean_q: 5.087904
 25718/100000: episode: 2625, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 5.039, mean reward: 0.504 [0.339, 0.566], mean action: 46.000 [0.000, 98.000], mean observation: 3.157 [-1.609, 10.239], loss: 1.139653, mae: 4.782838, mean_q: 5.088834
 25728/100000: episode: 2626, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.239, mean reward: 0.424 [0.314, 0.505], mean action: 30.500 [0.000, 75.000], mean observation: 3.150 [-1.414, 10.435], loss: 1.199426, mae: 4.783398, mean_q: 5.090392
 25738/100000: episode: 2627, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.424, mean reward: 0.442 [0.338, 0.549], mean action: 22.000 [0.000, 87.000], mean observation: 3.152 [-1.570, 10.291], loss: 0.939294, mae: 4.783231, mean_q: 5.082944
 25748/100000: episode: 2628, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.942, mean reward: 0.394 [0.329, 0.522], mean action: 23.000 [0.000, 68.000], mean observation: 3.148 [-1.314, 10.252], loss: 1.302746, mae: 4.784789, mean_q: 5.081210
 25758/100000: episode: 2629, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.182, mean reward: 0.418 [0.366, 0.531], mean action: 26.300 [10.000, 85.000], mean observation: 3.154 [-1.938, 10.320], loss: 0.932083, mae: 4.783805, mean_q: 5.082038
 25768/100000: episode: 2630, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.343, mean reward: 0.434 [0.347, 0.495], mean action: 42.200 [1.000, 100.000], mean observation: 3.158 [-1.945, 10.373], loss: 0.996036, mae: 4.784513, mean_q: 5.082950
 25778/100000: episode: 2631, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.209, mean reward: 0.421 [0.333, 0.484], mean action: 34.300 [7.000, 92.000], mean observation: 3.149 [-1.405, 10.358], loss: 1.334407, mae: 4.786215, mean_q: 5.083967
 25788/100000: episode: 2632, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.065, mean reward: 0.407 [0.369, 0.481], mean action: 40.100 [19.000, 91.000], mean observation: 3.149 [-1.326, 10.423], loss: 0.978391, mae: 4.785189, mean_q: 5.084722
 25798/100000: episode: 2633, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.121, mean reward: 0.412 [0.308, 0.469], mean action: 38.500 [21.000, 91.000], mean observation: 3.157 [-1.496, 10.352], loss: 1.394039, mae: 4.787372, mean_q: 5.085306
 25808/100000: episode: 2634, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.928, mean reward: 0.393 [0.331, 0.485], mean action: 35.800 [21.000, 92.000], mean observation: 3.153 [-1.351, 10.327], loss: 1.405532, mae: 4.788089, mean_q: 5.086319
 25818/100000: episode: 2635, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.883, mean reward: 0.488 [0.441, 0.509], mean action: 29.700 [9.000, 75.000], mean observation: 3.163 [-1.978, 10.320], loss: 1.201849, mae: 4.787108, mean_q: 5.087686
 25828/100000: episode: 2636, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.007, mean reward: 0.401 [0.298, 0.516], mean action: 46.800 [21.000, 83.000], mean observation: 3.163 [-1.671, 10.391], loss: 1.119774, mae: 4.787139, mean_q: 5.089681
 25838/100000: episode: 2637, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.486, mean reward: 0.449 [0.393, 0.589], mean action: 48.100 [21.000, 101.000], mean observation: 3.152 [-1.205, 10.247], loss: 1.019655, mae: 4.787084, mean_q: 5.091037
 25848/100000: episode: 2638, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.061, mean reward: 0.406 [0.341, 0.427], mean action: 49.800 [4.000, 98.000], mean observation: 3.153 [-2.290, 10.428], loss: 1.055352, mae: 4.787475, mean_q: 5.091989
 25858/100000: episode: 2639, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.138, mean reward: 0.414 [0.324, 0.482], mean action: 38.900 [9.000, 96.000], mean observation: 3.153 [-1.399, 10.328], loss: 0.876723, mae: 4.787512, mean_q: 5.093339
 25868/100000: episode: 2640, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.210, mean reward: 0.421 [0.367, 0.522], mean action: 34.000 [12.000, 76.000], mean observation: 3.153 [-1.284, 10.452], loss: 1.494820, mae: 4.790360, mean_q: 5.095027
 25878/100000: episode: 2641, duration: 0.223s, episode steps: 10, steps per second: 45, episode reward: 3.994, mean reward: 0.399 [0.344, 0.502], mean action: 19.200 [3.000, 35.000], mean observation: 3.155 [-2.060, 10.321], loss: 1.060311, mae: 4.788464, mean_q: 5.096419
 25888/100000: episode: 2642, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 4.031, mean reward: 0.403 [0.389, 0.466], mean action: 28.800 [21.000, 51.000], mean observation: 3.166 [-1.394, 10.318], loss: 1.043998, mae: 4.788659, mean_q: 5.097960
 25898/100000: episode: 2643, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.992, mean reward: 0.499 [0.344, 0.516], mean action: 45.100 [3.000, 98.000], mean observation: 3.173 [-1.656, 10.396], loss: 1.080961, mae: 4.789464, mean_q: 5.099795
 25908/100000: episode: 2644, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.984, mean reward: 0.398 [0.387, 0.459], mean action: 38.300 [21.000, 82.000], mean observation: 3.160 [-1.682, 10.330], loss: 1.240381, mae: 4.790144, mean_q: 5.102293
 25918/100000: episode: 2645, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.739, mean reward: 0.474 [0.469, 0.509], mean action: 28.400 [21.000, 71.000], mean observation: 3.160 [-1.467, 10.369], loss: 1.172257, mae: 4.790281, mean_q: 5.104597
 25928/100000: episode: 2646, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.421, mean reward: 0.442 [0.371, 0.493], mean action: 28.900 [8.000, 67.000], mean observation: 3.159 [-1.171, 10.374], loss: 1.096492, mae: 4.790544, mean_q: 5.106923
 25938/100000: episode: 2647, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.245, mean reward: 0.425 [0.394, 0.495], mean action: 34.600 [9.000, 92.000], mean observation: 3.157 [-1.622, 10.401], loss: 1.072895, mae: 4.790768, mean_q: 5.109768
 25948/100000: episode: 2648, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.044, mean reward: 0.404 [0.327, 0.493], mean action: 27.500 [0.000, 63.000], mean observation: 3.156 [-1.068, 10.339], loss: 1.400614, mae: 4.792357, mean_q: 5.114149
 25958/100000: episode: 2649, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.214, mean reward: 0.421 [0.321, 0.500], mean action: 24.400 [1.000, 90.000], mean observation: 3.152 [-1.609, 10.430], loss: 0.922399, mae: 4.790930, mean_q: 5.116871
 25968/100000: episode: 2650, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.051, mean reward: 0.405 [0.346, 0.491], mean action: 31.400 [4.000, 61.000], mean observation: 3.161 [-1.279, 10.370], loss: 1.009949, mae: 4.791836, mean_q: 5.120447
 25978/100000: episode: 2651, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.017, mean reward: 0.402 [0.327, 0.510], mean action: 29.200 [21.000, 54.000], mean observation: 3.148 [-1.773, 10.290], loss: 1.253522, mae: 4.793519, mean_q: 5.120725
 25988/100000: episode: 2652, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.139, mean reward: 0.414 [0.258, 0.550], mean action: 27.900 [2.000, 79.000], mean observation: 3.145 [-1.351, 10.260], loss: 1.188624, mae: 4.793668, mean_q: 5.117219
 25998/100000: episode: 2653, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.037, mean reward: 0.404 [0.300, 0.587], mean action: 30.500 [21.000, 76.000], mean observation: 3.154 [-2.164, 10.346], loss: 1.169431, mae: 4.794458, mean_q: 5.116444
 26008/100000: episode: 2654, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.870, mean reward: 0.387 [0.335, 0.425], mean action: 42.200 [12.000, 90.000], mean observation: 3.160 [-1.105, 10.285], loss: 1.333229, mae: 4.795554, mean_q: 5.117389
 26018/100000: episode: 2655, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.989, mean reward: 0.399 [0.380, 0.452], mean action: 39.800 [21.000, 91.000], mean observation: 3.167 [-1.079, 10.292], loss: 1.369009, mae: 4.795780, mean_q: 5.108937
 26028/100000: episode: 2656, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 3.711, mean reward: 0.371 [0.313, 0.447], mean action: 29.600 [4.000, 96.000], mean observation: 3.156 [-1.666, 10.334], loss: 1.191123, mae: 4.795063, mean_q: 5.103310
 26038/100000: episode: 2657, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.900, mean reward: 0.390 [0.352, 0.445], mean action: 58.900 [12.000, 100.000], mean observation: 3.156 [-1.769, 10.293], loss: 1.292243, mae: 4.795519, mean_q: 5.102353
 26048/100000: episode: 2658, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.713, mean reward: 0.471 [0.388, 0.577], mean action: 36.300 [4.000, 96.000], mean observation: 3.144 [-1.348, 10.479], loss: 1.477075, mae: 4.796533, mean_q: 5.101614
 26058/100000: episode: 2659, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.869, mean reward: 0.387 [0.321, 0.483], mean action: 36.700 [22.000, 90.000], mean observation: 3.146 [-1.738, 10.335], loss: 1.059998, mae: 4.794831, mean_q: 5.097832
 26068/100000: episode: 2660, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.251, mean reward: 0.425 [0.354, 0.592], mean action: 49.500 [0.000, 101.000], mean observation: 3.164 [-1.029, 10.521], loss: 1.248625, mae: 4.795821, mean_q: 5.097273
 26078/100000: episode: 2661, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.506, mean reward: 0.351 [0.312, 0.445], mean action: 40.900 [21.000, 79.000], mean observation: 3.149 [-2.218, 10.381], loss: 1.148670, mae: 4.795505, mean_q: 5.099176
 26088/100000: episode: 2662, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.231, mean reward: 0.423 [0.288, 0.558], mean action: 37.500 [7.000, 91.000], mean observation: 3.151 [-0.918, 10.252], loss: 1.057107, mae: 4.795588, mean_q: 5.096375
 26098/100000: episode: 2663, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.681, mean reward: 0.368 [0.345, 0.473], mean action: 43.800 [17.000, 101.000], mean observation: 3.150 [-1.119, 10.306], loss: 1.199671, mae: 4.796714, mean_q: 5.094350
 26108/100000: episode: 2664, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.937, mean reward: 0.394 [0.341, 0.511], mean action: 54.300 [22.000, 99.000], mean observation: 3.146 [-1.277, 10.273], loss: 1.035909, mae: 4.796085, mean_q: 5.094635
 26118/100000: episode: 2665, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.388, mean reward: 0.439 [0.422, 0.501], mean action: 29.400 [12.000, 93.000], mean observation: 3.153 [-1.701, 10.267], loss: 1.140903, mae: 4.796885, mean_q: 5.093013
 26128/100000: episode: 2666, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.174, mean reward: 0.417 [0.316, 0.483], mean action: 24.000 [0.000, 96.000], mean observation: 3.154 [-1.509, 10.286], loss: 0.935459, mae: 4.796741, mean_q: 5.089320
 26138/100000: episode: 2667, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.377, mean reward: 0.438 [0.306, 0.530], mean action: 51.900 [22.000, 98.000], mean observation: 3.152 [-1.244, 10.284], loss: 1.551585, mae: 4.799731, mean_q: 5.089025
 26148/100000: episode: 2668, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.066, mean reward: 0.407 [0.289, 0.510], mean action: 29.700 [2.000, 94.000], mean observation: 3.148 [-1.680, 10.305], loss: 1.197582, mae: 4.798674, mean_q: 5.089957
 26158/100000: episode: 2669, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.172, mean reward: 0.417 [0.349, 0.475], mean action: 24.400 [0.000, 85.000], mean observation: 3.156 [-1.209, 10.488], loss: 1.041226, mae: 4.798036, mean_q: 5.093169
 26168/100000: episode: 2670, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.127, mean reward: 0.413 [0.350, 0.511], mean action: 28.700 [0.000, 65.000], mean observation: 3.146 [-1.017, 10.231], loss: 1.254377, mae: 4.799516, mean_q: 5.095104
 26178/100000: episode: 2671, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.278, mean reward: 0.428 [0.384, 0.544], mean action: 31.400 [2.000, 87.000], mean observation: 3.157 [-1.822, 10.347], loss: 1.082439, mae: 4.799184, mean_q: 5.096622
 26188/100000: episode: 2672, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.121, mean reward: 0.412 [0.384, 0.525], mean action: 29.600 [0.000, 82.000], mean observation: 3.166 [-1.819, 10.413], loss: 1.144640, mae: 4.799671, mean_q: 5.098127
 26198/100000: episode: 2673, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.877, mean reward: 0.388 [0.344, 0.439], mean action: 30.500 [19.000, 75.000], mean observation: 3.160 [-1.296, 10.362], loss: 1.179418, mae: 4.800359, mean_q: 5.099185
 26208/100000: episode: 2674, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.056, mean reward: 0.406 [0.347, 0.501], mean action: 34.500 [0.000, 72.000], mean observation: 3.158 [-1.317, 10.275], loss: 1.293706, mae: 4.801064, mean_q: 5.097602
 26218/100000: episode: 2675, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.147, mean reward: 0.415 [0.332, 0.574], mean action: 28.400 [8.000, 82.000], mean observation: 3.149 [-1.905, 10.274], loss: 1.146532, mae: 4.800525, mean_q: 5.098805
 26228/100000: episode: 2676, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 4.235, mean reward: 0.423 [0.298, 0.555], mean action: 18.000 [0.000, 97.000], mean observation: 3.155 [-1.909, 10.503], loss: 1.298281, mae: 4.801177, mean_q: 5.097295
 26238/100000: episode: 2677, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.521, mean reward: 0.452 [0.392, 0.516], mean action: 22.900 [0.000, 72.000], mean observation: 3.153 [-1.325, 10.259], loss: 1.607000, mae: 4.802762, mean_q: 5.097519
 26248/100000: episode: 2678, duration: 0.153s, episode steps: 10, steps per second: 66, episode reward: 3.796, mean reward: 0.380 [0.333, 0.449], mean action: 51.000 [10.000, 98.000], mean observation: 3.154 [-1.100, 10.292], loss: 0.808885, mae: 4.799275, mean_q: 5.099546
 26258/100000: episode: 2679, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.972, mean reward: 0.397 [0.300, 0.472], mean action: 42.500 [9.000, 88.000], mean observation: 3.159 [-1.484, 10.505], loss: 1.145010, mae: 4.801131, mean_q: 5.098422
 26268/100000: episode: 2680, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.087, mean reward: 0.409 [0.339, 0.474], mean action: 54.200 [34.000, 91.000], mean observation: 3.157 [-1.382, 10.300], loss: 1.376164, mae: 4.802295, mean_q: 5.097499
 26278/100000: episode: 2681, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.044, mean reward: 0.404 [0.345, 0.540], mean action: 30.400 [0.000, 70.000], mean observation: 3.158 [-1.623, 10.343], loss: 1.076819, mae: 4.801068, mean_q: 5.098518
 26288/100000: episode: 2682, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.458, mean reward: 0.446 [0.362, 0.501], mean action: 24.600 [0.000, 81.000], mean observation: 3.160 [-1.475, 10.377], loss: 1.092847, mae: 4.801849, mean_q: 5.101298
 26298/100000: episode: 2683, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 4.295, mean reward: 0.429 [0.319, 0.532], mean action: 20.800 [0.000, 99.000], mean observation: 3.153 [-1.302, 10.302], loss: 1.290656, mae: 4.802617, mean_q: 5.099699
 26308/100000: episode: 2684, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.878, mean reward: 0.388 [0.329, 0.477], mean action: 40.800 [0.000, 93.000], mean observation: 3.159 [-1.086, 10.198], loss: 1.130138, mae: 4.802145, mean_q: 5.098586
 26318/100000: episode: 2685, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.688, mean reward: 0.369 [0.275, 0.443], mean action: 26.500 [0.000, 100.000], mean observation: 3.157 [-1.322, 10.479], loss: 1.332506, mae: 4.803113, mean_q: 5.100497
 26328/100000: episode: 2686, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.847, mean reward: 0.385 [0.302, 0.422], mean action: 27.700 [0.000, 84.000], mean observation: 3.153 [-1.099, 10.300], loss: 1.364196, mae: 4.803283, mean_q: 5.103173
 26338/100000: episode: 2687, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.171, mean reward: 0.417 [0.345, 0.508], mean action: 20.100 [0.000, 54.000], mean observation: 3.150 [-1.575, 10.542], loss: 1.337332, mae: 4.803411, mean_q: 5.103147
 26348/100000: episode: 2688, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 4.306, mean reward: 0.431 [0.361, 0.478], mean action: 10.000 [0.000, 71.000], mean observation: 3.156 [-1.230, 10.356], loss: 1.504947, mae: 4.804091, mean_q: 5.101471
 26358/100000: episode: 2689, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.218, mean reward: 0.422 [0.411, 0.469], mean action: 46.900 [0.000, 94.000], mean observation: 3.164 [-1.019, 10.336], loss: 1.594253, mae: 4.804315, mean_q: 5.099160
 26368/100000: episode: 2690, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.019, mean reward: 0.402 [0.322, 0.470], mean action: 35.300 [19.000, 52.000], mean observation: 3.156 [-1.885, 10.284], loss: 1.348918, mae: 4.803235, mean_q: 5.101708
 26378/100000: episode: 2691, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.391, mean reward: 0.439 [0.397, 0.586], mean action: 34.400 [1.000, 62.000], mean observation: 3.156 [-1.407, 10.347], loss: 1.381750, mae: 4.803742, mean_q: 5.107728
 26388/100000: episode: 2692, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.925, mean reward: 0.392 [0.339, 0.432], mean action: 40.000 [36.000, 68.000], mean observation: 3.155 [-1.059, 10.248], loss: 1.181781, mae: 4.802892, mean_q: 5.110613
 26398/100000: episode: 2693, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.788, mean reward: 0.379 [0.323, 0.472], mean action: 45.300 [36.000, 89.000], mean observation: 3.149 [-1.155, 10.224], loss: 1.372746, mae: 4.803576, mean_q: 5.112514
 26399/100000: episode: 2694, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 4.000 [4.000, 4.000], mean observation: 3.159 [-1.028, 10.695], loss: 0.804082, mae: 4.801472, mean_q: 5.109775
 26409/100000: episode: 2695, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 5.081, mean reward: 0.508 [0.508, 0.508], mean action: 60.400 [36.000, 100.000], mean observation: 3.137 [-1.090, 10.230], loss: 1.128796, mae: 4.802938, mean_q: 5.104528
 26419/100000: episode: 2696, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.581, mean reward: 0.358 [0.338, 0.381], mean action: 56.200 [30.000, 99.000], mean observation: 3.156 [-1.395, 10.430], loss: 1.152602, mae: 4.803325, mean_q: 5.096064
 26429/100000: episode: 2697, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.208, mean reward: 0.421 [0.375, 0.466], mean action: 35.300 [2.000, 71.000], mean observation: 3.154 [-1.985, 10.336], loss: 1.328787, mae: 4.804529, mean_q: 5.091436
 26439/100000: episode: 2698, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.702, mean reward: 0.370 [0.272, 0.462], mean action: 31.200 [10.000, 36.000], mean observation: 3.147 [-1.213, 10.390], loss: 1.108951, mae: 4.803553, mean_q: 5.088311
 26449/100000: episode: 2699, duration: 0.215s, episode steps: 10, steps per second: 46, episode reward: 4.296, mean reward: 0.430 [0.303, 0.516], mean action: 13.900 [0.000, 59.000], mean observation: 3.161 [-1.176, 10.319], loss: 1.502750, mae: 4.805358, mean_q: 5.089087
 26459/100000: episode: 2700, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.103, mean reward: 0.410 [0.309, 0.513], mean action: 29.600 [0.000, 96.000], mean observation: 3.161 [-1.438, 10.296], loss: 1.345908, mae: 4.804446, mean_q: 5.091420
 26469/100000: episode: 2701, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.245, mean reward: 0.424 [0.356, 0.485], mean action: 24.100 [0.000, 83.000], mean observation: 3.149 [-1.444, 10.381], loss: 0.927113, mae: 4.802594, mean_q: 5.088052
 26479/100000: episode: 2702, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.057, mean reward: 0.406 [0.356, 0.455], mean action: 31.800 [12.000, 82.000], mean observation: 3.158 [-1.272, 10.270], loss: 0.913392, mae: 4.802920, mean_q: 5.090575
 26489/100000: episode: 2703, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.329, mean reward: 0.433 [0.336, 0.563], mean action: 32.400 [12.000, 76.000], mean observation: 3.148 [-1.578, 10.346], loss: 1.381501, mae: 4.805662, mean_q: 5.088140
 26499/100000: episode: 2704, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 3.997, mean reward: 0.400 [0.364, 0.528], mean action: 28.200 [0.000, 78.000], mean observation: 3.148 [-1.462, 10.338], loss: 1.235090, mae: 4.805308, mean_q: 5.089676
 26509/100000: episode: 2705, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.141, mean reward: 0.414 [0.330, 0.558], mean action: 32.900 [0.000, 95.000], mean observation: 3.149 [-1.390, 10.368], loss: 1.430358, mae: 4.806243, mean_q: 5.091094
 26519/100000: episode: 2706, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.992, mean reward: 0.399 [0.388, 0.477], mean action: 74.300 [28.000, 85.000], mean observation: 3.155 [-1.018, 10.360], loss: 1.234490, mae: 4.805723, mean_q: 5.092277
 26529/100000: episode: 2707, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.108, mean reward: 0.411 [0.296, 0.456], mean action: 45.600 [12.000, 86.000], mean observation: 3.161 [-1.634, 10.250], loss: 1.219846, mae: 4.805749, mean_q: 5.094156
 26539/100000: episode: 2708, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.881, mean reward: 0.488 [0.482, 0.543], mean action: 45.300 [12.000, 90.000], mean observation: 3.161 [-1.179, 10.339], loss: 1.227146, mae: 4.805869, mean_q: 5.095799
 26549/100000: episode: 2709, duration: 0.223s, episode steps: 10, steps per second: 45, episode reward: 3.769, mean reward: 0.377 [0.343, 0.451], mean action: 19.200 [4.000, 60.000], mean observation: 3.158 [-1.468, 10.282], loss: 1.402612, mae: 4.806534, mean_q: 5.097669
 26559/100000: episode: 2710, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.919, mean reward: 0.392 [0.326, 0.515], mean action: 68.300 [22.000, 88.000], mean observation: 3.152 [-1.506, 10.370], loss: 1.067409, mae: 4.805367, mean_q: 5.097998
 26569/100000: episode: 2711, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.699, mean reward: 0.470 [0.341, 0.536], mean action: 73.800 [13.000, 85.000], mean observation: 3.166 [-0.943, 10.272], loss: 0.983190, mae: 4.805053, mean_q: 5.102233
 26579/100000: episode: 2712, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.358, mean reward: 0.436 [0.380, 0.517], mean action: 43.200 [0.000, 85.000], mean observation: 3.170 [-1.652, 10.345], loss: 0.993223, mae: 4.805626, mean_q: 5.108513
 26589/100000: episode: 2713, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.689, mean reward: 0.369 [0.312, 0.445], mean action: 71.300 [17.000, 99.000], mean observation: 3.143 [-0.854, 10.175], loss: 1.373949, mae: 4.807441, mean_q: 5.111149
 26599/100000: episode: 2714, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.447, mean reward: 0.445 [0.330, 0.520], mean action: 51.500 [8.000, 85.000], mean observation: 3.150 [-1.458, 10.270], loss: 1.296610, mae: 4.807316, mean_q: 5.112240
 26609/100000: episode: 2715, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.280, mean reward: 0.428 [0.424, 0.446], mean action: 64.500 [9.000, 85.000], mean observation: 3.161 [-2.161, 10.275], loss: 1.078979, mae: 4.806673, mean_q: 5.113695
 26619/100000: episode: 2716, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.380, mean reward: 0.438 [0.316, 0.493], mean action: 63.700 [21.000, 88.000], mean observation: 3.149 [-1.536, 10.252], loss: 1.038117, mae: 4.806977, mean_q: 5.115400
 26629/100000: episode: 2717, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 3.221, mean reward: 0.322 [0.296, 0.386], mean action: 63.800 [24.000, 98.000], mean observation: 3.156 [-1.189, 10.299], loss: 1.462535, mae: 4.808867, mean_q: 5.116396
 26639/100000: episode: 2718, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.064, mean reward: 0.406 [0.375, 0.465], mean action: 53.800 [12.000, 85.000], mean observation: 3.156 [-1.124, 10.486], loss: 0.914767, mae: 4.807135, mean_q: 5.112748
 26649/100000: episode: 2719, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 4.450, mean reward: 0.445 [0.366, 0.454], mean action: 78.800 [4.000, 99.000], mean observation: 3.155 [-1.215, 10.354], loss: 1.224836, mae: 4.808777, mean_q: 5.111189
 26659/100000: episode: 2720, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 3.744, mean reward: 0.374 [0.361, 0.399], mean action: 72.600 [14.000, 101.000], mean observation: 3.162 [-0.939, 10.297], loss: 0.833642, mae: 4.807876, mean_q: 5.109767
 26669/100000: episode: 2721, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 3.771, mean reward: 0.377 [0.364, 0.432], mean action: 74.600 [46.000, 85.000], mean observation: 3.173 [-1.229, 10.259], loss: 1.187050, mae: 4.809473, mean_q: 5.106191
 26679/100000: episode: 2722, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.752, mean reward: 0.375 [0.333, 0.429], mean action: 53.100 [12.000, 89.000], mean observation: 3.155 [-2.019, 10.372], loss: 1.246696, mae: 4.810285, mean_q: 5.106595
 26689/100000: episode: 2723, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.893, mean reward: 0.389 [0.332, 0.456], mean action: 66.100 [22.000, 85.000], mean observation: 3.147 [-1.276, 10.303], loss: 0.994757, mae: 4.809316, mean_q: 5.104975
 26699/100000: episode: 2724, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.785, mean reward: 0.479 [0.474, 0.495], mean action: 76.000 [34.000, 95.000], mean observation: 3.147 [-1.532, 10.363], loss: 1.013878, mae: 4.809903, mean_q: 5.101478
 26709/100000: episode: 2725, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.739, mean reward: 0.474 [0.446, 0.492], mean action: 71.500 [7.000, 100.000], mean observation: 3.150 [-1.178, 10.462], loss: 0.799299, mae: 4.809344, mean_q: 5.101535
 26719/100000: episode: 2726, duration: 0.114s, episode steps: 10, steps per second: 87, episode reward: 4.245, mean reward: 0.425 [0.413, 0.451], mean action: 78.000 [59.000, 94.000], mean observation: 3.164 [-1.718, 10.336], loss: 0.979363, mae: 4.810987, mean_q: 5.102733
 26729/100000: episode: 2727, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.852, mean reward: 0.385 [0.333, 0.428], mean action: 62.300 [4.000, 88.000], mean observation: 3.154 [-1.156, 10.267], loss: 1.104210, mae: 4.811866, mean_q: 5.104010
 26739/100000: episode: 2728, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.213, mean reward: 0.421 [0.361, 0.472], mean action: 78.800 [24.000, 94.000], mean observation: 3.136 [-1.021, 10.338], loss: 1.311491, mae: 4.812928, mean_q: 5.104933
 26749/100000: episode: 2729, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.352, mean reward: 0.435 [0.312, 0.466], mean action: 55.600 [1.000, 85.000], mean observation: 3.179 [-1.175, 10.333], loss: 1.295273, mae: 4.812947, mean_q: 5.103230
 26759/100000: episode: 2730, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.471, mean reward: 0.447 [0.390, 0.569], mean action: 55.000 [14.000, 93.000], mean observation: 3.153 [-1.581, 10.359], loss: 1.022455, mae: 4.812412, mean_q: 5.100750
 26769/100000: episode: 2731, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.079, mean reward: 0.408 [0.365, 0.520], mean action: 66.100 [15.000, 85.000], mean observation: 3.163 [-1.434, 10.349], loss: 1.219863, mae: 4.813840, mean_q: 5.101058
 26779/100000: episode: 2732, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 5.016, mean reward: 0.502 [0.502, 0.502], mean action: 60.000 [4.000, 85.000], mean observation: 3.151 [-1.281, 10.354], loss: 1.497405, mae: 4.814902, mean_q: 5.102870
 26789/100000: episode: 2733, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.368, mean reward: 0.437 [0.400, 0.504], mean action: 58.600 [9.000, 85.000], mean observation: 3.171 [-1.612, 10.423], loss: 1.329010, mae: 4.814763, mean_q: 5.103997
 26799/100000: episode: 2734, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.581, mean reward: 0.358 [0.327, 0.455], mean action: 61.300 [2.000, 85.000], mean observation: 3.174 [-1.221, 10.317], loss: 1.325336, mae: 4.814715, mean_q: 5.101016
 26809/100000: episode: 2735, duration: 0.109s, episode steps: 10, steps per second: 91, episode reward: 4.559, mean reward: 0.456 [0.441, 0.496], mean action: 79.100 [36.000, 97.000], mean observation: 3.154 [-1.496, 10.387], loss: 1.249836, mae: 4.814676, mean_q: 5.101477
 26819/100000: episode: 2736, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.510, mean reward: 0.451 [0.378, 0.476], mean action: 70.400 [1.000, 85.000], mean observation: 3.140 [-1.112, 10.320], loss: 1.078993, mae: 4.813865, mean_q: 5.102571
 26829/100000: episode: 2737, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.322, mean reward: 0.432 [0.339, 0.503], mean action: 82.800 [38.000, 101.000], mean observation: 3.124 [-0.732, 10.193], loss: 1.233793, mae: 4.815126, mean_q: 5.102405
 26839/100000: episode: 2738, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 5.779, mean reward: 0.578 [0.578, 0.578], mean action: 74.300 [28.000, 99.000], mean observation: 3.145 [-0.961, 10.336], loss: 1.157923, mae: 4.814804, mean_q: 5.099139
 26849/100000: episode: 2739, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 3.743, mean reward: 0.374 [0.321, 0.426], mean action: 64.900 [22.000, 89.000], mean observation: 3.145 [-1.600, 10.353], loss: 1.414445, mae: 4.816129, mean_q: 5.098365
 26859/100000: episode: 2740, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.918, mean reward: 0.492 [0.492, 0.492], mean action: 75.800 [2.000, 97.000], mean observation: 3.147 [-1.112, 10.297], loss: 0.919533, mae: 4.814709, mean_q: 5.099450
 26869/100000: episode: 2741, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.910, mean reward: 0.391 [0.391, 0.391], mean action: 73.800 [39.000, 85.000], mean observation: 3.146 [-1.192, 10.220], loss: 1.054981, mae: 4.815545, mean_q: 5.102241
 26879/100000: episode: 2742, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.992, mean reward: 0.399 [0.386, 0.431], mean action: 68.100 [18.000, 93.000], mean observation: 3.161 [-1.712, 10.364], loss: 1.314980, mae: 4.817077, mean_q: 5.105117
 26889/100000: episode: 2743, duration: 0.102s, episode steps: 10, steps per second: 98, episode reward: 4.601, mean reward: 0.460 [0.459, 0.472], mean action: 88.700 [77.000, 101.000], mean observation: 3.164 [-1.568, 10.356], loss: 0.907170, mae: 4.816104, mean_q: 5.107261
 26899/100000: episode: 2744, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.875, mean reward: 0.388 [0.341, 0.567], mean action: 55.600 [10.000, 85.000], mean observation: 3.164 [-2.106, 10.587], loss: 1.134650, mae: 4.817568, mean_q: 5.108306
 26909/100000: episode: 2745, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.882, mean reward: 0.388 [0.312, 0.505], mean action: 71.000 [7.000, 98.000], mean observation: 3.156 [-1.276, 10.250], loss: 1.397405, mae: 4.818795, mean_q: 5.106441
 26919/100000: episode: 2746, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 4.180, mean reward: 0.418 [0.368, 0.572], mean action: 74.400 [34.000, 85.000], mean observation: 3.163 [-1.122, 10.504], loss: 1.241982, mae: 4.818543, mean_q: 5.107562
 26929/100000: episode: 2747, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 4.027, mean reward: 0.403 [0.340, 0.424], mean action: 80.400 [30.000, 91.000], mean observation: 3.141 [-1.120, 10.370], loss: 0.938630, mae: 4.817492, mean_q: 5.107259
 26939/100000: episode: 2748, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.057, mean reward: 0.406 [0.356, 0.466], mean action: 67.400 [28.000, 85.000], mean observation: 3.157 [-1.099, 10.268], loss: 1.079086, mae: 4.817937, mean_q: 5.103926
 26949/100000: episode: 2749, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.654, mean reward: 0.365 [0.339, 0.436], mean action: 57.800 [7.000, 85.000], mean observation: 3.146 [-2.177, 10.187], loss: 1.230815, mae: 4.819105, mean_q: 5.102312
 26959/100000: episode: 2750, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 4.371, mean reward: 0.437 [0.425, 0.465], mean action: 74.400 [36.000, 90.000], mean observation: 3.163 [-1.563, 10.362], loss: 1.310562, mae: 4.819855, mean_q: 5.099832
 26969/100000: episode: 2751, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.769, mean reward: 0.477 [0.457, 0.523], mean action: 61.600 [10.000, 99.000], mean observation: 3.150 [-1.188, 10.343], loss: 0.957796, mae: 4.818741, mean_q: 5.099969
 26979/100000: episode: 2752, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.233, mean reward: 0.423 [0.423, 0.423], mean action: 61.500 [17.000, 85.000], mean observation: 3.146 [-0.923, 10.233], loss: 1.047240, mae: 4.819818, mean_q: 5.101844
 26989/100000: episode: 2753, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 3.889, mean reward: 0.389 [0.379, 0.430], mean action: 76.200 [42.000, 101.000], mean observation: 3.165 [-1.517, 10.189], loss: 1.058690, mae: 4.820487, mean_q: 5.104078
 26999/100000: episode: 2754, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.830, mean reward: 0.483 [0.468, 0.519], mean action: 78.800 [56.000, 99.000], mean observation: 3.160 [-1.421, 10.232], loss: 1.258346, mae: 4.821565, mean_q: 5.106314
 27009/100000: episode: 2755, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.755, mean reward: 0.376 [0.340, 0.437], mean action: 75.100 [0.000, 96.000], mean observation: 3.160 [-1.521, 10.260], loss: 1.230402, mae: 4.822327, mean_q: 5.110217
 27019/100000: episode: 2756, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.333, mean reward: 0.433 [0.368, 0.522], mean action: 67.300 [5.000, 85.000], mean observation: 3.164 [-1.421, 10.396], loss: 1.209036, mae: 4.822143, mean_q: 5.112647
 27029/100000: episode: 2757, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.979, mean reward: 0.398 [0.348, 0.482], mean action: 60.300 [7.000, 94.000], mean observation: 3.149 [-1.807, 10.208], loss: 1.293927, mae: 4.822450, mean_q: 5.116673
 27039/100000: episode: 2758, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.252, mean reward: 0.425 [0.396, 0.464], mean action: 72.500 [10.000, 85.000], mean observation: 3.154 [-1.416, 10.338], loss: 1.317963, mae: 4.823081, mean_q: 5.122272
 27049/100000: episode: 2759, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.356, mean reward: 0.436 [0.334, 0.502], mean action: 69.000 [5.000, 85.000], mean observation: 3.169 [-0.877, 10.255], loss: 1.204696, mae: 4.823018, mean_q: 5.125667
 27059/100000: episode: 2760, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.014, mean reward: 0.401 [0.375, 0.416], mean action: 59.900 [11.000, 94.000], mean observation: 3.153 [-1.134, 10.472], loss: 1.350226, mae: 4.823871, mean_q: 5.124889
 27069/100000: episode: 2761, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 3.693, mean reward: 0.369 [0.328, 0.504], mean action: 64.000 [2.000, 85.000], mean observation: 3.160 [-1.112, 10.430], loss: 1.376219, mae: 4.824162, mean_q: 5.126313
 27079/100000: episode: 2762, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.069, mean reward: 0.407 [0.397, 0.418], mean action: 70.200 [8.000, 88.000], mean observation: 3.160 [-0.776, 10.297], loss: 1.435540, mae: 4.824389, mean_q: 5.126172
 27089/100000: episode: 2763, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.964, mean reward: 0.496 [0.394, 0.558], mean action: 58.300 [18.000, 93.000], mean observation: 3.167 [-1.764, 10.465], loss: 1.140034, mae: 4.823522, mean_q: 5.127790
 27099/100000: episode: 2764, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.078, mean reward: 0.408 [0.353, 0.447], mean action: 69.000 [15.000, 85.000], mean observation: 3.152 [-1.278, 10.231], loss: 1.067794, mae: 4.823632, mean_q: 5.130006
 27109/100000: episode: 2765, duration: 0.117s, episode steps: 10, steps per second: 86, episode reward: 3.969, mean reward: 0.397 [0.363, 0.501], mean action: 79.100 [13.000, 98.000], mean observation: 3.166 [-1.978, 10.298], loss: 1.434537, mae: 4.825230, mean_q: 5.131408
 27119/100000: episode: 2766, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.056, mean reward: 0.406 [0.394, 0.450], mean action: 55.600 [16.000, 90.000], mean observation: 3.163 [-1.017, 10.243], loss: 1.104224, mae: 4.823934, mean_q: 5.130296
 27129/100000: episode: 2767, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.576, mean reward: 0.358 [0.284, 0.421], mean action: 79.300 [31.000, 96.000], mean observation: 3.165 [-1.699, 10.313], loss: 1.026227, mae: 4.824138, mean_q: 5.131425
 27139/100000: episode: 2768, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.731, mean reward: 0.373 [0.348, 0.382], mean action: 67.100 [1.000, 85.000], mean observation: 3.153 [-1.436, 10.242], loss: 1.292218, mae: 4.825657, mean_q: 5.134253
 27142/100000: episode: 2769, duration: 0.064s, episode steps: 3, steps per second: 47, episode reward: 10.874, mean reward: 3.625 [0.437, 10.000], mean action: 36.667 [11.000, 80.000], mean observation: 3.154 [-0.939, 10.213], loss: 1.676789, mae: 4.827970, mean_q: 5.135380
 27152/100000: episode: 2770, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 5.027, mean reward: 0.503 [0.325, 0.571], mean action: 62.700 [1.000, 87.000], mean observation: 3.145 [-0.679, 10.414], loss: 0.937854, mae: 4.825086, mean_q: 5.136203
 27162/100000: episode: 2771, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.868, mean reward: 0.387 [0.309, 0.518], mean action: 59.100 [7.000, 85.000], mean observation: 3.157 [-1.175, 10.322], loss: 1.049063, mae: 4.826002, mean_q: 5.137568
 27172/100000: episode: 2772, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.862, mean reward: 0.386 [0.348, 0.470], mean action: 66.600 [5.000, 91.000], mean observation: 3.148 [-1.333, 10.277], loss: 0.991656, mae: 4.826396, mean_q: 5.139966
 27182/100000: episode: 2773, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.124, mean reward: 0.412 [0.389, 0.526], mean action: 64.500 [8.000, 85.000], mean observation: 3.140 [-1.275, 10.315], loss: 1.223474, mae: 4.827472, mean_q: 5.141407
 27192/100000: episode: 2774, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.003, mean reward: 0.400 [0.360, 0.479], mean action: 63.300 [1.000, 98.000], mean observation: 3.147 [-1.713, 10.265], loss: 0.974689, mae: 4.827140, mean_q: 5.140031
 27202/100000: episode: 2775, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.449, mean reward: 0.345 [0.326, 0.419], mean action: 75.500 [33.000, 85.000], mean observation: 3.158 [-1.165, 10.214], loss: 1.186762, mae: 4.828578, mean_q: 5.141570
 27212/100000: episode: 2776, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.149, mean reward: 0.415 [0.377, 0.459], mean action: 69.400 [14.000, 100.000], mean observation: 3.155 [-1.111, 10.388], loss: 1.267010, mae: 4.829021, mean_q: 5.140554
 27222/100000: episode: 2777, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 2.952, mean reward: 0.295 [0.276, 0.343], mean action: 80.300 [49.000, 85.000], mean observation: 3.155 [-0.868, 10.288], loss: 1.073888, mae: 4.828569, mean_q: 5.141491
 27232/100000: episode: 2778, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.240, mean reward: 0.424 [0.392, 0.587], mean action: 73.600 [31.000, 85.000], mean observation: 3.156 [-1.015, 10.181], loss: 0.928560, mae: 4.828625, mean_q: 5.143680
 27242/100000: episode: 2779, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 4.206, mean reward: 0.421 [0.349, 0.540], mean action: 83.600 [55.000, 101.000], mean observation: 3.158 [-0.786, 10.302], loss: 1.004992, mae: 4.829085, mean_q: 5.144059
 27252/100000: episode: 2780, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 3.712, mean reward: 0.371 [0.305, 0.482], mean action: 69.900 [5.000, 85.000], mean observation: 3.157 [-1.178, 10.337], loss: 0.996391, mae: 4.829846, mean_q: 5.144351
 27262/100000: episode: 2781, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.321, mean reward: 0.432 [0.430, 0.450], mean action: 70.100 [10.000, 85.000], mean observation: 3.159 [-0.762, 10.410], loss: 1.259912, mae: 4.831410, mean_q: 5.146029
 27272/100000: episode: 2782, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.772, mean reward: 0.377 [0.349, 0.395], mean action: 56.200 [5.000, 85.000], mean observation: 3.131 [-1.188, 10.288], loss: 1.242060, mae: 4.831992, mean_q: 5.145957
 27282/100000: episode: 2783, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.962, mean reward: 0.396 [0.376, 0.414], mean action: 72.900 [15.000, 85.000], mean observation: 3.151 [-1.305, 10.301], loss: 0.840438, mae: 4.830691, mean_q: 5.144741
 27292/100000: episode: 2784, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.957, mean reward: 0.396 [0.362, 0.468], mean action: 78.000 [22.000, 99.000], mean observation: 3.170 [-1.382, 10.456], loss: 1.200986, mae: 4.833060, mean_q: 5.139678
 27302/100000: episode: 2785, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.938, mean reward: 0.494 [0.494, 0.494], mean action: 64.300 [29.000, 85.000], mean observation: 3.148 [-2.183, 10.318], loss: 0.851959, mae: 4.832223, mean_q: 5.134842
 27312/100000: episode: 2786, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.971, mean reward: 0.497 [0.432, 0.548], mean action: 57.900 [12.000, 93.000], mean observation: 3.150 [-1.665, 10.306], loss: 1.213533, mae: 4.834116, mean_q: 5.134574
 27322/100000: episode: 2787, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.061, mean reward: 0.406 [0.355, 0.525], mean action: 49.700 [3.000, 85.000], mean observation: 3.163 [-1.333, 10.387], loss: 1.005902, mae: 4.833690, mean_q: 5.137057
 27332/100000: episode: 2788, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.666, mean reward: 0.467 [0.397, 0.520], mean action: 55.000 [14.000, 85.000], mean observation: 3.162 [-0.981, 10.384], loss: 1.289830, mae: 4.834800, mean_q: 5.138258
 27342/100000: episode: 2789, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.187, mean reward: 0.419 [0.385, 0.488], mean action: 65.100 [4.000, 85.000], mean observation: 3.163 [-1.126, 10.279], loss: 1.133303, mae: 4.834553, mean_q: 5.131914
 27352/100000: episode: 2790, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.338, mean reward: 0.434 [0.421, 0.484], mean action: 60.700 [24.000, 85.000], mean observation: 3.158 [-2.495, 10.246], loss: 1.395574, mae: 4.835979, mean_q: 5.122832
 27362/100000: episode: 2791, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.913, mean reward: 0.391 [0.328, 0.472], mean action: 59.800 [12.000, 100.000], mean observation: 3.154 [-0.851, 10.457], loss: 1.109107, mae: 4.834919, mean_q: 5.116412
 27372/100000: episode: 2792, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.607, mean reward: 0.461 [0.448, 0.517], mean action: 69.100 [14.000, 85.000], mean observation: 3.154 [-1.253, 10.332], loss: 1.240654, mae: 4.835763, mean_q: 5.117036
 27382/100000: episode: 2793, duration: 0.123s, episode steps: 10, steps per second: 82, episode reward: 13.408, mean reward: 1.341 [0.300, 10.000], mean action: 67.600 [24.000, 85.000], mean observation: 3.146 [-1.694, 10.334], loss: 1.200366, mae: 4.836576, mean_q: 5.116956
 27392/100000: episode: 2794, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 3.829, mean reward: 0.383 [0.349, 0.528], mean action: 82.100 [54.000, 90.000], mean observation: 3.151 [-1.020, 10.346], loss: 1.091771, mae: 4.836180, mean_q: 5.116744
 27402/100000: episode: 2795, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.876, mean reward: 0.388 [0.375, 0.443], mean action: 65.200 [11.000, 87.000], mean observation: 3.150 [-0.962, 10.290], loss: 1.246076, mae: 4.837355, mean_q: 5.117327
 27412/100000: episode: 2796, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.127, mean reward: 0.413 [0.321, 0.434], mean action: 65.300 [28.000, 85.000], mean observation: 3.151 [-1.801, 10.348], loss: 1.112605, mae: 4.837094, mean_q: 5.118808
 27422/100000: episode: 2797, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 3.943, mean reward: 0.394 [0.392, 0.418], mean action: 74.500 [39.000, 100.000], mean observation: 3.157 [-1.124, 10.373], loss: 1.052709, mae: 4.836670, mean_q: 5.119465
 27432/100000: episode: 2798, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 4.163, mean reward: 0.416 [0.301, 0.462], mean action: 81.200 [28.000, 101.000], mean observation: 3.166 [-1.470, 10.368], loss: 0.989223, mae: 4.837382, mean_q: 5.120252
 27442/100000: episode: 2799, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.534, mean reward: 0.353 [0.302, 0.461], mean action: 45.800 [0.000, 100.000], mean observation: 3.150 [-1.714, 10.236], loss: 1.273066, mae: 4.838408, mean_q: 5.118415
 27452/100000: episode: 2800, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.798, mean reward: 0.380 [0.314, 0.514], mean action: 63.900 [9.000, 85.000], mean observation: 3.163 [-1.225, 10.338], loss: 1.009700, mae: 4.837659, mean_q: 5.118358
 27462/100000: episode: 2801, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 6.352, mean reward: 0.635 [0.635, 0.635], mean action: 73.300 [12.000, 94.000], mean observation: 3.173 [-0.833, 10.369], loss: 1.114151, mae: 4.838409, mean_q: 5.118055
 27472/100000: episode: 2802, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.026, mean reward: 0.403 [0.320, 0.477], mean action: 53.400 [2.000, 85.000], mean observation: 3.154 [-1.010, 10.276], loss: 0.989794, mae: 4.838517, mean_q: 5.119735
 27482/100000: episode: 2803, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.079, mean reward: 0.408 [0.301, 0.470], mean action: 49.900 [11.000, 89.000], mean observation: 3.149 [-1.477, 10.399], loss: 1.115352, mae: 4.839231, mean_q: 5.117377
 27492/100000: episode: 2804, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.335, mean reward: 0.433 [0.370, 0.532], mean action: 62.200 [24.000, 91.000], mean observation: 3.164 [-1.711, 10.350], loss: 1.149832, mae: 4.839800, mean_q: 5.110133
 27498/100000: episode: 2805, duration: 0.086s, episode steps: 6, steps per second: 70, episode reward: 12.180, mean reward: 2.030 [0.436, 10.000], mean action: 69.500 [24.000, 92.000], mean observation: 3.174 [-1.280, 10.249], loss: 0.933099, mae: 4.839186, mean_q: 5.103524
 27508/100000: episode: 2806, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.013, mean reward: 0.401 [0.335, 0.493], mean action: 36.500 [24.000, 100.000], mean observation: 3.146 [-1.277, 10.354], loss: 1.129040, mae: 4.840062, mean_q: 5.101560
 27518/100000: episode: 2807, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.383, mean reward: 0.438 [0.390, 0.565], mean action: 50.300 [24.000, 101.000], mean observation: 3.164 [-1.782, 10.376], loss: 1.356424, mae: 4.841249, mean_q: 5.102116
 27528/100000: episode: 2808, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.187, mean reward: 0.419 [0.374, 0.495], mean action: 32.200 [7.000, 86.000], mean observation: 3.151 [-1.265, 10.302], loss: 1.250613, mae: 4.841038, mean_q: 5.103860
 27532/100000: episode: 2809, duration: 0.089s, episode steps: 4, steps per second: 45, episode reward: 11.192, mean reward: 2.798 [0.387, 10.000], mean action: 28.250 [16.000, 49.000], mean observation: 3.163 [-1.070, 10.336], loss: 1.165809, mae: 4.840796, mean_q: 5.104735
 27537/100000: episode: 2810, duration: 0.117s, episode steps: 5, steps per second: 43, episode reward: 11.498, mean reward: 2.300 [0.344, 10.000], mean action: 27.800 [24.000, 42.000], mean observation: 3.152 [-1.156, 10.286], loss: 1.271277, mae: 4.841165, mean_q: 5.105007
 27547/100000: episode: 2811, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.923, mean reward: 0.392 [0.321, 0.458], mean action: 33.600 [2.000, 77.000], mean observation: 3.151 [-1.301, 10.314], loss: 1.230409, mae: 4.841460, mean_q: 5.105720
 27557/100000: episode: 2812, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.925, mean reward: 0.393 [0.378, 0.428], mean action: 43.300 [10.000, 91.000], mean observation: 3.162 [-1.383, 10.317], loss: 1.194009, mae: 4.841143, mean_q: 5.107678
 27567/100000: episode: 2813, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.440, mean reward: 0.444 [0.383, 0.575], mean action: 27.800 [1.000, 68.000], mean observation: 3.153 [-1.248, 10.465], loss: 1.626850, mae: 4.843156, mean_q: 5.108975
 27577/100000: episode: 2814, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.957, mean reward: 0.396 [0.352, 0.473], mean action: 32.400 [13.000, 80.000], mean observation: 3.146 [-1.487, 10.284], loss: 1.299528, mae: 4.841712, mean_q: 5.111097
 27587/100000: episode: 2815, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.341, mean reward: 0.434 [0.354, 0.515], mean action: 54.400 [21.000, 99.000], mean observation: 3.167 [-0.860, 10.348], loss: 1.284906, mae: 4.841317, mean_q: 5.107372
 27597/100000: episode: 2816, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.672, mean reward: 0.367 [0.330, 0.404], mean action: 47.300 [0.000, 101.000], mean observation: 3.153 [-1.450, 10.362], loss: 1.487127, mae: 4.841975, mean_q: 5.105628
 27607/100000: episode: 2817, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.012, mean reward: 0.401 [0.285, 0.558], mean action: 41.500 [24.000, 91.000], mean observation: 3.163 [-1.375, 10.282], loss: 1.270053, mae: 4.840875, mean_q: 5.105353
 27617/100000: episode: 2818, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.113, mean reward: 0.411 [0.332, 0.460], mean action: 32.000 [6.000, 78.000], mean observation: 3.155 [-2.119, 10.346], loss: 1.047319, mae: 4.840094, mean_q: 5.105670
 27627/100000: episode: 2819, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.517, mean reward: 0.452 [0.388, 0.523], mean action: 34.500 [8.000, 98.000], mean observation: 3.153 [-1.428, 10.341], loss: 1.417259, mae: 4.841731, mean_q: 5.107405
 27637/100000: episode: 2820, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.839, mean reward: 0.384 [0.371, 0.438], mean action: 41.600 [24.000, 100.000], mean observation: 3.152 [-2.260, 10.425], loss: 0.891923, mae: 4.839660, mean_q: 5.110806
 27647/100000: episode: 2821, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 4.304, mean reward: 0.430 [0.335, 0.590], mean action: 29.300 [24.000, 59.000], mean observation: 3.156 [-1.495, 10.277], loss: 1.001247, mae: 4.840690, mean_q: 5.119504
 27657/100000: episode: 2822, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.377, mean reward: 0.438 [0.351, 0.545], mean action: 33.100 [0.000, 83.000], mean observation: 3.147 [-1.345, 10.313], loss: 1.154426, mae: 4.841524, mean_q: 5.125407
 27667/100000: episode: 2823, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.467, mean reward: 0.447 [0.365, 0.491], mean action: 47.100 [14.000, 97.000], mean observation: 3.153 [-1.285, 10.272], loss: 1.106198, mae: 4.842120, mean_q: 5.128818
 27677/100000: episode: 2824, duration: 0.223s, episode steps: 10, steps per second: 45, episode reward: 4.056, mean reward: 0.406 [0.317, 0.456], mean action: 32.100 [7.000, 94.000], mean observation: 3.150 [-1.733, 10.264], loss: 1.073964, mae: 4.842628, mean_q: 5.131560
 27687/100000: episode: 2825, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.447, mean reward: 0.445 [0.344, 0.526], mean action: 35.200 [20.000, 91.000], mean observation: 3.151 [-1.389, 10.503], loss: 1.007097, mae: 4.842706, mean_q: 5.134718
 27697/100000: episode: 2826, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.375, mean reward: 0.438 [0.303, 0.527], mean action: 41.200 [9.000, 91.000], mean observation: 3.158 [-1.283, 10.492], loss: 1.269973, mae: 4.844252, mean_q: 5.137165
 27707/100000: episode: 2827, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.278, mean reward: 0.428 [0.358, 0.524], mean action: 30.000 [6.000, 76.000], mean observation: 3.152 [-1.038, 10.315], loss: 0.888825, mae: 4.843143, mean_q: 5.139369
 27717/100000: episode: 2828, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.218, mean reward: 0.422 [0.352, 0.470], mean action: 16.100 [0.000, 24.000], mean observation: 3.164 [-1.890, 10.419], loss: 1.209801, mae: 4.844987, mean_q: 5.141411
 27724/100000: episode: 2829, duration: 0.143s, episode steps: 7, steps per second: 49, episode reward: 12.258, mean reward: 1.751 [0.371, 10.000], mean action: 34.143 [8.000, 71.000], mean observation: 3.153 [-1.071, 10.539], loss: 0.949475, mae: 4.844458, mean_q: 5.144498
 27734/100000: episode: 2830, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.725, mean reward: 0.373 [0.296, 0.458], mean action: 35.300 [24.000, 90.000], mean observation: 3.151 [-1.284, 10.197], loss: 1.334127, mae: 4.846112, mean_q: 5.146865
 27744/100000: episode: 2831, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 4.689, mean reward: 0.469 [0.395, 0.555], mean action: 37.200 [6.000, 79.000], mean observation: 3.154 [-1.731, 10.278], loss: 1.349168, mae: 4.846089, mean_q: 5.142517
 27754/100000: episode: 2832, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.139, mean reward: 0.414 [0.337, 0.511], mean action: 30.500 [14.000, 72.000], mean observation: 3.146 [-1.544, 10.201], loss: 1.167043, mae: 4.845414, mean_q: 5.135867
 27763/100000: episode: 2833, duration: 0.170s, episode steps: 9, steps per second: 53, episode reward: 13.244, mean reward: 1.472 [0.363, 10.000], mean action: 39.667 [20.000, 85.000], mean observation: 3.169 [-1.609, 10.434], loss: 0.914644, mae: 4.844965, mean_q: 5.134866
 27773/100000: episode: 2834, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.294, mean reward: 0.429 [0.363, 0.523], mean action: 31.400 [8.000, 65.000], mean observation: 3.167 [-2.075, 10.321], loss: 1.154883, mae: 4.846118, mean_q: 5.126672
 27783/100000: episode: 2835, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.041, mean reward: 0.404 [0.316, 0.505], mean action: 36.000 [15.000, 85.000], mean observation: 3.154 [-1.226, 10.232], loss: 1.275627, mae: 4.847144, mean_q: 5.123153
 27793/100000: episode: 2836, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.222, mean reward: 0.422 [0.386, 0.551], mean action: 37.500 [20.000, 91.000], mean observation: 3.154 [-1.296, 10.347], loss: 1.282708, mae: 4.847307, mean_q: 5.120542
 27803/100000: episode: 2837, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.886, mean reward: 0.389 [0.301, 0.513], mean action: 37.700 [18.000, 77.000], mean observation: 3.155 [-1.168, 10.369], loss: 1.029243, mae: 4.846564, mean_q: 5.118700
 27813/100000: episode: 2838, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.150, mean reward: 0.415 [0.347, 0.483], mean action: 42.600 [11.000, 88.000], mean observation: 3.156 [-1.592, 10.321], loss: 1.523019, mae: 4.848859, mean_q: 5.121334
 27823/100000: episode: 2839, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.056, mean reward: 0.406 [0.346, 0.519], mean action: 29.600 [1.000, 92.000], mean observation: 3.156 [-0.854, 10.383], loss: 1.116544, mae: 4.847102, mean_q: 5.125760
 27833/100000: episode: 2840, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.512, mean reward: 0.451 [0.324, 0.557], mean action: 48.500 [24.000, 96.000], mean observation: 3.171 [-2.008, 10.406], loss: 1.291793, mae: 4.847609, mean_q: 5.123221
 27843/100000: episode: 2841, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.930, mean reward: 0.393 [0.312, 0.475], mean action: 25.300 [2.000, 56.000], mean observation: 3.149 [-1.678, 10.302], loss: 1.241158, mae: 4.847656, mean_q: 5.120946
 27853/100000: episode: 2842, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.285, mean reward: 0.429 [0.265, 0.572], mean action: 32.200 [1.000, 62.000], mean observation: 3.155 [-1.537, 10.289], loss: 1.161379, mae: 4.847534, mean_q: 5.122048
 27863/100000: episode: 2843, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 5.364, mean reward: 0.536 [0.529, 0.593], mean action: 29.200 [13.000, 89.000], mean observation: 3.168 [-1.583, 10.340], loss: 1.443963, mae: 4.848706, mean_q: 5.120191
 27873/100000: episode: 2844, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.794, mean reward: 0.379 [0.310, 0.435], mean action: 35.100 [16.000, 93.000], mean observation: 3.156 [-1.828, 10.304], loss: 0.752142, mae: 4.846080, mean_q: 5.114959
 27883/100000: episode: 2845, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.285, mean reward: 0.428 [0.370, 0.533], mean action: 36.900 [0.000, 92.000], mean observation: 3.168 [-2.118, 10.343], loss: 1.025147, mae: 4.847445, mean_q: 5.113914
 27893/100000: episode: 2846, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.354, mean reward: 0.435 [0.357, 0.479], mean action: 46.200 [22.000, 101.000], mean observation: 3.171 [-1.634, 10.435], loss: 1.028889, mae: 4.848067, mean_q: 5.114789
 27903/100000: episode: 2847, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.398, mean reward: 0.440 [0.352, 0.504], mean action: 36.300 [5.000, 99.000], mean observation: 3.171 [-1.430, 10.328], loss: 1.194052, mae: 4.848904, mean_q: 5.115219
 27913/100000: episode: 2848, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.385, mean reward: 0.438 [0.370, 0.517], mean action: 38.700 [7.000, 89.000], mean observation: 3.152 [-1.205, 10.264], loss: 0.900380, mae: 4.848105, mean_q: 5.113818
 27923/100000: episode: 2849, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.969, mean reward: 0.397 [0.351, 0.507], mean action: 51.500 [24.000, 98.000], mean observation: 3.170 [-1.695, 10.271], loss: 1.420848, mae: 4.851027, mean_q: 5.112330
 27933/100000: episode: 2850, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.706, mean reward: 0.371 [0.323, 0.490], mean action: 58.000 [8.000, 96.000], mean observation: 3.172 [-1.130, 10.282], loss: 1.352499, mae: 4.850929, mean_q: 5.112571
 27943/100000: episode: 2851, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.215, mean reward: 0.421 [0.345, 0.577], mean action: 48.800 [17.000, 94.000], mean observation: 3.151 [-1.305, 10.329], loss: 1.269024, mae: 4.851022, mean_q: 5.112848
 27953/100000: episode: 2852, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.957, mean reward: 0.396 [0.343, 0.439], mean action: 28.000 [14.000, 53.000], mean observation: 3.165 [-1.282, 10.450], loss: 1.248677, mae: 4.851026, mean_q: 5.113999
 27957/100000: episode: 2853, duration: 0.087s, episode steps: 4, steps per second: 46, episode reward: 11.137, mean reward: 2.784 [0.323, 10.000], mean action: 22.250 [11.000, 26.000], mean observation: 3.142 [-1.472, 10.100], loss: 1.088468, mae: 4.850659, mean_q: 5.112628
 27967/100000: episode: 2854, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.469, mean reward: 0.447 [0.413, 0.482], mean action: 42.000 [26.000, 84.000], mean observation: 3.157 [-1.600, 10.460], loss: 0.837449, mae: 4.850049, mean_q: 5.111667
 27977/100000: episode: 2855, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.814, mean reward: 0.381 [0.322, 0.434], mean action: 47.900 [18.000, 95.000], mean observation: 3.162 [-1.793, 10.445], loss: 0.946171, mae: 4.851014, mean_q: 5.112607
 27987/100000: episode: 2856, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.175, mean reward: 0.417 [0.365, 0.499], mean action: 59.700 [3.000, 98.000], mean observation: 3.159 [-0.827, 10.315], loss: 1.139108, mae: 4.852263, mean_q: 5.114660
 27997/100000: episode: 2857, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.569, mean reward: 0.457 [0.436, 0.484], mean action: 35.300 [16.000, 87.000], mean observation: 3.159 [-1.961, 10.430], loss: 1.337345, mae: 4.853215, mean_q: 5.113544
 28007/100000: episode: 2858, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.348, mean reward: 0.435 [0.422, 0.541], mean action: 38.400 [20.000, 87.000], mean observation: 3.153 [-1.149, 10.368], loss: 1.169706, mae: 4.852805, mean_q: 5.107879
 28017/100000: episode: 2859, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.651, mean reward: 0.465 [0.331, 0.531], mean action: 44.200 [17.000, 87.000], mean observation: 3.153 [-1.746, 10.425], loss: 1.282808, mae: 4.853312, mean_q: 5.104482
 28027/100000: episode: 2860, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.180, mean reward: 0.418 [0.353, 0.486], mean action: 21.900 [1.000, 53.000], mean observation: 3.156 [-1.530, 10.343], loss: 1.195552, mae: 4.853491, mean_q: 5.104381
 28037/100000: episode: 2861, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.265, mean reward: 0.427 [0.422, 0.438], mean action: 37.900 [26.000, 100.000], mean observation: 3.167 [-1.628, 10.430], loss: 0.932168, mae: 4.852592, mean_q: 5.105534
 28047/100000: episode: 2862, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.219, mean reward: 0.422 [0.346, 0.480], mean action: 48.300 [26.000, 88.000], mean observation: 3.156 [-1.359, 10.380], loss: 1.336200, mae: 4.854434, mean_q: 5.107307
 28057/100000: episode: 2863, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.635, mean reward: 0.464 [0.365, 0.517], mean action: 50.900 [26.000, 96.000], mean observation: 3.153 [-1.683, 10.398], loss: 1.154052, mae: 4.853888, mean_q: 5.112410
 28067/100000: episode: 2864, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.101, mean reward: 0.410 [0.354, 0.477], mean action: 47.800 [0.000, 92.000], mean observation: 3.156 [-0.656, 10.383], loss: 1.289619, mae: 4.854723, mean_q: 5.115465
 28077/100000: episode: 2865, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.232, mean reward: 0.423 [0.369, 0.527], mean action: 31.300 [1.000, 52.000], mean observation: 3.171 [-1.736, 10.211], loss: 1.421846, mae: 4.855112, mean_q: 5.113446
 28084/100000: episode: 2866, duration: 0.150s, episode steps: 7, steps per second: 47, episode reward: 12.367, mean reward: 1.767 [0.322, 10.000], mean action: 28.571 [6.000, 70.000], mean observation: 3.149 [-1.301, 10.339], loss: 1.285972, mae: 4.854137, mean_q: 5.112426
 28094/100000: episode: 2867, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.316, mean reward: 0.432 [0.372, 0.594], mean action: 33.100 [16.000, 93.000], mean observation: 3.152 [-2.242, 10.319], loss: 1.081567, mae: 4.853236, mean_q: 5.112735
 28104/100000: episode: 2868, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.961, mean reward: 0.396 [0.330, 0.478], mean action: 54.500 [17.000, 100.000], mean observation: 3.165 [-1.582, 10.364], loss: 1.174053, mae: 4.853921, mean_q: 5.113544
 28114/100000: episode: 2869, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.236, mean reward: 0.424 [0.342, 0.507], mean action: 31.800 [12.000, 98.000], mean observation: 3.153 [-2.238, 10.425], loss: 1.212406, mae: 4.854166, mean_q: 5.114595
 28124/100000: episode: 2870, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.747, mean reward: 0.375 [0.291, 0.446], mean action: 33.300 [7.000, 84.000], mean observation: 3.166 [-1.672, 10.317], loss: 1.400533, mae: 4.855016, mean_q: 5.115370
 28134/100000: episode: 2871, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.979, mean reward: 0.398 [0.344, 0.446], mean action: 38.100 [15.000, 87.000], mean observation: 3.158 [-1.231, 10.423], loss: 0.853732, mae: 4.852675, mean_q: 5.115870
 28144/100000: episode: 2872, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.975, mean reward: 0.398 [0.305, 0.472], mean action: 52.700 [26.000, 100.000], mean observation: 3.169 [-1.204, 10.414], loss: 1.098376, mae: 4.853765, mean_q: 5.116604
 28154/100000: episode: 2873, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.872, mean reward: 0.387 [0.376, 0.406], mean action: 66.500 [26.000, 101.000], mean observation: 3.149 [-1.258, 10.304], loss: 1.349926, mae: 4.854963, mean_q: 5.117913
 28164/100000: episode: 2874, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.964, mean reward: 0.396 [0.365, 0.466], mean action: 34.200 [12.000, 73.000], mean observation: 3.156 [-1.861, 10.319], loss: 1.249158, mae: 4.854908, mean_q: 5.116336
 28174/100000: episode: 2875, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.866, mean reward: 0.387 [0.359, 0.438], mean action: 42.700 [7.000, 94.000], mean observation: 3.155 [-1.044, 10.410], loss: 1.064094, mae: 4.853878, mean_q: 5.108409
 28184/100000: episode: 2876, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.991, mean reward: 0.399 [0.362, 0.436], mean action: 48.700 [40.000, 92.000], mean observation: 3.151 [-1.253, 10.237], loss: 1.146712, mae: 4.854340, mean_q: 5.108197
 28194/100000: episode: 2877, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.779, mean reward: 0.378 [0.322, 0.503], mean action: 37.500 [0.000, 69.000], mean observation: 3.156 [-1.666, 10.157], loss: 1.288527, mae: 4.855056, mean_q: 5.107423
 28204/100000: episode: 2878, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.022, mean reward: 0.402 [0.305, 0.484], mean action: 40.400 [0.000, 84.000], mean observation: 3.160 [-1.714, 10.512], loss: 1.483053, mae: 4.856311, mean_q: 5.103703
 28214/100000: episode: 2879, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.619, mean reward: 0.362 [0.330, 0.432], mean action: 61.400 [8.000, 100.000], mean observation: 3.134 [-1.460, 10.392], loss: 0.827268, mae: 4.853505, mean_q: 5.102588
 28224/100000: episode: 2880, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.039, mean reward: 0.404 [0.321, 0.518], mean action: 53.200 [12.000, 99.000], mean observation: 3.168 [-1.046, 10.363], loss: 1.272215, mae: 4.855470, mean_q: 5.102974
 28234/100000: episode: 2881, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.529, mean reward: 0.353 [0.317, 0.410], mean action: 42.900 [26.000, 86.000], mean observation: 3.156 [-1.270, 10.219], loss: 0.972686, mae: 4.854750, mean_q: 5.103390
 28244/100000: episode: 2882, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.798, mean reward: 0.380 [0.317, 0.573], mean action: 30.100 [3.000, 73.000], mean observation: 3.152 [-1.650, 10.277], loss: 0.918470, mae: 4.855298, mean_q: 5.105401
 28247/100000: episode: 2883, duration: 0.063s, episode steps: 3, steps per second: 48, episode reward: 10.969, mean reward: 3.656 [0.484, 10.000], mean action: 42.000 [26.000, 74.000], mean observation: 3.163 [-1.327, 10.212], loss: 1.045744, mae: 4.855824, mean_q: 5.107276
 28257/100000: episode: 2884, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.746, mean reward: 0.375 [0.274, 0.520], mean action: 42.000 [26.000, 99.000], mean observation: 3.149 [-2.282, 10.298], loss: 1.256007, mae: 4.857371, mean_q: 5.108869
 28267/100000: episode: 2885, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.608, mean reward: 0.461 [0.452, 0.509], mean action: 38.700 [17.000, 85.000], mean observation: 3.159 [-1.421, 10.578], loss: 1.109935, mae: 4.856514, mean_q: 5.111205
 28277/100000: episode: 2886, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.814, mean reward: 0.381 [0.312, 0.484], mean action: 32.800 [20.000, 90.000], mean observation: 3.160 [-1.040, 10.335], loss: 1.161039, mae: 4.857172, mean_q: 5.113448
 28287/100000: episode: 2887, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.189, mean reward: 0.419 [0.364, 0.559], mean action: 37.000 [17.000, 65.000], mean observation: 3.161 [-1.142, 10.229], loss: 1.370627, mae: 4.858025, mean_q: 5.114604
 28297/100000: episode: 2888, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.838, mean reward: 0.484 [0.476, 0.557], mean action: 33.200 [16.000, 76.000], mean observation: 3.149 [-1.054, 10.292], loss: 1.151243, mae: 4.857537, mean_q: 5.113421
 28307/100000: episode: 2889, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.590, mean reward: 0.459 [0.384, 0.491], mean action: 41.100 [18.000, 90.000], mean observation: 3.152 [-1.627, 10.349], loss: 1.209023, mae: 4.857466, mean_q: 5.113885
 28317/100000: episode: 2890, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.214, mean reward: 0.421 [0.408, 0.456], mean action: 53.200 [40.000, 93.000], mean observation: 3.152 [-1.660, 10.460], loss: 1.185432, mae: 4.858145, mean_q: 5.114167
 28327/100000: episode: 2891, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 5.306, mean reward: 0.531 [0.531, 0.531], mean action: 39.300 [14.000, 65.000], mean observation: 3.169 [-1.484, 10.368], loss: 1.286763, mae: 4.858657, mean_q: 5.115151
 28335/100000: episode: 2892, duration: 0.123s, episode steps: 8, steps per second: 65, episode reward: 12.892, mean reward: 1.611 [0.367, 10.000], mean action: 51.500 [33.000, 95.000], mean observation: 3.167 [-1.253, 10.378], loss: 1.138503, mae: 4.858299, mean_q: 5.116270
 28345/100000: episode: 2893, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.650, mean reward: 0.365 [0.329, 0.473], mean action: 56.300 [24.000, 94.000], mean observation: 3.166 [-1.259, 10.355], loss: 1.146193, mae: 4.858338, mean_q: 5.117807
 28355/100000: episode: 2894, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.227, mean reward: 0.423 [0.391, 0.534], mean action: 53.500 [24.000, 94.000], mean observation: 3.144 [-1.253, 10.613], loss: 1.168633, mae: 4.858836, mean_q: 5.118740
 28365/100000: episode: 2895, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.314, mean reward: 0.431 [0.406, 0.522], mean action: 52.000 [40.000, 90.000], mean observation: 3.156 [-1.308, 10.248], loss: 1.162171, mae: 4.859057, mean_q: 5.115155
 28375/100000: episode: 2896, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.404, mean reward: 0.340 [0.301, 0.406], mean action: 61.100 [40.000, 93.000], mean observation: 3.152 [-1.078, 10.314], loss: 1.342159, mae: 4.859815, mean_q: 5.112695
 28385/100000: episode: 2897, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.103, mean reward: 0.410 [0.308, 0.599], mean action: 50.400 [26.000, 97.000], mean observation: 3.148 [-1.423, 10.327], loss: 0.995416, mae: 4.858639, mean_q: 5.108960
 28395/100000: episode: 2898, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.135, mean reward: 0.413 [0.370, 0.540], mean action: 48.600 [12.000, 93.000], mean observation: 3.148 [-1.092, 10.233], loss: 1.262605, mae: 4.859613, mean_q: 5.109304
 28405/100000: episode: 2899, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.087, mean reward: 0.409 [0.339, 0.495], mean action: 34.700 [24.000, 82.000], mean observation: 3.163 [-1.409, 10.321], loss: 1.139229, mae: 4.859643, mean_q: 5.110762
 28415/100000: episode: 2900, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.079, mean reward: 0.408 [0.308, 0.457], mean action: 34.600 [3.000, 87.000], mean observation: 3.154 [-1.592, 10.206], loss: 1.118129, mae: 4.859999, mean_q: 5.112553
 28425/100000: episode: 2901, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.877, mean reward: 0.388 [0.294, 0.543], mean action: 32.100 [7.000, 79.000], mean observation: 3.158 [-1.417, 10.298], loss: 1.369339, mae: 4.861262, mean_q: 5.113367
 28435/100000: episode: 2902, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.857, mean reward: 0.386 [0.362, 0.415], mean action: 34.600 [19.000, 96.000], mean observation: 3.164 [-1.418, 10.339], loss: 1.049893, mae: 4.860181, mean_q: 5.110672
 28445/100000: episode: 2903, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.449, mean reward: 0.445 [0.370, 0.516], mean action: 30.700 [0.000, 80.000], mean observation: 3.151 [-2.314, 10.402], loss: 0.902657, mae: 4.859840, mean_q: 5.109388
 28455/100000: episode: 2904, duration: 0.220s, episode steps: 10, steps per second: 46, episode reward: 3.856, mean reward: 0.386 [0.335, 0.456], mean action: 34.600 [7.000, 51.000], mean observation: 3.157 [-1.619, 10.435], loss: 1.083469, mae: 4.860929, mean_q: 5.106599
 28465/100000: episode: 2905, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.102, mean reward: 0.410 [0.392, 0.481], mean action: 44.700 [40.000, 86.000], mean observation: 3.169 [-1.609, 10.199], loss: 1.645029, mae: 4.863397, mean_q: 5.107913
 28475/100000: episode: 2906, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.931, mean reward: 0.393 [0.347, 0.517], mean action: 57.100 [30.000, 100.000], mean observation: 3.152 [-1.145, 10.362], loss: 1.226847, mae: 4.861811, mean_q: 5.109009
 28485/100000: episode: 2907, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.050, mean reward: 0.405 [0.331, 0.454], mean action: 43.900 [3.000, 86.000], mean observation: 3.147 [-1.354, 10.275], loss: 1.455651, mae: 4.862572, mean_q: 5.106225
 28495/100000: episode: 2908, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.449, mean reward: 0.445 [0.377, 0.482], mean action: 36.000 [3.000, 100.000], mean observation: 3.151 [-2.074, 10.290], loss: 1.157652, mae: 4.861271, mean_q: 5.106437
 28505/100000: episode: 2909, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.022, mean reward: 0.402 [0.322, 0.473], mean action: 56.800 [35.000, 90.000], mean observation: 3.176 [-1.878, 10.271], loss: 1.388996, mae: 4.862208, mean_q: 5.108441
 28515/100000: episode: 2910, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.818, mean reward: 0.382 [0.345, 0.460], mean action: 57.000 [40.000, 101.000], mean observation: 3.146 [-1.163, 10.277], loss: 1.301305, mae: 4.861947, mean_q: 5.109723
 28525/100000: episode: 2911, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.053, mean reward: 0.405 [0.335, 0.478], mean action: 42.900 [23.000, 86.000], mean observation: 3.154 [-1.478, 10.287], loss: 1.158778, mae: 4.861102, mean_q: 5.111619
 28535/100000: episode: 2912, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.186, mean reward: 0.419 [0.289, 0.500], mean action: 47.300 [8.000, 89.000], mean observation: 3.147 [-1.281, 10.312], loss: 1.185980, mae: 4.861360, mean_q: 5.113339
 28545/100000: episode: 2913, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.841, mean reward: 0.384 [0.318, 0.440], mean action: 30.600 [0.000, 59.000], mean observation: 3.153 [-0.950, 10.281], loss: 1.174270, mae: 4.861320, mean_q: 5.114607
 28555/100000: episode: 2914, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.046, mean reward: 0.405 [0.404, 0.412], mean action: 43.400 [23.000, 74.000], mean observation: 3.164 [-0.744, 10.232], loss: 1.285255, mae: 4.862042, mean_q: 5.113071
 28565/100000: episode: 2915, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.889, mean reward: 0.389 [0.345, 0.465], mean action: 39.400 [12.000, 60.000], mean observation: 3.157 [-1.374, 10.261], loss: 1.111098, mae: 4.861407, mean_q: 5.110831
 28575/100000: episode: 2916, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.839, mean reward: 0.384 [0.310, 0.453], mean action: 50.000 [40.000, 88.000], mean observation: 3.157 [-1.514, 10.263], loss: 1.373763, mae: 4.862669, mean_q: 5.111521
 28585/100000: episode: 2917, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.271, mean reward: 0.427 [0.376, 0.457], mean action: 54.700 [18.000, 86.000], mean observation: 3.150 [-1.418, 10.222], loss: 1.339176, mae: 4.863102, mean_q: 5.113368
 28595/100000: episode: 2918, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.083, mean reward: 0.408 [0.330, 0.529], mean action: 39.600 [15.000, 68.000], mean observation: 3.157 [-1.720, 10.340], loss: 1.164642, mae: 4.862426, mean_q: 5.116478
 28605/100000: episode: 2919, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.828, mean reward: 0.483 [0.349, 0.524], mean action: 41.400 [1.000, 91.000], mean observation: 3.152 [-1.050, 10.201], loss: 1.079514, mae: 4.862702, mean_q: 5.120020
 28615/100000: episode: 2920, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.744, mean reward: 0.474 [0.474, 0.474], mean action: 35.600 [6.000, 59.000], mean observation: 3.141 [-2.292, 10.215], loss: 1.352040, mae: 4.864307, mean_q: 5.122851
 28625/100000: episode: 2921, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.104, mean reward: 0.410 [0.343, 0.499], mean action: 48.500 [13.000, 97.000], mean observation: 3.161 [-1.538, 10.462], loss: 0.962932, mae: 4.863106, mean_q: 5.120305
 28635/100000: episode: 2922, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.895, mean reward: 0.389 [0.356, 0.446], mean action: 47.300 [20.000, 89.000], mean observation: 3.150 [-1.475, 10.313], loss: 1.487128, mae: 4.865470, mean_q: 5.121160
 28645/100000: episode: 2923, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.126, mean reward: 0.413 [0.356, 0.485], mean action: 46.600 [8.000, 73.000], mean observation: 3.150 [-1.319, 10.350], loss: 1.210655, mae: 4.864456, mean_q: 5.123006
 28655/100000: episode: 2924, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.842, mean reward: 0.384 [0.301, 0.449], mean action: 46.700 [40.000, 61.000], mean observation: 3.155 [-1.892, 10.175], loss: 1.032787, mae: 4.864034, mean_q: 5.126023
 28665/100000: episode: 2925, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.182, mean reward: 0.418 [0.355, 0.508], mean action: 47.200 [11.000, 97.000], mean observation: 3.150 [-1.571, 10.310], loss: 1.419969, mae: 4.865428, mean_q: 5.129596
 28675/100000: episode: 2926, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.251, mean reward: 0.425 [0.361, 0.598], mean action: 49.300 [2.000, 101.000], mean observation: 3.154 [-1.587, 10.357], loss: 1.248312, mae: 4.864873, mean_q: 5.131828
 28685/100000: episode: 2927, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.921, mean reward: 0.392 [0.328, 0.476], mean action: 41.500 [8.000, 98.000], mean observation: 3.166 [-1.776, 10.341], loss: 1.551205, mae: 4.866105, mean_q: 5.129928
 28695/100000: episode: 2928, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.977, mean reward: 0.398 [0.376, 0.437], mean action: 47.000 [15.000, 84.000], mean observation: 3.152 [-0.868, 10.288], loss: 1.176272, mae: 4.864180, mean_q: 5.127569
 28705/100000: episode: 2929, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.980, mean reward: 0.398 [0.365, 0.503], mean action: 48.500 [40.000, 84.000], mean observation: 3.155 [-1.275, 10.428], loss: 1.441936, mae: 4.865492, mean_q: 5.124494
 28715/100000: episode: 2930, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.552, mean reward: 0.355 [0.294, 0.415], mean action: 49.200 [28.000, 94.000], mean observation: 3.152 [-1.179, 10.306], loss: 1.153087, mae: 4.864277, mean_q: 5.129071
 28725/100000: episode: 2931, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.419, mean reward: 0.342 [0.288, 0.449], mean action: 50.200 [6.000, 99.000], mean observation: 3.153 [-1.374, 10.244], loss: 1.092124, mae: 4.864312, mean_q: 5.132171
 28735/100000: episode: 2932, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.799, mean reward: 0.380 [0.284, 0.477], mean action: 50.000 [40.000, 92.000], mean observation: 3.158 [-0.895, 10.293], loss: 1.361948, mae: 4.865534, mean_q: 5.132705
 28745/100000: episode: 2933, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.665, mean reward: 0.466 [0.462, 0.476], mean action: 54.700 [23.000, 96.000], mean observation: 3.155 [-1.717, 10.520], loss: 1.199782, mae: 4.864933, mean_q: 5.129951
 28755/100000: episode: 2934, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.862, mean reward: 0.386 [0.331, 0.422], mean action: 46.300 [1.000, 93.000], mean observation: 3.169 [-1.474, 10.326], loss: 1.014607, mae: 4.864653, mean_q: 5.131374
 28756/100000: episode: 2935, duration: 0.025s, episode steps: 1, steps per second: 41, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 40.000 [40.000, 40.000], mean observation: 3.115 [-1.161, 10.100], loss: 1.728366, mae: 4.867699, mean_q: 5.133282
 28766/100000: episode: 2936, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.201, mean reward: 0.420 [0.383, 0.524], mean action: 54.800 [40.000, 94.000], mean observation: 3.158 [-1.107, 10.312], loss: 1.426903, mae: 4.866548, mean_q: 5.135489
 28776/100000: episode: 2937, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.553, mean reward: 0.455 [0.440, 0.465], mean action: 51.500 [1.000, 97.000], mean observation: 3.163 [-1.813, 10.470], loss: 1.309549, mae: 4.866368, mean_q: 5.137810
 28786/100000: episode: 2938, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 3.879, mean reward: 0.388 [0.317, 0.429], mean action: 36.100 [15.000, 51.000], mean observation: 3.148 [-1.854, 10.363], loss: 1.088657, mae: 4.865210, mean_q: 5.129710
 28796/100000: episode: 2939, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.270, mean reward: 0.427 [0.383, 0.506], mean action: 31.900 [0.000, 40.000], mean observation: 3.162 [-1.251, 10.524], loss: 1.219517, mae: 4.865903, mean_q: 5.124324
 28806/100000: episode: 2940, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.961, mean reward: 0.396 [0.391, 0.427], mean action: 39.900 [17.000, 64.000], mean observation: 3.155 [-1.617, 10.293], loss: 1.460196, mae: 4.867071, mean_q: 5.122382
 28816/100000: episode: 2941, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.002, mean reward: 0.400 [0.304, 0.484], mean action: 51.400 [11.000, 92.000], mean observation: 3.161 [-0.957, 10.289], loss: 1.059767, mae: 4.865455, mean_q: 5.118546
 28826/100000: episode: 2942, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.214, mean reward: 0.421 [0.399, 0.457], mean action: 41.500 [5.000, 77.000], mean observation: 3.156 [-1.634, 10.330], loss: 1.170537, mae: 4.866223, mean_q: 5.118886
 28836/100000: episode: 2943, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.455, mean reward: 0.445 [0.402, 0.474], mean action: 45.900 [19.000, 84.000], mean observation: 3.172 [-1.293, 10.258], loss: 1.032821, mae: 4.866160, mean_q: 5.120768
 28846/100000: episode: 2944, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.338, mean reward: 0.434 [0.392, 0.496], mean action: 36.500 [0.000, 89.000], mean observation: 3.149 [-1.596, 10.311], loss: 0.786603, mae: 4.865362, mean_q: 5.123071
 28856/100000: episode: 2945, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.357, mean reward: 0.436 [0.376, 0.509], mean action: 43.300 [4.000, 96.000], mean observation: 3.157 [-1.483, 10.370], loss: 1.276544, mae: 4.867930, mean_q: 5.122049
 28866/100000: episode: 2946, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.187, mean reward: 0.419 [0.379, 0.468], mean action: 39.500 [5.000, 76.000], mean observation: 3.151 [-1.747, 10.318], loss: 0.988154, mae: 4.867236, mean_q: 5.121995
 28876/100000: episode: 2947, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.864, mean reward: 0.386 [0.304, 0.520], mean action: 36.000 [0.000, 40.000], mean observation: 3.157 [-1.535, 10.323], loss: 1.165015, mae: 4.868315, mean_q: 5.121492
 28886/100000: episode: 2948, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.735, mean reward: 0.373 [0.278, 0.432], mean action: 47.000 [40.000, 100.000], mean observation: 3.165 [-1.832, 10.235], loss: 1.102766, mae: 4.868543, mean_q: 5.118285
 28896/100000: episode: 2949, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.082, mean reward: 0.408 [0.301, 0.480], mean action: 37.600 [4.000, 77.000], mean observation: 3.154 [-1.520, 10.473], loss: 1.096043, mae: 4.868728, mean_q: 5.113142
 28906/100000: episode: 2950, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 3.988, mean reward: 0.399 [0.310, 0.528], mean action: 36.800 [17.000, 39.000], mean observation: 3.154 [-2.733, 10.608], loss: 1.193653, mae: 4.868957, mean_q: 5.111583
 28916/100000: episode: 2951, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.064, mean reward: 0.406 [0.352, 0.471], mean action: 52.200 [14.000, 101.000], mean observation: 3.153 [-1.052, 10.172], loss: 1.399188, mae: 4.869721, mean_q: 5.113493
 28926/100000: episode: 2952, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.479, mean reward: 0.448 [0.419, 0.572], mean action: 48.400 [38.000, 91.000], mean observation: 3.155 [-1.113, 10.313], loss: 1.404808, mae: 4.869745, mean_q: 5.114863
 28936/100000: episode: 2953, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.190, mean reward: 0.419 [0.372, 0.505], mean action: 34.700 [0.000, 45.000], mean observation: 3.157 [-1.049, 10.362], loss: 1.124615, mae: 4.868293, mean_q: 5.116656
 28946/100000: episode: 2954, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 3.645, mean reward: 0.365 [0.278, 0.488], mean action: 48.000 [3.000, 89.000], mean observation: 3.156 [-1.085, 10.430], loss: 1.309739, mae: 4.868741, mean_q: 5.117939
 28956/100000: episode: 2955, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.781, mean reward: 0.378 [0.322, 0.521], mean action: 49.900 [14.000, 83.000], mean observation: 3.151 [-1.758, 10.230], loss: 1.223426, mae: 4.868486, mean_q: 5.117750
 28966/100000: episode: 2956, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 5.652, mean reward: 0.565 [0.565, 0.565], mean action: 44.600 [14.000, 94.000], mean observation: 3.162 [-1.860, 10.272], loss: 1.499813, mae: 4.869431, mean_q: 5.114400
 28976/100000: episode: 2957, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.094, mean reward: 0.409 [0.317, 0.520], mean action: 45.100 [3.000, 88.000], mean observation: 3.167 [-0.897, 10.318], loss: 1.047319, mae: 4.867563, mean_q: 5.113940
 28986/100000: episode: 2958, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.525, mean reward: 0.452 [0.355, 0.561], mean action: 40.600 [7.000, 95.000], mean observation: 3.155 [-1.262, 10.436], loss: 1.073635, mae: 4.867835, mean_q: 5.115146
 28996/100000: episode: 2959, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.969, mean reward: 0.397 [0.327, 0.470], mean action: 43.900 [22.000, 69.000], mean observation: 3.151 [-1.530, 10.383], loss: 1.349891, mae: 4.869260, mean_q: 5.113170
 29006/100000: episode: 2960, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.009, mean reward: 0.401 [0.386, 0.442], mean action: 44.200 [28.000, 82.000], mean observation: 3.152 [-1.389, 10.310], loss: 1.318898, mae: 4.869098, mean_q: 5.109323
 29016/100000: episode: 2961, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.735, mean reward: 0.474 [0.358, 0.566], mean action: 33.900 [2.000, 80.000], mean observation: 3.150 [-1.450, 10.216], loss: 1.161927, mae: 4.868556, mean_q: 5.110126
 29026/100000: episode: 2962, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.621, mean reward: 0.462 [0.448, 0.572], mean action: 32.100 [8.000, 90.000], mean observation: 3.157 [-1.600, 10.297], loss: 1.068006, mae: 4.868193, mean_q: 5.113110
 29036/100000: episode: 2963, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.722, mean reward: 0.472 [0.425, 0.498], mean action: 37.900 [8.000, 86.000], mean observation: 3.180 [-1.428, 10.419], loss: 1.127843, mae: 4.868823, mean_q: 5.116239
 29046/100000: episode: 2964, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.789, mean reward: 0.379 [0.346, 0.471], mean action: 46.400 [7.000, 99.000], mean observation: 3.155 [-1.252, 10.329], loss: 1.191784, mae: 4.869952, mean_q: 5.122530
 29056/100000: episode: 2965, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.840, mean reward: 0.384 [0.372, 0.419], mean action: 54.500 [39.000, 98.000], mean observation: 3.153 [-1.596, 10.270], loss: 0.980687, mae: 4.869331, mean_q: 5.126610
 29066/100000: episode: 2966, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.084, mean reward: 0.408 [0.280, 0.536], mean action: 42.200 [23.000, 79.000], mean observation: 3.157 [-1.463, 10.268], loss: 1.197017, mae: 4.870924, mean_q: 5.126907
 29076/100000: episode: 2967, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 4.453, mean reward: 0.445 [0.439, 0.473], mean action: 41.500 [39.000, 64.000], mean observation: 3.138 [-1.414, 10.297], loss: 1.068998, mae: 4.870570, mean_q: 5.123389
 29086/100000: episode: 2968, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.180, mean reward: 0.418 [0.332, 0.579], mean action: 38.400 [4.000, 61.000], mean observation: 3.158 [-1.623, 10.294], loss: 1.233983, mae: 4.871795, mean_q: 5.122397
 29096/100000: episode: 2969, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.714, mean reward: 0.371 [0.327, 0.407], mean action: 49.500 [0.000, 98.000], mean observation: 3.157 [-1.140, 10.458], loss: 1.189859, mae: 4.872008, mean_q: 5.122404
 29106/100000: episode: 2970, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.769, mean reward: 0.377 [0.281, 0.443], mean action: 37.700 [6.000, 58.000], mean observation: 3.164 [-1.154, 10.340], loss: 1.152956, mae: 4.872149, mean_q: 5.122847
 29116/100000: episode: 2971, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.792, mean reward: 0.479 [0.479, 0.479], mean action: 53.500 [16.000, 96.000], mean observation: 3.146 [-1.363, 10.350], loss: 1.040148, mae: 4.871795, mean_q: 5.124394
 29126/100000: episode: 2972, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.218, mean reward: 0.422 [0.390, 0.515], mean action: 36.000 [10.000, 50.000], mean observation: 3.154 [-1.672, 10.333], loss: 1.056996, mae: 4.872313, mean_q: 5.126219
 29136/100000: episode: 2973, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.118, mean reward: 0.412 [0.366, 0.538], mean action: 54.700 [9.000, 99.000], mean observation: 3.168 [-1.505, 10.378], loss: 1.086342, mae: 4.872934, mean_q: 5.127763
 29146/100000: episode: 2974, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.822, mean reward: 0.382 [0.343, 0.468], mean action: 41.500 [1.000, 96.000], mean observation: 3.149 [-1.900, 10.309], loss: 1.419382, mae: 4.874720, mean_q: 5.126237
 29156/100000: episode: 2975, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.991, mean reward: 0.399 [0.320, 0.487], mean action: 44.400 [12.000, 99.000], mean observation: 3.149 [-1.555, 10.422], loss: 1.042406, mae: 4.873376, mean_q: 5.124976
 29166/100000: episode: 2976, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.645, mean reward: 0.464 [0.420, 0.471], mean action: 48.900 [3.000, 100.000], mean observation: 3.165 [-0.730, 10.429], loss: 1.131113, mae: 4.874199, mean_q: 5.126128
 29176/100000: episode: 2977, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.547, mean reward: 0.455 [0.442, 0.530], mean action: 45.100 [31.000, 92.000], mean observation: 3.162 [-1.444, 10.382], loss: 1.027438, mae: 4.873963, mean_q: 5.127654
 29186/100000: episode: 2978, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.959, mean reward: 0.396 [0.343, 0.458], mean action: 37.800 [19.000, 83.000], mean observation: 3.157 [-1.351, 10.355], loss: 1.543944, mae: 4.876262, mean_q: 5.129410
 29196/100000: episode: 2979, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.429, mean reward: 0.443 [0.336, 0.546], mean action: 38.300 [11.000, 60.000], mean observation: 3.165 [-1.682, 10.462], loss: 1.344210, mae: 4.875332, mean_q: 5.130973
 29206/100000: episode: 2980, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.125, mean reward: 0.412 [0.367, 0.497], mean action: 42.500 [3.000, 80.000], mean observation: 3.143 [-1.127, 10.426], loss: 1.405538, mae: 4.875466, mean_q: 5.132843
 29216/100000: episode: 2981, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.300, mean reward: 0.430 [0.352, 0.541], mean action: 36.100 [11.000, 49.000], mean observation: 3.166 [-1.034, 10.379], loss: 1.146924, mae: 4.874414, mean_q: 5.134875
 29226/100000: episode: 2982, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.003, mean reward: 0.400 [0.325, 0.464], mean action: 56.500 [1.000, 99.000], mean observation: 3.152 [-1.548, 10.292], loss: 1.200745, mae: 4.874752, mean_q: 5.131788
 29236/100000: episode: 2983, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.881, mean reward: 0.388 [0.328, 0.451], mean action: 37.300 [8.000, 87.000], mean observation: 3.139 [-1.163, 10.277], loss: 1.255390, mae: 4.875134, mean_q: 5.127734
 29246/100000: episode: 2984, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.189, mean reward: 0.419 [0.371, 0.511], mean action: 22.100 [8.000, 85.000], mean observation: 3.149 [-1.313, 10.397], loss: 1.195877, mae: 4.875219, mean_q: 5.128148
 29249/100000: episode: 2985, duration: 0.074s, episode steps: 3, steps per second: 40, episode reward: 10.962, mean reward: 3.654 [0.398, 10.000], mean action: 8.667 [8.000, 10.000], mean observation: 3.156 [-1.373, 10.634], loss: 1.215448, mae: 4.875294, mean_q: 5.127970
 29259/100000: episode: 2986, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.851, mean reward: 0.385 [0.305, 0.508], mean action: 24.900 [4.000, 92.000], mean observation: 3.155 [-2.395, 10.332], loss: 1.185100, mae: 4.875106, mean_q: 5.128429
 29269/100000: episode: 2987, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.240, mean reward: 0.424 [0.353, 0.514], mean action: 21.000 [0.000, 68.000], mean observation: 3.152 [-1.506, 10.394], loss: 1.018802, mae: 4.875119, mean_q: 5.129592
 29279/100000: episode: 2988, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.682, mean reward: 0.468 [0.359, 0.624], mean action: 38.900 [8.000, 101.000], mean observation: 3.164 [-0.965, 10.368], loss: 1.228650, mae: 4.876165, mean_q: 5.128381
 29289/100000: episode: 2989, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.995, mean reward: 0.399 [0.319, 0.467], mean action: 47.800 [8.000, 100.000], mean observation: 3.155 [-2.284, 10.321], loss: 0.998679, mae: 4.875453, mean_q: 5.128652
 29299/100000: episode: 2990, duration: 0.221s, episode steps: 10, steps per second: 45, episode reward: 5.187, mean reward: 0.519 [0.519, 0.519], mean action: 19.400 [7.000, 70.000], mean observation: 3.152 [-1.475, 10.458], loss: 1.421692, mae: 4.877511, mean_q: 5.130908
 29305/100000: episode: 2991, duration: 0.140s, episode steps: 6, steps per second: 43, episode reward: 12.081, mean reward: 2.014 [0.363, 10.000], mean action: 8.167 [8.000, 9.000], mean observation: 3.155 [-1.179, 10.490], loss: 1.483299, mae: 4.877880, mean_q: 5.128771
 29315/100000: episode: 2992, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 3.660, mean reward: 0.366 [0.291, 0.556], mean action: 46.200 [8.000, 92.000], mean observation: 3.151 [-1.515, 10.337], loss: 1.258607, mae: 4.877254, mean_q: 5.123487
 29325/100000: episode: 2993, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.386, mean reward: 0.439 [0.380, 0.498], mean action: 46.600 [30.000, 89.000], mean observation: 3.142 [-1.483, 10.384], loss: 1.304255, mae: 4.877280, mean_q: 5.121633
 29335/100000: episode: 2994, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.916, mean reward: 0.492 [0.492, 0.492], mean action: 41.500 [39.000, 61.000], mean observation: 3.148 [-1.370, 10.343], loss: 1.381372, mae: 4.877584, mean_q: 5.122876
 29345/100000: episode: 2995, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.370, mean reward: 0.437 [0.437, 0.437], mean action: 50.800 [31.000, 96.000], mean observation: 3.141 [-1.201, 10.471], loss: 1.433107, mae: 4.877522, mean_q: 5.121650
 29355/100000: episode: 2996, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.172, mean reward: 0.417 [0.408, 0.425], mean action: 49.100 [4.000, 74.000], mean observation: 3.158 [-1.596, 10.325], loss: 1.292957, mae: 4.876960, mean_q: 5.120694
 29365/100000: episode: 2997, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.963, mean reward: 0.396 [0.389, 0.451], mean action: 63.000 [22.000, 71.000], mean observation: 3.152 [-0.768, 10.259], loss: 0.967697, mae: 4.875783, mean_q: 5.121917
 29375/100000: episode: 2998, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.022, mean reward: 0.402 [0.379, 0.480], mean action: 69.000 [69.000, 69.000], mean observation: 3.167 [-1.628, 10.355], loss: 1.268442, mae: 4.877233, mean_q: 5.123085
 29385/100000: episode: 2999, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.650, mean reward: 0.365 [0.334, 0.458], mean action: 58.400 [3.000, 101.000], mean observation: 3.156 [-1.608, 10.192], loss: 0.990233, mae: 4.876229, mean_q: 5.122344
 29395/100000: episode: 3000, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.284, mean reward: 0.428 [0.390, 0.550], mean action: 58.400 [12.000, 69.000], mean observation: 3.159 [-1.679, 10.250], loss: 1.221943, mae: 4.877450, mean_q: 5.123250
 29405/100000: episode: 3001, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.349, mean reward: 0.435 [0.360, 0.487], mean action: 67.900 [1.000, 101.000], mean observation: 3.159 [-1.508, 10.315], loss: 1.419444, mae: 4.878276, mean_q: 5.123878
 29415/100000: episode: 3002, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.611, mean reward: 0.461 [0.364, 0.479], mean action: 67.200 [38.000, 94.000], mean observation: 3.154 [-1.469, 10.301], loss: 0.963035, mae: 4.876597, mean_q: 5.125047
 29425/100000: episode: 3003, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.908, mean reward: 0.391 [0.377, 0.457], mean action: 65.000 [65.000, 65.000], mean observation: 3.145 [-1.237, 10.322], loss: 1.162515, mae: 4.877397, mean_q: 5.126842
 29435/100000: episode: 3004, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.222, mean reward: 0.422 [0.417, 0.456], mean action: 48.900 [7.000, 74.000], mean observation: 3.157 [-1.511, 10.268], loss: 1.277146, mae: 4.877933, mean_q: 5.127654
 29445/100000: episode: 3005, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.142, mean reward: 0.414 [0.297, 0.489], mean action: 50.900 [10.000, 80.000], mean observation: 3.160 [-1.432, 10.372], loss: 1.311656, mae: 4.878498, mean_q: 5.127956
 29455/100000: episode: 3006, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.664, mean reward: 0.366 [0.315, 0.442], mean action: 54.100 [11.000, 75.000], mean observation: 3.149 [-1.581, 10.363], loss: 1.492063, mae: 4.879163, mean_q: 5.128874
 29465/100000: episode: 3007, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.573, mean reward: 0.457 [0.382, 0.546], mean action: 52.100 [6.000, 76.000], mean observation: 3.153 [-1.569, 10.501], loss: 1.028649, mae: 4.877572, mean_q: 5.130192
 29475/100000: episode: 3008, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.211, mean reward: 0.421 [0.369, 0.501], mean action: 61.300 [31.000, 96.000], mean observation: 3.159 [-1.815, 10.308], loss: 1.069804, mae: 4.877995, mean_q: 5.130942
 29485/100000: episode: 3009, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.910, mean reward: 0.391 [0.371, 0.428], mean action: 54.000 [27.000, 65.000], mean observation: 3.166 [-1.094, 10.348], loss: 1.288351, mae: 4.879155, mean_q: 5.131588
 29495/100000: episode: 3010, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.384, mean reward: 0.438 [0.410, 0.481], mean action: 65.800 [32.000, 95.000], mean observation: 3.152 [-1.950, 10.315], loss: 1.422713, mae: 4.879699, mean_q: 5.132143
 29505/100000: episode: 3011, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.782, mean reward: 0.378 [0.294, 0.545], mean action: 52.800 [1.000, 69.000], mean observation: 3.161 [-1.269, 10.265], loss: 1.037976, mae: 4.878199, mean_q: 5.134222
 29515/100000: episode: 3012, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.617, mean reward: 0.362 [0.321, 0.434], mean action: 60.300 [5.000, 100.000], mean observation: 3.166 [-1.091, 10.220], loss: 1.019242, mae: 4.878200, mean_q: 5.135563
 29525/100000: episode: 3013, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.659, mean reward: 0.366 [0.323, 0.443], mean action: 63.400 [36.000, 96.000], mean observation: 3.152 [-1.594, 10.222], loss: 1.370361, mae: 4.879773, mean_q: 5.136545
 29535/100000: episode: 3014, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.451, mean reward: 0.445 [0.445, 0.445], mean action: 59.300 [11.000, 90.000], mean observation: 3.149 [-1.214, 10.275], loss: 1.029266, mae: 4.878309, mean_q: 5.137528
 29545/100000: episode: 3015, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.854, mean reward: 0.385 [0.321, 0.484], mean action: 55.200 [10.000, 75.000], mean observation: 3.166 [-0.861, 10.417], loss: 1.279593, mae: 4.879755, mean_q: 5.138440
 29555/100000: episode: 3016, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 3.735, mean reward: 0.374 [0.344, 0.486], mean action: 71.200 [41.000, 95.000], mean observation: 3.154 [-1.956, 10.397], loss: 1.349556, mae: 4.880404, mean_q: 5.133601
 29565/100000: episode: 3017, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.311, mean reward: 0.431 [0.364, 0.479], mean action: 43.100 [4.000, 65.000], mean observation: 3.163 [-1.148, 10.238], loss: 1.255452, mae: 4.879972, mean_q: 5.131883
 29575/100000: episode: 3018, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.940, mean reward: 0.394 [0.341, 0.436], mean action: 34.500 [0.000, 93.000], mean observation: 3.156 [-1.484, 10.417], loss: 1.166373, mae: 4.879975, mean_q: 5.132645
 29585/100000: episode: 3019, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 3.997, mean reward: 0.400 [0.293, 0.578], mean action: 38.500 [4.000, 88.000], mean observation: 3.155 [-1.272, 10.215], loss: 0.881203, mae: 4.879372, mean_q: 5.133529
 29595/100000: episode: 3020, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.124, mean reward: 0.412 [0.354, 0.534], mean action: 43.600 [19.000, 80.000], mean observation: 3.166 [-1.138, 10.465], loss: 1.410012, mae: 4.881622, mean_q: 5.133915
 29605/100000: episode: 3021, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.265, mean reward: 0.427 [0.366, 0.477], mean action: 43.300 [23.000, 73.000], mean observation: 3.154 [-1.849, 10.401], loss: 1.238670, mae: 4.881402, mean_q: 5.136894
 29615/100000: episode: 3022, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.910, mean reward: 0.391 [0.350, 0.526], mean action: 54.200 [21.000, 96.000], mean observation: 3.171 [-1.679, 10.299], loss: 1.351825, mae: 4.881605, mean_q: 5.138782
 29625/100000: episode: 3023, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.970, mean reward: 0.397 [0.358, 0.454], mean action: 47.800 [3.000, 91.000], mean observation: 3.153 [-1.339, 10.338], loss: 1.201389, mae: 4.881219, mean_q: 5.140334
 29635/100000: episode: 3024, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.900, mean reward: 0.390 [0.344, 0.498], mean action: 53.500 [39.000, 97.000], mean observation: 3.166 [-1.233, 10.351], loss: 1.008377, mae: 4.880250, mean_q: 5.140993
 29645/100000: episode: 3025, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.132, mean reward: 0.413 [0.377, 0.534], mean action: 51.200 [39.000, 94.000], mean observation: 3.142 [-1.220, 10.255], loss: 1.133635, mae: 4.881070, mean_q: 5.142086
 29655/100000: episode: 3026, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.038, mean reward: 0.404 [0.338, 0.495], mean action: 52.800 [4.000, 89.000], mean observation: 3.150 [-1.914, 10.238], loss: 1.309273, mae: 4.881813, mean_q: 5.143174
 29665/100000: episode: 3027, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.015, mean reward: 0.401 [0.366, 0.467], mean action: 45.500 [39.000, 86.000], mean observation: 3.162 [-1.536, 10.336], loss: 1.032827, mae: 4.880726, mean_q: 5.144855
 29669/100000: episode: 3028, duration: 0.076s, episode steps: 4, steps per second: 53, episode reward: 11.053, mean reward: 2.763 [0.346, 10.000], mean action: 55.250 [39.000, 87.000], mean observation: 3.150 [-1.274, 10.197], loss: 0.900914, mae: 4.880153, mean_q: 5.145765
 29679/100000: episode: 3029, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.407, mean reward: 0.441 [0.351, 0.497], mean action: 35.900 [13.000, 39.000], mean observation: 3.156 [-1.350, 10.399], loss: 1.577026, mae: 4.882892, mean_q: 5.143523
 29689/100000: episode: 3030, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.891, mean reward: 0.389 [0.316, 0.442], mean action: 45.100 [18.000, 69.000], mean observation: 3.162 [-1.301, 10.363], loss: 1.080677, mae: 4.880944, mean_q: 5.143212
 29699/100000: episode: 3031, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.936, mean reward: 0.394 [0.336, 0.503], mean action: 42.200 [39.000, 62.000], mean observation: 3.156 [-1.259, 10.334], loss: 1.461851, mae: 4.882586, mean_q: 5.144864
 29709/100000: episode: 3032, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.878, mean reward: 0.388 [0.312, 0.458], mean action: 38.700 [10.000, 65.000], mean observation: 3.141 [-1.326, 10.250], loss: 1.135900, mae: 4.881553, mean_q: 5.147353
 29719/100000: episode: 3033, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.009, mean reward: 0.401 [0.361, 0.523], mean action: 38.500 [8.000, 98.000], mean observation: 3.165 [-0.958, 10.227], loss: 1.116346, mae: 4.881636, mean_q: 5.144321
 29729/100000: episode: 3034, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.474, mean reward: 0.447 [0.345, 0.542], mean action: 47.900 [27.000, 100.000], mean observation: 3.157 [-0.913, 10.290], loss: 1.308935, mae: 4.882746, mean_q: 5.143762
 29739/100000: episode: 3035, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.465, mean reward: 0.447 [0.344, 0.550], mean action: 33.400 [4.000, 68.000], mean observation: 3.160 [-1.107, 10.235], loss: 1.326356, mae: 4.883374, mean_q: 5.148322
 29749/100000: episode: 3036, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 5.321, mean reward: 0.532 [0.334, 0.586], mean action: 37.300 [2.000, 87.000], mean observation: 3.158 [-1.665, 10.326], loss: 1.346105, mae: 4.883780, mean_q: 5.151349
 29759/100000: episode: 3037, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.756, mean reward: 0.376 [0.301, 0.441], mean action: 47.200 [26.000, 95.000], mean observation: 3.153 [-1.559, 10.243], loss: 0.997362, mae: 4.882586, mean_q: 5.152786
 29769/100000: episode: 3038, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.357, mean reward: 0.436 [0.329, 0.550], mean action: 36.000 [0.000, 81.000], mean observation: 3.160 [-1.315, 10.280], loss: 1.012460, mae: 4.883198, mean_q: 5.154855
 29779/100000: episode: 3039, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.677, mean reward: 0.368 [0.299, 0.426], mean action: 62.500 [39.000, 98.000], mean observation: 3.151 [-1.340, 10.283], loss: 1.288067, mae: 4.884538, mean_q: 5.157964
 29789/100000: episode: 3040, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.913, mean reward: 0.391 [0.357, 0.479], mean action: 66.300 [31.000, 98.000], mean observation: 3.156 [-1.297, 10.409], loss: 0.891474, mae: 4.882972, mean_q: 5.160198
 29799/100000: episode: 3041, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.180, mean reward: 0.418 [0.346, 0.547], mean action: 53.100 [39.000, 91.000], mean observation: 3.163 [-1.609, 10.536], loss: 1.278299, mae: 4.884946, mean_q: 5.162021
 29809/100000: episode: 3042, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 3.962, mean reward: 0.396 [0.333, 0.463], mean action: 36.200 [4.000, 73.000], mean observation: 3.157 [-1.486, 10.492], loss: 0.884375, mae: 4.883795, mean_q: 5.157430
 29819/100000: episode: 3043, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.465, mean reward: 0.447 [0.373, 0.524], mean action: 43.200 [22.000, 73.000], mean observation: 3.159 [-1.442, 10.349], loss: 1.344851, mae: 4.886120, mean_q: 5.154445
 29829/100000: episode: 3044, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.561, mean reward: 0.456 [0.362, 0.549], mean action: 47.400 [39.000, 87.000], mean observation: 3.152 [-1.844, 10.403], loss: 0.916868, mae: 4.884482, mean_q: 5.149200
 29839/100000: episode: 3045, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 6.309, mean reward: 0.631 [0.631, 0.631], mean action: 42.000 [24.000, 68.000], mean observation: 3.175 [-1.096, 10.399], loss: 0.830445, mae: 4.884710, mean_q: 5.149007
 29849/100000: episode: 3046, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.264, mean reward: 0.426 [0.348, 0.521], mean action: 41.400 [0.000, 100.000], mean observation: 3.158 [-1.500, 10.374], loss: 1.224782, mae: 4.886906, mean_q: 5.150301
 29859/100000: episode: 3047, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.935, mean reward: 0.493 [0.493, 0.493], mean action: 42.700 [9.000, 79.000], mean observation: 3.138 [-2.279, 10.248], loss: 1.251092, mae: 4.887016, mean_q: 5.151344
 29869/100000: episode: 3048, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.573, mean reward: 0.457 [0.390, 0.588], mean action: 58.500 [39.000, 99.000], mean observation: 3.157 [-1.662, 10.416], loss: 1.307856, mae: 4.887337, mean_q: 5.152175
 29879/100000: episode: 3049, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.806, mean reward: 0.381 [0.315, 0.488], mean action: 40.900 [4.000, 101.000], mean observation: 3.157 [-1.039, 10.243], loss: 1.123743, mae: 4.886833, mean_q: 5.151660
 29889/100000: episode: 3050, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.933, mean reward: 0.393 [0.368, 0.494], mean action: 41.000 [39.000, 59.000], mean observation: 3.158 [-2.364, 10.316], loss: 1.590880, mae: 4.888806, mean_q: 5.151219
 29899/100000: episode: 3051, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.201, mean reward: 0.420 [0.397, 0.533], mean action: 44.900 [39.000, 85.000], mean observation: 3.142 [-1.750, 10.434], loss: 1.254139, mae: 4.887348, mean_q: 5.149206
 29909/100000: episode: 3052, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.066, mean reward: 0.407 [0.349, 0.503], mean action: 35.900 [13.000, 59.000], mean observation: 3.160 [-1.069, 10.370], loss: 1.111355, mae: 4.886948, mean_q: 5.146617
 29919/100000: episode: 3053, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.173, mean reward: 0.417 [0.361, 0.491], mean action: 47.600 [4.000, 79.000], mean observation: 3.150 [-1.336, 10.247], loss: 1.260359, mae: 4.887785, mean_q: 5.144109
 29929/100000: episode: 3054, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.063, mean reward: 0.406 [0.298, 0.486], mean action: 40.200 [3.000, 68.000], mean observation: 3.164 [-1.084, 10.368], loss: 1.131022, mae: 4.887257, mean_q: 5.144380
 29939/100000: episode: 3055, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.122, mean reward: 0.412 [0.348, 0.487], mean action: 59.200 [26.000, 101.000], mean observation: 3.168 [-1.755, 10.407], loss: 1.212125, mae: 4.887816, mean_q: 5.145389
 29949/100000: episode: 3056, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 4.137, mean reward: 0.414 [0.363, 0.545], mean action: 56.900 [24.000, 88.000], mean observation: 3.157 [-0.934, 10.521], loss: 1.157703, mae: 4.887610, mean_q: 5.147177
 29959/100000: episode: 3057, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.754, mean reward: 0.375 [0.304, 0.541], mean action: 43.500 [19.000, 81.000], mean observation: 3.155 [-1.243, 10.293], loss: 1.287718, mae: 4.888432, mean_q: 5.150033
 29969/100000: episode: 3058, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.329, mean reward: 0.433 [0.354, 0.521], mean action: 44.000 [25.000, 92.000], mean observation: 3.156 [-1.489, 10.319], loss: 1.249038, mae: 4.888364, mean_q: 5.151460
 29979/100000: episode: 3059, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.244, mean reward: 0.424 [0.287, 0.536], mean action: 47.200 [2.000, 95.000], mean observation: 3.156 [-1.799, 10.368], loss: 1.338047, mae: 4.888957, mean_q: 5.148296
 29989/100000: episode: 3060, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.385, mean reward: 0.439 [0.399, 0.558], mean action: 43.800 [15.000, 100.000], mean observation: 3.167 [-1.310, 10.203], loss: 1.454708, mae: 4.889503, mean_q: 5.146831
 29999/100000: episode: 3061, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.125, mean reward: 0.412 [0.355, 0.533], mean action: 39.900 [9.000, 79.000], mean observation: 3.142 [-1.348, 10.279], loss: 1.359389, mae: 4.888869, mean_q: 5.144594
 30009/100000: episode: 3062, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.197, mean reward: 0.420 [0.356, 0.519], mean action: 38.400 [17.000, 67.000], mean observation: 3.144 [-1.696, 10.255], loss: 1.016397, mae: 4.888015, mean_q: 5.142163
 30019/100000: episode: 3063, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.015, mean reward: 0.402 [0.298, 0.496], mean action: 33.800 [5.000, 54.000], mean observation: 3.152 [-1.514, 10.269], loss: 1.119078, mae: 4.888467, mean_q: 5.140835
 30029/100000: episode: 3064, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.170, mean reward: 0.417 [0.400, 0.492], mean action: 46.300 [15.000, 95.000], mean observation: 3.140 [-1.941, 10.296], loss: 1.469082, mae: 4.890379, mean_q: 5.140240
 30039/100000: episode: 3065, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.651, mean reward: 0.365 [0.338, 0.434], mean action: 40.400 [12.000, 87.000], mean observation: 3.158 [-1.330, 10.363], loss: 0.995256, mae: 4.888464, mean_q: 5.145118
 30049/100000: episode: 3066, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.296, mean reward: 0.430 [0.427, 0.458], mean action: 38.300 [8.000, 89.000], mean observation: 3.154 [-1.740, 10.242], loss: 1.157073, mae: 4.889760, mean_q: 5.147580
 30059/100000: episode: 3067, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.888, mean reward: 0.389 [0.351, 0.449], mean action: 41.100 [22.000, 62.000], mean observation: 3.171 [-1.956, 10.268], loss: 0.994749, mae: 4.889009, mean_q: 5.146242
 30069/100000: episode: 3068, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.765, mean reward: 0.377 [0.327, 0.501], mean action: 39.300 [31.000, 56.000], mean observation: 3.156 [-1.005, 10.238], loss: 1.467813, mae: 4.891377, mean_q: 5.144676
 30079/100000: episode: 3069, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.117, mean reward: 0.412 [0.379, 0.512], mean action: 52.600 [26.000, 99.000], mean observation: 3.149 [-1.579, 10.259], loss: 0.977511, mae: 4.889386, mean_q: 5.145534
 30089/100000: episode: 3070, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.013, mean reward: 0.401 [0.350, 0.452], mean action: 38.400 [5.000, 64.000], mean observation: 3.154 [-1.622, 10.298], loss: 1.272583, mae: 4.891036, mean_q: 5.146876
 30099/100000: episode: 3071, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.660, mean reward: 0.466 [0.455, 0.510], mean action: 43.900 [14.000, 87.000], mean observation: 3.154 [-1.715, 10.521], loss: 1.452862, mae: 4.891993, mean_q: 5.148320
 30109/100000: episode: 3072, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.942, mean reward: 0.394 [0.383, 0.407], mean action: 57.000 [2.000, 100.000], mean observation: 3.154 [-1.729, 10.357], loss: 1.052136, mae: 4.890585, mean_q: 5.148556
 30119/100000: episode: 3073, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 5.153, mean reward: 0.515 [0.515, 0.515], mean action: 47.600 [39.000, 77.000], mean observation: 3.135 [-0.891, 10.281], loss: 1.062346, mae: 4.890937, mean_q: 5.145136
 30129/100000: episode: 3074, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.191, mean reward: 0.419 [0.336, 0.518], mean action: 54.500 [26.000, 97.000], mean observation: 3.159 [-0.969, 10.239], loss: 1.126429, mae: 4.891422, mean_q: 5.144552
 30139/100000: episode: 3075, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.009, mean reward: 0.401 [0.379, 0.464], mean action: 41.800 [5.000, 86.000], mean observation: 3.152 [-1.694, 10.213], loss: 1.345166, mae: 4.892568, mean_q: 5.143565
 30149/100000: episode: 3076, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.898, mean reward: 0.390 [0.340, 0.469], mean action: 54.100 [39.000, 95.000], mean observation: 3.145 [-1.328, 10.457], loss: 1.087979, mae: 4.891783, mean_q: 5.140536
 30159/100000: episode: 3077, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.068, mean reward: 0.407 [0.401, 0.448], mean action: 51.600 [15.000, 99.000], mean observation: 3.160 [-1.820, 10.469], loss: 1.444446, mae: 4.893487, mean_q: 5.139551
 30169/100000: episode: 3078, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.813, mean reward: 0.381 [0.332, 0.497], mean action: 61.000 [10.000, 96.000], mean observation: 3.148 [-1.359, 10.321], loss: 1.190113, mae: 4.892997, mean_q: 5.139980
 30179/100000: episode: 3079, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.520, mean reward: 0.452 [0.394, 0.530], mean action: 65.000 [25.000, 97.000], mean observation: 3.145 [-0.956, 10.230], loss: 1.294560, mae: 4.893487, mean_q: 5.142018
 30181/100000: episode: 3080, duration: 0.038s, episode steps: 2, steps per second: 52, episode reward: 10.395, mean reward: 5.197 [0.395, 10.000], mean action: 58.500 [48.000, 69.000], mean observation: 3.171 [-0.766, 10.268], loss: 1.105807, mae: 4.892828, mean_q: 5.141865
 30191/100000: episode: 3081, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.083, mean reward: 0.408 [0.378, 0.458], mean action: 49.900 [3.000, 81.000], mean observation: 3.148 [-1.530, 10.456], loss: 1.207367, mae: 4.893614, mean_q: 5.140778
 30201/100000: episode: 3082, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.529, mean reward: 0.453 [0.340, 0.583], mean action: 60.500 [12.000, 101.000], mean observation: 3.167 [-1.081, 10.392], loss: 1.373552, mae: 4.894245, mean_q: 5.139031
 30211/100000: episode: 3083, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.912, mean reward: 0.391 [0.338, 0.487], mean action: 54.700 [4.000, 91.000], mean observation: 3.169 [-1.386, 10.248], loss: 1.278717, mae: 4.894116, mean_q: 5.138881
 30221/100000: episode: 3084, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.050, mean reward: 0.405 [0.347, 0.442], mean action: 67.800 [23.000, 87.000], mean observation: 3.155 [-1.000, 10.363], loss: 1.220460, mae: 4.893796, mean_q: 5.139781
 30231/100000: episode: 3085, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.260, mean reward: 0.426 [0.374, 0.540], mean action: 51.800 [20.000, 78.000], mean observation: 3.165 [-1.621, 10.342], loss: 1.039565, mae: 4.892944, mean_q: 5.142043
 30241/100000: episode: 3086, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.709, mean reward: 0.371 [0.364, 0.404], mean action: 46.300 [9.000, 100.000], mean observation: 3.152 [-1.193, 10.350], loss: 1.291338, mae: 4.894295, mean_q: 5.142061
 30251/100000: episode: 3087, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.235, mean reward: 0.423 [0.406, 0.442], mean action: 63.600 [3.000, 99.000], mean observation: 3.178 [-1.430, 10.273], loss: 1.532105, mae: 4.895308, mean_q: 5.143350
 30261/100000: episode: 3088, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 3.720, mean reward: 0.372 [0.334, 0.440], mean action: 60.600 [39.000, 91.000], mean observation: 3.156 [-0.878, 10.267], loss: 1.205035, mae: 4.893912, mean_q: 5.145468
 30271/100000: episode: 3089, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 14.038, mean reward: 1.404 [0.399, 10.000], mean action: 28.700 [6.000, 47.000], mean observation: 3.151 [-1.686, 10.346], loss: 1.408160, mae: 4.894860, mean_q: 5.148304
 30281/100000: episode: 3090, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.247, mean reward: 0.425 [0.323, 0.560], mean action: 39.900 [16.000, 91.000], mean observation: 3.153 [-1.878, 10.425], loss: 1.295867, mae: 4.894217, mean_q: 5.145833
 30291/100000: episode: 3091, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.713, mean reward: 0.371 [0.261, 0.420], mean action: 61.500 [10.000, 81.000], mean observation: 3.159 [-1.723, 10.202], loss: 1.124352, mae: 4.893701, mean_q: 5.145920
 30301/100000: episode: 3092, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.179, mean reward: 0.418 [0.385, 0.468], mean action: 59.000 [16.000, 69.000], mean observation: 3.155 [-2.237, 10.335], loss: 1.082224, mae: 4.893713, mean_q: 5.147756
 30311/100000: episode: 3093, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.112, mean reward: 0.411 [0.410, 0.421], mean action: 61.900 [14.000, 75.000], mean observation: 3.164 [-1.100, 10.284], loss: 1.308028, mae: 4.894870, mean_q: 5.149331
 30321/100000: episode: 3094, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.710, mean reward: 0.371 [0.325, 0.429], mean action: 63.700 [5.000, 99.000], mean observation: 3.171 [-1.109, 10.346], loss: 1.267861, mae: 4.894785, mean_q: 5.150235
 30331/100000: episode: 3095, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.434, mean reward: 0.443 [0.436, 0.451], mean action: 60.500 [26.000, 92.000], mean observation: 3.159 [-1.102, 10.294], loss: 1.313196, mae: 4.894958, mean_q: 5.150262
 30341/100000: episode: 3096, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.832, mean reward: 0.383 [0.339, 0.415], mean action: 61.800 [31.000, 99.000], mean observation: 3.155 [-1.298, 10.431], loss: 1.267450, mae: 4.894667, mean_q: 5.152244
 30351/100000: episode: 3097, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.878, mean reward: 0.388 [0.355, 0.441], mean action: 45.800 [4.000, 79.000], mean observation: 3.153 [-2.262, 10.340], loss: 0.900317, mae: 4.893341, mean_q: 5.155524
 30361/100000: episode: 3098, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.363, mean reward: 0.436 [0.407, 0.466], mean action: 48.100 [0.000, 93.000], mean observation: 3.149 [-2.044, 10.352], loss: 1.567531, mae: 4.896163, mean_q: 5.158927
 30371/100000: episode: 3099, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.192, mean reward: 0.419 [0.364, 0.459], mean action: 65.500 [37.000, 99.000], mean observation: 3.147 [-1.265, 10.260], loss: 1.185955, mae: 4.894520, mean_q: 5.161578
 30381/100000: episode: 3100, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.638, mean reward: 0.464 [0.329, 0.576], mean action: 53.400 [12.000, 98.000], mean observation: 3.164 [-1.047, 10.314], loss: 1.197670, mae: 4.894755, mean_q: 5.158230
 30391/100000: episode: 3101, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 4.014, mean reward: 0.401 [0.366, 0.496], mean action: 58.800 [15.000, 94.000], mean observation: 3.170 [-1.472, 10.338], loss: 1.341679, mae: 4.895551, mean_q: 5.155813
 30401/100000: episode: 3102, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.352, mean reward: 0.435 [0.387, 0.518], mean action: 41.600 [3.000, 95.000], mean observation: 3.146 [-1.640, 10.459], loss: 1.433728, mae: 4.896197, mean_q: 5.153437
 30403/100000: episode: 3103, duration: 0.036s, episode steps: 2, steps per second: 56, episode reward: 10.350, mean reward: 5.175 [0.350, 10.000], mean action: 69.000 [69.000, 69.000], mean observation: 3.155 [-0.780, 10.222], loss: 1.558945, mae: 4.896281, mean_q: 5.150064
 30413/100000: episode: 3104, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.855, mean reward: 0.486 [0.486, 0.486], mean action: 62.900 [5.000, 100.000], mean observation: 3.152 [-1.432, 10.316], loss: 1.145540, mae: 4.894999, mean_q: 5.147405
 30423/100000: episode: 3105, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.874, mean reward: 0.387 [0.356, 0.464], mean action: 59.300 [2.000, 87.000], mean observation: 3.143 [-1.703, 10.331], loss: 1.103259, mae: 4.894811, mean_q: 5.144402
 30433/100000: episode: 3106, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.337, mean reward: 0.434 [0.373, 0.483], mean action: 53.600 [0.000, 69.000], mean observation: 3.156 [-1.526, 10.378], loss: 1.145573, mae: 4.895366, mean_q: 5.143243
 30443/100000: episode: 3107, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.135, mean reward: 0.413 [0.364, 0.500], mean action: 68.200 [23.000, 95.000], mean observation: 3.149 [-1.403, 10.269], loss: 1.221666, mae: 4.896114, mean_q: 5.142614
 30453/100000: episode: 3108, duration: 0.225s, episode steps: 10, steps per second: 44, episode reward: 4.197, mean reward: 0.420 [0.340, 0.517], mean action: 19.000 [4.000, 89.000], mean observation: 3.155 [-1.850, 10.187], loss: 1.038294, mae: 4.895622, mean_q: 5.146885
 30463/100000: episode: 3109, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.054, mean reward: 0.405 [0.338, 0.487], mean action: 16.500 [4.000, 92.000], mean observation: 3.154 [-1.270, 10.347], loss: 1.156823, mae: 4.896462, mean_q: 5.149531
 30473/100000: episode: 3110, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.367, mean reward: 0.437 [0.328, 0.483], mean action: 29.300 [4.000, 97.000], mean observation: 3.162 [-1.291, 10.437], loss: 1.325642, mae: 4.897234, mean_q: 5.150411
 30483/100000: episode: 3111, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.179, mean reward: 0.418 [0.331, 0.484], mean action: 22.600 [1.000, 90.000], mean observation: 3.162 [-1.081, 10.551], loss: 1.054607, mae: 4.896101, mean_q: 5.151326
 30493/100000: episode: 3112, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.794, mean reward: 0.379 [0.364, 0.394], mean action: 30.300 [4.000, 93.000], mean observation: 3.157 [-1.360, 10.343], loss: 1.017997, mae: 4.895993, mean_q: 5.148701
 30503/100000: episode: 3113, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.438, mean reward: 0.444 [0.326, 0.531], mean action: 26.300 [2.000, 78.000], mean observation: 3.165 [-1.379, 10.403], loss: 1.115328, mae: 4.897005, mean_q: 5.140876
 30513/100000: episode: 3114, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.788, mean reward: 0.379 [0.278, 0.499], mean action: 54.500 [6.000, 69.000], mean observation: 3.158 [-1.814, 10.278], loss: 1.193749, mae: 4.897723, mean_q: 5.138652
 30523/100000: episode: 3115, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.585, mean reward: 0.458 [0.454, 0.465], mean action: 54.100 [9.000, 96.000], mean observation: 3.158 [-1.464, 10.366], loss: 1.044442, mae: 4.897342, mean_q: 5.139529
 30533/100000: episode: 3116, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.237, mean reward: 0.424 [0.394, 0.491], mean action: 68.200 [18.000, 100.000], mean observation: 3.170 [-1.393, 10.425], loss: 1.112876, mae: 4.897836, mean_q: 5.140584
 30543/100000: episode: 3117, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.964, mean reward: 0.396 [0.382, 0.448], mean action: 64.800 [19.000, 78.000], mean observation: 3.157 [-1.162, 10.361], loss: 1.121197, mae: 4.898316, mean_q: 5.139330
 30552/100000: episode: 3118, duration: 0.131s, episode steps: 9, steps per second: 69, episode reward: 13.312, mean reward: 1.479 [0.377, 10.000], mean action: 49.444 [5.000, 70.000], mean observation: 3.164 [-1.187, 10.246], loss: 0.960252, mae: 4.897816, mean_q: 5.139544
 30562/100000: episode: 3119, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.813, mean reward: 0.381 [0.358, 0.406], mean action: 58.500 [28.000, 80.000], mean observation: 3.155 [-1.231, 10.186], loss: 1.453354, mae: 4.900365, mean_q: 5.141488
 30572/100000: episode: 3120, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.608, mean reward: 0.361 [0.320, 0.421], mean action: 63.400 [28.000, 69.000], mean observation: 3.148 [-1.453, 10.254], loss: 1.074213, mae: 4.899016, mean_q: 5.144106
 30582/100000: episode: 3121, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.213, mean reward: 0.421 [0.402, 0.560], mean action: 74.400 [61.000, 96.000], mean observation: 3.149 [-1.277, 10.365], loss: 1.166945, mae: 4.899888, mean_q: 5.144659
 30592/100000: episode: 3122, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.550, mean reward: 0.355 [0.309, 0.413], mean action: 62.700 [31.000, 84.000], mean observation: 3.152 [-1.156, 10.402], loss: 1.211963, mae: 4.900114, mean_q: 5.140189
 30602/100000: episode: 3123, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.254, mean reward: 0.425 [0.363, 0.511], mean action: 57.300 [9.000, 91.000], mean observation: 3.152 [-1.704, 10.241], loss: 1.420439, mae: 4.901087, mean_q: 5.139698
 30612/100000: episode: 3124, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.845, mean reward: 0.384 [0.349, 0.470], mean action: 51.700 [1.000, 95.000], mean observation: 3.159 [-1.663, 10.300], loss: 1.037214, mae: 4.899261, mean_q: 5.139853
 30622/100000: episode: 3125, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.991, mean reward: 0.399 [0.396, 0.423], mean action: 58.600 [17.000, 101.000], mean observation: 3.166 [-1.245, 10.307], loss: 1.372112, mae: 4.900548, mean_q: 5.139803
 30632/100000: episode: 3126, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 5.493, mean reward: 0.549 [0.474, 0.581], mean action: 65.900 [10.000, 101.000], mean observation: 3.154 [-2.800, 10.277], loss: 1.076667, mae: 4.899443, mean_q: 5.139749
 30642/100000: episode: 3127, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.338, mean reward: 0.434 [0.434, 0.434], mean action: 62.400 [30.000, 89.000], mean observation: 3.145 [-0.767, 10.337], loss: 1.338121, mae: 4.900605, mean_q: 5.139284
 30652/100000: episode: 3128, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.860, mean reward: 0.386 [0.374, 0.415], mean action: 60.700 [31.000, 99.000], mean observation: 3.162 [-1.470, 10.395], loss: 1.325321, mae: 4.900619, mean_q: 5.137567
 30662/100000: episode: 3129, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.142, mean reward: 0.414 [0.326, 0.519], mean action: 45.200 [11.000, 74.000], mean observation: 3.157 [-1.243, 10.225], loss: 1.004837, mae: 4.899273, mean_q: 5.133242
 30672/100000: episode: 3130, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.222, mean reward: 0.422 [0.411, 0.449], mean action: 60.500 [11.000, 101.000], mean observation: 3.162 [-0.551, 10.280], loss: 0.979137, mae: 4.899339, mean_q: 5.131149
 30682/100000: episode: 3131, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.172, mean reward: 0.417 [0.323, 0.528], mean action: 36.500 [3.000, 95.000], mean observation: 3.160 [-1.244, 10.419], loss: 1.135570, mae: 4.900411, mean_q: 5.128159
 30692/100000: episode: 3132, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.627, mean reward: 0.463 [0.317, 0.559], mean action: 47.000 [32.000, 99.000], mean observation: 3.167 [-1.633, 10.342], loss: 1.172827, mae: 4.900771, mean_q: 5.128686
 30702/100000: episode: 3133, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.426, mean reward: 0.443 [0.315, 0.499], mean action: 30.200 [1.000, 69.000], mean observation: 3.160 [-0.978, 10.535], loss: 1.338642, mae: 4.902040, mean_q: 5.129689
 30712/100000: episode: 3134, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.939, mean reward: 0.394 [0.357, 0.432], mean action: 38.100 [23.000, 70.000], mean observation: 3.154 [-1.130, 10.345], loss: 1.337491, mae: 4.902132, mean_q: 5.130091
 30716/100000: episode: 3135, duration: 0.064s, episode steps: 4, steps per second: 62, episode reward: 11.201, mean reward: 2.800 [0.366, 10.000], mean action: 49.750 [23.000, 84.000], mean observation: 3.172 [-0.891, 10.300], loss: 1.004518, mae: 4.900839, mean_q: 5.130224
 30726/100000: episode: 3136, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.329, mean reward: 0.433 [0.371, 0.532], mean action: 40.500 [20.000, 76.000], mean observation: 3.158 [-0.803, 10.228], loss: 1.156369, mae: 4.901524, mean_q: 5.130794
 30736/100000: episode: 3137, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.538, mean reward: 0.454 [0.392, 0.519], mean action: 53.300 [7.000, 99.000], mean observation: 3.168 [-0.886, 10.350], loss: 1.109574, mae: 4.901334, mean_q: 5.131560
 30746/100000: episode: 3138, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.878, mean reward: 0.388 [0.348, 0.463], mean action: 30.300 [18.000, 32.000], mean observation: 3.163 [-2.136, 10.354], loss: 1.126503, mae: 4.901152, mean_q: 5.132638
 30756/100000: episode: 3139, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.318, mean reward: 0.432 [0.410, 0.452], mean action: 59.700 [14.000, 95.000], mean observation: 3.159 [-0.807, 10.285], loss: 1.026801, mae: 4.901110, mean_q: 5.133549
 30766/100000: episode: 3140, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.200, mean reward: 0.420 [0.329, 0.501], mean action: 42.000 [21.000, 82.000], mean observation: 3.167 [-2.056, 10.255], loss: 1.001129, mae: 4.901192, mean_q: 5.136592
 30776/100000: episode: 3141, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.056, mean reward: 0.406 [0.351, 0.471], mean action: 41.700 [11.000, 89.000], mean observation: 3.154 [-1.343, 10.187], loss: 1.184330, mae: 4.902344, mean_q: 5.139095
 30786/100000: episode: 3142, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.089, mean reward: 0.409 [0.399, 0.440], mean action: 47.400 [27.000, 87.000], mean observation: 3.154 [-1.366, 10.310], loss: 0.999816, mae: 4.901507, mean_q: 5.140465
 30796/100000: episode: 3143, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.917, mean reward: 0.392 [0.330, 0.530], mean action: 37.100 [6.000, 71.000], mean observation: 3.156 [-1.461, 10.287], loss: 1.207229, mae: 4.902701, mean_q: 5.141811
 30806/100000: episode: 3144, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.080, mean reward: 0.408 [0.371, 0.453], mean action: 35.200 [7.000, 86.000], mean observation: 3.153 [-1.108, 10.212], loss: 1.322995, mae: 4.903401, mean_q: 5.143977
 30816/100000: episode: 3145, duration: 0.220s, episode steps: 10, steps per second: 45, episode reward: 4.135, mean reward: 0.414 [0.354, 0.480], mean action: 26.700 [0.000, 53.000], mean observation: 3.154 [-2.241, 10.269], loss: 1.101515, mae: 4.902732, mean_q: 5.139800
 30826/100000: episode: 3146, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.411, mean reward: 0.441 [0.384, 0.509], mean action: 39.100 [5.000, 96.000], mean observation: 3.154 [-1.592, 10.395], loss: 0.996053, mae: 4.902267, mean_q: 5.137585
 30836/100000: episode: 3147, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.189, mean reward: 0.419 [0.359, 0.510], mean action: 38.300 [19.000, 84.000], mean observation: 3.168 [-1.223, 10.328], loss: 1.101243, mae: 4.902992, mean_q: 5.137847
 30846/100000: episode: 3148, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.113, mean reward: 0.411 [0.313, 0.556], mean action: 38.400 [9.000, 98.000], mean observation: 3.153 [-1.474, 10.360], loss: 1.117871, mae: 4.903353, mean_q: 5.139607
 30856/100000: episode: 3149, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.802, mean reward: 0.480 [0.465, 0.543], mean action: 41.100 [27.000, 85.000], mean observation: 3.148 [-1.711, 10.376], loss: 1.053155, mae: 4.903800, mean_q: 5.136599
 30866/100000: episode: 3150, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 5.108, mean reward: 0.511 [0.504, 0.573], mean action: 50.300 [30.000, 100.000], mean observation: 3.151 [-1.497, 10.242], loss: 1.244272, mae: 4.904488, mean_q: 5.134458
 30876/100000: episode: 3151, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.869, mean reward: 0.387 [0.291, 0.591], mean action: 54.300 [8.000, 96.000], mean observation: 3.174 [-1.629, 10.259], loss: 1.297753, mae: 4.904984, mean_q: 5.134264
 30886/100000: episode: 3152, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.279, mean reward: 0.428 [0.346, 0.540], mean action: 28.500 [3.000, 32.000], mean observation: 3.160 [-1.243, 10.372], loss: 1.243092, mae: 4.905138, mean_q: 5.134723
 30895/100000: episode: 3153, duration: 0.188s, episode steps: 9, steps per second: 48, episode reward: 13.517, mean reward: 1.502 [0.402, 10.000], mean action: 27.000 [3.000, 32.000], mean observation: 3.157 [-1.331, 10.440], loss: 1.219946, mae: 4.904959, mean_q: 5.136339
 30905/100000: episode: 3154, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.307, mean reward: 0.431 [0.347, 0.568], mean action: 49.500 [32.000, 100.000], mean observation: 3.151 [-1.896, 10.461], loss: 1.128621, mae: 4.904408, mean_q: 5.137558
 30915/100000: episode: 3155, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.939, mean reward: 0.394 [0.344, 0.497], mean action: 35.600 [7.000, 74.000], mean observation: 3.159 [-1.246, 10.397], loss: 0.882615, mae: 4.903741, mean_q: 5.138320
 30925/100000: episode: 3156, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.379, mean reward: 0.438 [0.354, 0.567], mean action: 30.100 [4.000, 101.000], mean observation: 3.153 [-1.438, 10.124], loss: 1.140898, mae: 4.905099, mean_q: 5.137044
 30935/100000: episode: 3157, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.858, mean reward: 0.386 [0.291, 0.511], mean action: 33.000 [4.000, 94.000], mean observation: 3.159 [-2.131, 10.281], loss: 1.031389, mae: 4.905057, mean_q: 5.139181
 30945/100000: episode: 3158, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.084, mean reward: 0.408 [0.368, 0.481], mean action: 41.500 [4.000, 99.000], mean observation: 3.170 [-1.201, 10.372], loss: 1.318173, mae: 4.906316, mean_q: 5.140805
 30946/100000: episode: 3159, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 4.000 [4.000, 4.000], mean observation: 3.168 [-0.655, 10.152], loss: 0.784520, mae: 4.904734, mean_q: 5.141485
 30956/100000: episode: 3160, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.191, mean reward: 0.419 [0.373, 0.527], mean action: 10.500 [4.000, 69.000], mean observation: 3.145 [-1.669, 10.356], loss: 1.459152, mae: 4.907435, mean_q: 5.142091
 30966/100000: episode: 3161, duration: 0.215s, episode steps: 10, steps per second: 46, episode reward: 4.563, mean reward: 0.456 [0.338, 0.561], mean action: 23.200 [4.000, 89.000], mean observation: 3.154 [-1.750, 10.300], loss: 1.148789, mae: 4.906106, mean_q: 5.142897
 30976/100000: episode: 3162, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.856, mean reward: 0.386 [0.341, 0.437], mean action: 17.700 [2.000, 84.000], mean observation: 3.158 [-1.327, 10.241], loss: 1.398956, mae: 4.907632, mean_q: 5.144059
 30986/100000: episode: 3163, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.994, mean reward: 0.399 [0.322, 0.494], mean action: 42.000 [0.000, 83.000], mean observation: 3.163 [-1.046, 10.242], loss: 1.218379, mae: 4.906861, mean_q: 5.145290
 30987/100000: episode: 3164, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 32.000 [32.000, 32.000], mean observation: 3.186 [-0.886, 10.653], loss: 0.433024, mae: 4.903911, mean_q: 5.144490
 30997/100000: episode: 3165, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.089, mean reward: 0.409 [0.354, 0.569], mean action: 34.800 [9.000, 92.000], mean observation: 3.161 [-1.676, 10.635], loss: 1.077049, mae: 4.906471, mean_q: 5.142438
 31007/100000: episode: 3166, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.539, mean reward: 0.354 [0.306, 0.440], mean action: 39.600 [0.000, 91.000], mean observation: 3.147 [-1.464, 10.300], loss: 1.119024, mae: 4.906514, mean_q: 5.142332
 31017/100000: episode: 3167, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.823, mean reward: 0.382 [0.333, 0.445], mean action: 39.300 [13.000, 96.000], mean observation: 3.155 [-1.096, 10.329], loss: 1.248243, mae: 4.907397, mean_q: 5.143349
 31027/100000: episode: 3168, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.994, mean reward: 0.399 [0.305, 0.462], mean action: 22.900 [4.000, 78.000], mean observation: 3.159 [-1.745, 10.339], loss: 1.234711, mae: 4.907489, mean_q: 5.143020
 31037/100000: episode: 3169, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 3.879, mean reward: 0.388 [0.262, 0.474], mean action: 21.100 [4.000, 74.000], mean observation: 3.154 [-1.529, 10.326], loss: 0.964492, mae: 4.906161, mean_q: 5.144439
 31047/100000: episode: 3170, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.947, mean reward: 0.395 [0.334, 0.519], mean action: 28.400 [4.000, 86.000], mean observation: 3.153 [-1.656, 10.265], loss: 1.398760, mae: 4.908145, mean_q: 5.146692
 31057/100000: episode: 3171, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.766, mean reward: 0.377 [0.295, 0.459], mean action: 18.000 [4.000, 70.000], mean observation: 3.160 [-1.459, 10.414], loss: 1.223400, mae: 4.907150, mean_q: 5.149007
 31067/100000: episode: 3172, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 3.992, mean reward: 0.399 [0.312, 0.486], mean action: 22.500 [4.000, 101.000], mean observation: 3.159 [-1.506, 10.380], loss: 0.690167, mae: 4.905578, mean_q: 5.151286
 31077/100000: episode: 3173, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.947, mean reward: 0.395 [0.364, 0.474], mean action: 29.900 [3.000, 101.000], mean observation: 3.151 [-1.232, 10.352], loss: 1.312345, mae: 4.908620, mean_q: 5.148120
 31087/100000: episode: 3174, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.621, mean reward: 0.462 [0.407, 0.517], mean action: 38.300 [4.000, 89.000], mean observation: 3.149 [-1.330, 10.268], loss: 1.013439, mae: 4.907706, mean_q: 5.146379
 31097/100000: episode: 3175, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.092, mean reward: 0.409 [0.323, 0.478], mean action: 27.000 [4.000, 92.000], mean observation: 3.150 [-1.532, 10.314], loss: 0.975128, mae: 4.908046, mean_q: 5.147472
 31107/100000: episode: 3176, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.638, mean reward: 0.464 [0.391, 0.526], mean action: 29.400 [4.000, 82.000], mean observation: 3.150 [-1.274, 10.208], loss: 1.146358, mae: 4.909125, mean_q: 5.150279
 31117/100000: episode: 3177, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.119, mean reward: 0.412 [0.346, 0.439], mean action: 50.200 [31.000, 87.000], mean observation: 3.168 [-1.155, 10.360], loss: 0.996352, mae: 4.908584, mean_q: 5.155581
 31127/100000: episode: 3178, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.171, mean reward: 0.417 [0.302, 0.535], mean action: 36.500 [1.000, 97.000], mean observation: 3.164 [-1.409, 10.395], loss: 1.009815, mae: 4.909004, mean_q: 5.158904
 31137/100000: episode: 3179, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.834, mean reward: 0.483 [0.433, 0.519], mean action: 37.300 [3.000, 95.000], mean observation: 3.160 [-1.381, 10.387], loss: 1.175572, mae: 4.910131, mean_q: 5.159717
 31147/100000: episode: 3180, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.193, mean reward: 0.419 [0.321, 0.473], mean action: 28.600 [4.000, 88.000], mean observation: 3.161 [-1.438, 10.392], loss: 1.267332, mae: 4.910770, mean_q: 5.162134
 31157/100000: episode: 3181, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.278, mean reward: 0.428 [0.377, 0.469], mean action: 19.100 [4.000, 76.000], mean observation: 3.145 [-1.895, 10.182], loss: 0.968558, mae: 4.909793, mean_q: 5.173560
 31167/100000: episode: 3182, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.172, mean reward: 0.417 [0.362, 0.480], mean action: 26.700 [4.000, 94.000], mean observation: 3.155 [-1.703, 10.436], loss: 1.306147, mae: 4.911555, mean_q: 5.180255
 31177/100000: episode: 3183, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.255, mean reward: 0.425 [0.365, 0.472], mean action: 33.500 [4.000, 101.000], mean observation: 3.162 [-1.529, 10.291], loss: 1.125156, mae: 4.911356, mean_q: 5.185085
 31178/100000: episode: 3184, duration: 0.035s, episode steps: 1, steps per second: 29, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 4.000 [4.000, 4.000], mean observation: 3.139 [-1.024, 10.100], loss: 1.707925, mae: 4.913864, mean_q: 5.187146
 31188/100000: episode: 3185, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.671, mean reward: 0.367 [0.317, 0.547], mean action: 55.300 [4.000, 101.000], mean observation: 3.145 [-1.621, 10.190], loss: 1.542498, mae: 4.913062, mean_q: 5.188870
 31198/100000: episode: 3186, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.825, mean reward: 0.382 [0.323, 0.483], mean action: 38.400 [4.000, 91.000], mean observation: 3.159 [-1.594, 10.286], loss: 1.186748, mae: 4.911876, mean_q: 5.187119
 31208/100000: episode: 3187, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.576, mean reward: 0.458 [0.398, 0.509], mean action: 31.800 [4.000, 90.000], mean observation: 3.162 [-1.619, 10.330], loss: 0.921350, mae: 4.910967, mean_q: 5.186540
 31218/100000: episode: 3188, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.372, mean reward: 0.437 [0.371, 0.541], mean action: 15.200 [4.000, 89.000], mean observation: 3.152 [-1.142, 10.357], loss: 0.724601, mae: 4.910626, mean_q: 5.184030
 31228/100000: episode: 3189, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.991, mean reward: 0.399 [0.339, 0.478], mean action: 28.600 [2.000, 100.000], mean observation: 3.157 [-1.246, 10.320], loss: 1.253808, mae: 4.913176, mean_q: 5.182019
 31238/100000: episode: 3190, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.752, mean reward: 0.375 [0.331, 0.427], mean action: 39.300 [4.000, 100.000], mean observation: 3.165 [-1.269, 10.377], loss: 1.103060, mae: 4.912867, mean_q: 5.182624
 31248/100000: episode: 3191, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.026, mean reward: 0.403 [0.301, 0.467], mean action: 33.100 [4.000, 101.000], mean observation: 3.159 [-1.586, 10.368], loss: 1.198007, mae: 4.913367, mean_q: 5.185331
 31258/100000: episode: 3192, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.201, mean reward: 0.420 [0.366, 0.478], mean action: 34.600 [4.000, 80.000], mean observation: 3.168 [-1.427, 10.403], loss: 1.051761, mae: 4.913390, mean_q: 5.193202
 31268/100000: episode: 3193, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.192, mean reward: 0.419 [0.340, 0.484], mean action: 19.200 [4.000, 94.000], mean observation: 3.155 [-1.885, 10.369], loss: 1.485855, mae: 4.914970, mean_q: 5.198242
 31278/100000: episode: 3194, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.057, mean reward: 0.406 [0.397, 0.479], mean action: 23.000 [4.000, 68.000], mean observation: 3.158 [-1.891, 10.292], loss: 1.126730, mae: 4.913493, mean_q: 5.201186
 31288/100000: episode: 3195, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.340, mean reward: 0.434 [0.368, 0.493], mean action: 49.900 [4.000, 99.000], mean observation: 3.148 [-1.384, 10.357], loss: 1.364474, mae: 4.914527, mean_q: 5.203656
 31298/100000: episode: 3196, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.928, mean reward: 0.393 [0.307, 0.427], mean action: 23.000 [4.000, 89.000], mean observation: 3.157 [-1.014, 10.266], loss: 1.258019, mae: 4.914134, mean_q: 5.206473
 31308/100000: episode: 3197, duration: 0.232s, episode steps: 10, steps per second: 43, episode reward: 4.049, mean reward: 0.405 [0.309, 0.500], mean action: 9.500 [4.000, 59.000], mean observation: 3.162 [-1.552, 10.393], loss: 1.025318, mae: 4.913212, mean_q: 5.206994
 31318/100000: episode: 3198, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.897, mean reward: 0.390 [0.337, 0.503], mean action: 37.500 [4.000, 83.000], mean observation: 3.158 [-1.078, 10.581], loss: 1.247336, mae: 4.914248, mean_q: 5.204027
 31328/100000: episode: 3199, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.283, mean reward: 0.428 [0.373, 0.474], mean action: 23.400 [4.000, 81.000], mean observation: 3.154 [-1.756, 10.300], loss: 1.469049, mae: 4.915494, mean_q: 5.200088
 31338/100000: episode: 3200, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.145, mean reward: 0.415 [0.348, 0.525], mean action: 9.100 [3.000, 56.000], mean observation: 3.154 [-1.421, 10.360], loss: 1.143783, mae: 4.914069, mean_q: 5.199092
 31348/100000: episode: 3201, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.001, mean reward: 0.400 [0.324, 0.526], mean action: 29.100 [1.000, 87.000], mean observation: 3.159 [-1.420, 10.305], loss: 1.255296, mae: 4.914670, mean_q: 5.200965
 31358/100000: episode: 3202, duration: 0.234s, episode steps: 10, steps per second: 43, episode reward: 4.443, mean reward: 0.444 [0.389, 0.532], mean action: 4.900 [4.000, 13.000], mean observation: 3.162 [-1.739, 10.447], loss: 1.091272, mae: 4.914568, mean_q: 5.204438
 31368/100000: episode: 3203, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.096, mean reward: 0.410 [0.338, 0.463], mean action: 26.700 [4.000, 101.000], mean observation: 3.156 [-1.467, 10.306], loss: 1.135665, mae: 4.915111, mean_q: 5.207830
 31378/100000: episode: 3204, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.402, mean reward: 0.440 [0.342, 0.499], mean action: 52.200 [4.000, 95.000], mean observation: 3.155 [-1.467, 10.387], loss: 1.283789, mae: 4.915867, mean_q: 5.211394
 31388/100000: episode: 3205, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.608, mean reward: 0.461 [0.354, 0.540], mean action: 19.500 [4.000, 70.000], mean observation: 3.165 [-1.905, 10.303], loss: 1.323205, mae: 4.916296, mean_q: 5.214706
 31398/100000: episode: 3206, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 3.972, mean reward: 0.397 [0.324, 0.521], mean action: 24.300 [1.000, 100.000], mean observation: 3.163 [-1.830, 10.403], loss: 1.389076, mae: 4.916715, mean_q: 5.209589
 31408/100000: episode: 3207, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.717, mean reward: 0.472 [0.453, 0.564], mean action: 19.900 [4.000, 96.000], mean observation: 3.150 [-1.375, 10.324], loss: 1.174324, mae: 4.915994, mean_q: 5.207643
 31418/100000: episode: 3208, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.501, mean reward: 0.450 [0.371, 0.522], mean action: 16.200 [4.000, 49.000], mean observation: 3.142 [-1.223, 10.292], loss: 1.160587, mae: 4.916423, mean_q: 5.209280
 31428/100000: episode: 3209, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.036, mean reward: 0.404 [0.342, 0.477], mean action: 19.900 [4.000, 86.000], mean observation: 3.158 [-1.733, 10.346], loss: 1.265146, mae: 4.916918, mean_q: 5.211781
 31438/100000: episode: 3210, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.157, mean reward: 0.416 [0.308, 0.525], mean action: 5.500 [3.000, 20.000], mean observation: 3.152 [-2.079, 10.310], loss: 1.263036, mae: 4.917296, mean_q: 5.210391
 31448/100000: episode: 3211, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.304, mean reward: 0.430 [0.427, 0.438], mean action: 41.100 [4.000, 89.000], mean observation: 3.149 [-1.721, 10.359], loss: 1.084085, mae: 4.917009, mean_q: 5.210265
 31458/100000: episode: 3212, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.163, mean reward: 0.416 [0.333, 0.443], mean action: 22.300 [4.000, 95.000], mean observation: 3.157 [-1.139, 10.222], loss: 0.857809, mae: 4.916455, mean_q: 5.211504
 31468/100000: episode: 3213, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.049, mean reward: 0.405 [0.318, 0.571], mean action: 19.800 [4.000, 94.000], mean observation: 3.161 [-1.278, 10.539], loss: 1.274044, mae: 4.918654, mean_q: 5.216193
 31478/100000: episode: 3214, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.474, mean reward: 0.447 [0.349, 0.543], mean action: 25.000 [4.000, 97.000], mean observation: 3.159 [-1.170, 10.476], loss: 1.271892, mae: 4.919454, mean_q: 5.223243
 31488/100000: episode: 3215, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.128, mean reward: 0.413 [0.330, 0.473], mean action: 23.100 [4.000, 82.000], mean observation: 3.163 [-1.779, 10.563], loss: 1.111631, mae: 4.919117, mean_q: 5.226712
 31498/100000: episode: 3216, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 3.970, mean reward: 0.397 [0.303, 0.508], mean action: 14.400 [4.000, 55.000], mean observation: 3.162 [-0.919, 10.248], loss: 1.478924, mae: 4.921141, mean_q: 5.229274
 31508/100000: episode: 3217, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.808, mean reward: 0.381 [0.335, 0.476], mean action: 27.000 [4.000, 80.000], mean observation: 3.157 [-1.340, 10.333], loss: 1.339358, mae: 4.920454, mean_q: 5.230504
 31518/100000: episode: 3218, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.284, mean reward: 0.428 [0.327, 0.574], mean action: 18.000 [3.000, 76.000], mean observation: 3.153 [-1.757, 10.290], loss: 1.196469, mae: 4.920272, mean_q: 5.228510
 31528/100000: episode: 3219, duration: 0.217s, episode steps: 10, steps per second: 46, episode reward: 4.050, mean reward: 0.405 [0.335, 0.468], mean action: 31.500 [0.000, 98.000], mean observation: 3.154 [-2.311, 10.228], loss: 1.715360, mae: 4.922257, mean_q: 5.227147
 31538/100000: episode: 3220, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.266, mean reward: 0.427 [0.309, 0.555], mean action: 20.800 [4.000, 67.000], mean observation: 3.155 [-1.542, 10.530], loss: 1.225498, mae: 4.920382, mean_q: 5.218649
 31546/100000: episode: 3221, duration: 0.170s, episode steps: 8, steps per second: 47, episode reward: 12.615, mean reward: 1.577 [0.329, 10.000], mean action: 27.125 [4.000, 100.000], mean observation: 3.157 [-1.861, 10.365], loss: 1.187955, mae: 4.920208, mean_q: 5.214836
 31556/100000: episode: 3222, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.014, mean reward: 0.401 [0.352, 0.454], mean action: 28.900 [4.000, 92.000], mean observation: 3.167 [-1.308, 10.328], loss: 1.108956, mae: 4.920358, mean_q: 5.213458
 31566/100000: episode: 3223, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.307, mean reward: 0.431 [0.321, 0.550], mean action: 38.200 [4.000, 90.000], mean observation: 3.157 [-1.449, 10.369], loss: 1.203339, mae: 4.921014, mean_q: 5.212881
 31576/100000: episode: 3224, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.130, mean reward: 0.413 [0.355, 0.499], mean action: 31.100 [4.000, 92.000], mean observation: 3.156 [-1.208, 10.454], loss: 1.456095, mae: 4.922053, mean_q: 5.214209
 31586/100000: episode: 3225, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 3.930, mean reward: 0.393 [0.328, 0.446], mean action: 23.600 [4.000, 76.000], mean observation: 3.149 [-1.731, 10.390], loss: 1.410619, mae: 4.922005, mean_q: 5.214455
 31596/100000: episode: 3226, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.072, mean reward: 0.407 [0.322, 0.521], mean action: 37.100 [1.000, 101.000], mean observation: 3.153 [-1.993, 10.330], loss: 0.874759, mae: 4.920017, mean_q: 5.213377
 31606/100000: episode: 3227, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.865, mean reward: 0.386 [0.329, 0.490], mean action: 20.700 [4.000, 99.000], mean observation: 3.146 [-1.542, 10.381], loss: 1.338973, mae: 4.922321, mean_q: 5.214577
 31616/100000: episode: 3228, duration: 0.215s, episode steps: 10, steps per second: 47, episode reward: 4.039, mean reward: 0.404 [0.305, 0.487], mean action: 8.800 [0.000, 32.000], mean observation: 3.151 [-1.231, 10.243], loss: 1.404144, mae: 4.922652, mean_q: 5.215966
 31626/100000: episode: 3229, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.657, mean reward: 0.466 [0.366, 0.532], mean action: 24.600 [3.000, 86.000], mean observation: 3.152 [-1.847, 10.244], loss: 1.277207, mae: 4.922337, mean_q: 5.214157
 31636/100000: episode: 3230, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.301, mean reward: 0.430 [0.399, 0.450], mean action: 41.900 [4.000, 93.000], mean observation: 3.165 [-1.411, 10.396], loss: 1.013788, mae: 4.921795, mean_q: 5.214793
 31646/100000: episode: 3231, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.148, mean reward: 0.415 [0.360, 0.481], mean action: 15.200 [4.000, 42.000], mean observation: 3.155 [-1.345, 10.279], loss: 1.162567, mae: 4.922807, mean_q: 5.216028
 31656/100000: episode: 3232, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.044, mean reward: 0.404 [0.339, 0.497], mean action: 19.600 [4.000, 90.000], mean observation: 3.154 [-1.154, 10.330], loss: 1.099014, mae: 4.922754, mean_q: 5.218257
 31666/100000: episode: 3233, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.919, mean reward: 0.392 [0.379, 0.440], mean action: 39.800 [4.000, 72.000], mean observation: 3.150 [-1.938, 10.245], loss: 1.117879, mae: 4.923472, mean_q: 5.216702
 31676/100000: episode: 3234, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.123, mean reward: 0.412 [0.323, 0.557], mean action: 40.900 [4.000, 100.000], mean observation: 3.151 [-1.054, 10.355], loss: 1.268239, mae: 4.924508, mean_q: 5.215833
 31686/100000: episode: 3235, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.117, mean reward: 0.412 [0.354, 0.549], mean action: 21.700 [4.000, 59.000], mean observation: 3.155 [-1.559, 10.344], loss: 1.431006, mae: 4.925502, mean_q: 5.211948
 31696/100000: episode: 3236, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.264, mean reward: 0.426 [0.347, 0.526], mean action: 29.300 [4.000, 98.000], mean observation: 3.149 [-1.746, 10.367], loss: 1.448999, mae: 4.925447, mean_q: 5.207577
 31706/100000: episode: 3237, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 3.751, mean reward: 0.375 [0.304, 0.432], mean action: 20.400 [4.000, 101.000], mean observation: 3.150 [-1.397, 10.252], loss: 1.247743, mae: 4.924972, mean_q: 5.199100
 31716/100000: episode: 3238, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 3.882, mean reward: 0.388 [0.317, 0.453], mean action: 29.200 [1.000, 99.000], mean observation: 3.152 [-1.560, 10.229], loss: 0.838365, mae: 4.923820, mean_q: 5.193345
 31726/100000: episode: 3239, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 4.539, mean reward: 0.454 [0.387, 0.541], mean action: 19.800 [4.000, 90.000], mean observation: 3.143 [-1.568, 10.328], loss: 1.124325, mae: 4.925325, mean_q: 5.190379
 31736/100000: episode: 3240, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.765, mean reward: 0.377 [0.327, 0.460], mean action: 31.600 [4.000, 97.000], mean observation: 3.149 [-1.621, 10.330], loss: 1.342372, mae: 4.926779, mean_q: 5.187424
 31746/100000: episode: 3241, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.629, mean reward: 0.363 [0.306, 0.405], mean action: 21.300 [4.000, 89.000], mean observation: 3.149 [-1.664, 10.309], loss: 1.444944, mae: 4.927540, mean_q: 5.187772
 31756/100000: episode: 3242, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.955, mean reward: 0.395 [0.323, 0.454], mean action: 26.700 [4.000, 83.000], mean observation: 3.158 [-1.010, 10.269], loss: 1.031079, mae: 4.926154, mean_q: 5.189692
 31766/100000: episode: 3243, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.855, mean reward: 0.385 [0.342, 0.445], mean action: 19.500 [4.000, 100.000], mean observation: 3.160 [-1.535, 10.379], loss: 1.177125, mae: 4.927006, mean_q: 5.191803
 31773/100000: episode: 3244, duration: 0.138s, episode steps: 7, steps per second: 51, episode reward: 12.487, mean reward: 1.784 [0.356, 10.000], mean action: 38.857 [4.000, 89.000], mean observation: 3.166 [-2.080, 10.314], loss: 0.607956, mae: 4.925100, mean_q: 5.193559
 31783/100000: episode: 3245, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.663, mean reward: 0.366 [0.337, 0.420], mean action: 22.800 [4.000, 77.000], mean observation: 3.160 [-1.824, 10.294], loss: 1.566654, mae: 4.929358, mean_q: 5.196229
 31793/100000: episode: 3246, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.641, mean reward: 0.364 [0.293, 0.459], mean action: 42.800 [4.000, 82.000], mean observation: 3.152 [-0.969, 10.240], loss: 1.392249, mae: 4.928927, mean_q: 5.198913
 31803/100000: episode: 3247, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.252, mean reward: 0.425 [0.358, 0.452], mean action: 41.900 [4.000, 85.000], mean observation: 3.157 [-1.220, 10.389], loss: 1.175789, mae: 4.928419, mean_q: 5.197234
 31813/100000: episode: 3248, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.100, mean reward: 0.410 [0.364, 0.468], mean action: 25.700 [4.000, 83.000], mean observation: 3.160 [-1.646, 10.445], loss: 1.536031, mae: 4.930256, mean_q: 5.199793
 31823/100000: episode: 3249, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.577, mean reward: 0.458 [0.333, 0.559], mean action: 21.500 [4.000, 89.000], mean observation: 3.150 [-1.467, 10.390], loss: 1.362133, mae: 4.929902, mean_q: 5.201210
 31833/100000: episode: 3250, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.092, mean reward: 0.409 [0.364, 0.510], mean action: 24.300 [3.000, 64.000], mean observation: 3.162 [-1.592, 10.290], loss: 1.356607, mae: 4.929941, mean_q: 5.202081
 31843/100000: episode: 3251, duration: 0.221s, episode steps: 10, steps per second: 45, episode reward: 4.351, mean reward: 0.435 [0.359, 0.524], mean action: 17.300 [2.000, 72.000], mean observation: 3.150 [-1.458, 10.515], loss: 1.145636, mae: 4.929359, mean_q: 5.200466
 31853/100000: episode: 3252, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.164, mean reward: 0.416 [0.384, 0.479], mean action: 36.000 [4.000, 95.000], mean observation: 3.155 [-1.642, 10.385], loss: 1.266460, mae: 4.929995, mean_q: 5.200758
 31863/100000: episode: 3253, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.902, mean reward: 0.390 [0.278, 0.475], mean action: 34.700 [4.000, 95.000], mean observation: 3.153 [-1.992, 10.363], loss: 1.215591, mae: 4.929980, mean_q: 5.203264
 31873/100000: episode: 3254, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.027, mean reward: 0.403 [0.331, 0.467], mean action: 32.300 [2.000, 92.000], mean observation: 3.149 [-1.368, 10.339], loss: 1.033577, mae: 4.929397, mean_q: 5.206944
 31883/100000: episode: 3255, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.155, mean reward: 0.415 [0.346, 0.536], mean action: 20.100 [4.000, 80.000], mean observation: 3.147 [-1.766, 10.465], loss: 1.526814, mae: 4.931442, mean_q: 5.210353
 31893/100000: episode: 3256, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.186, mean reward: 0.419 [0.324, 0.501], mean action: 25.500 [4.000, 98.000], mean observation: 3.146 [-1.428, 10.354], loss: 1.385419, mae: 4.931041, mean_q: 5.210167
 31903/100000: episode: 3257, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.011, mean reward: 0.401 [0.330, 0.544], mean action: 23.000 [4.000, 85.000], mean observation: 3.151 [-1.157, 10.278], loss: 1.263212, mae: 4.930633, mean_q: 5.206792
 31913/100000: episode: 3258, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.054, mean reward: 0.405 [0.317, 0.438], mean action: 49.600 [4.000, 96.000], mean observation: 3.156 [-1.695, 10.428], loss: 1.042474, mae: 4.929942, mean_q: 5.199722
 31923/100000: episode: 3259, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.385, mean reward: 0.438 [0.347, 0.514], mean action: 27.700 [4.000, 74.000], mean observation: 3.146 [-1.257, 10.384], loss: 1.198359, mae: 4.930845, mean_q: 5.197816
 31924/100000: episode: 3260, duration: 0.026s, episode steps: 1, steps per second: 38, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 16.000 [16.000, 16.000], mean observation: 3.138 [-1.010, 10.100], loss: 1.187217, mae: 4.930951, mean_q: 5.195065
 31934/100000: episode: 3261, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.194, mean reward: 0.419 [0.385, 0.473], mean action: 25.400 [4.000, 100.000], mean observation: 3.150 [-1.471, 10.189], loss: 0.999778, mae: 4.930254, mean_q: 5.190871
 31944/100000: episode: 3262, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.738, mean reward: 0.374 [0.314, 0.459], mean action: 51.900 [4.000, 100.000], mean observation: 3.150 [-0.908, 10.229], loss: 1.238321, mae: 4.931433, mean_q: 5.189818
 31954/100000: episode: 3263, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.293, mean reward: 0.429 [0.329, 0.490], mean action: 27.800 [0.000, 91.000], mean observation: 3.155 [-1.787, 10.383], loss: 1.326459, mae: 4.932055, mean_q: 5.190077
 31964/100000: episode: 3264, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.863, mean reward: 0.386 [0.291, 0.459], mean action: 28.300 [4.000, 99.000], mean observation: 3.151 [-1.225, 10.357], loss: 0.924033, mae: 4.930879, mean_q: 5.192904
 31974/100000: episode: 3265, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.212, mean reward: 0.421 [0.327, 0.462], mean action: 19.000 [4.000, 91.000], mean observation: 3.162 [-1.122, 10.320], loss: 1.079012, mae: 4.931720, mean_q: 5.196090
 31984/100000: episode: 3266, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.294, mean reward: 0.429 [0.359, 0.486], mean action: 28.500 [4.000, 54.000], mean observation: 3.157 [-1.354, 10.375], loss: 1.054193, mae: 4.932049, mean_q: 5.200042
 31994/100000: episode: 3267, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.883, mean reward: 0.388 [0.350, 0.428], mean action: 33.100 [4.000, 98.000], mean observation: 3.158 [-1.472, 10.249], loss: 1.417987, mae: 4.933848, mean_q: 5.201404
 32004/100000: episode: 3268, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 3.792, mean reward: 0.379 [0.352, 0.429], mean action: 15.100 [4.000, 93.000], mean observation: 3.157 [-1.687, 10.239], loss: 1.432831, mae: 4.933861, mean_q: 5.201303
 32014/100000: episode: 3269, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.974, mean reward: 0.397 [0.343, 0.487], mean action: 31.100 [4.000, 92.000], mean observation: 3.157 [-1.116, 10.358], loss: 1.466501, mae: 4.933749, mean_q: 5.195137
 32024/100000: episode: 3270, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.105, mean reward: 0.411 [0.364, 0.493], mean action: 46.000 [4.000, 99.000], mean observation: 3.159 [-1.172, 10.343], loss: 1.396646, mae: 4.933559, mean_q: 5.190343
 32034/100000: episode: 3271, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.252, mean reward: 0.425 [0.344, 0.553], mean action: 19.800 [4.000, 92.000], mean observation: 3.155 [-1.822, 10.344], loss: 1.424955, mae: 4.933697, mean_q: 5.190387
 32035/100000: episode: 3272, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 4.000 [4.000, 4.000], mean observation: 3.169 [-0.287, 10.100], loss: 1.101485, mae: 4.932298, mean_q: 5.188867
 32045/100000: episode: 3273, duration: 0.251s, episode steps: 10, steps per second: 40, episode reward: 4.240, mean reward: 0.424 [0.340, 0.534], mean action: 16.100 [4.000, 77.000], mean observation: 3.150 [-1.546, 10.413], loss: 1.470937, mae: 4.934140, mean_q: 5.188560
 32055/100000: episode: 3274, duration: 0.234s, episode steps: 10, steps per second: 43, episode reward: 3.899, mean reward: 0.390 [0.323, 0.463], mean action: 16.300 [0.000, 80.000], mean observation: 3.156 [-1.305, 10.467], loss: 0.990785, mae: 4.932454, mean_q: 5.189852
 32065/100000: episode: 3275, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 3.992, mean reward: 0.399 [0.315, 0.453], mean action: 11.500 [2.000, 42.000], mean observation: 3.152 [-1.222, 10.275], loss: 1.310219, mae: 4.933712, mean_q: 5.188505
 32075/100000: episode: 3276, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.298, mean reward: 0.430 [0.324, 0.570], mean action: 21.400 [0.000, 101.000], mean observation: 3.148 [-2.017, 10.159], loss: 0.986431, mae: 4.933000, mean_q: 5.182363
 32085/100000: episode: 3277, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.202, mean reward: 0.420 [0.386, 0.457], mean action: 25.700 [4.000, 99.000], mean observation: 3.160 [-1.792, 10.549], loss: 1.208931, mae: 4.934509, mean_q: 5.179911
 32095/100000: episode: 3278, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.104, mean reward: 0.410 [0.385, 0.488], mean action: 24.100 [4.000, 83.000], mean observation: 3.168 [-1.581, 10.380], loss: 1.303418, mae: 4.934880, mean_q: 5.178195
 32105/100000: episode: 3279, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.195, mean reward: 0.419 [0.325, 0.536], mean action: 28.000 [4.000, 97.000], mean observation: 3.147 [-1.404, 10.296], loss: 1.202760, mae: 4.934733, mean_q: 5.178268
 32115/100000: episode: 3280, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 4.239, mean reward: 0.424 [0.355, 0.519], mean action: 18.100 [1.000, 100.000], mean observation: 3.158 [-1.660, 10.410], loss: 1.089656, mae: 4.934358, mean_q: 5.180554
 32118/100000: episode: 3281, duration: 0.075s, episode steps: 3, steps per second: 40, episode reward: 10.761, mean reward: 3.587 [0.361, 10.000], mean action: 17.667 [4.000, 45.000], mean observation: 3.164 [-1.893, 10.551], loss: 1.438901, mae: 4.935676, mean_q: 5.182343
 32128/100000: episode: 3282, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 4.854, mean reward: 0.485 [0.416, 0.580], mean action: 21.900 [3.000, 49.000], mean observation: 3.145 [-1.812, 10.221], loss: 1.484086, mae: 4.935796, mean_q: 5.182244
 32138/100000: episode: 3283, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.370, mean reward: 0.437 [0.331, 0.502], mean action: 37.900 [4.000, 82.000], mean observation: 3.158 [-0.890, 10.350], loss: 1.019716, mae: 4.933985, mean_q: 5.182827
 32148/100000: episode: 3284, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.363, mean reward: 0.436 [0.325, 0.486], mean action: 26.100 [4.000, 77.000], mean observation: 3.167 [-1.660, 10.462], loss: 1.365582, mae: 4.935469, mean_q: 5.185509
 32158/100000: episode: 3285, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.881, mean reward: 0.388 [0.342, 0.492], mean action: 30.200 [2.000, 99.000], mean observation: 3.152 [-1.626, 10.364], loss: 1.094959, mae: 4.934446, mean_q: 5.189048
 32168/100000: episode: 3286, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.012, mean reward: 0.401 [0.354, 0.510], mean action: 19.600 [3.000, 82.000], mean observation: 3.155 [-1.328, 10.288], loss: 1.366145, mae: 4.935765, mean_q: 5.191768
 32178/100000: episode: 3287, duration: 0.237s, episode steps: 10, steps per second: 42, episode reward: 3.780, mean reward: 0.378 [0.305, 0.426], mean action: 22.000 [4.000, 93.000], mean observation: 3.160 [-1.623, 10.275], loss: 1.211704, mae: 4.935375, mean_q: 5.191555
 32188/100000: episode: 3288, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.968, mean reward: 0.397 [0.372, 0.535], mean action: 55.600 [4.000, 100.000], mean observation: 3.152 [-1.110, 10.272], loss: 1.237480, mae: 4.935912, mean_q: 5.190081
 32198/100000: episode: 3289, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.289, mean reward: 0.429 [0.371, 0.594], mean action: 23.500 [4.000, 60.000], mean observation: 3.168 [-1.880, 10.227], loss: 1.195810, mae: 4.935567, mean_q: 5.187308
 32208/100000: episode: 3290, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.198, mean reward: 0.420 [0.354, 0.565], mean action: 22.900 [4.000, 91.000], mean observation: 3.155 [-0.959, 10.364], loss: 1.365396, mae: 4.936436, mean_q: 5.186336
 32218/100000: episode: 3291, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.280, mean reward: 0.428 [0.329, 0.512], mean action: 31.000 [4.000, 92.000], mean observation: 3.156 [-1.327, 10.286], loss: 1.396761, mae: 4.936736, mean_q: 5.187799
 32228/100000: episode: 3292, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.137, mean reward: 0.414 [0.359, 0.529], mean action: 24.300 [4.000, 88.000], mean observation: 3.168 [-1.163, 10.404], loss: 1.231173, mae: 4.936179, mean_q: 5.190177
 32238/100000: episode: 3293, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 3.959, mean reward: 0.396 [0.337, 0.540], mean action: 16.700 [2.000, 73.000], mean observation: 3.151 [-1.088, 10.394], loss: 1.237277, mae: 4.936388, mean_q: 5.190070
 32248/100000: episode: 3294, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.158, mean reward: 0.416 [0.375, 0.479], mean action: 32.700 [4.000, 84.000], mean observation: 3.161 [-1.444, 10.349], loss: 1.350900, mae: 4.937102, mean_q: 5.188746
 32258/100000: episode: 3295, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.054, mean reward: 0.405 [0.308, 0.461], mean action: 28.500 [1.000, 96.000], mean observation: 3.151 [-1.829, 10.188], loss: 1.238898, mae: 4.936555, mean_q: 5.184129
 32268/100000: episode: 3296, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.413, mean reward: 0.441 [0.377, 0.550], mean action: 48.300 [1.000, 79.000], mean observation: 3.153 [-1.431, 10.268], loss: 1.316216, mae: 4.936833, mean_q: 5.175816
 32278/100000: episode: 3297, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.403, mean reward: 0.440 [0.384, 0.544], mean action: 71.100 [8.000, 91.000], mean observation: 3.173 [-1.295, 10.240], loss: 0.929430, mae: 4.935527, mean_q: 5.176909
 32288/100000: episode: 3298, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.448, mean reward: 0.345 [0.317, 0.440], mean action: 80.500 [64.000, 91.000], mean observation: 3.158 [-1.344, 10.284], loss: 1.511138, mae: 4.937982, mean_q: 5.178803
 32298/100000: episode: 3299, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.984, mean reward: 0.398 [0.359, 0.446], mean action: 59.000 [20.000, 79.000], mean observation: 3.156 [-1.714, 10.336], loss: 1.412521, mae: 4.937595, mean_q: 5.182133
 32308/100000: episode: 3300, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.621, mean reward: 0.462 [0.358, 0.544], mean action: 48.900 [10.000, 79.000], mean observation: 3.156 [-1.146, 10.220], loss: 1.367099, mae: 4.937310, mean_q: 5.185109
 32318/100000: episode: 3301, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.009, mean reward: 0.401 [0.342, 0.484], mean action: 60.700 [14.000, 83.000], mean observation: 3.163 [-1.226, 10.306], loss: 1.499820, mae: 4.937814, mean_q: 5.187371
 32328/100000: episode: 3302, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 4.341, mean reward: 0.434 [0.351, 0.539], mean action: 69.500 [29.000, 95.000], mean observation: 3.172 [-0.809, 10.326], loss: 1.253705, mae: 4.936829, mean_q: 5.187819
 32338/100000: episode: 3303, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.019, mean reward: 0.402 [0.402, 0.402], mean action: 73.000 [32.000, 94.000], mean observation: 3.144 [-1.025, 10.299], loss: 1.273695, mae: 4.937247, mean_q: 5.183069
 32348/100000: episode: 3304, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.007, mean reward: 0.401 [0.371, 0.422], mean action: 70.800 [7.000, 101.000], mean observation: 3.163 [-1.150, 10.294], loss: 1.188304, mae: 4.936549, mean_q: 5.181363
 32358/100000: episode: 3305, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.626, mean reward: 0.363 [0.321, 0.428], mean action: 70.900 [18.000, 91.000], mean observation: 3.165 [-2.302, 10.316], loss: 1.196128, mae: 4.936428, mean_q: 5.178589
 32368/100000: episode: 3306, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.621, mean reward: 0.362 [0.332, 0.414], mean action: 62.400 [14.000, 85.000], mean observation: 3.161 [-1.014, 10.290], loss: 1.031110, mae: 4.935711, mean_q: 5.176368
 32378/100000: episode: 3307, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.871, mean reward: 0.387 [0.384, 0.401], mean action: 64.700 [30.000, 93.000], mean observation: 3.157 [-1.725, 10.298], loss: 1.447770, mae: 4.937432, mean_q: 5.177196
 32388/100000: episode: 3308, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.640, mean reward: 0.364 [0.313, 0.415], mean action: 67.200 [6.000, 79.000], mean observation: 3.157 [-1.376, 10.318], loss: 1.169704, mae: 4.936416, mean_q: 5.178142
 32398/100000: episode: 3309, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.978, mean reward: 0.398 [0.344, 0.446], mean action: 63.300 [24.000, 79.000], mean observation: 3.164 [-1.748, 10.256], loss: 1.192423, mae: 4.936310, mean_q: 5.177913
 32408/100000: episode: 3310, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.581, mean reward: 0.358 [0.302, 0.388], mean action: 54.100 [8.000, 93.000], mean observation: 3.140 [-1.665, 10.272], loss: 1.159536, mae: 4.936238, mean_q: 5.173567
 32418/100000: episode: 3311, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.169, mean reward: 0.417 [0.380, 0.433], mean action: 51.100 [5.000, 79.000], mean observation: 3.141 [-1.480, 10.330], loss: 1.152221, mae: 4.936271, mean_q: 5.173255
 32428/100000: episode: 3312, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.276, mean reward: 0.428 [0.375, 0.484], mean action: 64.900 [15.000, 82.000], mean observation: 3.158 [-1.766, 10.295], loss: 1.371408, mae: 4.937230, mean_q: 5.170611
 32438/100000: episode: 3313, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.117, mean reward: 0.412 [0.402, 0.474], mean action: 63.300 [0.000, 83.000], mean observation: 3.158 [-1.481, 10.323], loss: 1.125010, mae: 4.936248, mean_q: 5.166897
 32448/100000: episode: 3314, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.119, mean reward: 0.412 [0.392, 0.454], mean action: 59.100 [12.000, 79.000], mean observation: 3.159 [-1.409, 10.301], loss: 1.109482, mae: 4.936112, mean_q: 5.161511
 32458/100000: episode: 3315, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.694, mean reward: 0.369 [0.281, 0.415], mean action: 71.800 [5.000, 90.000], mean observation: 3.160 [-0.870, 10.343], loss: 0.938432, mae: 4.935442, mean_q: 5.160291
 32468/100000: episode: 3316, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.505, mean reward: 0.450 [0.357, 0.461], mean action: 62.300 [3.000, 87.000], mean observation: 3.170 [-1.012, 10.216], loss: 1.116961, mae: 4.936189, mean_q: 5.160594
 32478/100000: episode: 3317, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.960, mean reward: 0.396 [0.372, 0.465], mean action: 67.400 [20.000, 92.000], mean observation: 3.155 [-1.139, 10.285], loss: 0.981234, mae: 4.935898, mean_q: 5.156944
 32488/100000: episode: 3318, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.148, mean reward: 0.415 [0.381, 0.461], mean action: 67.600 [18.000, 88.000], mean observation: 3.175 [-1.303, 10.278], loss: 0.859138, mae: 4.935903, mean_q: 5.155185
 32498/100000: episode: 3319, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.337, mean reward: 0.434 [0.405, 0.521], mean action: 45.700 [8.000, 99.000], mean observation: 3.161 [-1.579, 10.404], loss: 0.893758, mae: 4.937078, mean_q: 5.156160
 32508/100000: episode: 3320, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 5.175, mean reward: 0.518 [0.516, 0.529], mean action: 38.300 [7.000, 99.000], mean observation: 3.150 [-1.114, 10.269], loss: 1.288362, mae: 4.939003, mean_q: 5.158473
 32518/100000: episode: 3321, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.820, mean reward: 0.382 [0.348, 0.507], mean action: 43.100 [18.000, 84.000], mean observation: 3.156 [-1.452, 10.438], loss: 0.956876, mae: 4.938040, mean_q: 5.160716
 32528/100000: episode: 3322, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.356, mean reward: 0.436 [0.323, 0.501], mean action: 33.900 [18.000, 67.000], mean observation: 3.143 [-1.670, 10.281], loss: 1.448594, mae: 4.939890, mean_q: 5.162673
 32538/100000: episode: 3323, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 3.952, mean reward: 0.395 [0.349, 0.554], mean action: 32.900 [32.000, 41.000], mean observation: 3.163 [-1.767, 10.258], loss: 1.174660, mae: 4.938701, mean_q: 5.164207
 32548/100000: episode: 3324, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.762, mean reward: 0.376 [0.333, 0.437], mean action: 40.200 [11.000, 75.000], mean observation: 3.147 [-1.852, 10.353], loss: 1.242064, mae: 4.939586, mean_q: 5.166183
 32558/100000: episode: 3325, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.989, mean reward: 0.399 [0.354, 0.470], mean action: 40.700 [5.000, 70.000], mean observation: 3.157 [-1.875, 10.287], loss: 1.059252, mae: 4.938806, mean_q: 5.167858
 32568/100000: episode: 3326, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.963, mean reward: 0.396 [0.374, 0.459], mean action: 42.500 [13.000, 93.000], mean observation: 3.150 [-1.312, 10.504], loss: 1.166003, mae: 4.939407, mean_q: 5.169407
 32578/100000: episode: 3327, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.311, mean reward: 0.431 [0.413, 0.450], mean action: 57.400 [23.000, 98.000], mean observation: 3.157 [-0.841, 10.278], loss: 1.356189, mae: 4.940263, mean_q: 5.170957
 32588/100000: episode: 3328, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.022, mean reward: 0.402 [0.322, 0.496], mean action: 31.900 [3.000, 81.000], mean observation: 3.171 [-1.059, 10.371], loss: 1.148453, mae: 4.939395, mean_q: 5.172867
 32598/100000: episode: 3329, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.140, mean reward: 0.414 [0.366, 0.479], mean action: 44.200 [13.000, 88.000], mean observation: 3.159 [-1.268, 10.310], loss: 0.618664, mae: 4.937364, mean_q: 5.175352
 32604/100000: episode: 3330, duration: 0.138s, episode steps: 6, steps per second: 44, episode reward: 12.266, mean reward: 2.044 [0.391, 10.000], mean action: 29.333 [18.000, 32.000], mean observation: 3.144 [-1.150, 10.307], loss: 0.975012, mae: 4.939107, mean_q: 5.177179
 32614/100000: episode: 3331, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 3.702, mean reward: 0.370 [0.326, 0.476], mean action: 51.100 [29.000, 90.000], mean observation: 3.145 [-1.012, 10.361], loss: 1.320170, mae: 4.940854, mean_q: 5.179292
 32624/100000: episode: 3332, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.689, mean reward: 0.369 [0.345, 0.442], mean action: 42.200 [13.000, 85.000], mean observation: 3.166 [-1.516, 10.244], loss: 0.952516, mae: 4.939964, mean_q: 5.185053
 32634/100000: episode: 3333, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.913, mean reward: 0.391 [0.332, 0.472], mean action: 30.700 [19.000, 32.000], mean observation: 3.165 [-1.376, 10.483], loss: 1.356810, mae: 4.941282, mean_q: 5.186503
 32644/100000: episode: 3334, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.200, mean reward: 0.420 [0.333, 0.481], mean action: 36.600 [2.000, 57.000], mean observation: 3.157 [-2.027, 10.308], loss: 1.407784, mae: 4.941673, mean_q: 5.185269
 32654/100000: episode: 3335, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.734, mean reward: 0.373 [0.306, 0.542], mean action: 38.100 [32.000, 77.000], mean observation: 3.155 [-2.216, 10.261], loss: 1.027419, mae: 4.940367, mean_q: 5.184594
 32664/100000: episode: 3336, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.123, mean reward: 0.412 [0.301, 0.525], mean action: 45.200 [27.000, 86.000], mean observation: 3.159 [-1.000, 10.515], loss: 1.226506, mae: 4.941466, mean_q: 5.186353
 32674/100000: episode: 3337, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.959, mean reward: 0.396 [0.319, 0.490], mean action: 51.700 [29.000, 98.000], mean observation: 3.146 [-1.455, 10.335], loss: 1.003563, mae: 4.940967, mean_q: 5.187904
 32684/100000: episode: 3338, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.433, mean reward: 0.443 [0.429, 0.492], mean action: 40.900 [32.000, 101.000], mean observation: 3.171 [-1.271, 10.287], loss: 1.166364, mae: 4.942261, mean_q: 5.192525
 32694/100000: episode: 3339, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.260, mean reward: 0.426 [0.402, 0.483], mean action: 35.600 [18.000, 74.000], mean observation: 3.161 [-1.189, 10.471], loss: 1.371976, mae: 4.943316, mean_q: 5.192496
 32704/100000: episode: 3340, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.781, mean reward: 0.478 [0.473, 0.497], mean action: 44.600 [27.000, 97.000], mean observation: 3.152 [-2.080, 10.236], loss: 1.241059, mae: 4.943001, mean_q: 5.190059
 32714/100000: episode: 3341, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.820, mean reward: 0.482 [0.420, 0.558], mean action: 40.600 [28.000, 84.000], mean observation: 3.159 [-1.147, 10.383], loss: 1.321776, mae: 4.943233, mean_q: 5.187790
 32724/100000: episode: 3342, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.114, mean reward: 0.411 [0.278, 0.479], mean action: 45.200 [20.000, 101.000], mean observation: 3.166 [-1.263, 10.370], loss: 1.420167, mae: 4.943652, mean_q: 5.186255
 32734/100000: episode: 3343, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.184, mean reward: 0.418 [0.367, 0.569], mean action: 39.100 [16.000, 92.000], mean observation: 3.162 [-2.124, 10.451], loss: 1.121999, mae: 4.942356, mean_q: 5.183348
 32744/100000: episode: 3344, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.214, mean reward: 0.421 [0.398, 0.500], mean action: 36.500 [9.000, 57.000], mean observation: 3.162 [-1.287, 10.342], loss: 1.103575, mae: 4.942478, mean_q: 5.181087
 32754/100000: episode: 3345, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.449, mean reward: 0.445 [0.344, 0.509], mean action: 41.400 [6.000, 92.000], mean observation: 3.166 [-1.776, 10.348], loss: 0.932231, mae: 4.942039, mean_q: 5.181626
 32764/100000: episode: 3346, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.295, mean reward: 0.429 [0.372, 0.560], mean action: 38.900 [0.000, 87.000], mean observation: 3.151 [-1.157, 10.346], loss: 1.068396, mae: 4.943007, mean_q: 5.178236
 32774/100000: episode: 3347, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.061, mean reward: 0.406 [0.389, 0.471], mean action: 42.700 [9.000, 77.000], mean observation: 3.138 [-1.809, 10.365], loss: 1.085995, mae: 4.943421, mean_q: 5.177991
 32784/100000: episode: 3348, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.420, mean reward: 0.442 [0.397, 0.539], mean action: 37.300 [6.000, 60.000], mean observation: 3.157 [-1.716, 10.365], loss: 1.297976, mae: 4.944460, mean_q: 5.179274
 32785/100000: episode: 3349, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 49.000 [49.000, 49.000], mean observation: 3.128 [-0.823, 10.100], loss: 0.126027, mae: 4.939958, mean_q: 5.180005
 32795/100000: episode: 3350, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.945, mean reward: 0.395 [0.324, 0.449], mean action: 56.100 [23.000, 93.000], mean observation: 3.153 [-1.339, 10.402], loss: 1.325123, mae: 4.944808, mean_q: 5.180792
 32805/100000: episode: 3351, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.343, mean reward: 0.434 [0.399, 0.482], mean action: 41.700 [7.000, 70.000], mean observation: 3.142 [-1.817, 10.233], loss: 1.159392, mae: 4.944364, mean_q: 5.181893
 32815/100000: episode: 3352, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.517, mean reward: 0.452 [0.417, 0.527], mean action: 53.900 [8.000, 95.000], mean observation: 3.173 [-1.131, 10.268], loss: 1.430571, mae: 4.945408, mean_q: 5.183928
 32825/100000: episode: 3353, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.847, mean reward: 0.385 [0.346, 0.461], mean action: 57.600 [17.000, 101.000], mean observation: 3.164 [-1.406, 10.311], loss: 1.190458, mae: 4.944858, mean_q: 5.185437
 32835/100000: episode: 3354, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.763, mean reward: 0.376 [0.336, 0.437], mean action: 45.600 [22.000, 63.000], mean observation: 3.157 [-1.372, 10.412], loss: 1.310701, mae: 4.945500, mean_q: 5.186518
 32845/100000: episode: 3355, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.958, mean reward: 0.396 [0.345, 0.457], mean action: 40.000 [18.000, 59.000], mean observation: 3.154 [-1.364, 10.471], loss: 1.536223, mae: 4.946464, mean_q: 5.184029
 32855/100000: episode: 3356, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.963, mean reward: 0.496 [0.496, 0.496], mean action: 51.900 [45.000, 83.000], mean observation: 3.146 [-1.577, 10.336], loss: 1.076316, mae: 4.944347, mean_q: 5.181020
 32865/100000: episode: 3357, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.387, mean reward: 0.439 [0.346, 0.589], mean action: 48.900 [0.000, 97.000], mean observation: 3.138 [-1.282, 10.104], loss: 1.392354, mae: 4.945903, mean_q: 5.181954
 32875/100000: episode: 3358, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.039, mean reward: 0.404 [0.349, 0.522], mean action: 56.600 [49.000, 92.000], mean observation: 3.169 [-1.000, 10.298], loss: 1.193160, mae: 4.944935, mean_q: 5.183150
 32885/100000: episode: 3359, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.119, mean reward: 0.412 [0.379, 0.498], mean action: 28.300 [1.000, 54.000], mean observation: 3.150 [-1.521, 10.471], loss: 1.130904, mae: 4.944775, mean_q: 5.185254
 32895/100000: episode: 3360, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.963, mean reward: 0.396 [0.330, 0.490], mean action: 46.600 [13.000, 77.000], mean observation: 3.166 [-1.719, 10.185], loss: 1.132236, mae: 4.944917, mean_q: 5.188434
 32905/100000: episode: 3361, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.912, mean reward: 0.391 [0.371, 0.464], mean action: 65.000 [49.000, 99.000], mean observation: 3.151 [-1.700, 10.293], loss: 1.126006, mae: 4.945122, mean_q: 5.190534
 32915/100000: episode: 3362, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.026, mean reward: 0.403 [0.333, 0.480], mean action: 49.800 [24.000, 71.000], mean observation: 3.167 [-1.076, 10.322], loss: 1.249706, mae: 4.945581, mean_q: 5.187508
 32925/100000: episode: 3363, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.629, mean reward: 0.363 [0.323, 0.473], mean action: 46.700 [26.000, 49.000], mean observation: 3.165 [-1.664, 10.210], loss: 1.269959, mae: 4.945756, mean_q: 5.184137
 32935/100000: episode: 3364, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.803, mean reward: 0.380 [0.306, 0.438], mean action: 56.300 [12.000, 95.000], mean observation: 3.151 [-2.023, 10.376], loss: 1.214516, mae: 4.945271, mean_q: 5.184047
 32945/100000: episode: 3365, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.731, mean reward: 0.373 [0.291, 0.425], mean action: 52.900 [11.000, 96.000], mean observation: 3.159 [-1.924, 10.481], loss: 1.227598, mae: 4.945148, mean_q: 5.185346
 32955/100000: episode: 3366, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.358, mean reward: 0.436 [0.341, 0.527], mean action: 56.400 [20.000, 90.000], mean observation: 3.149 [-1.837, 10.399], loss: 1.473470, mae: 4.946008, mean_q: 5.187821
 32963/100000: episode: 3367, duration: 0.140s, episode steps: 8, steps per second: 57, episode reward: 12.530, mean reward: 1.566 [0.339, 10.000], mean action: 49.375 [27.000, 94.000], mean observation: 3.139 [-0.911, 10.399], loss: 1.268887, mae: 4.945237, mean_q: 5.186085
 32973/100000: episode: 3368, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.820, mean reward: 0.382 [0.297, 0.535], mean action: 46.100 [3.000, 86.000], mean observation: 3.150 [-1.186, 10.308], loss: 1.408217, mae: 4.945764, mean_q: 5.178476
 32983/100000: episode: 3369, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.910, mean reward: 0.391 [0.320, 0.506], mean action: 38.200 [11.000, 67.000], mean observation: 3.157 [-1.486, 10.380], loss: 1.200008, mae: 4.944861, mean_q: 5.176043
 32993/100000: episode: 3370, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.623, mean reward: 0.462 [0.370, 0.495], mean action: 43.000 [3.000, 79.000], mean observation: 3.160 [-1.026, 10.383], loss: 1.296652, mae: 4.945222, mean_q: 5.176161
 33003/100000: episode: 3371, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.361, mean reward: 0.436 [0.353, 0.509], mean action: 51.700 [1.000, 95.000], mean observation: 3.153 [-1.009, 10.327], loss: 0.992855, mae: 4.944030, mean_q: 5.177267
 33013/100000: episode: 3372, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.128, mean reward: 0.413 [0.384, 0.458], mean action: 50.000 [17.000, 90.000], mean observation: 3.158 [-1.633, 10.501], loss: 1.190509, mae: 4.945382, mean_q: 5.178700
 33023/100000: episode: 3373, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.662, mean reward: 0.366 [0.322, 0.433], mean action: 58.700 [43.000, 84.000], mean observation: 3.153 [-1.560, 10.376], loss: 1.170165, mae: 4.945591, mean_q: 5.180745
 33033/100000: episode: 3374, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.889, mean reward: 0.389 [0.314, 0.561], mean action: 46.500 [23.000, 85.000], mean observation: 3.149 [-1.604, 10.379], loss: 1.166153, mae: 4.945673, mean_q: 5.179840
 33043/100000: episode: 3375, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.206, mean reward: 0.421 [0.420, 0.424], mean action: 57.100 [49.000, 88.000], mean observation: 3.151 [-1.193, 10.322], loss: 1.517419, mae: 4.947304, mean_q: 5.181331
 33053/100000: episode: 3376, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.248, mean reward: 0.425 [0.330, 0.493], mean action: 58.100 [12.000, 101.000], mean observation: 3.143 [-1.650, 10.276], loss: 1.391551, mae: 4.946652, mean_q: 5.184342
 33063/100000: episode: 3377, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.953, mean reward: 0.395 [0.382, 0.515], mean action: 54.300 [5.000, 99.000], mean observation: 3.159 [-1.100, 10.282], loss: 1.195107, mae: 4.945651, mean_q: 5.186236
 33073/100000: episode: 3378, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.620, mean reward: 0.462 [0.296, 0.497], mean action: 53.800 [21.000, 96.000], mean observation: 3.149 [-1.489, 10.341], loss: 1.192466, mae: 4.945852, mean_q: 5.187654
 33083/100000: episode: 3379, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.113, mean reward: 0.411 [0.361, 0.470], mean action: 58.100 [13.000, 94.000], mean observation: 3.171 [-1.094, 10.224], loss: 1.248245, mae: 4.946295, mean_q: 5.189121
 33084/100000: episode: 3380, duration: 0.026s, episode steps: 1, steps per second: 38, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 98.000 [98.000, 98.000], mean observation: 3.142 [-1.420, 10.136], loss: 1.462969, mae: 4.947234, mean_q: 5.190639
 33094/100000: episode: 3381, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 13.437, mean reward: 1.344 [0.341, 10.000], mean action: 52.300 [23.000, 91.000], mean observation: 3.156 [-1.478, 10.625], loss: 1.365053, mae: 4.946712, mean_q: 5.189978
 33104/100000: episode: 3382, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.498, mean reward: 0.450 [0.370, 0.527], mean action: 59.500 [42.000, 100.000], mean observation: 3.158 [-1.441, 10.264], loss: 1.339221, mae: 4.946465, mean_q: 5.188970
 33114/100000: episode: 3383, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.978, mean reward: 0.498 [0.498, 0.498], mean action: 51.300 [30.000, 78.000], mean observation: 3.141 [-1.552, 10.314], loss: 1.236609, mae: 4.946322, mean_q: 5.191041
 33124/100000: episode: 3384, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.086, mean reward: 0.409 [0.402, 0.447], mean action: 51.600 [33.000, 91.000], mean observation: 3.167 [-1.131, 10.320], loss: 1.544739, mae: 4.947163, mean_q: 5.191516
 33134/100000: episode: 3385, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.439, mean reward: 0.444 [0.406, 0.490], mean action: 55.600 [11.000, 91.000], mean observation: 3.141 [-2.142, 10.194], loss: 1.278395, mae: 4.946122, mean_q: 5.187959
 33144/100000: episode: 3386, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.710, mean reward: 0.371 [0.300, 0.468], mean action: 57.300 [28.000, 97.000], mean observation: 3.164 [-1.428, 10.324], loss: 1.036678, mae: 4.945397, mean_q: 5.187178
 33154/100000: episode: 3387, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.149, mean reward: 0.415 [0.377, 0.475], mean action: 43.400 [1.000, 96.000], mean observation: 3.165 [-1.341, 10.294], loss: 1.412218, mae: 4.947055, mean_q: 5.187320
 33164/100000: episode: 3388, duration: 0.150s, episode steps: 10, steps per second: 66, episode reward: 3.558, mean reward: 0.356 [0.320, 0.512], mean action: 62.100 [29.000, 96.000], mean observation: 3.148 [-1.189, 10.369], loss: 1.193173, mae: 4.946185, mean_q: 5.189329
 33174/100000: episode: 3389, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.821, mean reward: 0.382 [0.366, 0.426], mean action: 42.600 [2.000, 92.000], mean observation: 3.162 [-1.360, 10.368], loss: 1.111215, mae: 4.945827, mean_q: 5.192914
 33184/100000: episode: 3390, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.070, mean reward: 0.407 [0.382, 0.411], mean action: 52.300 [38.000, 71.000], mean observation: 3.156 [-1.151, 10.406], loss: 1.241848, mae: 4.946290, mean_q: 5.195532
 33194/100000: episode: 3391, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.109, mean reward: 0.411 [0.352, 0.493], mean action: 49.400 [0.000, 101.000], mean observation: 3.159 [-1.260, 10.171], loss: 1.661294, mae: 4.947771, mean_q: 5.194467
 33204/100000: episode: 3392, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.868, mean reward: 0.387 [0.312, 0.497], mean action: 46.400 [29.000, 53.000], mean observation: 3.155 [-1.567, 10.311], loss: 1.486648, mae: 4.946539, mean_q: 5.195405
 33214/100000: episode: 3393, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.365, mean reward: 0.436 [0.387, 0.526], mean action: 53.400 [24.000, 85.000], mean observation: 3.158 [-1.420, 10.239], loss: 1.274523, mae: 4.945924, mean_q: 5.197966
 33224/100000: episode: 3394, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.312, mean reward: 0.431 [0.405, 0.524], mean action: 47.800 [30.000, 58.000], mean observation: 3.171 [-1.761, 10.219], loss: 1.094681, mae: 4.945068, mean_q: 5.200210
 33234/100000: episode: 3395, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.258, mean reward: 0.426 [0.339, 0.518], mean action: 38.400 [9.000, 49.000], mean observation: 3.147 [-1.395, 10.273], loss: 0.996456, mae: 4.944827, mean_q: 5.201931
 33244/100000: episode: 3396, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.699, mean reward: 0.370 [0.309, 0.399], mean action: 51.900 [31.000, 71.000], mean observation: 3.152 [-1.415, 10.314], loss: 1.116760, mae: 4.945580, mean_q: 5.203889
 33254/100000: episode: 3397, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.822, mean reward: 0.382 [0.340, 0.541], mean action: 52.000 [22.000, 91.000], mean observation: 3.153 [-1.183, 10.328], loss: 1.517720, mae: 4.947506, mean_q: 5.203283
 33264/100000: episode: 3398, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.757, mean reward: 0.376 [0.363, 0.423], mean action: 56.700 [13.000, 100.000], mean observation: 3.163 [-1.284, 10.386], loss: 1.338700, mae: 4.946517, mean_q: 5.197122
 33274/100000: episode: 3399, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.165, mean reward: 0.416 [0.330, 0.462], mean action: 41.900 [0.000, 78.000], mean observation: 3.158 [-1.359, 10.440], loss: 1.133891, mae: 4.945595, mean_q: 5.195988
 33284/100000: episode: 3400, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.364, mean reward: 0.436 [0.419, 0.560], mean action: 49.800 [34.000, 68.000], mean observation: 3.186 [-1.267, 10.269], loss: 1.132265, mae: 4.945779, mean_q: 5.196598
 33294/100000: episode: 3401, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.895, mean reward: 0.389 [0.350, 0.428], mean action: 50.500 [36.000, 85.000], mean observation: 3.155 [-1.110, 10.453], loss: 1.168343, mae: 4.946243, mean_q: 5.198265
 33304/100000: episode: 3402, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.062, mean reward: 0.406 [0.312, 0.493], mean action: 54.300 [9.000, 95.000], mean observation: 3.165 [-1.076, 10.450], loss: 1.600911, mae: 4.948202, mean_q: 5.200404
 33314/100000: episode: 3403, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.140, mean reward: 0.414 [0.400, 0.460], mean action: 54.800 [22.000, 101.000], mean observation: 3.164 [-1.213, 10.249], loss: 1.096029, mae: 4.946332, mean_q: 5.202735
 33324/100000: episode: 3404, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.034, mean reward: 0.403 [0.349, 0.462], mean action: 48.300 [32.000, 59.000], mean observation: 3.158 [-1.790, 10.328], loss: 1.166825, mae: 4.946823, mean_q: 5.204114
 33334/100000: episode: 3405, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.717, mean reward: 0.372 [0.334, 0.467], mean action: 48.500 [1.000, 85.000], mean observation: 3.161 [-0.773, 10.353], loss: 1.204087, mae: 4.947034, mean_q: 5.205457
 33344/100000: episode: 3406, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 5.003, mean reward: 0.500 [0.500, 0.500], mean action: 60.500 [49.000, 92.000], mean observation: 3.149 [-1.884, 10.298], loss: 1.116730, mae: 4.947090, mean_q: 5.208080
 33354/100000: episode: 3407, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.958, mean reward: 0.396 [0.309, 0.465], mean action: 59.900 [31.000, 99.000], mean observation: 3.160 [-1.423, 10.301], loss: 1.679114, mae: 4.949186, mean_q: 5.211301
 33364/100000: episode: 3408, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.630, mean reward: 0.363 [0.309, 0.429], mean action: 56.700 [45.000, 93.000], mean observation: 3.146 [-1.133, 10.320], loss: 1.229001, mae: 4.947164, mean_q: 5.209082
 33374/100000: episode: 3409, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.873, mean reward: 0.387 [0.354, 0.539], mean action: 56.300 [30.000, 96.000], mean observation: 3.153 [-1.875, 10.537], loss: 1.411635, mae: 4.947837, mean_q: 5.204457
 33384/100000: episode: 3410, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.839, mean reward: 0.384 [0.326, 0.475], mean action: 47.400 [22.000, 77.000], mean observation: 3.160 [-1.055, 10.225], loss: 1.136208, mae: 4.946593, mean_q: 5.203485
 33394/100000: episode: 3411, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.292, mean reward: 0.429 [0.373, 0.445], mean action: 36.800 [9.000, 56.000], mean observation: 3.159 [-1.789, 10.284], loss: 1.344350, mae: 4.947329, mean_q: 5.201362
 33404/100000: episode: 3412, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 5.171, mean reward: 0.517 [0.434, 0.551], mean action: 45.800 [3.000, 76.000], mean observation: 3.150 [-1.445, 10.261], loss: 0.852235, mae: 4.945697, mean_q: 5.201669
 33414/100000: episode: 3413, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.469, mean reward: 0.447 [0.415, 0.494], mean action: 44.300 [14.000, 78.000], mean observation: 3.151 [-1.489, 10.328], loss: 1.323743, mae: 4.947892, mean_q: 5.202998
 33424/100000: episode: 3414, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.052, mean reward: 0.405 [0.354, 0.452], mean action: 42.400 [20.000, 67.000], mean observation: 3.153 [-1.618, 10.396], loss: 1.409372, mae: 4.948518, mean_q: 5.205127
 33434/100000: episode: 3415, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.111, mean reward: 0.411 [0.336, 0.570], mean action: 45.700 [16.000, 49.000], mean observation: 3.156 [-1.275, 10.287], loss: 1.672886, mae: 4.949400, mean_q: 5.208941
 33444/100000: episode: 3416, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.315, mean reward: 0.431 [0.326, 0.492], mean action: 44.100 [23.000, 85.000], mean observation: 3.162 [-1.510, 10.290], loss: 1.340056, mae: 4.948064, mean_q: 5.211805
 33454/100000: episode: 3417, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.717, mean reward: 0.372 [0.332, 0.462], mean action: 45.300 [12.000, 68.000], mean observation: 3.166 [-1.623, 10.311], loss: 0.998728, mae: 4.946446, mean_q: 5.214356
 33464/100000: episode: 3418, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.874, mean reward: 0.387 [0.326, 0.505], mean action: 48.700 [21.000, 82.000], mean observation: 3.153 [-1.315, 10.264], loss: 1.663171, mae: 4.949663, mean_q: 5.217052
 33474/100000: episode: 3419, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.879, mean reward: 0.488 [0.362, 0.542], mean action: 56.900 [14.000, 101.000], mean observation: 3.168 [-0.983, 10.314], loss: 1.310161, mae: 4.947840, mean_q: 5.211833
 33484/100000: episode: 3420, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.359, mean reward: 0.436 [0.350, 0.549], mean action: 46.900 [15.000, 90.000], mean observation: 3.146 [-1.016, 10.466], loss: 1.043662, mae: 4.947001, mean_q: 5.210490
 33494/100000: episode: 3421, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.947, mean reward: 0.395 [0.312, 0.501], mean action: 44.200 [11.000, 67.000], mean observation: 3.155 [-2.160, 10.308], loss: 1.592298, mae: 4.949655, mean_q: 5.211978
 33504/100000: episode: 3422, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.905, mean reward: 0.490 [0.490, 0.490], mean action: 49.200 [21.000, 71.000], mean observation: 3.143 [-1.505, 10.288], loss: 0.871904, mae: 4.946634, mean_q: 5.208087
 33514/100000: episode: 3423, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.074, mean reward: 0.407 [0.338, 0.478], mean action: 61.300 [22.000, 97.000], mean observation: 3.163 [-0.875, 10.352], loss: 1.030096, mae: 4.947806, mean_q: 5.209018
 33524/100000: episode: 3424, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.168, mean reward: 0.417 [0.390, 0.491], mean action: 48.000 [8.000, 98.000], mean observation: 3.143 [-1.830, 10.303], loss: 1.254998, mae: 4.949114, mean_q: 5.211408
 33534/100000: episode: 3425, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.076, mean reward: 0.408 [0.353, 0.497], mean action: 45.500 [3.000, 94.000], mean observation: 3.170 [-1.527, 10.208], loss: 1.081000, mae: 4.948696, mean_q: 5.213957
 33544/100000: episode: 3426, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 3.983, mean reward: 0.398 [0.305, 0.495], mean action: 44.100 [1.000, 70.000], mean observation: 3.147 [-1.463, 10.248], loss: 1.259308, mae: 4.949729, mean_q: 5.215360
 33554/100000: episode: 3427, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.102, mean reward: 0.410 [0.370, 0.469], mean action: 43.400 [7.000, 84.000], mean observation: 3.160 [-1.704, 10.394], loss: 1.218751, mae: 4.949473, mean_q: 5.216380
 33564/100000: episode: 3428, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 5.243, mean reward: 0.524 [0.524, 0.524], mean action: 61.400 [49.000, 86.000], mean observation: 3.164 [-2.294, 10.309], loss: 1.430533, mae: 4.950316, mean_q: 5.219104
 33574/100000: episode: 3429, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.059, mean reward: 0.406 [0.377, 0.463], mean action: 57.100 [49.000, 86.000], mean observation: 3.152 [-1.077, 10.322], loss: 1.348337, mae: 4.949716, mean_q: 5.220437
 33584/100000: episode: 3430, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.847, mean reward: 0.385 [0.362, 0.431], mean action: 47.700 [24.000, 61.000], mean observation: 3.156 [-1.638, 10.397], loss: 1.205787, mae: 4.949188, mean_q: 5.221506
 33594/100000: episode: 3431, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.890, mean reward: 0.389 [0.320, 0.507], mean action: 36.700 [2.000, 93.000], mean observation: 3.162 [-1.363, 10.406], loss: 1.338569, mae: 4.949669, mean_q: 5.224265
 33604/100000: episode: 3432, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.915, mean reward: 0.391 [0.328, 0.476], mean action: 51.800 [9.000, 101.000], mean observation: 3.156 [-0.632, 10.399], loss: 1.031166, mae: 4.948449, mean_q: 5.219838
 33607/100000: episode: 3433, duration: 0.062s, episode steps: 3, steps per second: 48, episode reward: 10.697, mean reward: 3.566 [0.348, 10.000], mean action: 58.333 [49.000, 77.000], mean observation: 3.150 [-1.596, 10.232], loss: 1.474070, mae: 4.950211, mean_q: 5.217763
 33617/100000: episode: 3434, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.644, mean reward: 0.364 [0.335, 0.421], mean action: 42.000 [0.000, 60.000], mean observation: 3.147 [-1.459, 10.199], loss: 1.431088, mae: 4.950200, mean_q: 5.216936
 33627/100000: episode: 3435, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.739, mean reward: 0.474 [0.474, 0.475], mean action: 49.600 [8.000, 81.000], mean observation: 3.157 [-1.380, 10.396], loss: 1.104852, mae: 4.949046, mean_q: 5.214547
 33637/100000: episode: 3436, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.394, mean reward: 0.439 [0.436, 0.451], mean action: 48.900 [17.000, 78.000], mean observation: 3.150 [-1.576, 10.324], loss: 1.230763, mae: 4.949608, mean_q: 5.214606
 33647/100000: episode: 3437, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.937, mean reward: 0.494 [0.394, 0.530], mean action: 46.200 [5.000, 97.000], mean observation: 3.165 [-1.292, 10.390], loss: 1.201852, mae: 4.949389, mean_q: 5.216309
 33657/100000: episode: 3438, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.244, mean reward: 0.424 [0.297, 0.476], mean action: 48.900 [14.000, 85.000], mean observation: 3.153 [-1.719, 10.287], loss: 1.094413, mae: 4.949410, mean_q: 5.218394
 33660/100000: episode: 3439, duration: 0.067s, episode steps: 3, steps per second: 45, episode reward: 10.862, mean reward: 3.621 [0.389, 10.000], mean action: 60.000 [49.000, 82.000], mean observation: 3.134 [-0.725, 10.238], loss: 1.410753, mae: 4.951329, mean_q: 5.219608
 33670/100000: episode: 3440, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.084, mean reward: 0.408 [0.328, 0.590], mean action: 43.700 [0.000, 91.000], mean observation: 3.159 [-1.259, 10.555], loss: 1.349442, mae: 4.950856, mean_q: 5.220977
 33680/100000: episode: 3441, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.717, mean reward: 0.372 [0.346, 0.476], mean action: 40.700 [18.000, 52.000], mean observation: 3.152 [-1.147, 10.270], loss: 1.130189, mae: 4.950102, mean_q: 5.220782
 33690/100000: episode: 3442, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.339, mean reward: 0.434 [0.368, 0.482], mean action: 41.400 [1.000, 90.000], mean observation: 3.166 [-1.605, 10.302], loss: 1.303708, mae: 4.950905, mean_q: 5.220305
 33700/100000: episode: 3443, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.040, mean reward: 0.404 [0.385, 0.478], mean action: 57.000 [30.000, 98.000], mean observation: 3.145 [-1.249, 10.371], loss: 1.047557, mae: 4.950021, mean_q: 5.220014
 33710/100000: episode: 3444, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.531, mean reward: 0.353 [0.284, 0.464], mean action: 47.200 [21.000, 99.000], mean observation: 3.153 [-1.112, 10.298], loss: 1.273879, mae: 4.951399, mean_q: 5.219805
 33720/100000: episode: 3445, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.026, mean reward: 0.403 [0.303, 0.538], mean action: 51.800 [39.000, 93.000], mean observation: 3.149 [-1.238, 10.205], loss: 1.163657, mae: 4.951084, mean_q: 5.219593
 33730/100000: episode: 3446, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.325, mean reward: 0.433 [0.325, 0.573], mean action: 45.100 [5.000, 54.000], mean observation: 3.151 [-0.974, 10.314], loss: 1.338725, mae: 4.951872, mean_q: 5.219243
 33740/100000: episode: 3447, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.061, mean reward: 0.406 [0.276, 0.595], mean action: 45.200 [22.000, 49.000], mean observation: 3.160 [-1.373, 10.372], loss: 1.206711, mae: 4.951640, mean_q: 5.220312
 33750/100000: episode: 3448, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.926, mean reward: 0.393 [0.361, 0.588], mean action: 46.200 [19.000, 51.000], mean observation: 3.157 [-1.248, 10.534], loss: 1.213897, mae: 4.951725, mean_q: 5.225451
 33760/100000: episode: 3449, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.515, mean reward: 0.451 [0.422, 0.525], mean action: 52.700 [26.000, 80.000], mean observation: 3.140 [-1.849, 10.328], loss: 0.848954, mae: 4.950904, mean_q: 5.231193
 33770/100000: episode: 3450, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.945, mean reward: 0.395 [0.332, 0.469], mean action: 56.900 [41.000, 99.000], mean observation: 3.159 [-1.282, 10.304], loss: 1.156102, mae: 4.952720, mean_q: 5.235557
 33780/100000: episode: 3451, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.686, mean reward: 0.369 [0.347, 0.411], mean action: 42.800 [16.000, 93.000], mean observation: 3.152 [-1.212, 10.209], loss: 1.474030, mae: 4.954244, mean_q: 5.237470
 33790/100000: episode: 3452, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.030, mean reward: 0.403 [0.362, 0.464], mean action: 43.900 [12.000, 79.000], mean observation: 3.159 [-1.913, 10.391], loss: 1.455305, mae: 4.954446, mean_q: 5.234792
 33800/100000: episode: 3453, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.387, mean reward: 0.439 [0.378, 0.528], mean action: 34.000 [2.000, 49.000], mean observation: 3.149 [-1.624, 10.427], loss: 1.299345, mae: 4.953962, mean_q: 5.234574
 33810/100000: episode: 3454, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.051, mean reward: 0.405 [0.368, 0.480], mean action: 44.000 [14.000, 84.000], mean observation: 3.153 [-1.709, 10.335], loss: 0.873081, mae: 4.952325, mean_q: 5.233772
 33820/100000: episode: 3455, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.403, mean reward: 0.440 [0.364, 0.472], mean action: 47.000 [20.000, 84.000], mean observation: 3.164 [-1.408, 10.277], loss: 1.358312, mae: 4.955012, mean_q: 5.232741
 33830/100000: episode: 3456, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.768, mean reward: 0.477 [0.389, 0.556], mean action: 48.500 [12.000, 75.000], mean observation: 3.146 [-1.250, 10.275], loss: 1.445301, mae: 4.955550, mean_q: 5.232820
 33840/100000: episode: 3457, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.999, mean reward: 0.400 [0.347, 0.450], mean action: 41.600 [4.000, 52.000], mean observation: 3.130 [-1.523, 10.334], loss: 1.225213, mae: 4.954843, mean_q: 5.232394
 33850/100000: episode: 3458, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.254, mean reward: 0.425 [0.377, 0.489], mean action: 49.900 [11.000, 80.000], mean observation: 3.154 [-1.266, 10.271], loss: 1.121962, mae: 4.955155, mean_q: 5.229560
 33860/100000: episode: 3459, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.882, mean reward: 0.388 [0.348, 0.413], mean action: 46.200 [21.000, 90.000], mean observation: 3.150 [-1.260, 10.318], loss: 0.978114, mae: 4.955138, mean_q: 5.230857
 33870/100000: episode: 3460, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.037, mean reward: 0.404 [0.373, 0.506], mean action: 48.200 [23.000, 69.000], mean observation: 3.154 [-1.568, 10.389], loss: 1.146752, mae: 4.956787, mean_q: 5.233199
 33880/100000: episode: 3461, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.935, mean reward: 0.393 [0.381, 0.421], mean action: 52.200 [9.000, 89.000], mean observation: 3.162 [-1.646, 10.361], loss: 1.229939, mae: 4.957572, mean_q: 5.233747
 33890/100000: episode: 3462, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.959, mean reward: 0.496 [0.483, 0.556], mean action: 50.000 [15.000, 100.000], mean observation: 3.173 [-1.270, 10.240], loss: 1.250707, mae: 4.958296, mean_q: 5.232316
 33900/100000: episode: 3463, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.776, mean reward: 0.378 [0.329, 0.419], mean action: 40.600 [1.000, 76.000], mean observation: 3.156 [-1.475, 10.311], loss: 1.302189, mae: 4.958824, mean_q: 5.229985
 33910/100000: episode: 3464, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.320, mean reward: 0.432 [0.399, 0.463], mean action: 51.700 [8.000, 90.000], mean observation: 3.168 [-1.307, 10.285], loss: 1.205611, mae: 4.959006, mean_q: 5.230011
 33920/100000: episode: 3465, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.625, mean reward: 0.363 [0.291, 0.390], mean action: 67.000 [37.000, 96.000], mean observation: 3.168 [-1.071, 10.234], loss: 1.498697, mae: 4.960472, mean_q: 5.229996
 33930/100000: episode: 3466, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.497, mean reward: 0.450 [0.348, 0.531], mean action: 50.600 [34.000, 80.000], mean observation: 3.157 [-1.209, 10.356], loss: 1.297660, mae: 4.959543, mean_q: 5.229590
 33940/100000: episode: 3467, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.673, mean reward: 0.367 [0.312, 0.455], mean action: 50.500 [23.000, 85.000], mean observation: 3.154 [-1.424, 10.320], loss: 1.017687, mae: 4.958633, mean_q: 5.229056
 33950/100000: episode: 3468, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.255, mean reward: 0.425 [0.361, 0.523], mean action: 43.400 [9.000, 49.000], mean observation: 3.155 [-1.968, 10.234], loss: 1.210906, mae: 4.959766, mean_q: 5.225283
 33960/100000: episode: 3469, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 5.114, mean reward: 0.511 [0.504, 0.576], mean action: 50.700 [13.000, 91.000], mean observation: 3.143 [-1.277, 10.122], loss: 1.214676, mae: 4.960052, mean_q: 5.222834
 33970/100000: episode: 3470, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.018, mean reward: 0.402 [0.338, 0.482], mean action: 51.100 [25.000, 80.000], mean observation: 3.139 [-1.610, 10.361], loss: 1.118977, mae: 4.960057, mean_q: 5.217174
 33980/100000: episode: 3471, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.400, mean reward: 0.440 [0.331, 0.500], mean action: 46.700 [5.000, 74.000], mean observation: 3.159 [-1.613, 10.376], loss: 1.388929, mae: 4.961284, mean_q: 5.211992
 33990/100000: episode: 3472, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.146, mean reward: 0.415 [0.410, 0.445], mean action: 56.700 [38.000, 80.000], mean observation: 3.163 [-1.044, 10.372], loss: 0.890668, mae: 4.959495, mean_q: 5.206480
 34000/100000: episode: 3473, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 5.050, mean reward: 0.505 [0.496, 0.588], mean action: 48.000 [30.000, 68.000], mean observation: 3.152 [-1.829, 10.261], loss: 1.451979, mae: 4.961932, mean_q: 5.200943
 34010/100000: episode: 3474, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.719, mean reward: 0.372 [0.343, 0.417], mean action: 49.800 [14.000, 71.000], mean observation: 3.156 [-1.058, 10.363], loss: 1.097645, mae: 4.960742, mean_q: 5.195296
 34020/100000: episode: 3475, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.354, mean reward: 0.435 [0.418, 0.477], mean action: 59.600 [12.000, 74.000], mean observation: 3.144 [-2.207, 10.250], loss: 1.114166, mae: 4.960846, mean_q: 5.193986
 34030/100000: episode: 3476, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.386, mean reward: 0.439 [0.339, 0.505], mean action: 46.600 [1.000, 71.000], mean observation: 3.158 [-1.258, 10.355], loss: 1.297876, mae: 4.961595, mean_q: 5.195120
 34040/100000: episode: 3477, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.963, mean reward: 0.396 [0.360, 0.451], mean action: 64.600 [27.000, 100.000], mean observation: 3.143 [-1.773, 10.387], loss: 1.037173, mae: 4.960567, mean_q: 5.196217
 34050/100000: episode: 3478, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.121, mean reward: 0.412 [0.374, 0.566], mean action: 63.500 [9.000, 93.000], mean observation: 3.145 [-1.161, 10.268], loss: 1.473636, mae: 4.961957, mean_q: 5.196265
 34060/100000: episode: 3479, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.953, mean reward: 0.395 [0.343, 0.480], mean action: 51.500 [12.000, 92.000], mean observation: 3.156 [-2.562, 10.375], loss: 1.070308, mae: 4.960796, mean_q: 5.194742
 34070/100000: episode: 3480, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.694, mean reward: 0.469 [0.382, 0.534], mean action: 48.800 [17.000, 77.000], mean observation: 3.150 [-1.474, 10.358], loss: 1.018138, mae: 4.960733, mean_q: 5.192555
 34080/100000: episode: 3481, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.661, mean reward: 0.366 [0.341, 0.469], mean action: 75.000 [24.000, 96.000], mean observation: 3.164 [-1.327, 10.338], loss: 1.109397, mae: 4.961009, mean_q: 5.192514
 34090/100000: episode: 3482, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.883, mean reward: 0.388 [0.328, 0.445], mean action: 37.300 [24.000, 98.000], mean observation: 3.147 [-1.840, 10.281], loss: 1.300771, mae: 4.962214, mean_q: 5.194934
 34091/100000: episode: 3483, duration: 0.039s, episode steps: 1, steps per second: 25, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 24.000 [24.000, 24.000], mean observation: 3.135 [-1.494, 10.100], loss: 1.770541, mae: 4.964333, mean_q: 5.196107
 34101/100000: episode: 3484, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.972, mean reward: 0.397 [0.376, 0.428], mean action: 40.600 [24.000, 83.000], mean observation: 3.141 [-1.923, 10.320], loss: 0.951715, mae: 4.961312, mean_q: 5.197409
 34111/100000: episode: 3485, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.229, mean reward: 0.423 [0.368, 0.501], mean action: 37.200 [22.000, 98.000], mean observation: 3.151 [-1.498, 10.278], loss: 1.163559, mae: 4.962128, mean_q: 5.198642
 34121/100000: episode: 3486, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.028, mean reward: 0.403 [0.340, 0.505], mean action: 30.600 [1.000, 65.000], mean observation: 3.155 [-1.815, 10.335], loss: 1.188477, mae: 4.962549, mean_q: 5.200470
 34131/100000: episode: 3487, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.205, mean reward: 0.420 [0.367, 0.500], mean action: 29.700 [6.000, 68.000], mean observation: 3.160 [-1.471, 10.282], loss: 1.395504, mae: 4.963373, mean_q: 5.205034
 34141/100000: episode: 3488, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.138, mean reward: 0.414 [0.306, 0.554], mean action: 32.600 [21.000, 62.000], mean observation: 3.144 [-2.009, 10.364], loss: 1.325854, mae: 4.962945, mean_q: 5.207887
 34151/100000: episode: 3489, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.351, mean reward: 0.435 [0.377, 0.511], mean action: 49.400 [24.000, 96.000], mean observation: 3.161 [-1.364, 10.304], loss: 1.391851, mae: 4.963273, mean_q: 5.209708
 34161/100000: episode: 3490, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.353, mean reward: 0.435 [0.361, 0.535], mean action: 45.500 [18.000, 92.000], mean observation: 3.161 [-1.476, 10.355], loss: 1.049273, mae: 4.961741, mean_q: 5.210341
 34171/100000: episode: 3491, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.968, mean reward: 0.397 [0.327, 0.446], mean action: 44.300 [24.000, 92.000], mean observation: 3.147 [-1.322, 10.268], loss: 1.093377, mae: 4.962099, mean_q: 5.209234
 34179/100000: episode: 3492, duration: 0.160s, episode steps: 8, steps per second: 50, episode reward: 13.333, mean reward: 1.667 [0.469, 10.000], mean action: 35.000 [24.000, 81.000], mean observation: 3.149 [-1.278, 10.299], loss: 1.024565, mae: 4.962489, mean_q: 5.210047
 34189/100000: episode: 3493, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.732, mean reward: 0.373 [0.323, 0.451], mean action: 33.700 [2.000, 99.000], mean observation: 3.151 [-1.458, 10.490], loss: 1.015316, mae: 4.962747, mean_q: 5.210903
 34199/100000: episode: 3494, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.122, mean reward: 0.412 [0.327, 0.511], mean action: 39.500 [24.000, 99.000], mean observation: 3.159 [-1.037, 10.216], loss: 1.569262, mae: 4.965365, mean_q: 5.211746
 34209/100000: episode: 3495, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.376, mean reward: 0.438 [0.348, 0.532], mean action: 45.000 [22.000, 93.000], mean observation: 3.154 [-2.237, 10.365], loss: 1.240746, mae: 4.964073, mean_q: 5.207261
 34219/100000: episode: 3496, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.915, mean reward: 0.391 [0.326, 0.520], mean action: 37.500 [3.000, 100.000], mean observation: 3.159 [-1.330, 10.339], loss: 1.127468, mae: 4.963720, mean_q: 5.205913
 34229/100000: episode: 3497, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.315, mean reward: 0.431 [0.417, 0.477], mean action: 39.300 [16.000, 86.000], mean observation: 3.147 [-1.809, 10.437], loss: 1.206747, mae: 4.964491, mean_q: 5.203753
 34239/100000: episode: 3498, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.236, mean reward: 0.424 [0.337, 0.476], mean action: 40.900 [10.000, 95.000], mean observation: 3.161 [-1.270, 10.451], loss: 0.954275, mae: 4.963562, mean_q: 5.204404
 34249/100000: episode: 3499, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.602, mean reward: 0.460 [0.405, 0.519], mean action: 47.300 [19.000, 83.000], mean observation: 3.152 [-1.750, 10.293], loss: 1.102786, mae: 4.964105, mean_q: 5.206688
 34259/100000: episode: 3500, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.778, mean reward: 0.478 [0.473, 0.512], mean action: 33.200 [15.000, 91.000], mean observation: 3.163 [-1.479, 10.355], loss: 1.262707, mae: 4.965153, mean_q: 5.208865
 34269/100000: episode: 3501, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.803, mean reward: 0.380 [0.303, 0.415], mean action: 46.300 [24.000, 95.000], mean observation: 3.154 [-1.418, 10.269], loss: 1.531533, mae: 4.966042, mean_q: 5.211374
 34279/100000: episode: 3502, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.305, mean reward: 0.431 [0.283, 0.559], mean action: 31.100 [0.000, 89.000], mean observation: 3.161 [-1.133, 10.283], loss: 0.872731, mae: 4.963351, mean_q: 5.213147
 34289/100000: episode: 3503, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.811, mean reward: 0.381 [0.333, 0.501], mean action: 36.900 [15.000, 88.000], mean observation: 3.159 [-1.677, 10.241], loss: 1.095330, mae: 4.964549, mean_q: 5.211535
 34299/100000: episode: 3504, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.143, mean reward: 0.414 [0.412, 0.430], mean action: 37.800 [21.000, 78.000], mean observation: 3.163 [-1.176, 10.234], loss: 1.400871, mae: 4.965819, mean_q: 5.208842
 34309/100000: episode: 3505, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.881, mean reward: 0.388 [0.320, 0.495], mean action: 41.400 [18.000, 91.000], mean observation: 3.151 [-1.781, 10.322], loss: 1.113406, mae: 4.965015, mean_q: 5.208324
 34319/100000: episode: 3506, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.110, mean reward: 0.411 [0.341, 0.572], mean action: 28.400 [24.000, 48.000], mean observation: 3.167 [-1.553, 10.348], loss: 0.942053, mae: 4.964903, mean_q: 5.209124
 34329/100000: episode: 3507, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.793, mean reward: 0.479 [0.473, 0.541], mean action: 56.600 [12.000, 99.000], mean observation: 3.155 [-1.389, 10.297], loss: 1.300514, mae: 4.966842, mean_q: 5.211593
 34339/100000: episode: 3508, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.959, mean reward: 0.396 [0.332, 0.498], mean action: 40.300 [24.000, 84.000], mean observation: 3.160 [-1.221, 10.544], loss: 1.197352, mae: 4.966678, mean_q: 5.214172
 34349/100000: episode: 3509, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.537, mean reward: 0.454 [0.364, 0.586], mean action: 26.200 [24.000, 46.000], mean observation: 3.146 [-1.671, 10.368], loss: 1.001644, mae: 4.966037, mean_q: 5.213044
 34359/100000: episode: 3510, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.679, mean reward: 0.368 [0.313, 0.479], mean action: 36.000 [6.000, 84.000], mean observation: 3.168 [-0.972, 10.438], loss: 1.094177, mae: 4.966874, mean_q: 5.210485
 34369/100000: episode: 3511, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 3.966, mean reward: 0.397 [0.341, 0.468], mean action: 39.600 [16.000, 83.000], mean observation: 3.165 [-1.162, 10.307], loss: 1.328041, mae: 4.968055, mean_q: 5.211060
 34379/100000: episode: 3512, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.846, mean reward: 0.385 [0.310, 0.479], mean action: 43.600 [22.000, 75.000], mean observation: 3.143 [-2.134, 10.248], loss: 0.978593, mae: 4.966842, mean_q: 5.210921
 34389/100000: episode: 3513, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.600, mean reward: 0.460 [0.382, 0.553], mean action: 30.300 [1.000, 70.000], mean observation: 3.162 [-2.140, 10.369], loss: 1.244856, mae: 4.968464, mean_q: 5.212013
 34399/100000: episode: 3514, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.986, mean reward: 0.399 [0.339, 0.519], mean action: 37.300 [24.000, 84.000], mean observation: 3.147 [-1.502, 10.612], loss: 1.161066, mae: 4.968428, mean_q: 5.211648
 34409/100000: episode: 3515, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.037, mean reward: 0.404 [0.372, 0.520], mean action: 36.400 [24.000, 95.000], mean observation: 3.167 [-0.866, 10.423], loss: 1.269033, mae: 4.968853, mean_q: 5.209564
 34419/100000: episode: 3516, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.780, mean reward: 0.378 [0.347, 0.431], mean action: 36.400 [24.000, 94.000], mean observation: 3.159 [-1.291, 10.293], loss: 0.909555, mae: 4.967587, mean_q: 5.211135
 34429/100000: episode: 3517, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.167, mean reward: 0.417 [0.409, 0.478], mean action: 43.600 [24.000, 93.000], mean observation: 3.160 [-1.494, 10.408], loss: 1.180772, mae: 4.969339, mean_q: 5.215371
 34439/100000: episode: 3518, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.475, mean reward: 0.448 [0.331, 0.567], mean action: 32.800 [2.000, 96.000], mean observation: 3.168 [-1.954, 10.455], loss: 1.236115, mae: 4.969685, mean_q: 5.217936
 34449/100000: episode: 3519, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 3.981, mean reward: 0.398 [0.335, 0.501], mean action: 26.400 [7.000, 67.000], mean observation: 3.162 [-1.960, 10.442], loss: 1.284243, mae: 4.969888, mean_q: 5.220470
 34459/100000: episode: 3520, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.814, mean reward: 0.381 [0.346, 0.424], mean action: 39.300 [16.000, 82.000], mean observation: 3.160 [-1.271, 10.384], loss: 1.304426, mae: 4.970418, mean_q: 5.221430
 34469/100000: episode: 3521, duration: 0.223s, episode steps: 10, steps per second: 45, episode reward: 4.405, mean reward: 0.440 [0.364, 0.529], mean action: 25.200 [3.000, 77.000], mean observation: 3.154 [-1.050, 10.276], loss: 1.347201, mae: 4.970500, mean_q: 5.220179
 34479/100000: episode: 3522, duration: 0.220s, episode steps: 10, steps per second: 45, episode reward: 3.556, mean reward: 0.356 [0.297, 0.417], mean action: 29.800 [21.000, 70.000], mean observation: 3.146 [-1.431, 10.348], loss: 1.073542, mae: 4.969609, mean_q: 5.217973
 34489/100000: episode: 3523, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.895, mean reward: 0.390 [0.328, 0.480], mean action: 43.100 [2.000, 99.000], mean observation: 3.164 [-1.103, 10.493], loss: 1.476422, mae: 4.971500, mean_q: 5.213422
 34499/100000: episode: 3524, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.536, mean reward: 0.454 [0.405, 0.495], mean action: 41.800 [13.000, 92.000], mean observation: 3.160 [-3.068, 10.574], loss: 1.236803, mae: 4.970525, mean_q: 5.214160
 34509/100000: episode: 3525, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.946, mean reward: 0.395 [0.387, 0.425], mean action: 38.000 [15.000, 93.000], mean observation: 3.159 [-2.155, 10.354], loss: 1.119074, mae: 4.970290, mean_q: 5.217937
 34519/100000: episode: 3526, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.781, mean reward: 0.378 [0.326, 0.446], mean action: 53.400 [4.000, 99.000], mean observation: 3.158 [-1.558, 10.364], loss: 1.377858, mae: 4.971494, mean_q: 5.223198
 34529/100000: episode: 3527, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.345, mean reward: 0.434 [0.375, 0.495], mean action: 45.800 [6.000, 93.000], mean observation: 3.151 [-1.390, 10.449], loss: 1.260558, mae: 4.971280, mean_q: 5.228302
 34539/100000: episode: 3528, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.933, mean reward: 0.393 [0.338, 0.449], mean action: 42.000 [1.000, 91.000], mean observation: 3.154 [-1.702, 10.491], loss: 1.322438, mae: 4.971494, mean_q: 5.230124
 34549/100000: episode: 3529, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.997, mean reward: 0.400 [0.329, 0.552], mean action: 48.500 [24.000, 101.000], mean observation: 3.148 [-1.543, 10.346], loss: 1.290102, mae: 4.971498, mean_q: 5.225338
 34559/100000: episode: 3530, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.202, mean reward: 0.420 [0.339, 0.542], mean action: 32.600 [23.000, 89.000], mean observation: 3.148 [-1.718, 10.215], loss: 1.103967, mae: 4.970787, mean_q: 5.221507
 34569/100000: episode: 3531, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.764, mean reward: 0.376 [0.333, 0.397], mean action: 32.700 [10.000, 69.000], mean observation: 3.147 [-1.398, 10.216], loss: 1.364338, mae: 4.972255, mean_q: 5.220819
 34579/100000: episode: 3532, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.563, mean reward: 0.456 [0.430, 0.516], mean action: 22.500 [7.000, 32.000], mean observation: 3.166 [-2.189, 10.310], loss: 1.158056, mae: 4.971547, mean_q: 5.219719
 34589/100000: episode: 3533, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 5.143, mean reward: 0.514 [0.514, 0.514], mean action: 37.900 [24.000, 97.000], mean observation: 3.144 [-1.078, 10.274], loss: 1.556749, mae: 4.973104, mean_q: 5.220690
 34590/100000: episode: 3534, duration: 0.022s, episode steps: 1, steps per second: 45, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 11.000 [11.000, 11.000], mean observation: 3.162 [-1.110, 10.100], loss: 1.793650, mae: 4.973877, mean_q: 5.220247
 34600/100000: episode: 3535, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.301, mean reward: 0.430 [0.351, 0.543], mean action: 36.300 [24.000, 58.000], mean observation: 3.158 [-2.132, 10.331], loss: 1.412237, mae: 4.972522, mean_q: 5.220559
 34610/100000: episode: 3536, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 4.065, mean reward: 0.406 [0.314, 0.457], mean action: 25.500 [6.000, 41.000], mean observation: 3.153 [-1.169, 10.470], loss: 1.523955, mae: 4.972691, mean_q: 5.221999
 34614/100000: episode: 3537, duration: 0.104s, episode steps: 4, steps per second: 38, episode reward: 11.017, mean reward: 2.754 [0.338, 10.000], mean action: 18.500 [2.000, 24.000], mean observation: 3.156 [-1.823, 10.250], loss: 0.868913, mae: 4.969975, mean_q: 5.223642
 34624/100000: episode: 3538, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.259, mean reward: 0.426 [0.315, 0.599], mean action: 53.900 [24.000, 85.000], mean observation: 3.153 [-1.141, 10.246], loss: 1.219987, mae: 4.971655, mean_q: 5.223876
 34634/100000: episode: 3539, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.628, mean reward: 0.463 [0.327, 0.532], mean action: 40.100 [1.000, 92.000], mean observation: 3.154 [-1.264, 10.258], loss: 1.207719, mae: 4.971890, mean_q: 5.221014
 34644/100000: episode: 3540, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.148, mean reward: 0.415 [0.307, 0.445], mean action: 29.400 [12.000, 85.000], mean observation: 3.145 [-1.448, 10.296], loss: 1.196739, mae: 4.971840, mean_q: 5.217812
 34654/100000: episode: 3541, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.601, mean reward: 0.460 [0.457, 0.492], mean action: 41.900 [21.000, 100.000], mean observation: 3.141 [-1.583, 10.502], loss: 1.218230, mae: 4.972425, mean_q: 5.219348
 34656/100000: episode: 3542, duration: 0.062s, episode steps: 2, steps per second: 32, episode reward: 10.460, mean reward: 5.230 [0.460, 10.000], mean action: 21.500 [19.000, 24.000], mean observation: 3.144 [-1.194, 10.281], loss: 0.491485, mae: 4.969424, mean_q: 5.219413
 34666/100000: episode: 3543, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.289, mean reward: 0.429 [0.333, 0.517], mean action: 22.300 [5.000, 48.000], mean observation: 3.163 [-1.813, 10.372], loss: 0.919106, mae: 4.971338, mean_q: 5.217752
 34676/100000: episode: 3544, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.038, mean reward: 0.404 [0.351, 0.512], mean action: 34.100 [24.000, 75.000], mean observation: 3.149 [-1.211, 10.377], loss: 1.249516, mae: 4.972913, mean_q: 5.214484
 34686/100000: episode: 3545, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.090, mean reward: 0.409 [0.307, 0.488], mean action: 35.100 [0.000, 89.000], mean observation: 3.166 [-1.475, 10.306], loss: 0.868566, mae: 4.972123, mean_q: 5.214721
 34696/100000: episode: 3546, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.422, mean reward: 0.442 [0.376, 0.634], mean action: 40.000 [24.000, 89.000], mean observation: 3.170 [-1.260, 10.330], loss: 1.271216, mae: 4.973992, mean_q: 5.216110
 34706/100000: episode: 3547, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.044, mean reward: 0.404 [0.379, 0.590], mean action: 32.300 [14.000, 100.000], mean observation: 3.163 [-1.407, 10.337], loss: 1.164182, mae: 4.973826, mean_q: 5.218369
 34716/100000: episode: 3548, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.912, mean reward: 0.391 [0.339, 0.463], mean action: 44.000 [9.000, 98.000], mean observation: 3.161 [-1.315, 10.340], loss: 1.071410, mae: 4.973782, mean_q: 5.221249
 34726/100000: episode: 3549, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.073, mean reward: 0.407 [0.326, 0.482], mean action: 41.500 [24.000, 91.000], mean observation: 3.156 [-1.248, 10.406], loss: 1.002026, mae: 4.973917, mean_q: 5.222116
 34736/100000: episode: 3550, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.994, mean reward: 0.399 [0.350, 0.464], mean action: 27.700 [0.000, 69.000], mean observation: 3.158 [-1.031, 10.306], loss: 1.269148, mae: 4.974890, mean_q: 5.222532
 34746/100000: episode: 3551, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.435, mean reward: 0.443 [0.404, 0.579], mean action: 40.600 [24.000, 92.000], mean observation: 3.168 [-1.218, 10.318], loss: 1.308591, mae: 4.975338, mean_q: 5.221709
 34756/100000: episode: 3552, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.073, mean reward: 0.407 [0.299, 0.527], mean action: 39.100 [11.000, 89.000], mean observation: 3.172 [-1.043, 10.471], loss: 1.542164, mae: 4.976538, mean_q: 5.221684
 34766/100000: episode: 3553, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.258, mean reward: 0.426 [0.382, 0.506], mean action: 32.400 [10.000, 87.000], mean observation: 3.151 [-1.296, 10.279], loss: 1.479671, mae: 4.976279, mean_q: 5.223649
 34776/100000: episode: 3554, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.734, mean reward: 0.473 [0.440, 0.564], mean action: 38.900 [12.000, 93.000], mean observation: 3.148 [-1.733, 10.406], loss: 0.878391, mae: 4.974214, mean_q: 5.222983
 34786/100000: episode: 3555, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.223, mean reward: 0.422 [0.306, 0.538], mean action: 41.700 [16.000, 99.000], mean observation: 3.150 [-1.229, 10.334], loss: 1.258477, mae: 4.976013, mean_q: 5.221977
 34796/100000: episode: 3556, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.872, mean reward: 0.387 [0.313, 0.449], mean action: 35.200 [17.000, 79.000], mean observation: 3.156 [-1.746, 10.241], loss: 1.239882, mae: 4.976032, mean_q: 5.221677
 34806/100000: episode: 3557, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.075, mean reward: 0.408 [0.359, 0.486], mean action: 23.900 [23.000, 24.000], mean observation: 3.151 [-1.413, 10.323], loss: 1.142654, mae: 4.975993, mean_q: 5.222416
 34816/100000: episode: 3558, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.896, mean reward: 0.390 [0.310, 0.442], mean action: 47.400 [24.000, 88.000], mean observation: 3.146 [-1.332, 10.257], loss: 1.204583, mae: 4.976238, mean_q: 5.221309
 34826/100000: episode: 3559, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.280, mean reward: 0.428 [0.323, 0.591], mean action: 34.100 [19.000, 96.000], mean observation: 3.145 [-1.638, 10.455], loss: 1.318338, mae: 4.976871, mean_q: 5.215431
 34836/100000: episode: 3560, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.297, mean reward: 0.430 [0.384, 0.540], mean action: 41.600 [24.000, 91.000], mean observation: 3.173 [-1.405, 10.359], loss: 1.377964, mae: 4.977386, mean_q: 5.212318
 34842/100000: episode: 3561, duration: 0.114s, episode steps: 6, steps per second: 53, episode reward: 12.207, mean reward: 2.035 [0.387, 10.000], mean action: 28.000 [3.000, 43.000], mean observation: 3.155 [-1.663, 10.198], loss: 1.089323, mae: 4.976124, mean_q: 5.212827
 34852/100000: episode: 3562, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.904, mean reward: 0.390 [0.345, 0.470], mean action: 36.200 [11.000, 78.000], mean observation: 3.176 [-1.444, 10.362], loss: 0.959247, mae: 4.975774, mean_q: 5.212046
 34862/100000: episode: 3563, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.664, mean reward: 0.466 [0.360, 0.478], mean action: 47.200 [7.000, 95.000], mean observation: 3.163 [-1.355, 10.334], loss: 1.206182, mae: 4.976987, mean_q: 5.211537
 34872/100000: episode: 3564, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 5.101, mean reward: 0.510 [0.510, 0.510], mean action: 48.200 [20.000, 89.000], mean observation: 3.156 [-1.451, 10.361], loss: 1.493659, mae: 4.978383, mean_q: 5.210364
 34882/100000: episode: 3565, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.240, mean reward: 0.424 [0.416, 0.449], mean action: 46.900 [3.000, 94.000], mean observation: 3.158 [-0.715, 10.428], loss: 1.178246, mae: 4.977132, mean_q: 5.204226
 34892/100000: episode: 3566, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.197, mean reward: 0.420 [0.398, 0.471], mean action: 43.400 [24.000, 100.000], mean observation: 3.164 [-2.242, 10.364], loss: 1.290732, mae: 4.977478, mean_q: 5.202844
 34902/100000: episode: 3567, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.113, mean reward: 0.411 [0.391, 0.453], mean action: 46.200 [6.000, 92.000], mean observation: 3.153 [-1.330, 10.280], loss: 0.977727, mae: 4.976273, mean_q: 5.206383
 34912/100000: episode: 3568, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.058, mean reward: 0.406 [0.298, 0.489], mean action: 29.200 [8.000, 71.000], mean observation: 3.152 [-0.900, 10.406], loss: 1.052967, mae: 4.976977, mean_q: 5.208956
 34922/100000: episode: 3569, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.223, mean reward: 0.422 [0.322, 0.518], mean action: 24.500 [1.000, 67.000], mean observation: 3.159 [-1.638, 10.331], loss: 1.355918, mae: 4.978206, mean_q: 5.209966
 34932/100000: episode: 3570, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.899, mean reward: 0.390 [0.377, 0.446], mean action: 45.400 [24.000, 101.000], mean observation: 3.141 [-1.135, 10.276], loss: 1.239312, mae: 4.977826, mean_q: 5.211114
 34942/100000: episode: 3571, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.944, mean reward: 0.394 [0.361, 0.456], mean action: 43.800 [18.000, 89.000], mean observation: 3.162 [-1.207, 10.358], loss: 1.342038, mae: 4.978276, mean_q: 5.209841
 34952/100000: episode: 3572, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.899, mean reward: 0.390 [0.335, 0.501], mean action: 52.800 [24.000, 98.000], mean observation: 3.158 [-1.388, 10.539], loss: 0.633395, mae: 4.975286, mean_q: 5.207246
 34962/100000: episode: 3573, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.179, mean reward: 0.418 [0.373, 0.504], mean action: 35.800 [9.000, 99.000], mean observation: 3.152 [-1.023, 10.393], loss: 1.059947, mae: 4.977121, mean_q: 5.207810
 34967/100000: episode: 3574, duration: 0.094s, episode steps: 5, steps per second: 53, episode reward: 12.002, mean reward: 2.400 [0.483, 10.000], mean action: 60.000 [15.000, 88.000], mean observation: 3.166 [-1.973, 10.324], loss: 1.365277, mae: 4.978642, mean_q: 5.207081
 34977/100000: episode: 3575, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.453, mean reward: 0.445 [0.424, 0.572], mean action: 37.600 [24.000, 74.000], mean observation: 3.147 [-1.678, 10.267], loss: 1.196878, mae: 4.978198, mean_q: 5.205319
 34987/100000: episode: 3576, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.980, mean reward: 0.398 [0.356, 0.494], mean action: 40.400 [24.000, 88.000], mean observation: 3.156 [-1.448, 10.258], loss: 1.267571, mae: 4.978564, mean_q: 5.205326
 34997/100000: episode: 3577, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 3.711, mean reward: 0.371 [0.288, 0.425], mean action: 29.900 [18.000, 66.000], mean observation: 3.151 [-1.717, 10.368], loss: 1.011693, mae: 4.977741, mean_q: 5.204976
 35007/100000: episode: 3578, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.019, mean reward: 0.402 [0.392, 0.489], mean action: 42.300 [24.000, 90.000], mean observation: 3.165 [-1.285, 10.191], loss: 1.398100, mae: 4.979270, mean_q: 5.206059
 35017/100000: episode: 3579, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.888, mean reward: 0.389 [0.362, 0.470], mean action: 36.900 [24.000, 68.000], mean observation: 3.160 [-1.656, 10.341], loss: 1.116820, mae: 4.978181, mean_q: 5.206364
 35027/100000: episode: 3580, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.235, mean reward: 0.424 [0.369, 0.495], mean action: 41.200 [15.000, 100.000], mean observation: 3.157 [-1.502, 10.277], loss: 1.290914, mae: 4.978929, mean_q: 5.207113
 35037/100000: episode: 3581, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.618, mean reward: 0.462 [0.395, 0.534], mean action: 49.000 [24.000, 94.000], mean observation: 3.147 [-1.446, 10.446], loss: 1.340716, mae: 4.979398, mean_q: 5.208546
 35047/100000: episode: 3582, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.430, mean reward: 0.443 [0.348, 0.525], mean action: 40.100 [17.000, 90.000], mean observation: 3.149 [-0.977, 10.315], loss: 1.083081, mae: 4.978506, mean_q: 5.210898
 35057/100000: episode: 3583, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.165, mean reward: 0.416 [0.410, 0.456], mean action: 43.000 [14.000, 78.000], mean observation: 3.153 [-1.802, 10.268], loss: 1.338771, mae: 4.979451, mean_q: 5.213062
 35067/100000: episode: 3584, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.207, mean reward: 0.421 [0.360, 0.457], mean action: 56.700 [14.000, 80.000], mean observation: 3.156 [-1.432, 10.336], loss: 1.223981, mae: 4.979018, mean_q: 5.213894
 35077/100000: episode: 3585, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.105, mean reward: 0.410 [0.369, 0.477], mean action: 40.100 [5.000, 92.000], mean observation: 3.162 [-1.448, 10.357], loss: 1.246180, mae: 4.978899, mean_q: 5.213612
 35087/100000: episode: 3586, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.017, mean reward: 0.402 [0.281, 0.487], mean action: 30.000 [0.000, 98.000], mean observation: 3.155 [-1.355, 10.339], loss: 1.172459, mae: 4.978899, mean_q: 5.211671
 35097/100000: episode: 3587, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.082, mean reward: 0.408 [0.347, 0.480], mean action: 41.800 [24.000, 97.000], mean observation: 3.153 [-1.156, 10.452], loss: 1.372174, mae: 4.979743, mean_q: 5.213418
 35107/100000: episode: 3588, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.204, mean reward: 0.420 [0.281, 0.571], mean action: 25.900 [2.000, 59.000], mean observation: 3.158 [-2.275, 10.424], loss: 1.165422, mae: 4.978998, mean_q: 5.216102
 35117/100000: episode: 3589, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.036, mean reward: 0.404 [0.306, 0.550], mean action: 38.500 [24.000, 75.000], mean observation: 3.164 [-1.025, 10.276], loss: 1.310022, mae: 4.979646, mean_q: 5.218424
 35127/100000: episode: 3590, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.773, mean reward: 0.377 [0.337, 0.478], mean action: 49.100 [24.000, 98.000], mean observation: 3.160 [-1.007, 10.313], loss: 0.939209, mae: 4.978379, mean_q: 5.222931
 35137/100000: episode: 3591, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.091, mean reward: 0.409 [0.306, 0.534], mean action: 35.600 [2.000, 101.000], mean observation: 3.149 [-1.547, 10.336], loss: 1.255194, mae: 4.979816, mean_q: 5.224414
 35147/100000: episode: 3592, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.143, mean reward: 0.414 [0.345, 0.535], mean action: 39.600 [1.000, 83.000], mean observation: 3.149 [-1.341, 10.284], loss: 1.468156, mae: 4.980727, mean_q: 5.222648
 35157/100000: episode: 3593, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.911, mean reward: 0.391 [0.317, 0.460], mean action: 38.600 [24.000, 94.000], mean observation: 3.152 [-1.720, 10.318], loss: 1.302029, mae: 4.979912, mean_q: 5.219520
 35167/100000: episode: 3594, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.777, mean reward: 0.378 [0.368, 0.441], mean action: 42.800 [24.000, 97.000], mean observation: 3.151 [-1.508, 10.244], loss: 1.364589, mae: 4.980035, mean_q: 5.218780
 35177/100000: episode: 3595, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.252, mean reward: 0.425 [0.322, 0.519], mean action: 39.700 [24.000, 91.000], mean observation: 3.169 [-1.400, 10.296], loss: 1.049327, mae: 4.978652, mean_q: 5.216496
 35187/100000: episode: 3596, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.166, mean reward: 0.417 [0.380, 0.449], mean action: 34.500 [2.000, 73.000], mean observation: 3.164 [-2.495, 10.240], loss: 1.368328, mae: 4.979976, mean_q: 5.211464
 35197/100000: episode: 3597, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.093, mean reward: 0.409 [0.339, 0.499], mean action: 46.000 [24.000, 96.000], mean observation: 3.150 [-1.196, 10.298], loss: 1.136068, mae: 4.979415, mean_q: 5.206621
 35207/100000: episode: 3598, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.181, mean reward: 0.418 [0.342, 0.548], mean action: 26.300 [19.000, 53.000], mean observation: 3.158 [-1.401, 10.316], loss: 1.137599, mae: 4.979403, mean_q: 5.203616
 35217/100000: episode: 3599, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.965, mean reward: 0.396 [0.276, 0.474], mean action: 24.800 [18.000, 40.000], mean observation: 3.155 [-1.196, 10.367], loss: 1.342784, mae: 4.980237, mean_q: 5.202541
 35227/100000: episode: 3600, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.381, mean reward: 0.438 [0.404, 0.508], mean action: 21.000 [3.000, 41.000], mean observation: 3.153 [-1.483, 10.395], loss: 1.474001, mae: 4.980350, mean_q: 5.199949
 35237/100000: episode: 3601, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.089, mean reward: 0.409 [0.360, 0.508], mean action: 33.300 [24.000, 84.000], mean observation: 3.151 [-1.493, 10.176], loss: 1.350523, mae: 4.979693, mean_q: 5.197325
 35247/100000: episode: 3602, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.813, mean reward: 0.381 [0.353, 0.428], mean action: 58.500 [22.000, 97.000], mean observation: 3.154 [-0.898, 10.325], loss: 1.109834, mae: 4.978701, mean_q: 5.198304
 35257/100000: episode: 3603, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.005, mean reward: 0.400 [0.386, 0.459], mean action: 52.700 [24.000, 85.000], mean observation: 3.174 [-1.349, 10.266], loss: 1.084016, mae: 4.978677, mean_q: 5.200059
 35267/100000: episode: 3604, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.548, mean reward: 0.455 [0.375, 0.558], mean action: 24.300 [5.000, 68.000], mean observation: 3.159 [-1.484, 10.361], loss: 1.148779, mae: 4.979066, mean_q: 5.201033
 35277/100000: episode: 3605, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.474, mean reward: 0.447 [0.319, 0.514], mean action: 40.200 [6.000, 99.000], mean observation: 3.157 [-1.516, 10.381], loss: 1.258909, mae: 4.979741, mean_q: 5.201063
 35287/100000: episode: 3606, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.762, mean reward: 0.376 [0.337, 0.468], mean action: 35.600 [17.000, 74.000], mean observation: 3.162 [-1.271, 10.267], loss: 1.134228, mae: 4.979471, mean_q: 5.200093
 35297/100000: episode: 3607, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.101, mean reward: 0.410 [0.349, 0.490], mean action: 30.400 [18.000, 90.000], mean observation: 3.147 [-1.708, 10.246], loss: 1.346458, mae: 4.980498, mean_q: 5.198712
 35307/100000: episode: 3608, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.951, mean reward: 0.395 [0.374, 0.455], mean action: 45.700 [3.000, 100.000], mean observation: 3.157 [-1.186, 10.271], loss: 1.086932, mae: 4.979553, mean_q: 5.198099
 35317/100000: episode: 3609, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.077, mean reward: 0.408 [0.323, 0.516], mean action: 39.100 [2.000, 75.000], mean observation: 3.162 [-1.605, 10.304], loss: 1.373068, mae: 4.980820, mean_q: 5.196497
 35327/100000: episode: 3610, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.774, mean reward: 0.377 [0.308, 0.454], mean action: 53.400 [16.000, 99.000], mean observation: 3.163 [-1.679, 10.238], loss: 1.421334, mae: 4.980935, mean_q: 5.192877
 35337/100000: episode: 3611, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.932, mean reward: 0.393 [0.324, 0.523], mean action: 40.200 [24.000, 94.000], mean observation: 3.157 [-1.186, 10.212], loss: 1.226222, mae: 4.980089, mean_q: 5.190144
 35347/100000: episode: 3612, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.100, mean reward: 0.410 [0.313, 0.592], mean action: 16.400 [0.000, 28.000], mean observation: 3.161 [-1.984, 10.310], loss: 1.130179, mae: 4.979326, mean_q: 5.189073
 35357/100000: episode: 3613, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.142, mean reward: 0.414 [0.389, 0.478], mean action: 40.600 [11.000, 101.000], mean observation: 3.150 [-1.377, 10.358], loss: 1.157048, mae: 4.979552, mean_q: 5.189224
 35367/100000: episode: 3614, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.188, mean reward: 0.419 [0.332, 0.535], mean action: 27.900 [11.000, 81.000], mean observation: 3.159 [-1.767, 10.454], loss: 1.097723, mae: 4.979254, mean_q: 5.189766
 35377/100000: episode: 3615, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.333, mean reward: 0.433 [0.369, 0.520], mean action: 44.600 [11.000, 95.000], mean observation: 3.157 [-1.889, 10.409], loss: 0.886796, mae: 4.978692, mean_q: 5.190236
 35387/100000: episode: 3616, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.070, mean reward: 0.407 [0.335, 0.546], mean action: 24.100 [6.000, 86.000], mean observation: 3.149 [-1.516, 10.358], loss: 1.029837, mae: 4.979674, mean_q: 5.188776
 35397/100000: episode: 3617, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.074, mean reward: 0.407 [0.377, 0.482], mean action: 54.900 [24.000, 97.000], mean observation: 3.160 [-1.041, 10.278], loss: 1.637149, mae: 4.982258, mean_q: 5.188363
 35407/100000: episode: 3618, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.095, mean reward: 0.410 [0.384, 0.449], mean action: 42.300 [24.000, 92.000], mean observation: 3.155 [-1.442, 10.399], loss: 1.287595, mae: 4.980499, mean_q: 5.189000
 35417/100000: episode: 3619, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.027, mean reward: 0.403 [0.349, 0.525], mean action: 35.900 [2.000, 77.000], mean observation: 3.164 [-1.460, 10.281], loss: 1.192834, mae: 4.980259, mean_q: 5.188942
 35427/100000: episode: 3620, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.337, mean reward: 0.434 [0.391, 0.545], mean action: 43.300 [24.000, 93.000], mean observation: 3.169 [-1.313, 10.423], loss: 1.765778, mae: 4.981920, mean_q: 5.186238
 35437/100000: episode: 3621, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.162, mean reward: 0.416 [0.342, 0.460], mean action: 35.500 [12.000, 89.000], mean observation: 3.159 [-2.164, 10.335], loss: 1.146318, mae: 4.979211, mean_q: 5.184832
 35447/100000: episode: 3622, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.277, mean reward: 0.428 [0.360, 0.525], mean action: 34.900 [2.000, 88.000], mean observation: 3.156 [-2.097, 10.446], loss: 1.345693, mae: 4.980016, mean_q: 5.186626
 35457/100000: episode: 3623, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.811, mean reward: 0.381 [0.317, 0.469], mean action: 46.300 [24.000, 93.000], mean observation: 3.147 [-1.007, 10.312], loss: 1.351718, mae: 4.979800, mean_q: 5.187073
 35467/100000: episode: 3624, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.323, mean reward: 0.432 [0.373, 0.545], mean action: 35.600 [4.000, 91.000], mean observation: 3.162 [-1.563, 10.361], loss: 1.281866, mae: 4.979208, mean_q: 5.181626
 35477/100000: episode: 3625, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.351, mean reward: 0.435 [0.334, 0.463], mean action: 54.800 [17.000, 93.000], mean observation: 3.161 [-1.377, 10.351], loss: 1.289178, mae: 4.979056, mean_q: 5.178286
 35487/100000: episode: 3626, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.725, mean reward: 0.373 [0.312, 0.553], mean action: 35.600 [5.000, 87.000], mean observation: 3.148 [-1.329, 10.364], loss: 1.389221, mae: 4.979464, mean_q: 5.177886
 35497/100000: episode: 3627, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.322, mean reward: 0.432 [0.344, 0.528], mean action: 41.600 [7.000, 97.000], mean observation: 3.163 [-1.472, 10.356], loss: 1.043267, mae: 4.978006, mean_q: 5.177542
 35507/100000: episode: 3628, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.239, mean reward: 0.424 [0.297, 0.543], mean action: 48.500 [16.000, 99.000], mean observation: 3.159 [-1.065, 10.232], loss: 1.378074, mae: 4.979359, mean_q: 5.175771
 35508/100000: episode: 3629, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 39.000 [39.000, 39.000], mean observation: 3.161 [-0.396, 10.490], loss: 1.464113, mae: 4.979186, mean_q: 5.173554
 35518/100000: episode: 3630, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.043, mean reward: 0.404 [0.344, 0.565], mean action: 42.700 [0.000, 96.000], mean observation: 3.155 [-1.029, 10.385], loss: 1.466743, mae: 4.979545, mean_q: 5.172622
 35528/100000: episode: 3631, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.106, mean reward: 0.411 [0.329, 0.437], mean action: 65.600 [16.000, 101.000], mean observation: 3.153 [-1.278, 10.248], loss: 1.152943, mae: 4.978064, mean_q: 5.173956
 35538/100000: episode: 3632, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.748, mean reward: 0.375 [0.325, 0.433], mean action: 40.300 [2.000, 95.000], mean observation: 3.153 [-1.089, 10.286], loss: 1.011816, mae: 4.977690, mean_q: 5.175688
 35548/100000: episode: 3633, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.144, mean reward: 0.414 [0.305, 0.509], mean action: 44.100 [20.000, 95.000], mean observation: 3.149 [-1.512, 10.446], loss: 1.296291, mae: 4.978787, mean_q: 5.177220
 35558/100000: episode: 3634, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.885, mean reward: 0.388 [0.349, 0.485], mean action: 45.000 [35.000, 89.000], mean observation: 3.157 [-1.681, 10.437], loss: 1.007457, mae: 4.977876, mean_q: 5.179585
 35568/100000: episode: 3635, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.951, mean reward: 0.395 [0.262, 0.474], mean action: 44.100 [7.000, 101.000], mean observation: 3.150 [-1.022, 10.397], loss: 1.340843, mae: 4.978974, mean_q: 5.179312
 35578/100000: episode: 3636, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.121, mean reward: 0.412 [0.317, 0.511], mean action: 49.100 [2.000, 101.000], mean observation: 3.153 [-1.151, 10.195], loss: 1.054956, mae: 4.977952, mean_q: 5.175205
 35588/100000: episode: 3637, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.395, mean reward: 0.439 [0.363, 0.565], mean action: 52.100 [14.000, 101.000], mean observation: 3.168 [-1.369, 10.383], loss: 1.349341, mae: 4.979151, mean_q: 5.171744
 35598/100000: episode: 3638, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.010, mean reward: 0.401 [0.396, 0.447], mean action: 58.500 [39.000, 95.000], mean observation: 3.140 [-0.916, 10.181], loss: 1.044770, mae: 4.977715, mean_q: 5.169661
 35608/100000: episode: 3639, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.793, mean reward: 0.479 [0.297, 0.559], mean action: 38.000 [7.000, 80.000], mean observation: 3.146 [-2.169, 10.383], loss: 1.053130, mae: 4.977975, mean_q: 5.167946
 35618/100000: episode: 3640, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.931, mean reward: 0.393 [0.351, 0.461], mean action: 39.300 [33.000, 67.000], mean observation: 3.161 [-0.984, 10.348], loss: 1.437837, mae: 4.979831, mean_q: 5.169111
 35628/100000: episode: 3641, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.996, mean reward: 0.400 [0.345, 0.454], mean action: 39.800 [30.000, 69.000], mean observation: 3.150 [-1.514, 10.187], loss: 1.102444, mae: 4.978419, mean_q: 5.170312
 35638/100000: episode: 3642, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.232, mean reward: 0.423 [0.382, 0.477], mean action: 36.400 [2.000, 84.000], mean observation: 3.154 [-1.383, 10.433], loss: 1.395637, mae: 4.979619, mean_q: 5.169963
 35648/100000: episode: 3643, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.356, mean reward: 0.436 [0.344, 0.514], mean action: 43.600 [4.000, 97.000], mean observation: 3.160 [-1.308, 10.293], loss: 1.140762, mae: 4.978504, mean_q: 5.168841
 35658/100000: episode: 3644, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.434, mean reward: 0.443 [0.387, 0.578], mean action: 39.400 [38.000, 44.000], mean observation: 3.162 [-1.022, 10.279], loss: 0.885707, mae: 4.977536, mean_q: 5.171018
 35668/100000: episode: 3645, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.221, mean reward: 0.422 [0.335, 0.504], mean action: 38.500 [4.000, 90.000], mean observation: 3.147 [-1.281, 10.342], loss: 1.259386, mae: 4.979101, mean_q: 5.172585
 35678/100000: episode: 3646, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.243, mean reward: 0.424 [0.329, 0.543], mean action: 49.000 [35.000, 89.000], mean observation: 3.174 [-1.550, 10.272], loss: 1.166498, mae: 4.978836, mean_q: 5.171985
 35688/100000: episode: 3647, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.016, mean reward: 0.402 [0.333, 0.461], mean action: 46.400 [15.000, 94.000], mean observation: 3.157 [-1.431, 10.404], loss: 1.231013, mae: 4.978705, mean_q: 5.173582
 35698/100000: episode: 3648, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.003, mean reward: 0.400 [0.387, 0.428], mean action: 67.400 [39.000, 101.000], mean observation: 3.152 [-1.544, 10.370], loss: 1.400207, mae: 4.979365, mean_q: 5.176347
 35708/100000: episode: 3649, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.051, mean reward: 0.405 [0.358, 0.564], mean action: 49.700 [19.000, 88.000], mean observation: 3.169 [-1.201, 10.357], loss: 1.244113, mae: 4.978134, mean_q: 5.178994
 35718/100000: episode: 3650, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.008, mean reward: 0.401 [0.355, 0.466], mean action: 51.500 [10.000, 98.000], mean observation: 3.150 [-1.272, 10.433], loss: 1.533301, mae: 4.979146, mean_q: 5.180921
 35728/100000: episode: 3651, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.004, mean reward: 0.400 [0.376, 0.467], mean action: 48.100 [27.000, 92.000], mean observation: 3.148 [-1.403, 10.385], loss: 1.480992, mae: 4.978935, mean_q: 5.182353
 35738/100000: episode: 3652, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.540, mean reward: 0.454 [0.435, 0.534], mean action: 42.000 [19.000, 93.000], mean observation: 3.158 [-1.771, 10.456], loss: 1.359337, mae: 4.977893, mean_q: 5.182716
 35748/100000: episode: 3653, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.092, mean reward: 0.409 [0.358, 0.461], mean action: 40.800 [17.000, 82.000], mean observation: 3.156 [-1.533, 10.420], loss: 1.100864, mae: 4.977048, mean_q: 5.181499
 35758/100000: episode: 3654, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.446, mean reward: 0.445 [0.328, 0.573], mean action: 36.500 [9.000, 61.000], mean observation: 3.143 [-1.812, 10.197], loss: 1.128766, mae: 4.977368, mean_q: 5.181282
 35768/100000: episode: 3655, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.340, mean reward: 0.434 [0.337, 0.536], mean action: 43.300 [11.000, 78.000], mean observation: 3.160 [-2.139, 10.382], loss: 1.064605, mae: 4.977051, mean_q: 5.178816
 35778/100000: episode: 3656, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.622, mean reward: 0.462 [0.456, 0.507], mean action: 48.700 [39.000, 82.000], mean observation: 3.150 [-1.682, 10.501], loss: 1.373417, mae: 4.978692, mean_q: 5.176382
 35788/100000: episode: 3657, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.056, mean reward: 0.406 [0.341, 0.490], mean action: 41.100 [4.000, 101.000], mean observation: 3.149 [-1.560, 10.374], loss: 1.025079, mae: 4.977320, mean_q: 5.173812
 35798/100000: episode: 3658, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.377, mean reward: 0.438 [0.392, 0.486], mean action: 33.300 [3.000, 67.000], mean observation: 3.152 [-2.464, 10.282], loss: 1.075800, mae: 4.977945, mean_q: 5.177652
 35808/100000: episode: 3659, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.491, mean reward: 0.449 [0.419, 0.526], mean action: 48.400 [39.000, 90.000], mean observation: 3.142 [-1.059, 10.295], loss: 1.093094, mae: 4.978489, mean_q: 5.181229
 35818/100000: episode: 3660, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.012, mean reward: 0.401 [0.317, 0.533], mean action: 46.500 [28.000, 86.000], mean observation: 3.164 [-1.439, 10.421], loss: 1.244270, mae: 4.979396, mean_q: 5.183956
 35828/100000: episode: 3661, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.674, mean reward: 0.367 [0.307, 0.533], mean action: 51.600 [25.000, 101.000], mean observation: 3.143 [-1.420, 10.264], loss: 0.818024, mae: 4.977943, mean_q: 5.186124
 35838/100000: episode: 3662, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.232, mean reward: 0.423 [0.387, 0.490], mean action: 66.200 [39.000, 100.000], mean observation: 3.169 [-0.943, 10.293], loss: 1.115864, mae: 4.979434, mean_q: 5.188304
 35848/100000: episode: 3663, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.624, mean reward: 0.362 [0.305, 0.502], mean action: 52.300 [23.000, 99.000], mean observation: 3.145 [-1.570, 10.327], loss: 1.123697, mae: 4.979802, mean_q: 5.190262
 35858/100000: episode: 3664, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.996, mean reward: 0.400 [0.349, 0.553], mean action: 40.400 [3.000, 78.000], mean observation: 3.157 [-1.510, 10.311], loss: 1.113596, mae: 4.980210, mean_q: 5.191915
 35868/100000: episode: 3665, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.948, mean reward: 0.395 [0.361, 0.501], mean action: 49.400 [39.000, 77.000], mean observation: 3.163 [-1.135, 10.436], loss: 1.287221, mae: 4.980861, mean_q: 5.193837
 35878/100000: episode: 3666, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 4.239, mean reward: 0.424 [0.316, 0.566], mean action: 22.200 [0.000, 39.000], mean observation: 3.160 [-1.567, 10.280], loss: 0.854392, mae: 4.979204, mean_q: 5.193962
 35888/100000: episode: 3667, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.726, mean reward: 0.373 [0.344, 0.457], mean action: 43.200 [39.000, 73.000], mean observation: 3.152 [-1.560, 10.191], loss: 1.322877, mae: 4.981509, mean_q: 5.189746
 35898/100000: episode: 3668, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 5.282, mean reward: 0.528 [0.528, 0.528], mean action: 37.500 [29.000, 45.000], mean observation: 3.163 [-1.409, 10.426], loss: 1.373968, mae: 4.981739, mean_q: 5.187556
 35908/100000: episode: 3669, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.575, mean reward: 0.457 [0.457, 0.465], mean action: 47.200 [16.000, 92.000], mean observation: 3.160 [-1.459, 10.233], loss: 1.467962, mae: 4.982136, mean_q: 5.188573
 35918/100000: episode: 3670, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.326, mean reward: 0.433 [0.331, 0.458], mean action: 44.400 [1.000, 98.000], mean observation: 3.150 [-2.511, 10.328], loss: 1.225396, mae: 4.980844, mean_q: 5.187926
 35928/100000: episode: 3671, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.831, mean reward: 0.383 [0.316, 0.565], mean action: 36.500 [12.000, 61.000], mean observation: 3.151 [-1.810, 10.254], loss: 0.988239, mae: 4.980110, mean_q: 5.186096
 35938/100000: episode: 3672, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.394, mean reward: 0.439 [0.405, 0.512], mean action: 40.600 [11.000, 85.000], mean observation: 3.150 [-1.509, 10.384], loss: 0.977270, mae: 4.980079, mean_q: 5.187620
 35948/100000: episode: 3673, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.620, mean reward: 0.462 [0.424, 0.477], mean action: 47.600 [16.000, 87.000], mean observation: 3.152 [-1.485, 10.248], loss: 1.079699, mae: 4.980596, mean_q: 5.189899
 35958/100000: episode: 3674, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.399, mean reward: 0.440 [0.325, 0.547], mean action: 39.000 [3.000, 84.000], mean observation: 3.148 [-1.586, 10.284], loss: 1.070717, mae: 4.981126, mean_q: 5.193279
 35968/100000: episode: 3675, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.894, mean reward: 0.389 [0.295, 0.471], mean action: 39.000 [39.000, 39.000], mean observation: 3.162 [-1.353, 10.308], loss: 1.101364, mae: 4.981335, mean_q: 5.196137
 35978/100000: episode: 3676, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.472, mean reward: 0.447 [0.368, 0.497], mean action: 50.400 [4.000, 99.000], mean observation: 3.151 [-1.925, 10.284], loss: 1.485813, mae: 4.982995, mean_q: 5.197239
 35988/100000: episode: 3677, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.935, mean reward: 0.494 [0.494, 0.494], mean action: 52.400 [17.000, 101.000], mean observation: 3.162 [-1.858, 10.216], loss: 1.383242, mae: 4.982893, mean_q: 5.195376
 35998/100000: episode: 3678, duration: 0.153s, episode steps: 10, steps per second: 66, episode reward: 4.158, mean reward: 0.416 [0.373, 0.453], mean action: 51.200 [39.000, 83.000], mean observation: 3.165 [-1.142, 10.452], loss: 1.393568, mae: 4.982705, mean_q: 5.192162
 36008/100000: episode: 3679, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.050, mean reward: 0.405 [0.357, 0.489], mean action: 47.400 [30.000, 82.000], mean observation: 3.136 [-1.707, 10.280], loss: 1.498388, mae: 4.982829, mean_q: 5.185545
 36018/100000: episode: 3680, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.902, mean reward: 0.490 [0.480, 0.522], mean action: 37.900 [2.000, 53.000], mean observation: 3.158 [-1.788, 10.382], loss: 0.993896, mae: 4.981066, mean_q: 5.182111
 36028/100000: episode: 3681, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.697, mean reward: 0.370 [0.301, 0.426], mean action: 45.400 [6.000, 93.000], mean observation: 3.162 [-1.101, 10.462], loss: 1.226200, mae: 4.982051, mean_q: 5.178761
 36038/100000: episode: 3682, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.114, mean reward: 0.411 [0.333, 0.537], mean action: 45.700 [18.000, 96.000], mean observation: 3.160 [-1.492, 10.322], loss: 1.462864, mae: 4.983051, mean_q: 5.178491
 36048/100000: episode: 3683, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.870, mean reward: 0.387 [0.327, 0.461], mean action: 41.800 [3.000, 97.000], mean observation: 3.161 [-1.326, 10.204], loss: 1.049475, mae: 4.981216, mean_q: 5.178540
 36058/100000: episode: 3684, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.498, mean reward: 0.450 [0.386, 0.470], mean action: 52.700 [9.000, 80.000], mean observation: 3.156 [-1.340, 10.316], loss: 1.166075, mae: 4.981648, mean_q: 5.179421
 36068/100000: episode: 3685, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.992, mean reward: 0.399 [0.381, 0.455], mean action: 48.000 [23.000, 88.000], mean observation: 3.161 [-1.040, 10.301], loss: 1.529315, mae: 4.983222, mean_q: 5.180497
 36078/100000: episode: 3686, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.859, mean reward: 0.386 [0.313, 0.484], mean action: 48.500 [34.000, 93.000], mean observation: 3.170 [-1.497, 10.314], loss: 1.163968, mae: 4.981740, mean_q: 5.177468
 36088/100000: episode: 3687, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.033, mean reward: 0.403 [0.359, 0.493], mean action: 36.400 [12.000, 62.000], mean observation: 3.161 [-1.854, 10.338], loss: 1.250092, mae: 4.981925, mean_q: 5.174695
 36098/100000: episode: 3688, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.307, mean reward: 0.431 [0.365, 0.501], mean action: 41.200 [6.000, 86.000], mean observation: 3.159 [-1.310, 10.396], loss: 1.245380, mae: 4.981927, mean_q: 5.174422
 36108/100000: episode: 3689, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.930, mean reward: 0.393 [0.298, 0.416], mean action: 34.100 [1.000, 61.000], mean observation: 3.153 [-2.285, 10.413], loss: 1.001550, mae: 4.981064, mean_q: 5.175711
 36117/100000: episode: 3690, duration: 0.177s, episode steps: 9, steps per second: 51, episode reward: 13.238, mean reward: 1.471 [0.328, 10.000], mean action: 41.333 [39.000, 60.000], mean observation: 3.150 [-1.121, 10.498], loss: 1.040990, mae: 4.981348, mean_q: 5.177893
 36127/100000: episode: 3691, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.496, mean reward: 0.450 [0.365, 0.554], mean action: 41.900 [3.000, 72.000], mean observation: 3.162 [-1.148, 10.335], loss: 1.538741, mae: 4.983291, mean_q: 5.178662
 36137/100000: episode: 3692, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.689, mean reward: 0.369 [0.324, 0.447], mean action: 41.100 [11.000, 83.000], mean observation: 3.146 [-1.736, 10.472], loss: 1.284767, mae: 4.982222, mean_q: 5.176843
 36147/100000: episode: 3693, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.648, mean reward: 0.465 [0.461, 0.483], mean action: 36.100 [15.000, 76.000], mean observation: 3.146 [-1.329, 10.364], loss: 1.075827, mae: 4.981407, mean_q: 5.178166
 36157/100000: episode: 3694, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.514, mean reward: 0.451 [0.347, 0.540], mean action: 32.400 [13.000, 73.000], mean observation: 3.157 [-2.747, 10.305], loss: 1.300850, mae: 4.982226, mean_q: 5.177207
 36167/100000: episode: 3695, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.050, mean reward: 0.405 [0.331, 0.505], mean action: 47.300 [21.000, 95.000], mean observation: 3.141 [-1.174, 10.305], loss: 1.213880, mae: 4.981801, mean_q: 5.178270
 36177/100000: episode: 3696, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 5.345, mean reward: 0.534 [0.529, 0.582], mean action: 49.000 [39.000, 80.000], mean observation: 3.174 [-1.858, 10.391], loss: 1.075855, mae: 4.981049, mean_q: 5.177194
 36187/100000: episode: 3697, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.696, mean reward: 0.370 [0.324, 0.392], mean action: 49.700 [17.000, 101.000], mean observation: 3.155 [-1.208, 10.311], loss: 0.996803, mae: 4.980855, mean_q: 5.176923
 36197/100000: episode: 3698, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.700, mean reward: 0.370 [0.294, 0.493], mean action: 38.200 [14.000, 73.000], mean observation: 3.148 [-1.541, 10.268], loss: 1.194545, mae: 4.982008, mean_q: 5.176083
 36207/100000: episode: 3699, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.665, mean reward: 0.466 [0.466, 0.466], mean action: 46.200 [25.000, 97.000], mean observation: 3.164 [-1.022, 10.427], loss: 0.987130, mae: 4.981359, mean_q: 5.175065
 36217/100000: episode: 3700, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.018, mean reward: 0.402 [0.330, 0.469], mean action: 52.200 [25.000, 92.000], mean observation: 3.138 [-1.172, 10.314], loss: 1.226599, mae: 4.982277, mean_q: 5.177362
 36227/100000: episode: 3701, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.116, mean reward: 0.412 [0.352, 0.479], mean action: 39.500 [25.000, 72.000], mean observation: 3.167 [-1.232, 10.278], loss: 1.421194, mae: 4.983291, mean_q: 5.178944
 36237/100000: episode: 3702, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.310, mean reward: 0.431 [0.328, 0.546], mean action: 38.300 [16.000, 78.000], mean observation: 3.156 [-1.492, 10.325], loss: 1.231266, mae: 4.982121, mean_q: 5.181009
 36247/100000: episode: 3703, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.882, mean reward: 0.388 [0.315, 0.543], mean action: 23.900 [7.000, 41.000], mean observation: 3.148 [-1.533, 10.252], loss: 1.163340, mae: 4.981777, mean_q: 5.182175
 36254/100000: episode: 3704, duration: 0.153s, episode steps: 7, steps per second: 46, episode reward: 12.604, mean reward: 1.801 [0.349, 10.000], mean action: 21.571 [8.000, 28.000], mean observation: 3.155 [-0.907, 10.353], loss: 1.174394, mae: 4.981781, mean_q: 5.182584
 36264/100000: episode: 3705, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.253, mean reward: 0.425 [0.333, 0.498], mean action: 42.900 [25.000, 98.000], mean observation: 3.148 [-0.861, 10.487], loss: 1.431033, mae: 4.982535, mean_q: 5.183501
 36274/100000: episode: 3706, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.622, mean reward: 0.462 [0.334, 0.582], mean action: 39.100 [5.000, 87.000], mean observation: 3.165 [-1.683, 10.328], loss: 1.225583, mae: 4.981707, mean_q: 5.185262
 36284/100000: episode: 3707, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.463, mean reward: 0.446 [0.346, 0.465], mean action: 34.400 [2.000, 87.000], mean observation: 3.161 [-1.269, 10.234], loss: 1.103485, mae: 4.981191, mean_q: 5.186935
 36294/100000: episode: 3708, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.314, mean reward: 0.431 [0.354, 0.542], mean action: 43.600 [25.000, 89.000], mean observation: 3.160 [-1.518, 10.355], loss: 1.331237, mae: 4.982146, mean_q: 5.184766
 36304/100000: episode: 3709, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.207, mean reward: 0.421 [0.381, 0.492], mean action: 38.900 [25.000, 84.000], mean observation: 3.168 [-1.428, 10.484], loss: 1.218077, mae: 4.981902, mean_q: 5.185519
 36314/100000: episode: 3710, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.991, mean reward: 0.399 [0.331, 0.527], mean action: 50.800 [25.000, 92.000], mean observation: 3.155 [-1.321, 10.181], loss: 1.769151, mae: 4.984084, mean_q: 5.186373
 36324/100000: episode: 3711, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.839, mean reward: 0.384 [0.308, 0.506], mean action: 38.300 [25.000, 89.000], mean observation: 3.141 [-1.447, 10.411], loss: 1.227581, mae: 4.981544, mean_q: 5.186710
 36334/100000: episode: 3712, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.320, mean reward: 0.432 [0.389, 0.488], mean action: 53.700 [17.000, 93.000], mean observation: 3.161 [-1.280, 10.497], loss: 0.982930, mae: 4.980631, mean_q: 5.187512
 36344/100000: episode: 3713, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.449, mean reward: 0.445 [0.441, 0.480], mean action: 45.100 [11.000, 94.000], mean observation: 3.156 [-1.601, 10.267], loss: 0.978901, mae: 4.980674, mean_q: 5.189599
 36354/100000: episode: 3714, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.160, mean reward: 0.416 [0.380, 0.513], mean action: 41.500 [16.000, 94.000], mean observation: 3.157 [-1.336, 10.515], loss: 1.485943, mae: 4.982641, mean_q: 5.191226
 36364/100000: episode: 3715, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.703, mean reward: 0.370 [0.310, 0.425], mean action: 29.800 [7.000, 62.000], mean observation: 3.157 [-1.156, 10.389], loss: 1.255879, mae: 4.981631, mean_q: 5.192546
 36374/100000: episode: 3716, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 5.297, mean reward: 0.530 [0.354, 0.674], mean action: 37.200 [2.000, 94.000], mean observation: 3.144 [-1.602, 10.363], loss: 1.423213, mae: 4.982064, mean_q: 5.192367
 36384/100000: episode: 3717, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.126, mean reward: 0.413 [0.378, 0.498], mean action: 33.900 [2.000, 80.000], mean observation: 3.150 [-1.460, 10.275], loss: 1.230974, mae: 4.981360, mean_q: 5.188209
 36394/100000: episode: 3718, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.280, mean reward: 0.428 [0.357, 0.525], mean action: 36.200 [25.000, 86.000], mean observation: 3.166 [-1.225, 10.315], loss: 1.094672, mae: 4.980797, mean_q: 5.187246
 36404/100000: episode: 3719, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.232, mean reward: 0.423 [0.361, 0.509], mean action: 34.200 [2.000, 90.000], mean observation: 3.161 [-1.292, 10.275], loss: 0.979733, mae: 4.980653, mean_q: 5.187402
 36414/100000: episode: 3720, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 3.710, mean reward: 0.371 [0.303, 0.462], mean action: 33.000 [9.000, 92.000], mean observation: 3.148 [-1.326, 10.316], loss: 1.540478, mae: 4.983066, mean_q: 5.188378
 36424/100000: episode: 3721, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.514, mean reward: 0.451 [0.331, 0.614], mean action: 48.000 [25.000, 90.000], mean observation: 3.163 [-1.584, 10.353], loss: 1.099710, mae: 4.981430, mean_q: 5.189288
 36434/100000: episode: 3722, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.264, mean reward: 0.426 [0.368, 0.486], mean action: 31.600 [25.000, 80.000], mean observation: 3.170 [-1.593, 10.348], loss: 1.415848, mae: 4.982602, mean_q: 5.189531
 36444/100000: episode: 3723, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.059, mean reward: 0.406 [0.325, 0.464], mean action: 37.700 [0.000, 66.000], mean observation: 3.160 [-1.284, 10.259], loss: 1.319910, mae: 4.982248, mean_q: 5.187636
 36454/100000: episode: 3724, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.191, mean reward: 0.419 [0.341, 0.565], mean action: 34.900 [5.000, 96.000], mean observation: 3.143 [-1.321, 10.177], loss: 1.110843, mae: 4.981347, mean_q: 5.187980
 36464/100000: episode: 3725, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.908, mean reward: 0.391 [0.295, 0.468], mean action: 38.600 [11.000, 90.000], mean observation: 3.158 [-1.356, 10.347], loss: 1.062033, mae: 4.981318, mean_q: 5.189203
 36474/100000: episode: 3726, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.080, mean reward: 0.408 [0.304, 0.489], mean action: 30.300 [0.000, 77.000], mean observation: 3.148 [-1.949, 10.421], loss: 1.223245, mae: 4.982076, mean_q: 5.190969
 36484/100000: episode: 3727, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.811, mean reward: 0.381 [0.287, 0.533], mean action: 44.000 [22.000, 84.000], mean observation: 3.164 [-1.428, 10.420], loss: 0.955247, mae: 4.981053, mean_q: 5.192188
 36494/100000: episode: 3728, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.834, mean reward: 0.383 [0.318, 0.472], mean action: 40.500 [2.000, 97.000], mean observation: 3.162 [-1.012, 10.498], loss: 1.185320, mae: 4.982148, mean_q: 5.193277
 36504/100000: episode: 3729, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.539, mean reward: 0.454 [0.383, 0.575], mean action: 44.200 [25.000, 101.000], mean observation: 3.164 [-0.937, 10.221], loss: 1.053514, mae: 4.981759, mean_q: 5.194453
 36514/100000: episode: 3730, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.052, mean reward: 0.405 [0.341, 0.527], mean action: 41.600 [3.000, 91.000], mean observation: 3.159 [-1.960, 10.309], loss: 1.271418, mae: 4.982606, mean_q: 5.190291
 36524/100000: episode: 3731, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.117, mean reward: 0.412 [0.307, 0.486], mean action: 32.400 [1.000, 87.000], mean observation: 3.162 [-1.565, 10.250], loss: 1.282672, mae: 4.982629, mean_q: 5.189117
 36534/100000: episode: 3732, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.342, mean reward: 0.434 [0.392, 0.512], mean action: 35.800 [25.000, 89.000], mean observation: 3.164 [-1.123, 10.336], loss: 1.157479, mae: 4.982272, mean_q: 5.186359
 36544/100000: episode: 3733, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.270, mean reward: 0.427 [0.389, 0.583], mean action: 38.200 [14.000, 91.000], mean observation: 3.157 [-1.104, 10.298], loss: 1.139345, mae: 4.982297, mean_q: 5.182753
 36554/100000: episode: 3734, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.792, mean reward: 0.379 [0.339, 0.459], mean action: 41.800 [5.000, 72.000], mean observation: 3.146 [-1.279, 10.236], loss: 1.223153, mae: 4.982920, mean_q: 5.182368
 36564/100000: episode: 3735, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.220, mean reward: 0.422 [0.333, 0.499], mean action: 25.300 [25.000, 28.000], mean observation: 3.156 [-1.350, 10.258], loss: 1.762649, mae: 4.985173, mean_q: 5.183906
 36574/100000: episode: 3736, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.533, mean reward: 0.453 [0.374, 0.479], mean action: 46.300 [7.000, 99.000], mean observation: 3.154 [-1.654, 10.250], loss: 1.496049, mae: 4.983599, mean_q: 5.185787
 36584/100000: episode: 3737, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.989, mean reward: 0.399 [0.367, 0.444], mean action: 41.200 [25.000, 94.000], mean observation: 3.157 [-1.404, 10.478], loss: 1.200303, mae: 4.982589, mean_q: 5.187441
 36594/100000: episode: 3738, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.480, mean reward: 0.348 [0.300, 0.391], mean action: 47.300 [9.000, 97.000], mean observation: 3.150 [-1.381, 10.316], loss: 1.031854, mae: 4.981718, mean_q: 5.190206
 36604/100000: episode: 3739, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.028, mean reward: 0.403 [0.372, 0.473], mean action: 39.600 [25.000, 89.000], mean observation: 3.153 [-1.787, 10.326], loss: 1.237119, mae: 4.982606, mean_q: 5.191873
 36614/100000: episode: 3740, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.848, mean reward: 0.385 [0.341, 0.453], mean action: 41.500 [10.000, 89.000], mean observation: 3.156 [-0.916, 10.339], loss: 1.195184, mae: 4.982706, mean_q: 5.183648
 36624/100000: episode: 3741, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.251, mean reward: 0.425 [0.329, 0.495], mean action: 35.000 [6.000, 92.000], mean observation: 3.157 [-1.164, 10.468], loss: 1.369396, mae: 4.983473, mean_q: 5.175786
 36634/100000: episode: 3742, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.321, mean reward: 0.432 [0.375, 0.493], mean action: 57.400 [16.000, 88.000], mean observation: 3.144 [-1.374, 10.297], loss: 1.174893, mae: 4.982596, mean_q: 5.173144
 36644/100000: episode: 3743, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 3.860, mean reward: 0.386 [0.318, 0.552], mean action: 60.800 [53.000, 100.000], mean observation: 3.141 [-0.992, 10.310], loss: 1.184291, mae: 4.982441, mean_q: 5.171987
 36654/100000: episode: 3744, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.847, mean reward: 0.485 [0.485, 0.485], mean action: 55.800 [27.000, 82.000], mean observation: 3.170 [-1.267, 10.290], loss: 0.943984, mae: 4.981587, mean_q: 5.169724
 36664/100000: episode: 3745, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.861, mean reward: 0.386 [0.309, 0.477], mean action: 45.500 [9.000, 87.000], mean observation: 3.143 [-1.117, 10.447], loss: 1.424515, mae: 4.983762, mean_q: 5.169444
 36674/100000: episode: 3746, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.167, mean reward: 0.417 [0.363, 0.548], mean action: 54.700 [12.000, 88.000], mean observation: 3.165 [-1.197, 10.310], loss: 1.078288, mae: 4.982351, mean_q: 5.169722
 36684/100000: episode: 3747, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.067, mean reward: 0.407 [0.357, 0.461], mean action: 44.700 [10.000, 78.000], mean observation: 3.164 [-1.485, 10.179], loss: 0.959189, mae: 4.981925, mean_q: 5.169961
 36694/100000: episode: 3748, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.044, mean reward: 0.404 [0.380, 0.471], mean action: 49.100 [2.000, 96.000], mean observation: 3.161 [-1.550, 10.342], loss: 1.002869, mae: 4.982469, mean_q: 5.171014
 36704/100000: episode: 3749, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.792, mean reward: 0.379 [0.323, 0.458], mean action: 29.200 [12.000, 53.000], mean observation: 3.156 [-1.481, 10.284], loss: 1.509069, mae: 4.984563, mean_q: 5.172125
 36714/100000: episode: 3750, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.923, mean reward: 0.492 [0.492, 0.492], mean action: 57.000 [53.000, 79.000], mean observation: 3.161 [-1.207, 10.248], loss: 1.325110, mae: 4.983929, mean_q: 5.173329
 36724/100000: episode: 3751, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.915, mean reward: 0.492 [0.485, 0.546], mean action: 50.200 [11.000, 74.000], mean observation: 3.156 [-1.947, 10.356], loss: 1.121954, mae: 4.983405, mean_q: 5.174795
 36734/100000: episode: 3752, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.947, mean reward: 0.395 [0.358, 0.515], mean action: 53.300 [13.000, 96.000], mean observation: 3.163 [-1.686, 10.389], loss: 0.857003, mae: 4.982711, mean_q: 5.176147
 36744/100000: episode: 3753, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.079, mean reward: 0.408 [0.401, 0.454], mean action: 58.800 [25.000, 95.000], mean observation: 3.163 [-1.452, 10.390], loss: 0.886804, mae: 4.983071, mean_q: 5.177117
 36754/100000: episode: 3754, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.880, mean reward: 0.488 [0.488, 0.488], mean action: 54.400 [43.000, 65.000], mean observation: 3.126 [-1.435, 10.310], loss: 1.367622, mae: 4.985355, mean_q: 5.178720
 36764/100000: episode: 3755, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.968, mean reward: 0.397 [0.352, 0.517], mean action: 51.600 [18.000, 64.000], mean observation: 3.164 [-1.361, 10.342], loss: 1.246273, mae: 4.985153, mean_q: 5.179975
 36774/100000: episode: 3756, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.512, mean reward: 0.451 [0.343, 0.490], mean action: 55.300 [26.000, 80.000], mean observation: 3.155 [-1.911, 10.331], loss: 1.248339, mae: 4.985018, mean_q: 5.180751
 36784/100000: episode: 3757, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.976, mean reward: 0.398 [0.338, 0.501], mean action: 45.200 [15.000, 62.000], mean observation: 3.147 [-1.250, 10.279], loss: 1.275482, mae: 4.984847, mean_q: 5.181666
 36794/100000: episode: 3758, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.022, mean reward: 0.402 [0.330, 0.539], mean action: 50.700 [26.000, 77.000], mean observation: 3.152 [-1.305, 10.260], loss: 1.240058, mae: 4.984784, mean_q: 5.182314
 36804/100000: episode: 3759, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.197, mean reward: 0.420 [0.414, 0.449], mean action: 43.600 [13.000, 74.000], mean observation: 3.169 [-1.060, 10.267], loss: 1.082696, mae: 4.983975, mean_q: 5.180233
 36814/100000: episode: 3760, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.796, mean reward: 0.380 [0.329, 0.549], mean action: 61.800 [40.000, 97.000], mean observation: 3.151 [-1.050, 10.132], loss: 1.020878, mae: 4.984130, mean_q: 5.178394
 36824/100000: episode: 3761, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.684, mean reward: 0.468 [0.359, 0.481], mean action: 47.300 [4.000, 97.000], mean observation: 3.162 [-2.116, 10.390], loss: 1.297401, mae: 4.985308, mean_q: 5.175309
 36834/100000: episode: 3762, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.188, mean reward: 0.419 [0.326, 0.495], mean action: 37.400 [7.000, 83.000], mean observation: 3.167 [-1.571, 10.380], loss: 1.483823, mae: 4.986159, mean_q: 5.176489
 36844/100000: episode: 3763, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.039, mean reward: 0.404 [0.351, 0.428], mean action: 35.900 [0.000, 91.000], mean observation: 3.145 [-1.372, 10.229], loss: 1.189601, mae: 4.985151, mean_q: 5.178068
 36854/100000: episode: 3764, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.976, mean reward: 0.398 [0.342, 0.462], mean action: 41.400 [33.000, 75.000], mean observation: 3.168 [-1.831, 10.259], loss: 1.449445, mae: 4.986407, mean_q: 5.179036
 36864/100000: episode: 3765, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.553, mean reward: 0.455 [0.440, 0.506], mean action: 32.900 [19.000, 45.000], mean observation: 3.152 [-1.029, 10.336], loss: 1.037133, mae: 4.984600, mean_q: 5.179582
 36874/100000: episode: 3766, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.041, mean reward: 0.404 [0.321, 0.505], mean action: 37.400 [2.000, 92.000], mean observation: 3.153 [-1.259, 10.268], loss: 1.063094, mae: 4.984855, mean_q: 5.180321
 36884/100000: episode: 3767, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.573, mean reward: 0.357 [0.265, 0.485], mean action: 55.000 [5.000, 96.000], mean observation: 3.151 [-1.164, 10.557], loss: 1.165489, mae: 4.985318, mean_q: 5.180887
 36894/100000: episode: 3768, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.753, mean reward: 0.375 [0.309, 0.435], mean action: 58.500 [33.000, 98.000], mean observation: 3.155 [-1.926, 10.250], loss: 0.914211, mae: 4.984609, mean_q: 5.181209
 36904/100000: episode: 3769, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.095, mean reward: 0.410 [0.348, 0.554], mean action: 39.900 [33.000, 71.000], mean observation: 3.156 [-1.279, 10.483], loss: 1.469174, mae: 4.986720, mean_q: 5.180246
 36914/100000: episode: 3770, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.831, mean reward: 0.383 [0.325, 0.431], mean action: 57.600 [33.000, 92.000], mean observation: 3.149 [-2.728, 10.332], loss: 1.449154, mae: 4.986837, mean_q: 5.176959
 36924/100000: episode: 3771, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.915, mean reward: 0.392 [0.352, 0.490], mean action: 53.100 [28.000, 97.000], mean observation: 3.154 [-1.522, 10.309], loss: 1.314134, mae: 4.985962, mean_q: 5.177083
 36934/100000: episode: 3772, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.160, mean reward: 0.416 [0.396, 0.495], mean action: 63.500 [45.000, 97.000], mean observation: 3.164 [-1.786, 10.320], loss: 1.177040, mae: 4.985208, mean_q: 5.178319
 36944/100000: episode: 3773, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.547, mean reward: 0.455 [0.297, 0.489], mean action: 73.000 [42.000, 97.000], mean observation: 3.160 [-0.773, 10.372], loss: 1.769230, mae: 4.987174, mean_q: 5.179190
 36954/100000: episode: 3774, duration: 0.088s, episode steps: 10, steps per second: 114, episode reward: 4.440, mean reward: 0.444 [0.444, 0.444], mean action: 94.600 [78.000, 97.000], mean observation: 3.134 [-0.876, 10.256], loss: 1.388058, mae: 4.985112, mean_q: 5.179874
 36964/100000: episode: 3775, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.750, mean reward: 0.475 [0.468, 0.536], mean action: 72.600 [18.000, 97.000], mean observation: 3.167 [-1.721, 10.373], loss: 1.056986, mae: 4.983839, mean_q: 5.180750
 36974/100000: episode: 3776, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 3.752, mean reward: 0.375 [0.356, 0.393], mean action: 77.100 [30.000, 97.000], mean observation: 3.141 [-1.215, 10.220], loss: 1.173776, mae: 4.984156, mean_q: 5.182110
 36984/100000: episode: 3777, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.006, mean reward: 0.401 [0.355, 0.489], mean action: 77.900 [14.000, 97.000], mean observation: 3.156 [-1.928, 10.190], loss: 1.169364, mae: 4.984098, mean_q: 5.183291
 36994/100000: episode: 3778, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 4.411, mean reward: 0.441 [0.404, 0.478], mean action: 84.500 [52.000, 97.000], mean observation: 3.162 [-1.063, 10.401], loss: 0.886786, mae: 4.983573, mean_q: 5.185271
 37004/100000: episode: 3779, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.131, mean reward: 0.413 [0.382, 0.556], mean action: 60.100 [5.000, 97.000], mean observation: 3.149 [-1.341, 10.386], loss: 1.170451, mae: 4.984936, mean_q: 5.181130
 37014/100000: episode: 3780, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.253, mean reward: 0.425 [0.374, 0.496], mean action: 58.400 [35.000, 100.000], mean observation: 3.163 [-1.368, 10.296], loss: 1.345896, mae: 4.985696, mean_q: 5.180375
 37024/100000: episode: 3781, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.681, mean reward: 0.368 [0.337, 0.486], mean action: 49.600 [21.000, 87.000], mean observation: 3.139 [-1.096, 10.251], loss: 1.298892, mae: 4.985655, mean_q: 5.179919
 37034/100000: episode: 3782, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.927, mean reward: 0.393 [0.307, 0.530], mean action: 46.800 [25.000, 92.000], mean observation: 3.156 [-1.214, 10.294], loss: 1.254300, mae: 4.985167, mean_q: 5.178572
 37044/100000: episode: 3783, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.961, mean reward: 0.396 [0.318, 0.529], mean action: 36.600 [5.000, 100.000], mean observation: 3.165 [-1.158, 10.405], loss: 0.968797, mae: 4.984241, mean_q: 5.176975
 37054/100000: episode: 3784, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.793, mean reward: 0.479 [0.395, 0.600], mean action: 36.500 [25.000, 95.000], mean observation: 3.160 [-0.926, 10.413], loss: 1.085767, mae: 4.984504, mean_q: 5.173472
 37064/100000: episode: 3785, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.703, mean reward: 0.370 [0.305, 0.436], mean action: 32.400 [23.000, 65.000], mean observation: 3.150 [-1.641, 10.405], loss: 1.492797, mae: 4.986122, mean_q: 5.171465
 37074/100000: episode: 3786, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.037, mean reward: 0.404 [0.355, 0.564], mean action: 31.900 [9.000, 59.000], mean observation: 3.170 [-1.256, 10.331], loss: 1.282028, mae: 4.985299, mean_q: 5.169864
 37084/100000: episode: 3787, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.868, mean reward: 0.387 [0.363, 0.426], mean action: 50.900 [27.000, 60.000], mean observation: 3.156 [-1.381, 10.272], loss: 1.121987, mae: 4.984782, mean_q: 5.170864
 37094/100000: episode: 3788, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.128, mean reward: 0.413 [0.410, 0.433], mean action: 60.400 [18.000, 92.000], mean observation: 3.149 [-1.302, 10.255], loss: 1.408138, mae: 4.985981, mean_q: 5.172393
 37104/100000: episode: 3789, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.730, mean reward: 0.373 [0.314, 0.413], mean action: 57.700 [13.000, 99.000], mean observation: 3.150 [-1.218, 10.189], loss: 1.346202, mae: 4.985575, mean_q: 5.172671
 37114/100000: episode: 3790, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.881, mean reward: 0.388 [0.309, 0.436], mean action: 46.900 [19.000, 91.000], mean observation: 3.147 [-1.387, 10.395], loss: 1.225173, mae: 4.985040, mean_q: 5.173903
 37124/100000: episode: 3791, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.024, mean reward: 0.402 [0.307, 0.457], mean action: 63.400 [25.000, 94.000], mean observation: 3.149 [-0.877, 10.322], loss: 1.461600, mae: 4.985531, mean_q: 5.176748
 37134/100000: episode: 3792, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.388, mean reward: 0.439 [0.378, 0.498], mean action: 45.300 [18.000, 86.000], mean observation: 3.148 [-1.702, 10.282], loss: 1.449514, mae: 4.985396, mean_q: 5.175206
 37144/100000: episode: 3793, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.752, mean reward: 0.375 [0.347, 0.463], mean action: 43.800 [15.000, 82.000], mean observation: 3.154 [-1.897, 10.369], loss: 1.155432, mae: 4.984220, mean_q: 5.175260
 37154/100000: episode: 3794, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.901, mean reward: 0.390 [0.337, 0.507], mean action: 34.200 [1.000, 97.000], mean observation: 3.157 [-1.088, 10.370], loss: 1.384601, mae: 4.984734, mean_q: 5.174207
 37164/100000: episode: 3795, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 4.140, mean reward: 0.414 [0.389, 0.486], mean action: 72.300 [10.000, 97.000], mean observation: 3.154 [-1.243, 10.315], loss: 1.380879, mae: 4.984771, mean_q: 5.176775
 37174/100000: episode: 3796, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.889, mean reward: 0.389 [0.354, 0.420], mean action: 65.200 [10.000, 97.000], mean observation: 3.176 [-1.102, 10.273], loss: 1.337423, mae: 4.984544, mean_q: 5.180095
 37184/100000: episode: 3797, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.957, mean reward: 0.396 [0.366, 0.432], mean action: 57.200 [0.000, 100.000], mean observation: 3.155 [-1.008, 10.352], loss: 1.053648, mae: 4.983000, mean_q: 5.182454
 37194/100000: episode: 3798, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 3.919, mean reward: 0.392 [0.326, 0.541], mean action: 76.100 [4.000, 97.000], mean observation: 3.166 [-1.179, 10.437], loss: 1.433421, mae: 4.984558, mean_q: 5.182123
 37204/100000: episode: 3799, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 4.658, mean reward: 0.466 [0.466, 0.466], mean action: 82.200 [18.000, 97.000], mean observation: 3.168 [-1.446, 10.254], loss: 1.189605, mae: 4.983589, mean_q: 5.178776
 37212/100000: episode: 3800, duration: 0.115s, episode steps: 8, steps per second: 70, episode reward: 12.800, mean reward: 1.600 [0.389, 10.000], mean action: 66.000 [0.000, 97.000], mean observation: 3.148 [-1.217, 10.333], loss: 1.334235, mae: 4.984221, mean_q: 5.178440
 37222/100000: episode: 3801, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.285, mean reward: 0.428 [0.384, 0.511], mean action: 62.000 [0.000, 97.000], mean observation: 3.150 [-1.331, 10.361], loss: 1.134625, mae: 4.983592, mean_q: 5.179447
 37232/100000: episode: 3802, duration: 0.107s, episode steps: 10, steps per second: 93, episode reward: 4.002, mean reward: 0.400 [0.397, 0.424], mean action: 89.200 [44.000, 97.000], mean observation: 3.169 [-1.098, 10.368], loss: 0.912654, mae: 4.982742, mean_q: 5.180760
 37242/100000: episode: 3803, duration: 0.107s, episode steps: 10, steps per second: 93, episode reward: 4.203, mean reward: 0.420 [0.348, 0.482], mean action: 77.800 [18.000, 97.000], mean observation: 3.167 [-1.792, 10.138], loss: 1.246456, mae: 4.984194, mean_q: 5.182250
 37252/100000: episode: 3804, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 3.995, mean reward: 0.399 [0.392, 0.469], mean action: 87.900 [23.000, 97.000], mean observation: 3.173 [-1.312, 10.338], loss: 1.179695, mae: 4.984249, mean_q: 5.183713
 37262/100000: episode: 3805, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.071, mean reward: 0.407 [0.378, 0.431], mean action: 78.900 [12.000, 97.000], mean observation: 3.145 [-1.472, 10.273], loss: 1.130268, mae: 4.983916, mean_q: 5.184973
 37263/100000: episode: 3806, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 26.000 [26.000, 26.000], mean observation: 3.138 [-0.971, 10.100], loss: 0.777611, mae: 4.982438, mean_q: 5.185567
 37273/100000: episode: 3807, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.659, mean reward: 0.466 [0.362, 0.523], mean action: 67.700 [4.000, 100.000], mean observation: 3.160 [-1.136, 10.310], loss: 1.167571, mae: 4.983828, mean_q: 5.185797
 37283/100000: episode: 3808, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.894, mean reward: 0.389 [0.380, 0.461], mean action: 48.500 [2.000, 81.000], mean observation: 3.160 [-1.928, 10.372], loss: 1.313472, mae: 4.984342, mean_q: 5.182766
 37293/100000: episode: 3809, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.936, mean reward: 0.394 [0.318, 0.494], mean action: 49.000 [5.000, 81.000], mean observation: 3.157 [-1.078, 10.334], loss: 1.115652, mae: 4.983213, mean_q: 5.184663
 37303/100000: episode: 3810, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 4.408, mean reward: 0.441 [0.429, 0.459], mean action: 83.100 [12.000, 97.000], mean observation: 3.130 [-1.250, 10.273], loss: 1.590594, mae: 4.985001, mean_q: 5.184372
 37313/100000: episode: 3811, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.135, mean reward: 0.414 [0.349, 0.432], mean action: 59.300 [15.000, 98.000], mean observation: 3.158 [-1.755, 10.353], loss: 0.891860, mae: 4.982057, mean_q: 5.186475
 37323/100000: episode: 3812, duration: 0.109s, episode steps: 10, steps per second: 91, episode reward: 3.757, mean reward: 0.376 [0.297, 0.434], mean action: 85.000 [48.000, 97.000], mean observation: 3.167 [-0.788, 10.232], loss: 1.264809, mae: 4.983892, mean_q: 5.188213
 37333/100000: episode: 3813, duration: 0.121s, episode steps: 10, steps per second: 82, episode reward: 4.133, mean reward: 0.413 [0.327, 0.457], mean action: 77.000 [14.000, 97.000], mean observation: 3.145 [-1.208, 10.333], loss: 1.465082, mae: 4.984459, mean_q: 5.190044
 37343/100000: episode: 3814, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.733, mean reward: 0.473 [0.398, 0.517], mean action: 71.500 [12.000, 98.000], mean observation: 3.168 [-1.452, 10.349], loss: 1.590304, mae: 4.984897, mean_q: 5.192237
 37353/100000: episode: 3815, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 3.856, mean reward: 0.386 [0.332, 0.409], mean action: 86.100 [13.000, 97.000], mean observation: 3.134 [-0.776, 10.291], loss: 1.418360, mae: 4.983901, mean_q: 5.190993
 37363/100000: episode: 3816, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.787, mean reward: 0.379 [0.334, 0.566], mean action: 53.200 [5.000, 97.000], mean observation: 3.151 [-1.741, 10.274], loss: 1.297789, mae: 4.982982, mean_q: 5.183462
 37373/100000: episode: 3817, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 6.337, mean reward: 0.634 [0.634, 0.634], mean action: 62.700 [9.000, 100.000], mean observation: 3.152 [-1.063, 10.270], loss: 1.190807, mae: 4.982339, mean_q: 5.183560
 37383/100000: episode: 3818, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.544, mean reward: 0.454 [0.407, 0.466], mean action: 57.500 [0.000, 99.000], mean observation: 3.163 [-1.566, 10.345], loss: 1.361161, mae: 4.982907, mean_q: 5.183349
 37393/100000: episode: 3819, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.076, mean reward: 0.408 [0.398, 0.478], mean action: 74.700 [9.000, 97.000], mean observation: 3.144 [-1.384, 10.385], loss: 1.101574, mae: 4.982003, mean_q: 5.182338
 37403/100000: episode: 3820, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.860, mean reward: 0.386 [0.300, 0.501], mean action: 68.200 [1.000, 97.000], mean observation: 3.158 [-0.920, 10.373], loss: 1.399560, mae: 4.983116, mean_q: 5.184018
 37413/100000: episode: 3821, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.353, mean reward: 0.435 [0.336, 0.540], mean action: 68.600 [6.000, 97.000], mean observation: 3.159 [-1.355, 10.432], loss: 1.255844, mae: 4.982667, mean_q: 5.185689
 37423/100000: episode: 3822, duration: 0.102s, episode steps: 10, steps per second: 98, episode reward: 4.530, mean reward: 0.453 [0.415, 0.469], mean action: 78.500 [12.000, 97.000], mean observation: 3.155 [-1.416, 10.393], loss: 1.296098, mae: 4.983093, mean_q: 5.186937
 37433/100000: episode: 3823, duration: 0.121s, episode steps: 10, steps per second: 82, episode reward: 4.397, mean reward: 0.440 [0.427, 0.489], mean action: 79.400 [23.000, 97.000], mean observation: 3.152 [-1.453, 10.455], loss: 1.355685, mae: 4.983115, mean_q: 5.188967
 37443/100000: episode: 3824, duration: 0.086s, episode steps: 10, steps per second: 116, episode reward: 3.708, mean reward: 0.371 [0.362, 0.443], mean action: 90.700 [61.000, 97.000], mean observation: 3.147 [-1.198, 10.341], loss: 0.722475, mae: 4.980737, mean_q: 5.191998
 37453/100000: episode: 3825, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.722, mean reward: 0.372 [0.293, 0.476], mean action: 72.400 [23.000, 97.000], mean observation: 3.169 [-1.175, 10.422], loss: 1.084400, mae: 4.982640, mean_q: 5.194999
 37463/100000: episode: 3826, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 3.757, mean reward: 0.376 [0.357, 0.437], mean action: 68.800 [6.000, 97.000], mean observation: 3.162 [-1.224, 10.287], loss: 1.127618, mae: 4.983067, mean_q: 5.196627
 37466/100000: episode: 3827, duration: 0.043s, episode steps: 3, steps per second: 69, episode reward: 10.800, mean reward: 3.600 [0.400, 10.000], mean action: 73.667 [27.000, 97.000], mean observation: 3.158 [-1.086, 10.167], loss: 1.233988, mae: 4.983542, mean_q: 5.197271
 37476/100000: episode: 3828, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 3.925, mean reward: 0.392 [0.342, 0.588], mean action: 79.600 [12.000, 99.000], mean observation: 3.149 [-1.040, 10.292], loss: 1.233192, mae: 4.984117, mean_q: 5.198834
 37486/100000: episode: 3829, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 4.135, mean reward: 0.414 [0.414, 0.414], mean action: 89.700 [58.000, 97.000], mean observation: 3.168 [-0.898, 10.393], loss: 1.231789, mae: 4.984334, mean_q: 5.200445
 37496/100000: episode: 3830, duration: 0.089s, episode steps: 10, steps per second: 113, episode reward: 3.965, mean reward: 0.396 [0.396, 0.396], mean action: 92.300 [50.000, 97.000], mean observation: 3.171 [-1.735, 10.344], loss: 1.269570, mae: 4.984712, mean_q: 5.201362
 37506/100000: episode: 3831, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.137, mean reward: 0.414 [0.397, 0.441], mean action: 55.400 [2.000, 97.000], mean observation: 3.152 [-1.507, 10.302], loss: 1.372344, mae: 4.985010, mean_q: 5.197783
 37507/100000: episode: 3832, duration: 0.025s, episode steps: 1, steps per second: 41, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 80.000 [80.000, 80.000], mean observation: 3.156 [-1.193, 10.100], loss: 1.803664, mae: 4.986466, mean_q: 5.196689
 37517/100000: episode: 3833, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.762, mean reward: 0.376 [0.327, 0.424], mean action: 75.300 [28.000, 97.000], mean observation: 3.153 [-1.716, 10.305], loss: 1.249410, mae: 4.984396, mean_q: 5.196657
 37527/100000: episode: 3834, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 5.458, mean reward: 0.546 [0.531, 0.559], mean action: 84.300 [55.000, 101.000], mean observation: 3.159 [-1.291, 10.312], loss: 1.441391, mae: 4.984769, mean_q: 5.196720
 37537/100000: episode: 3835, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.128, mean reward: 0.413 [0.357, 0.437], mean action: 88.500 [28.000, 97.000], mean observation: 3.141 [-1.352, 10.314], loss: 1.170933, mae: 4.983558, mean_q: 5.193403
 37547/100000: episode: 3836, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.333, mean reward: 0.433 [0.426, 0.496], mean action: 82.300 [28.000, 101.000], mean observation: 3.152 [-0.860, 10.344], loss: 0.930217, mae: 4.982541, mean_q: 5.190404
 37557/100000: episode: 3837, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.792, mean reward: 0.379 [0.317, 0.536], mean action: 57.500 [6.000, 97.000], mean observation: 3.149 [-1.445, 10.267], loss: 1.163070, mae: 4.983733, mean_q: 5.187056
 37567/100000: episode: 3838, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.738, mean reward: 0.374 [0.305, 0.437], mean action: 48.000 [5.000, 82.000], mean observation: 3.171 [-2.024, 10.436], loss: 1.301253, mae: 4.983938, mean_q: 5.189395
 37577/100000: episode: 3839, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.675, mean reward: 0.467 [0.410, 0.494], mean action: 60.900 [1.000, 100.000], mean observation: 3.158 [-1.252, 10.393], loss: 1.191950, mae: 4.984048, mean_q: 5.190330
 37587/100000: episode: 3840, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.171, mean reward: 0.417 [0.370, 0.463], mean action: 63.000 [25.000, 80.000], mean observation: 3.158 [-2.023, 10.401], loss: 0.895265, mae: 4.982791, mean_q: 5.189821
 37597/100000: episode: 3841, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.831, mean reward: 0.383 [0.355, 0.479], mean action: 53.500 [9.000, 82.000], mean observation: 3.158 [-1.244, 10.309], loss: 1.485756, mae: 4.985270, mean_q: 5.186147
 37607/100000: episode: 3842, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.703, mean reward: 0.370 [0.295, 0.582], mean action: 53.500 [6.000, 82.000], mean observation: 3.160 [-1.475, 10.387], loss: 0.942992, mae: 4.982898, mean_q: 5.184664
 37617/100000: episode: 3843, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.220, mean reward: 0.422 [0.398, 0.523], mean action: 58.300 [13.000, 69.000], mean observation: 3.171 [-1.270, 10.398], loss: 1.179360, mae: 4.983946, mean_q: 5.186643
 37627/100000: episode: 3844, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.000, mean reward: 0.400 [0.387, 0.467], mean action: 73.300 [65.000, 97.000], mean observation: 3.170 [-0.576, 10.325], loss: 1.126638, mae: 4.983952, mean_q: 5.190133
 37637/100000: episode: 3845, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.111, mean reward: 0.411 [0.404, 0.439], mean action: 60.700 [14.000, 79.000], mean observation: 3.164 [-0.902, 10.384], loss: 1.465436, mae: 4.985618, mean_q: 5.191245
 37647/100000: episode: 3846, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.301, mean reward: 0.330 [0.294, 0.390], mean action: 65.200 [33.000, 93.000], mean observation: 3.153 [-0.903, 10.274], loss: 1.339165, mae: 4.984849, mean_q: 5.192812
 37657/100000: episode: 3847, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.200, mean reward: 0.420 [0.382, 0.539], mean action: 64.300 [26.000, 98.000], mean observation: 3.159 [-0.903, 10.280], loss: 1.066898, mae: 4.983875, mean_q: 5.197814
 37663/100000: episode: 3848, duration: 0.104s, episode steps: 6, steps per second: 57, episode reward: 11.968, mean reward: 1.995 [0.348, 10.000], mean action: 65.000 [37.000, 82.000], mean observation: 3.159 [-1.117, 10.419], loss: 1.465364, mae: 4.985323, mean_q: 5.201353
 37673/100000: episode: 3849, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.052, mean reward: 0.405 [0.354, 0.498], mean action: 63.300 [12.000, 91.000], mean observation: 3.162 [-0.825, 10.282], loss: 1.152263, mae: 4.984303, mean_q: 5.204153
 37683/100000: episode: 3850, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.096, mean reward: 0.410 [0.376, 0.488], mean action: 60.700 [15.000, 93.000], mean observation: 3.153 [-1.258, 10.466], loss: 1.133644, mae: 4.984416, mean_q: 5.207524
 37693/100000: episode: 3851, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.835, mean reward: 0.383 [0.359, 0.450], mean action: 81.600 [66.000, 101.000], mean observation: 3.165 [-1.305, 10.365], loss: 1.415425, mae: 4.985607, mean_q: 5.209252
 37703/100000: episode: 3852, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.288, mean reward: 0.429 [0.397, 0.467], mean action: 52.500 [0.000, 97.000], mean observation: 3.162 [-0.890, 10.242], loss: 0.861880, mae: 4.983718, mean_q: 5.209293
 37713/100000: episode: 3853, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.103, mean reward: 0.410 [0.365, 0.457], mean action: 52.500 [0.000, 85.000], mean observation: 3.159 [-1.147, 10.537], loss: 1.333012, mae: 4.985968, mean_q: 5.209590
 37723/100000: episode: 3854, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.379, mean reward: 0.438 [0.388, 0.482], mean action: 60.800 [29.000, 69.000], mean observation: 3.152 [-1.614, 10.395], loss: 1.610646, mae: 4.987199, mean_q: 5.206492
 37733/100000: episode: 3855, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.178, mean reward: 0.418 [0.341, 0.516], mean action: 66.900 [42.000, 75.000], mean observation: 3.168 [-1.338, 10.188], loss: 1.098428, mae: 4.985165, mean_q: 5.205587
 37743/100000: episode: 3856, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.515, mean reward: 0.451 [0.451, 0.451], mean action: 66.700 [29.000, 95.000], mean observation: 3.149 [-1.312, 10.318], loss: 0.993563, mae: 4.984658, mean_q: 5.205226
 37753/100000: episode: 3857, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.033, mean reward: 0.403 [0.352, 0.458], mean action: 63.100 [1.000, 97.000], mean observation: 3.169 [-1.789, 10.274], loss: 1.387784, mae: 4.986618, mean_q: 5.206008
 37763/100000: episode: 3858, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.965, mean reward: 0.397 [0.321, 0.493], mean action: 61.900 [0.000, 69.000], mean observation: 3.156 [-1.326, 10.351], loss: 1.117383, mae: 4.985675, mean_q: 5.207700
 37773/100000: episode: 3859, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.870, mean reward: 0.387 [0.371, 0.436], mean action: 63.700 [38.000, 69.000], mean observation: 3.163 [-1.238, 10.322], loss: 1.297622, mae: 4.986225, mean_q: 5.209210
 37783/100000: episode: 3860, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.535, mean reward: 0.353 [0.253, 0.420], mean action: 60.800 [17.000, 69.000], mean observation: 3.161 [-1.099, 10.277], loss: 1.113578, mae: 4.985625, mean_q: 5.209231
 37793/100000: episode: 3861, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.993, mean reward: 0.399 [0.285, 0.471], mean action: 47.900 [11.000, 83.000], mean observation: 3.161 [-1.287, 10.253], loss: 1.354029, mae: 4.986773, mean_q: 5.207532
 37803/100000: episode: 3862, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.652, mean reward: 0.365 [0.320, 0.574], mean action: 67.400 [40.000, 98.000], mean observation: 3.161 [-1.683, 10.159], loss: 1.345469, mae: 4.986806, mean_q: 5.205346
 37813/100000: episode: 3863, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.467, mean reward: 0.347 [0.306, 0.390], mean action: 71.300 [56.000, 100.000], mean observation: 3.150 [-1.740, 10.203], loss: 1.145379, mae: 4.986176, mean_q: 5.204914
 37823/100000: episode: 3864, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.661, mean reward: 0.366 [0.324, 0.449], mean action: 51.100 [7.000, 69.000], mean observation: 3.156 [-1.370, 10.271], loss: 1.399480, mae: 4.987482, mean_q: 5.202993
 37833/100000: episode: 3865, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.014, mean reward: 0.401 [0.357, 0.548], mean action: 57.100 [6.000, 93.000], mean observation: 3.149 [-1.265, 10.351], loss: 1.195246, mae: 4.986753, mean_q: 5.201145
 37843/100000: episode: 3866, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.235, mean reward: 0.424 [0.300, 0.483], mean action: 67.100 [23.000, 96.000], mean observation: 3.147 [-0.785, 10.262], loss: 0.617945, mae: 4.984708, mean_q: 5.200448
 37853/100000: episode: 3867, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 5.399, mean reward: 0.540 [0.540, 0.540], mean action: 62.900 [11.000, 98.000], mean observation: 3.162 [-2.187, 10.300], loss: 1.369305, mae: 4.988020, mean_q: 5.201125
 37863/100000: episode: 3868, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.633, mean reward: 0.463 [0.430, 0.523], mean action: 65.900 [28.000, 92.000], mean observation: 3.165 [-1.730, 10.335], loss: 0.716169, mae: 4.985975, mean_q: 5.203283
 37873/100000: episode: 3869, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 3.918, mean reward: 0.392 [0.385, 0.414], mean action: 65.900 [20.000, 79.000], mean observation: 3.164 [-1.496, 10.325], loss: 1.294648, mae: 4.988689, mean_q: 5.205593
 37883/100000: episode: 3870, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.801, mean reward: 0.380 [0.375, 0.425], mean action: 65.100 [27.000, 83.000], mean observation: 3.155 [-1.364, 10.303], loss: 1.379023, mae: 4.989291, mean_q: 5.206815
 37893/100000: episode: 3871, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.273, mean reward: 0.427 [0.379, 0.598], mean action: 56.300 [5.000, 93.000], mean observation: 3.173 [-0.700, 10.317], loss: 1.196157, mae: 4.988689, mean_q: 5.208673
 37903/100000: episode: 3872, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.528, mean reward: 0.353 [0.345, 0.390], mean action: 72.900 [54.000, 100.000], mean observation: 3.161 [-1.443, 10.371], loss: 1.402923, mae: 4.989555, mean_q: 5.207580
 37913/100000: episode: 3873, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.875, mean reward: 0.387 [0.365, 0.462], mean action: 63.800 [13.000, 81.000], mean observation: 3.154 [-1.255, 10.380], loss: 0.926942, mae: 4.987747, mean_q: 5.207695
 37923/100000: episode: 3874, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.126, mean reward: 0.413 [0.331, 0.486], mean action: 68.100 [23.000, 98.000], mean observation: 3.172 [-1.707, 10.344], loss: 1.206364, mae: 4.989054, mean_q: 5.207860
 37933/100000: episode: 3875, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.012, mean reward: 0.401 [0.349, 0.501], mean action: 40.500 [0.000, 76.000], mean observation: 3.149 [-1.326, 10.248], loss: 1.331122, mae: 4.989409, mean_q: 5.202854
 37943/100000: episode: 3876, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.504, mean reward: 0.450 [0.387, 0.504], mean action: 62.300 [30.000, 69.000], mean observation: 3.150 [-1.644, 10.196], loss: 1.651607, mae: 4.990423, mean_q: 5.201429
 37953/100000: episode: 3877, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.892, mean reward: 0.489 [0.489, 0.490], mean action: 60.100 [9.000, 78.000], mean observation: 3.168 [-1.283, 10.341], loss: 1.197810, mae: 4.988826, mean_q: 5.197042
 37963/100000: episode: 3878, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.530, mean reward: 0.453 [0.395, 0.468], mean action: 37.500 [8.000, 59.000], mean observation: 3.149 [-1.663, 10.379], loss: 1.326625, mae: 4.989259, mean_q: 5.198893
 37973/100000: episode: 3879, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.896, mean reward: 0.390 [0.314, 0.565], mean action: 58.200 [36.000, 79.000], mean observation: 3.169 [-1.101, 10.407], loss: 1.321388, mae: 4.989386, mean_q: 5.200824
 37983/100000: episode: 3880, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.706, mean reward: 0.371 [0.306, 0.461], mean action: 34.400 [11.000, 68.000], mean observation: 3.161 [-3.171, 10.497], loss: 1.400959, mae: 4.989627, mean_q: 5.199804
 37993/100000: episode: 3881, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.877, mean reward: 0.488 [0.483, 0.515], mean action: 39.000 [15.000, 69.000], mean observation: 3.159 [-1.111, 10.167], loss: 1.180929, mae: 4.988548, mean_q: 5.194884
 38003/100000: episode: 3882, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.543, mean reward: 0.454 [0.401, 0.480], mean action: 39.900 [2.000, 86.000], mean observation: 3.158 [-1.535, 10.318], loss: 1.313368, mae: 4.988721, mean_q: 5.190300
 38012/100000: episode: 3883, duration: 0.142s, episode steps: 9, steps per second: 63, episode reward: 13.353, mean reward: 1.484 [0.396, 10.000], mean action: 48.778 [6.000, 99.000], mean observation: 3.165 [-0.968, 10.355], loss: 1.278800, mae: 4.988698, mean_q: 5.186918
 38022/100000: episode: 3884, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.683, mean reward: 0.368 [0.356, 0.414], mean action: 48.800 [1.000, 99.000], mean observation: 3.159 [-0.979, 10.302], loss: 1.152976, mae: 4.987873, mean_q: 5.188247
 38032/100000: episode: 3885, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.678, mean reward: 0.368 [0.323, 0.432], mean action: 41.300 [21.000, 78.000], mean observation: 3.148 [-1.018, 10.241], loss: 1.384446, mae: 4.988873, mean_q: 5.190791
 38042/100000: episode: 3886, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.627, mean reward: 0.463 [0.416, 0.488], mean action: 38.300 [0.000, 88.000], mean observation: 3.159 [-1.564, 10.485], loss: 0.858859, mae: 4.986856, mean_q: 5.191894
 38052/100000: episode: 3887, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.415, mean reward: 0.442 [0.415, 0.526], mean action: 33.800 [4.000, 75.000], mean observation: 3.164 [-1.667, 10.450], loss: 1.471945, mae: 4.989305, mean_q: 5.193486
 38062/100000: episode: 3888, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 5.044, mean reward: 0.504 [0.504, 0.504], mean action: 41.400 [18.000, 83.000], mean observation: 3.155 [-1.192, 10.261], loss: 1.367754, mae: 4.988683, mean_q: 5.196151
 38072/100000: episode: 3889, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.701, mean reward: 0.370 [0.349, 0.460], mean action: 37.800 [35.000, 48.000], mean observation: 3.141 [-1.450, 10.346], loss: 1.561094, mae: 4.989373, mean_q: 5.198399
 38082/100000: episode: 3890, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.212, mean reward: 0.421 [0.375, 0.546], mean action: 39.300 [34.000, 72.000], mean observation: 3.181 [-1.039, 10.410], loss: 1.292346, mae: 4.987870, mean_q: 5.199967
 38092/100000: episode: 3891, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.348, mean reward: 0.435 [0.434, 0.446], mean action: 56.500 [22.000, 96.000], mean observation: 3.157 [-1.069, 10.413], loss: 1.308581, mae: 4.987721, mean_q: 5.201341
 38102/100000: episode: 3892, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.793, mean reward: 0.379 [0.283, 0.448], mean action: 46.000 [36.000, 91.000], mean observation: 3.155 [-1.344, 10.326], loss: 1.324415, mae: 4.987773, mean_q: 5.202758
 38112/100000: episode: 3893, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.264, mean reward: 0.426 [0.373, 0.457], mean action: 30.800 [8.000, 51.000], mean observation: 3.164 [-1.740, 10.288], loss: 1.086882, mae: 4.987131, mean_q: 5.204217
 38122/100000: episode: 3894, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.957, mean reward: 0.396 [0.344, 0.462], mean action: 43.300 [25.000, 85.000], mean observation: 3.157 [-1.706, 10.302], loss: 1.220955, mae: 4.987697, mean_q: 5.201149
 38132/100000: episode: 3895, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.076, mean reward: 0.408 [0.316, 0.459], mean action: 51.000 [16.000, 95.000], mean observation: 3.158 [-1.935, 10.307], loss: 1.223154, mae: 4.987798, mean_q: 5.194395
 38142/100000: episode: 3896, duration: 0.121s, episode steps: 10, steps per second: 82, episode reward: 4.042, mean reward: 0.404 [0.370, 0.439], mean action: 70.500 [2.000, 95.000], mean observation: 3.154 [-1.392, 10.407], loss: 1.440385, mae: 4.988936, mean_q: 5.194752
 38152/100000: episode: 3897, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 4.816, mean reward: 0.482 [0.421, 0.544], mean action: 82.800 [18.000, 95.000], mean observation: 3.149 [-1.408, 10.191], loss: 1.618865, mae: 4.989304, mean_q: 5.195815
 38162/100000: episode: 3898, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.179, mean reward: 0.418 [0.405, 0.469], mean action: 64.800 [14.000, 95.000], mean observation: 3.168 [-1.227, 10.396], loss: 0.799933, mae: 4.985910, mean_q: 5.194219
 38172/100000: episode: 3899, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.093, mean reward: 0.409 [0.335, 0.533], mean action: 67.900 [15.000, 95.000], mean observation: 3.149 [-1.358, 10.460], loss: 1.271737, mae: 4.987948, mean_q: 5.194154
 38182/100000: episode: 3900, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.729, mean reward: 0.473 [0.412, 0.500], mean action: 69.100 [7.000, 99.000], mean observation: 3.148 [-1.650, 10.437], loss: 1.069391, mae: 4.987240, mean_q: 5.191816
 38192/100000: episode: 3901, duration: 0.083s, episode steps: 10, steps per second: 121, episode reward: 4.282, mean reward: 0.428 [0.425, 0.460], mean action: 95.100 [95.000, 96.000], mean observation: 3.146 [-1.595, 10.209], loss: 1.258561, mae: 4.988235, mean_q: 5.191683
 38202/100000: episode: 3902, duration: 0.121s, episode steps: 10, steps per second: 82, episode reward: 3.480, mean reward: 0.348 [0.338, 0.377], mean action: 85.000 [22.000, 95.000], mean observation: 3.160 [-1.392, 10.357], loss: 1.234640, mae: 4.988173, mean_q: 5.188949
 38212/100000: episode: 3903, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.072, mean reward: 0.407 [0.360, 0.545], mean action: 62.800 [7.000, 98.000], mean observation: 3.164 [-1.343, 10.300], loss: 1.054720, mae: 4.987571, mean_q: 5.186599
 38222/100000: episode: 3904, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.261, mean reward: 0.426 [0.333, 0.526], mean action: 58.500 [3.000, 101.000], mean observation: 3.154 [-1.579, 10.343], loss: 1.057824, mae: 4.987606, mean_q: 5.185967
 38232/100000: episode: 3905, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.158, mean reward: 0.416 [0.384, 0.479], mean action: 55.200 [21.000, 100.000], mean observation: 3.162 [-1.507, 10.364], loss: 1.174150, mae: 4.988480, mean_q: 5.186396
 38242/100000: episode: 3906, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.067, mean reward: 0.407 [0.326, 0.525], mean action: 55.800 [24.000, 101.000], mean observation: 3.146 [-0.856, 10.166], loss: 1.309115, mae: 4.989470, mean_q: 5.187547
 38252/100000: episode: 3907, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 5.015, mean reward: 0.502 [0.363, 0.524], mean action: 34.600 [6.000, 71.000], mean observation: 3.148 [-1.385, 10.300], loss: 1.300498, mae: 4.989262, mean_q: 5.187620
 38262/100000: episode: 3908, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.431, mean reward: 0.443 [0.319, 0.550], mean action: 49.400 [1.000, 101.000], mean observation: 3.151 [-1.402, 10.262], loss: 1.064101, mae: 4.988439, mean_q: 5.186688
 38272/100000: episode: 3909, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.362, mean reward: 0.436 [0.398, 0.479], mean action: 36.600 [5.000, 87.000], mean observation: 3.153 [-1.137, 10.218], loss: 1.448913, mae: 4.989726, mean_q: 5.186807
 38282/100000: episode: 3910, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 5.034, mean reward: 0.503 [0.497, 0.541], mean action: 41.900 [10.000, 100.000], mean observation: 3.146 [-1.189, 10.283], loss: 1.261313, mae: 4.988621, mean_q: 5.181484
 38292/100000: episode: 3911, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.269, mean reward: 0.427 [0.398, 0.514], mean action: 56.600 [17.000, 96.000], mean observation: 3.161 [-1.301, 10.267], loss: 1.234170, mae: 4.988417, mean_q: 5.179751
 38302/100000: episode: 3912, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.463, mean reward: 0.446 [0.386, 0.500], mean action: 52.400 [5.000, 85.000], mean observation: 3.167 [-1.937, 10.331], loss: 1.054403, mae: 4.987541, mean_q: 5.181252
 38312/100000: episode: 3913, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.950, mean reward: 0.395 [0.394, 0.408], mean action: 53.200 [44.000, 64.000], mean observation: 3.138 [-1.435, 10.323], loss: 1.222557, mae: 4.988342, mean_q: 5.183221
 38322/100000: episode: 3914, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.780, mean reward: 0.478 [0.464, 0.535], mean action: 46.200 [1.000, 91.000], mean observation: 3.159 [-1.218, 10.403], loss: 1.170640, mae: 4.988305, mean_q: 5.184926
 38332/100000: episode: 3915, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.014, mean reward: 0.401 [0.381, 0.500], mean action: 62.900 [33.000, 101.000], mean observation: 3.167 [-1.291, 10.329], loss: 1.022196, mae: 4.987692, mean_q: 5.186772
 38342/100000: episode: 3916, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.398, mean reward: 0.440 [0.435, 0.486], mean action: 46.200 [14.000, 53.000], mean observation: 3.149 [-1.563, 10.389], loss: 1.067753, mae: 4.988112, mean_q: 5.188029
 38352/100000: episode: 3917, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.998, mean reward: 0.400 [0.387, 0.460], mean action: 52.900 [28.000, 90.000], mean observation: 3.160 [-1.226, 10.279], loss: 1.402283, mae: 4.989602, mean_q: 5.185441
 38362/100000: episode: 3918, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.684, mean reward: 0.368 [0.295, 0.499], mean action: 52.000 [25.000, 98.000], mean observation: 3.165 [-1.432, 10.268], loss: 1.278090, mae: 4.989316, mean_q: 5.185046
 38372/100000: episode: 3919, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.899, mean reward: 0.390 [0.342, 0.509], mean action: 48.300 [4.000, 100.000], mean observation: 3.150 [-0.985, 10.326], loss: 1.240653, mae: 4.988781, mean_q: 5.186153
 38382/100000: episode: 3920, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.199, mean reward: 0.420 [0.411, 0.452], mean action: 62.000 [42.000, 89.000], mean observation: 3.147 [-1.124, 10.468], loss: 1.422432, mae: 4.989902, mean_q: 5.187256
 38392/100000: episode: 3921, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.199, mean reward: 0.420 [0.362, 0.541], mean action: 58.100 [41.000, 80.000], mean observation: 3.165 [-1.075, 10.302], loss: 1.046919, mae: 4.988046, mean_q: 5.185024
 38402/100000: episode: 3922, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.475, mean reward: 0.448 [0.448, 0.448], mean action: 60.500 [41.000, 98.000], mean observation: 3.165 [-0.925, 10.395], loss: 0.970037, mae: 4.988026, mean_q: 5.185067
 38412/100000: episode: 3923, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.773, mean reward: 0.377 [0.320, 0.482], mean action: 57.900 [22.000, 86.000], mean observation: 3.162 [-1.430, 10.251], loss: 1.381267, mae: 4.989861, mean_q: 5.186375
 38422/100000: episode: 3924, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.807, mean reward: 0.381 [0.355, 0.437], mean action: 66.600 [45.000, 93.000], mean observation: 3.159 [-1.102, 10.288], loss: 1.429635, mae: 4.990180, mean_q: 5.188121
 38432/100000: episode: 3925, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.279, mean reward: 0.428 [0.387, 0.454], mean action: 44.100 [12.000, 71.000], mean observation: 3.155 [-1.767, 10.345], loss: 1.026099, mae: 4.988331, mean_q: 5.189832
 38442/100000: episode: 3926, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.780, mean reward: 0.378 [0.353, 0.421], mean action: 44.200 [9.000, 53.000], mean observation: 3.143 [-1.239, 10.327], loss: 1.031870, mae: 4.988545, mean_q: 5.191022
 38452/100000: episode: 3927, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 4.480, mean reward: 0.448 [0.383, 0.472], mean action: 64.800 [28.000, 98.000], mean observation: 3.154 [-1.410, 10.396], loss: 1.380015, mae: 4.990002, mean_q: 5.193195
 38462/100000: episode: 3928, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.353, mean reward: 0.435 [0.376, 0.501], mean action: 48.800 [9.000, 99.000], mean observation: 3.150 [-1.377, 10.201], loss: 0.938783, mae: 4.988204, mean_q: 5.195600
 38472/100000: episode: 3929, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.077, mean reward: 0.408 [0.286, 0.586], mean action: 52.900 [3.000, 93.000], mean observation: 3.149 [-1.602, 10.225], loss: 0.915683, mae: 4.988086, mean_q: 5.198120
 38482/100000: episode: 3930, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.640, mean reward: 0.464 [0.409, 0.566], mean action: 43.100 [6.000, 53.000], mean observation: 3.156 [-1.672, 10.291], loss: 1.041870, mae: 4.989225, mean_q: 5.200200
 38492/100000: episode: 3931, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.511, mean reward: 0.451 [0.331, 0.529], mean action: 49.900 [20.000, 81.000], mean observation: 3.151 [-1.188, 10.406], loss: 0.916711, mae: 4.988870, mean_q: 5.201810
 38502/100000: episode: 3932, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.907, mean reward: 0.391 [0.349, 0.512], mean action: 59.900 [53.000, 94.000], mean observation: 3.132 [-1.494, 10.235], loss: 1.064132, mae: 4.989995, mean_q: 5.201962
 38512/100000: episode: 3933, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.784, mean reward: 0.378 [0.343, 0.438], mean action: 50.400 [20.000, 95.000], mean observation: 3.166 [-1.715, 10.452], loss: 1.450701, mae: 4.991665, mean_q: 5.204630
 38522/100000: episode: 3934, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.077, mean reward: 0.408 [0.384, 0.508], mean action: 46.600 [1.000, 90.000], mean observation: 3.148 [-1.033, 10.191], loss: 1.399729, mae: 4.991775, mean_q: 5.207990
 38532/100000: episode: 3935, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.084, mean reward: 0.408 [0.322, 0.506], mean action: 54.000 [11.000, 71.000], mean observation: 3.156 [-1.304, 10.360], loss: 0.931649, mae: 4.989980, mean_q: 5.210627
 38542/100000: episode: 3936, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.321, mean reward: 0.432 [0.363, 0.523], mean action: 43.100 [5.000, 69.000], mean observation: 3.170 [-2.748, 10.333], loss: 0.981882, mae: 4.990628, mean_q: 5.211926
 38552/100000: episode: 3937, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.961, mean reward: 0.396 [0.333, 0.473], mean action: 52.500 [20.000, 91.000], mean observation: 3.154 [-1.335, 10.264], loss: 1.065099, mae: 4.991093, mean_q: 5.210427
 38562/100000: episode: 3938, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 5.534, mean reward: 0.553 [0.553, 0.553], mean action: 59.400 [46.000, 93.000], mean observation: 3.180 [-1.281, 10.413], loss: 1.225348, mae: 4.991828, mean_q: 5.211168
 38572/100000: episode: 3939, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 13.386, mean reward: 1.339 [0.294, 10.000], mean action: 45.800 [13.000, 53.000], mean observation: 3.160 [-1.739, 10.406], loss: 1.186810, mae: 4.991971, mean_q: 5.212786
 38582/100000: episode: 3940, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 3.931, mean reward: 0.393 [0.366, 0.446], mean action: 41.700 [0.000, 65.000], mean observation: 3.158 [-1.833, 10.241], loss: 1.268282, mae: 4.992196, mean_q: 5.211248
 38592/100000: episode: 3941, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.169, mean reward: 0.417 [0.377, 0.554], mean action: 51.700 [25.000, 73.000], mean observation: 3.164 [-1.142, 10.513], loss: 1.134387, mae: 4.991557, mean_q: 5.208329
 38602/100000: episode: 3942, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.842, mean reward: 0.384 [0.303, 0.512], mean action: 43.800 [5.000, 79.000], mean observation: 3.166 [-1.051, 10.320], loss: 1.372640, mae: 4.992775, mean_q: 5.205995
 38612/100000: episode: 3943, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.779, mean reward: 0.478 [0.425, 0.532], mean action: 49.700 [16.000, 69.000], mean observation: 3.154 [-1.501, 10.362], loss: 1.269609, mae: 4.992398, mean_q: 5.206415
 38622/100000: episode: 3944, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.080, mean reward: 0.408 [0.371, 0.565], mean action: 53.800 [29.000, 85.000], mean observation: 3.160 [-1.367, 10.211], loss: 1.094270, mae: 4.991542, mean_q: 5.208431
 38632/100000: episode: 3945, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.538, mean reward: 0.454 [0.324, 0.521], mean action: 57.900 [0.000, 100.000], mean observation: 3.168 [-1.803, 10.372], loss: 1.136254, mae: 4.991910, mean_q: 5.212095
 38642/100000: episode: 3946, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.125, mean reward: 0.413 [0.323, 0.559], mean action: 40.900 [1.000, 101.000], mean observation: 3.151 [-1.128, 10.271], loss: 0.955116, mae: 4.991493, mean_q: 5.215520
 38652/100000: episode: 3947, duration: 0.153s, episode steps: 10, steps per second: 66, episode reward: 4.370, mean reward: 0.437 [0.355, 0.555], mean action: 40.000 [3.000, 53.000], mean observation: 3.153 [-1.676, 10.457], loss: 1.133067, mae: 4.992807, mean_q: 5.217631
 38662/100000: episode: 3948, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.236, mean reward: 0.424 [0.374, 0.501], mean action: 49.000 [11.000, 83.000], mean observation: 3.146 [-1.714, 10.264], loss: 1.206160, mae: 4.993213, mean_q: 5.210186
 38671/100000: episode: 3949, duration: 0.161s, episode steps: 9, steps per second: 56, episode reward: 13.068, mean reward: 1.452 [0.354, 10.000], mean action: 41.667 [4.000, 53.000], mean observation: 3.154 [-1.290, 10.346], loss: 1.234329, mae: 4.993065, mean_q: 5.206141
 38681/100000: episode: 3950, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.739, mean reward: 0.474 [0.318, 0.585], mean action: 52.300 [12.000, 84.000], mean observation: 3.185 [-1.333, 10.413], loss: 1.507652, mae: 4.994069, mean_q: 5.203904
 38691/100000: episode: 3951, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.287, mean reward: 0.429 [0.349, 0.517], mean action: 40.100 [15.000, 63.000], mean observation: 3.153 [-1.407, 10.246], loss: 1.334329, mae: 4.992956, mean_q: 5.197492
 38701/100000: episode: 3952, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.436, mean reward: 0.444 [0.409, 0.522], mean action: 51.800 [21.000, 94.000], mean observation: 3.156 [-1.758, 10.374], loss: 1.288111, mae: 4.992967, mean_q: 5.195889
 38711/100000: episode: 3953, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.191, mean reward: 0.419 [0.330, 0.506], mean action: 49.300 [19.000, 98.000], mean observation: 3.160 [-0.934, 10.316], loss: 1.130600, mae: 4.992274, mean_q: 5.197425
 38720/100000: episode: 3954, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 13.609, mean reward: 1.512 [0.435, 10.000], mean action: 50.333 [3.000, 92.000], mean observation: 3.164 [-1.027, 10.325], loss: 1.205311, mae: 4.992804, mean_q: 5.199257
 38730/100000: episode: 3955, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.676, mean reward: 0.468 [0.468, 0.468], mean action: 48.500 [23.000, 76.000], mean observation: 3.172 [-0.987, 10.277], loss: 1.245426, mae: 4.993007, mean_q: 5.198700
 38740/100000: episode: 3956, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.293, mean reward: 0.429 [0.349, 0.449], mean action: 45.700 [23.000, 79.000], mean observation: 3.169 [-1.405, 10.370], loss: 1.083933, mae: 4.992664, mean_q: 5.198848
 38750/100000: episode: 3957, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.773, mean reward: 0.377 [0.341, 0.529], mean action: 62.600 [39.000, 99.000], mean observation: 3.151 [-0.909, 10.266], loss: 1.450971, mae: 4.994090, mean_q: 5.200590
 38760/100000: episode: 3958, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.935, mean reward: 0.493 [0.493, 0.493], mean action: 38.800 [27.000, 49.000], mean observation: 3.157 [-1.162, 10.291], loss: 0.761647, mae: 4.991409, mean_q: 5.203412
 38770/100000: episode: 3959, duration: 0.127s, episode steps: 10, steps per second: 78, episode reward: 4.303, mean reward: 0.430 [0.418, 0.453], mean action: 61.000 [39.000, 91.000], mean observation: 3.147 [-1.120, 10.396], loss: 1.207853, mae: 4.993239, mean_q: 5.205023
 38780/100000: episode: 3960, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.172, mean reward: 0.417 [0.360, 0.433], mean action: 52.500 [18.000, 94.000], mean observation: 3.165 [-1.830, 10.305], loss: 1.542653, mae: 4.994777, mean_q: 5.205718
 38782/100000: episode: 3961, duration: 0.043s, episode steps: 2, steps per second: 46, episode reward: 10.458, mean reward: 5.229 [0.458, 10.000], mean action: 53.000 [53.000, 53.000], mean observation: 3.170 [-0.473, 10.285], loss: 1.309292, mae: 4.993377, mean_q: 5.204255
 38792/100000: episode: 3962, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.063, mean reward: 0.406 [0.353, 0.482], mean action: 57.500 [1.000, 92.000], mean observation: 3.166 [-1.721, 10.434], loss: 0.965348, mae: 4.992380, mean_q: 5.203400
 38802/100000: episode: 3963, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.798, mean reward: 0.480 [0.469, 0.538], mean action: 60.500 [19.000, 99.000], mean observation: 3.156 [-0.766, 10.301], loss: 1.208778, mae: 4.993548, mean_q: 5.203296
 38812/100000: episode: 3964, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.342, mean reward: 0.434 [0.388, 0.573], mean action: 55.900 [23.000, 101.000], mean observation: 3.147 [-1.295, 10.560], loss: 1.039504, mae: 4.993117, mean_q: 5.201728
 38822/100000: episode: 3965, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.405, mean reward: 0.441 [0.360, 0.462], mean action: 47.900 [5.000, 78.000], mean observation: 3.146 [-1.885, 10.244], loss: 1.261746, mae: 4.994214, mean_q: 5.199355
 38832/100000: episode: 3966, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.056, mean reward: 0.406 [0.340, 0.462], mean action: 48.800 [23.000, 58.000], mean observation: 3.160 [-1.969, 10.299], loss: 1.186946, mae: 4.993908, mean_q: 5.199887
 38842/100000: episode: 3967, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.241, mean reward: 0.424 [0.314, 0.570], mean action: 63.300 [24.000, 101.000], mean observation: 3.156 [-1.334, 10.323], loss: 1.592907, mae: 4.995667, mean_q: 5.202288
 38852/100000: episode: 3968, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.395, mean reward: 0.440 [0.356, 0.559], mean action: 47.900 [5.000, 84.000], mean observation: 3.161 [-1.261, 10.380], loss: 1.375565, mae: 4.994342, mean_q: 5.204817
 38862/100000: episode: 3969, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.867, mean reward: 0.387 [0.339, 0.453], mean action: 49.000 [10.000, 98.000], mean observation: 3.154 [-1.475, 10.318], loss: 1.162863, mae: 4.993182, mean_q: 5.207161
 38872/100000: episode: 3970, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 5.419, mean reward: 0.542 [0.542, 0.542], mean action: 68.700 [25.000, 100.000], mean observation: 3.140 [-1.944, 10.408], loss: 1.141078, mae: 4.993144, mean_q: 5.208879
 38882/100000: episode: 3971, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.874, mean reward: 0.387 [0.349, 0.434], mean action: 52.400 [18.000, 75.000], mean observation: 3.168 [-1.380, 10.387], loss: 1.170565, mae: 4.993297, mean_q: 5.204318
 38892/100000: episode: 3972, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.973, mean reward: 0.397 [0.339, 0.423], mean action: 45.300 [2.000, 70.000], mean observation: 3.155 [-1.034, 10.282], loss: 0.894085, mae: 4.992270, mean_q: 5.197733
 38902/100000: episode: 3973, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.900, mean reward: 0.390 [0.340, 0.438], mean action: 46.200 [0.000, 95.000], mean observation: 3.151 [-1.349, 10.318], loss: 1.092471, mae: 4.993704, mean_q: 5.197090
 38912/100000: episode: 3974, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.183, mean reward: 0.418 [0.385, 0.491], mean action: 38.400 [12.000, 89.000], mean observation: 3.161 [-1.461, 10.309], loss: 1.056418, mae: 4.993703, mean_q: 5.197607
 38922/100000: episode: 3975, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.806, mean reward: 0.481 [0.481, 0.481], mean action: 53.300 [19.000, 92.000], mean observation: 3.145 [-1.489, 10.343], loss: 1.261351, mae: 4.994862, mean_q: 5.197444
 38932/100000: episode: 3976, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.676, mean reward: 0.468 [0.429, 0.532], mean action: 40.700 [20.000, 77.000], mean observation: 3.172 [-1.266, 10.383], loss: 1.451980, mae: 4.995689, mean_q: 5.198393
 38942/100000: episode: 3977, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.653, mean reward: 0.465 [0.460, 0.512], mean action: 46.600 [18.000, 78.000], mean observation: 3.151 [-1.101, 10.351], loss: 1.320496, mae: 4.995133, mean_q: 5.198554
 38952/100000: episode: 3978, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.273, mean reward: 0.427 [0.345, 0.547], mean action: 58.900 [31.000, 101.000], mean observation: 3.178 [-0.711, 10.234], loss: 1.135472, mae: 4.994525, mean_q: 5.199098
 38962/100000: episode: 3979, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.560, mean reward: 0.356 [0.331, 0.402], mean action: 54.900 [27.000, 90.000], mean observation: 3.167 [-1.612, 10.262], loss: 1.204230, mae: 4.994876, mean_q: 5.197186
 38972/100000: episode: 3980, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.302, mean reward: 0.430 [0.330, 0.510], mean action: 45.900 [13.000, 53.000], mean observation: 3.158 [-1.791, 10.263], loss: 1.612603, mae: 4.996904, mean_q: 5.197824
 38982/100000: episode: 3981, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.955, mean reward: 0.395 [0.364, 0.467], mean action: 53.100 [4.000, 82.000], mean observation: 3.156 [-2.039, 10.213], loss: 1.000773, mae: 4.994357, mean_q: 5.192964
 38992/100000: episode: 3982, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.828, mean reward: 0.383 [0.315, 0.452], mean action: 55.300 [26.000, 97.000], mean observation: 3.145 [-1.443, 10.353], loss: 1.166100, mae: 4.995087, mean_q: 5.185977
 39002/100000: episode: 3983, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.850, mean reward: 0.385 [0.310, 0.494], mean action: 40.100 [15.000, 93.000], mean observation: 3.159 [-1.266, 10.531], loss: 1.636700, mae: 4.996883, mean_q: 5.189328
 39012/100000: episode: 3984, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.926, mean reward: 0.493 [0.493, 0.493], mean action: 49.300 [26.000, 74.000], mean observation: 3.165 [-1.454, 10.350], loss: 1.279296, mae: 4.995481, mean_q: 5.191460
 39022/100000: episode: 3985, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 4.210, mean reward: 0.421 [0.377, 0.512], mean action: 48.100 [24.000, 74.000], mean observation: 3.143 [-1.392, 10.257], loss: 1.290907, mae: 4.995548, mean_q: 5.194174
 39032/100000: episode: 3986, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.538, mean reward: 0.354 [0.336, 0.365], mean action: 51.000 [20.000, 98.000], mean observation: 3.168 [-1.485, 10.248], loss: 1.020945, mae: 4.994180, mean_q: 5.194012
 39042/100000: episode: 3987, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.405, mean reward: 0.440 [0.400, 0.521], mean action: 50.700 [10.000, 85.000], mean observation: 3.152 [-1.860, 10.200], loss: 0.888543, mae: 4.993760, mean_q: 5.190566
 39052/100000: episode: 3988, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 3.667, mean reward: 0.367 [0.346, 0.406], mean action: 71.300 [1.000, 95.000], mean observation: 3.151 [-1.826, 10.428], loss: 1.198020, mae: 4.995219, mean_q: 5.191447
 39062/100000: episode: 3989, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.049, mean reward: 0.405 [0.355, 0.430], mean action: 59.000 [3.000, 85.000], mean observation: 3.161 [-1.897, 10.324], loss: 1.152743, mae: 4.995397, mean_q: 5.192971
 39072/100000: episode: 3990, duration: 0.126s, episode steps: 10, steps per second: 80, episode reward: 4.360, mean reward: 0.436 [0.368, 0.444], mean action: 73.800 [13.000, 98.000], mean observation: 3.170 [-1.052, 10.384], loss: 1.472014, mae: 4.996622, mean_q: 5.194951
 39082/100000: episode: 3991, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.179, mean reward: 0.418 [0.409, 0.458], mean action: 65.600 [0.000, 94.000], mean observation: 3.154 [-1.561, 10.391], loss: 1.288603, mae: 4.995634, mean_q: 5.194007
 39092/100000: episode: 3992, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.278, mean reward: 0.428 [0.363, 0.527], mean action: 26.600 [0.000, 97.000], mean observation: 3.155 [-1.294, 10.457], loss: 1.454806, mae: 4.995763, mean_q: 5.194995
 39102/100000: episode: 3993, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.811, mean reward: 0.381 [0.304, 0.437], mean action: 37.200 [0.000, 76.000], mean observation: 3.156 [-1.974, 10.317], loss: 1.227504, mae: 4.994813, mean_q: 5.195837
 39112/100000: episode: 3994, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 3.739, mean reward: 0.374 [0.324, 0.411], mean action: 4.700 [0.000, 47.000], mean observation: 3.157 [-1.343, 10.328], loss: 1.303479, mae: 4.995082, mean_q: 5.196500
 39122/100000: episode: 3995, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.198, mean reward: 0.420 [0.347, 0.525], mean action: 23.900 [0.000, 88.000], mean observation: 3.156 [-2.067, 10.339], loss: 1.260889, mae: 4.994997, mean_q: 5.198840
 39132/100000: episode: 3996, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.951, mean reward: 0.395 [0.265, 0.577], mean action: 18.800 [0.000, 74.000], mean observation: 3.158 [-1.346, 10.597], loss: 1.163928, mae: 4.994562, mean_q: 5.199746
 39142/100000: episode: 3997, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.063, mean reward: 0.406 [0.361, 0.505], mean action: 25.700 [0.000, 93.000], mean observation: 3.155 [-1.538, 10.256], loss: 1.096042, mae: 4.994222, mean_q: 5.198687
 39152/100000: episode: 3998, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.953, mean reward: 0.395 [0.300, 0.422], mean action: 24.600 [0.000, 97.000], mean observation: 3.154 [-2.047, 10.456], loss: 1.339994, mae: 4.995338, mean_q: 5.198253
 39162/100000: episode: 3999, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.043, mean reward: 0.404 [0.322, 0.472], mean action: 23.000 [0.000, 65.000], mean observation: 3.154 [-1.404, 10.276], loss: 1.334011, mae: 4.995547, mean_q: 5.198795
 39172/100000: episode: 4000, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.546, mean reward: 0.455 [0.288, 0.581], mean action: 45.000 [0.000, 85.000], mean observation: 3.159 [-2.011, 10.355], loss: 1.219697, mae: 4.995028, mean_q: 5.198333
 39182/100000: episode: 4001, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.568, mean reward: 0.357 [0.332, 0.429], mean action: 63.000 [1.000, 85.000], mean observation: 3.163 [-1.790, 10.302], loss: 1.219348, mae: 4.994834, mean_q: 5.196997
 39192/100000: episode: 4002, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.392, mean reward: 0.439 [0.352, 0.481], mean action: 47.500 [0.000, 85.000], mean observation: 3.148 [-1.471, 10.245], loss: 1.281665, mae: 4.995017, mean_q: 5.198489
 39202/100000: episode: 4003, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.415, mean reward: 0.441 [0.331, 0.570], mean action: 46.500 [2.000, 85.000], mean observation: 3.166 [-1.996, 10.384], loss: 1.418195, mae: 4.995383, mean_q: 5.200460
 39212/100000: episode: 4004, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.714, mean reward: 0.471 [0.366, 0.558], mean action: 61.000 [0.000, 101.000], mean observation: 3.166 [-1.365, 10.342], loss: 1.237938, mae: 4.994711, mean_q: 5.202641
 39222/100000: episode: 4005, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.193, mean reward: 0.419 [0.353, 0.497], mean action: 74.900 [9.000, 85.000], mean observation: 3.159 [-1.058, 10.308], loss: 0.946784, mae: 4.993527, mean_q: 5.207188
 39232/100000: episode: 4006, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.911, mean reward: 0.391 [0.311, 0.556], mean action: 60.100 [24.000, 95.000], mean observation: 3.157 [-1.217, 10.310], loss: 1.081652, mae: 4.994461, mean_q: 5.211886
 39242/100000: episode: 4007, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 5.005, mean reward: 0.501 [0.448, 0.595], mean action: 71.900 [15.000, 99.000], mean observation: 3.152 [-1.184, 10.395], loss: 1.496217, mae: 4.996056, mean_q: 5.215121
 39252/100000: episode: 4008, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.612, mean reward: 0.361 [0.319, 0.501], mean action: 65.700 [4.000, 85.000], mean observation: 3.164 [-1.226, 10.320], loss: 1.227271, mae: 4.994855, mean_q: 5.217981
 39262/100000: episode: 4009, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.788, mean reward: 0.379 [0.364, 0.398], mean action: 65.900 [26.000, 85.000], mean observation: 3.151 [-1.135, 10.374], loss: 1.354372, mae: 4.995132, mean_q: 5.220387
 39272/100000: episode: 4010, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.951, mean reward: 0.395 [0.324, 0.469], mean action: 52.100 [9.000, 85.000], mean observation: 3.175 [-1.298, 10.404], loss: 0.906610, mae: 4.993687, mean_q: 5.223206
 39282/100000: episode: 4011, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.568, mean reward: 0.357 [0.351, 0.373], mean action: 72.000 [3.000, 85.000], mean observation: 3.150 [-1.253, 10.334], loss: 1.556848, mae: 4.996333, mean_q: 5.225848
 39292/100000: episode: 4012, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.887, mean reward: 0.389 [0.353, 0.458], mean action: 72.900 [23.000, 89.000], mean observation: 3.176 [-1.683, 10.292], loss: 1.199545, mae: 4.994971, mean_q: 5.228034
 39302/100000: episode: 4013, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.045, mean reward: 0.405 [0.310, 0.442], mean action: 59.100 [0.000, 85.000], mean observation: 3.163 [-1.143, 10.418], loss: 1.292073, mae: 4.995264, mean_q: 5.230219
 39312/100000: episode: 4014, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.872, mean reward: 0.387 [0.382, 0.403], mean action: 74.600 [51.000, 86.000], mean observation: 3.159 [-1.040, 10.384], loss: 1.007354, mae: 4.994124, mean_q: 5.232240
 39322/100000: episode: 4015, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.088, mean reward: 0.409 [0.363, 0.449], mean action: 63.600 [17.000, 85.000], mean observation: 3.146 [-1.746, 10.346], loss: 1.301830, mae: 4.995871, mean_q: 5.234195
 39332/100000: episode: 4016, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.233, mean reward: 0.423 [0.378, 0.488], mean action: 63.400 [20.000, 92.000], mean observation: 3.138 [-1.488, 10.388], loss: 1.429136, mae: 4.996183, mean_q: 5.235472
 39342/100000: episode: 4017, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.248, mean reward: 0.425 [0.404, 0.501], mean action: 58.700 [16.000, 85.000], mean observation: 3.161 [-1.351, 10.443], loss: 1.431065, mae: 4.995804, mean_q: 5.236766
 39352/100000: episode: 4018, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.780, mean reward: 0.378 [0.306, 0.463], mean action: 69.300 [4.000, 93.000], mean observation: 3.158 [-2.162, 10.382], loss: 1.281776, mae: 4.994927, mean_q: 5.236137
 39362/100000: episode: 4019, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.931, mean reward: 0.393 [0.301, 0.488], mean action: 59.000 [2.000, 101.000], mean observation: 3.148 [-0.969, 10.221], loss: 1.082488, mae: 4.994156, mean_q: 5.236065
 39372/100000: episode: 4020, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.856, mean reward: 0.386 [0.367, 0.394], mean action: 71.400 [5.000, 85.000], mean observation: 3.152 [-1.432, 10.333], loss: 1.314751, mae: 4.995063, mean_q: 5.237649
 39382/100000: episode: 4021, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.902, mean reward: 0.390 [0.306, 0.543], mean action: 67.600 [32.000, 96.000], mean observation: 3.153 [-1.158, 10.369], loss: 1.223659, mae: 4.994992, mean_q: 5.237071
 39392/100000: episode: 4022, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.961, mean reward: 0.396 [0.346, 0.435], mean action: 69.200 [28.000, 100.000], mean observation: 3.157 [-1.246, 10.355], loss: 1.154641, mae: 4.995090, mean_q: 5.234042
 39402/100000: episode: 4023, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.793, mean reward: 0.379 [0.333, 0.431], mean action: 61.600 [14.000, 85.000], mean observation: 3.155 [-0.920, 10.280], loss: 1.285926, mae: 4.995914, mean_q: 5.233298
 39412/100000: episode: 4024, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.683, mean reward: 0.468 [0.378, 0.487], mean action: 70.800 [15.000, 85.000], mean observation: 3.172 [-0.876, 10.368], loss: 1.094627, mae: 4.995230, mean_q: 5.231865
 39422/100000: episode: 4025, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.966, mean reward: 0.397 [0.340, 0.522], mean action: 67.800 [11.000, 98.000], mean observation: 3.160 [-1.365, 10.303], loss: 1.202309, mae: 4.995607, mean_q: 5.234838
 39432/100000: episode: 4026, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.139, mean reward: 0.414 [0.359, 0.551], mean action: 64.600 [19.000, 85.000], mean observation: 3.150 [-1.355, 10.144], loss: 0.906570, mae: 4.994918, mean_q: 5.238078
 39442/100000: episode: 4027, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.858, mean reward: 0.486 [0.456, 0.574], mean action: 61.800 [18.000, 100.000], mean observation: 3.171 [-1.086, 10.374], loss: 1.084713, mae: 4.995740, mean_q: 5.238149
 39452/100000: episode: 4028, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.385, mean reward: 0.439 [0.340, 0.450], mean action: 74.200 [10.000, 91.000], mean observation: 3.143 [-0.917, 10.353], loss: 1.034972, mae: 4.995930, mean_q: 5.235941
 39462/100000: episode: 4029, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.671, mean reward: 0.467 [0.363, 0.483], mean action: 68.000 [2.000, 85.000], mean observation: 3.156 [-1.281, 10.355], loss: 1.554888, mae: 4.998370, mean_q: 5.235725
 39472/100000: episode: 4030, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.235, mean reward: 0.424 [0.403, 0.452], mean action: 68.900 [4.000, 101.000], mean observation: 3.154 [-1.528, 10.394], loss: 1.074753, mae: 4.996662, mean_q: 5.235348
 39482/100000: episode: 4031, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.139, mean reward: 0.414 [0.397, 0.458], mean action: 82.000 [33.000, 100.000], mean observation: 3.154 [-1.457, 10.206], loss: 1.417863, mae: 4.998193, mean_q: 5.235919
 39492/100000: episode: 4032, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 4.611, mean reward: 0.461 [0.447, 0.514], mean action: 80.400 [14.000, 99.000], mean observation: 3.160 [-2.051, 10.315], loss: 1.265013, mae: 4.997787, mean_q: 5.238148
 39502/100000: episode: 4033, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.699, mean reward: 0.470 [0.348, 0.568], mean action: 76.000 [7.000, 100.000], mean observation: 3.148 [-1.203, 10.387], loss: 1.253980, mae: 4.997577, mean_q: 5.240380
 39512/100000: episode: 4034, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.494, mean reward: 0.449 [0.422, 0.559], mean action: 49.300 [0.000, 85.000], mean observation: 3.161 [-1.188, 10.339], loss: 1.292401, mae: 4.997825, mean_q: 5.242364
 39522/100000: episode: 4035, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.268, mean reward: 0.327 [0.302, 0.391], mean action: 80.000 [24.000, 96.000], mean observation: 3.161 [-1.600, 10.280], loss: 1.159257, mae: 4.997691, mean_q: 5.245080
 39532/100000: episode: 4036, duration: 0.121s, episode steps: 10, steps per second: 82, episode reward: 4.158, mean reward: 0.416 [0.411, 0.433], mean action: 77.900 [41.000, 99.000], mean observation: 3.162 [-1.386, 10.371], loss: 1.048796, mae: 4.997633, mean_q: 5.247924
 39542/100000: episode: 4037, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.614, mean reward: 0.461 [0.369, 0.518], mean action: 48.000 [1.000, 85.000], mean observation: 3.158 [-1.181, 10.399], loss: 1.533469, mae: 4.999440, mean_q: 5.248841
 39552/100000: episode: 4038, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.857, mean reward: 0.386 [0.343, 0.499], mean action: 56.700 [20.000, 85.000], mean observation: 3.158 [-1.382, 10.328], loss: 1.434980, mae: 4.998872, mean_q: 5.247810
 39553/100000: episode: 4039, duration: 0.026s, episode steps: 1, steps per second: 38, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 85.000 [85.000, 85.000], mean observation: 3.157 [-0.661, 10.263], loss: 1.091791, mae: 4.997733, mean_q: 5.249000
 39563/100000: episode: 4040, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.662, mean reward: 0.366 [0.353, 0.398], mean action: 83.100 [64.000, 93.000], mean observation: 3.160 [-0.802, 10.442], loss: 0.888838, mae: 4.996524, mean_q: 5.250531
 39573/100000: episode: 4041, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.374, mean reward: 0.437 [0.403, 0.558], mean action: 66.300 [3.000, 88.000], mean observation: 3.155 [-1.408, 10.333], loss: 1.293575, mae: 4.998187, mean_q: 5.251730
 39583/100000: episode: 4042, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.289, mean reward: 0.429 [0.371, 0.505], mean action: 78.600 [42.000, 92.000], mean observation: 3.167 [-1.028, 10.342], loss: 1.086621, mae: 4.997729, mean_q: 5.252299
 39593/100000: episode: 4043, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.214, mean reward: 0.421 [0.396, 0.492], mean action: 66.600 [10.000, 85.000], mean observation: 3.150 [-1.365, 10.325], loss: 1.263103, mae: 4.998828, mean_q: 5.251306
 39603/100000: episode: 4044, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 3.793, mean reward: 0.379 [0.297, 0.430], mean action: 64.000 [17.000, 85.000], mean observation: 3.160 [-1.294, 10.261], loss: 1.188869, mae: 4.998504, mean_q: 5.252690
 39613/100000: episode: 4045, duration: 0.113s, episode steps: 10, steps per second: 88, episode reward: 4.559, mean reward: 0.456 [0.361, 0.470], mean action: 75.500 [1.000, 100.000], mean observation: 3.168 [-0.506, 10.270], loss: 1.379048, mae: 4.999386, mean_q: 5.255359
 39623/100000: episode: 4046, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.362, mean reward: 0.436 [0.337, 0.502], mean action: 69.600 [2.000, 90.000], mean observation: 3.168 [-1.900, 10.339], loss: 1.271305, mae: 4.999411, mean_q: 5.258017
 39633/100000: episode: 4047, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 5.539, mean reward: 0.554 [0.554, 0.554], mean action: 80.100 [60.000, 85.000], mean observation: 3.172 [-1.808, 10.323], loss: 1.128778, mae: 4.999130, mean_q: 5.257607
 39643/100000: episode: 4048, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.726, mean reward: 0.373 [0.299, 0.508], mean action: 71.800 [6.000, 85.000], mean observation: 3.157 [-1.788, 10.481], loss: 1.086831, mae: 4.999280, mean_q: 5.255978
 39653/100000: episode: 4049, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.332, mean reward: 0.433 [0.384, 0.558], mean action: 62.000 [18.000, 85.000], mean observation: 3.151 [-1.637, 10.293], loss: 1.121433, mae: 5.000018, mean_q: 5.256140
 39663/100000: episode: 4050, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.390, mean reward: 0.439 [0.354, 0.518], mean action: 41.000 [2.000, 100.000], mean observation: 3.148 [-1.199, 10.496], loss: 1.047675, mae: 5.000282, mean_q: 5.256714
 39673/100000: episode: 4051, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.174, mean reward: 0.417 [0.390, 0.457], mean action: 57.400 [0.000, 98.000], mean observation: 3.162 [-0.966, 10.359], loss: 1.191732, mae: 5.001009, mean_q: 5.251793
 39683/100000: episode: 4052, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.306, mean reward: 0.431 [0.361, 0.461], mean action: 55.700 [5.000, 85.000], mean observation: 3.162 [-1.232, 10.260], loss: 1.170637, mae: 5.001314, mean_q: 5.247074
 39693/100000: episode: 4053, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 3.884, mean reward: 0.388 [0.322, 0.481], mean action: 69.900 [29.000, 96.000], mean observation: 3.168 [-0.843, 10.365], loss: 1.043067, mae: 5.001021, mean_q: 5.246663
 39703/100000: episode: 4054, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.545, mean reward: 0.354 [0.325, 0.465], mean action: 70.100 [20.000, 85.000], mean observation: 3.174 [-1.829, 10.351], loss: 1.300568, mae: 5.002233, mean_q: 5.248256
 39713/100000: episode: 4055, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.660, mean reward: 0.366 [0.353, 0.401], mean action: 66.200 [10.000, 95.000], mean observation: 3.169 [-1.839, 10.381], loss: 1.528655, mae: 5.003266, mean_q: 5.250743
 39723/100000: episode: 4056, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 3.943, mean reward: 0.394 [0.300, 0.497], mean action: 61.600 [12.000, 87.000], mean observation: 3.157 [-1.569, 10.356], loss: 1.143627, mae: 5.002090, mean_q: 5.251822
 39733/100000: episode: 4057, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.565, mean reward: 0.356 [0.297, 0.406], mean action: 69.400 [19.000, 95.000], mean observation: 3.160 [-1.252, 10.264], loss: 1.071821, mae: 5.002131, mean_q: 5.253725
 39743/100000: episode: 4058, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.730, mean reward: 0.373 [0.308, 0.434], mean action: 68.400 [36.000, 92.000], mean observation: 3.160 [-1.661, 10.275], loss: 1.140739, mae: 5.002855, mean_q: 5.256856
 39753/100000: episode: 4059, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.954, mean reward: 0.395 [0.313, 0.436], mean action: 73.500 [18.000, 100.000], mean observation: 3.151 [-0.981, 10.327], loss: 1.243671, mae: 5.003119, mean_q: 5.257140
 39763/100000: episode: 4060, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.265, mean reward: 0.426 [0.338, 0.580], mean action: 57.400 [4.000, 85.000], mean observation: 3.159 [-1.296, 10.323], loss: 1.170242, mae: 5.002917, mean_q: 5.258060
 39773/100000: episode: 4061, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.054, mean reward: 0.405 [0.392, 0.431], mean action: 64.000 [5.000, 93.000], mean observation: 3.168 [-1.042, 10.312], loss: 1.194140, mae: 5.003114, mean_q: 5.258132
 39783/100000: episode: 4062, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.882, mean reward: 0.388 [0.333, 0.444], mean action: 74.200 [19.000, 94.000], mean observation: 3.180 [-1.388, 10.317], loss: 1.461187, mae: 5.004140, mean_q: 5.260378
 39793/100000: episode: 4063, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.236, mean reward: 0.424 [0.353, 0.587], mean action: 53.900 [8.000, 85.000], mean observation: 3.168 [-1.167, 10.332], loss: 1.152725, mae: 5.003033, mean_q: 5.261762
 39803/100000: episode: 4064, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.839, mean reward: 0.384 [0.364, 0.445], mean action: 76.700 [39.000, 85.000], mean observation: 3.169 [-1.227, 10.281], loss: 1.127899, mae: 5.003278, mean_q: 5.260589
 39813/100000: episode: 4065, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.938, mean reward: 0.394 [0.361, 0.464], mean action: 62.400 [3.000, 94.000], mean observation: 3.155 [-1.892, 10.301], loss: 1.463905, mae: 5.004621, mean_q: 5.256041
 39823/100000: episode: 4066, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.570, mean reward: 0.357 [0.332, 0.377], mean action: 70.600 [25.000, 96.000], mean observation: 3.151 [-0.706, 10.254], loss: 1.270517, mae: 5.004167, mean_q: 5.246388
 39833/100000: episode: 4067, duration: 0.114s, episode steps: 10, steps per second: 87, episode reward: 4.664, mean reward: 0.466 [0.431, 0.504], mean action: 71.100 [18.000, 85.000], mean observation: 3.157 [-1.515, 10.348], loss: 1.171886, mae: 5.003975, mean_q: 5.246303
 39843/100000: episode: 4068, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.035, mean reward: 0.403 [0.384, 0.481], mean action: 69.900 [26.000, 95.000], mean observation: 3.152 [-0.851, 10.437], loss: 1.330367, mae: 5.004829, mean_q: 5.246932
 39853/100000: episode: 4069, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.859, mean reward: 0.386 [0.354, 0.424], mean action: 66.500 [0.000, 85.000], mean observation: 3.140 [-0.967, 10.281], loss: 1.205179, mae: 5.004619, mean_q: 5.244796
 39863/100000: episode: 4070, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 4.054, mean reward: 0.405 [0.326, 0.498], mean action: 68.000 [11.000, 96.000], mean observation: 3.164 [-1.277, 10.436], loss: 1.449535, mae: 5.005562, mean_q: 5.243821
 39873/100000: episode: 4071, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.872, mean reward: 0.387 [0.334, 0.418], mean action: 80.400 [46.000, 85.000], mean observation: 3.152 [-1.253, 10.305], loss: 1.245094, mae: 5.004845, mean_q: 5.244761
 39883/100000: episode: 4072, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 3.989, mean reward: 0.399 [0.348, 0.526], mean action: 78.000 [20.000, 89.000], mean observation: 3.169 [-1.285, 10.322], loss: 1.237739, mae: 5.004832, mean_q: 5.247568
 39893/100000: episode: 4073, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.549, mean reward: 0.455 [0.374, 0.581], mean action: 58.600 [9.000, 85.000], mean observation: 3.155 [-1.181, 10.298], loss: 1.435055, mae: 5.005322, mean_q: 5.248391
 39903/100000: episode: 4074, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.005, mean reward: 0.401 [0.375, 0.426], mean action: 65.500 [22.000, 85.000], mean observation: 3.151 [-1.388, 10.341], loss: 0.867724, mae: 5.003256, mean_q: 5.246753
 39913/100000: episode: 4075, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.797, mean reward: 0.480 [0.480, 0.480], mean action: 71.700 [21.000, 93.000], mean observation: 3.155 [-1.553, 10.440], loss: 1.239857, mae: 5.005276, mean_q: 5.246318
 39923/100000: episode: 4076, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 5.470, mean reward: 0.547 [0.547, 0.547], mean action: 61.200 [13.000, 90.000], mean observation: 3.154 [-1.196, 10.206], loss: 1.365051, mae: 5.005951, mean_q: 5.247652
 39933/100000: episode: 4077, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 3.981, mean reward: 0.398 [0.343, 0.595], mean action: 59.600 [16.000, 85.000], mean observation: 3.162 [-1.144, 10.316], loss: 1.191432, mae: 5.005579, mean_q: 5.246334
 39943/100000: episode: 4078, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.051, mean reward: 0.405 [0.341, 0.454], mean action: 72.200 [7.000, 97.000], mean observation: 3.142 [-1.752, 10.321], loss: 1.221942, mae: 5.005805, mean_q: 5.243678
 39953/100000: episode: 4079, duration: 0.114s, episode steps: 10, steps per second: 87, episode reward: 3.847, mean reward: 0.385 [0.378, 0.441], mean action: 79.500 [21.000, 93.000], mean observation: 3.162 [-0.689, 10.342], loss: 1.062234, mae: 5.005389, mean_q: 5.243461
 39963/100000: episode: 4080, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 3.881, mean reward: 0.388 [0.286, 0.522], mean action: 68.800 [28.000, 96.000], mean observation: 3.155 [-1.300, 10.374], loss: 1.313845, mae: 5.006591, mean_q: 5.245203
 39973/100000: episode: 4081, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.818, mean reward: 0.382 [0.356, 0.399], mean action: 73.200 [14.000, 93.000], mean observation: 3.162 [-0.770, 10.295], loss: 1.253447, mae: 5.006566, mean_q: 5.246160
 39983/100000: episode: 4082, duration: 0.121s, episode steps: 10, steps per second: 82, episode reward: 4.450, mean reward: 0.445 [0.445, 0.445], mean action: 79.800 [30.000, 96.000], mean observation: 3.162 [-1.409, 10.280], loss: 1.228931, mae: 5.006653, mean_q: 5.249264
 39993/100000: episode: 4083, duration: 0.107s, episode steps: 10, steps per second: 93, episode reward: 4.012, mean reward: 0.401 [0.359, 0.513], mean action: 76.100 [28.000, 100.000], mean observation: 3.157 [-1.616, 10.198], loss: 1.090900, mae: 5.006137, mean_q: 5.253302
 40003/100000: episode: 4084, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.691, mean reward: 0.469 [0.469, 0.469], mean action: 67.200 [34.000, 85.000], mean observation: 3.153 [-2.030, 10.313], loss: 1.204108, mae: 5.006938, mean_q: 5.254440
 40013/100000: episode: 4085, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 3.769, mean reward: 0.377 [0.373, 0.408], mean action: 80.800 [51.000, 98.000], mean observation: 3.158 [-0.911, 10.322], loss: 1.356108, mae: 5.007410, mean_q: 5.254832
 40023/100000: episode: 4086, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.286, mean reward: 0.429 [0.340, 0.468], mean action: 64.400 [11.000, 85.000], mean observation: 3.153 [-1.399, 10.342], loss: 1.198908, mae: 5.006806, mean_q: 5.252217
 40033/100000: episode: 4087, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.137, mean reward: 0.414 [0.341, 0.517], mean action: 61.900 [5.000, 93.000], mean observation: 3.153 [-0.815, 10.353], loss: 1.061248, mae: 5.006628, mean_q: 5.249904
 40043/100000: episode: 4088, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.992, mean reward: 0.399 [0.375, 0.577], mean action: 64.000 [12.000, 85.000], mean observation: 3.149 [-1.460, 10.442], loss: 0.876630, mae: 5.006337, mean_q: 5.249690
 40053/100000: episode: 4089, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.723, mean reward: 0.372 [0.308, 0.476], mean action: 79.100 [48.000, 93.000], mean observation: 3.121 [-0.961, 10.308], loss: 1.251171, mae: 5.008252, mean_q: 5.249584
 40063/100000: episode: 4090, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.959, mean reward: 0.396 [0.391, 0.419], mean action: 75.800 [6.000, 85.000], mean observation: 3.145 [-0.956, 10.231], loss: 1.057495, mae: 5.007832, mean_q: 5.249074
 40073/100000: episode: 4091, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.498, mean reward: 0.350 [0.296, 0.400], mean action: 66.700 [1.000, 85.000], mean observation: 3.144 [-1.059, 10.349], loss: 1.237290, mae: 5.008901, mean_q: 5.246554
 40083/100000: episode: 4092, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.073, mean reward: 0.407 [0.379, 0.443], mean action: 68.300 [0.000, 85.000], mean observation: 3.158 [-1.756, 10.361], loss: 1.281977, mae: 5.009186, mean_q: 5.244378
 40093/100000: episode: 4093, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.123, mean reward: 0.412 [0.339, 0.517], mean action: 71.600 [3.000, 90.000], mean observation: 3.175 [-0.825, 10.270], loss: 1.121265, mae: 5.008799, mean_q: 5.244979
 40103/100000: episode: 4094, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.026, mean reward: 0.403 [0.369, 0.523], mean action: 60.200 [0.000, 93.000], mean observation: 3.149 [-1.225, 10.249], loss: 1.369404, mae: 5.010011, mean_q: 5.244905
 40113/100000: episode: 4095, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 5.054, mean reward: 0.505 [0.369, 0.553], mean action: 65.300 [5.000, 85.000], mean observation: 3.158 [-1.501, 10.323], loss: 1.145156, mae: 5.009363, mean_q: 5.242230
 40123/100000: episode: 4096, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 4.545, mean reward: 0.455 [0.442, 0.467], mean action: 81.400 [39.000, 101.000], mean observation: 3.132 [-1.495, 10.239], loss: 1.201993, mae: 5.010003, mean_q: 5.242496
 40133/100000: episode: 4097, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.769, mean reward: 0.377 [0.297, 0.465], mean action: 66.800 [0.000, 85.000], mean observation: 3.167 [-1.855, 10.302], loss: 1.111189, mae: 5.009556, mean_q: 5.245134
 40143/100000: episode: 4098, duration: 0.150s, episode steps: 10, steps per second: 66, episode reward: 4.202, mean reward: 0.420 [0.384, 0.502], mean action: 57.200 [2.000, 85.000], mean observation: 3.153 [-1.310, 10.276], loss: 1.530868, mae: 5.011180, mean_q: 5.248034
 40153/100000: episode: 4099, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.871, mean reward: 0.387 [0.350, 0.433], mean action: 66.400 [4.000, 100.000], mean observation: 3.146 [-1.140, 10.169], loss: 1.106411, mae: 5.009768, mean_q: 5.249017
 40161/100000: episode: 4100, duration: 0.124s, episode steps: 8, steps per second: 65, episode reward: 12.770, mean reward: 1.596 [0.369, 10.000], mean action: 65.750 [6.000, 98.000], mean observation: 3.156 [-1.255, 10.589], loss: 1.547210, mae: 5.011383, mean_q: 5.250378
 40171/100000: episode: 4101, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.899, mean reward: 0.390 [0.307, 0.469], mean action: 58.700 [25.000, 85.000], mean observation: 3.150 [-1.024, 10.459], loss: 0.943343, mae: 5.008740, mean_q: 5.250063
 40181/100000: episode: 4102, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 4.001, mean reward: 0.400 [0.384, 0.478], mean action: 79.700 [56.000, 90.000], mean observation: 3.160 [-1.272, 10.319], loss: 1.419473, mae: 5.011016, mean_q: 5.251746
 40191/100000: episode: 4103, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.889, mean reward: 0.489 [0.359, 0.525], mean action: 63.300 [3.000, 85.000], mean observation: 3.157 [-0.820, 10.447], loss: 1.407015, mae: 5.010806, mean_q: 5.253124
 40201/100000: episode: 4104, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.793, mean reward: 0.379 [0.287, 0.436], mean action: 63.100 [11.000, 100.000], mean observation: 3.157 [-0.757, 10.273], loss: 1.057406, mae: 5.009327, mean_q: 5.254385
 40211/100000: episode: 4105, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 3.899, mean reward: 0.390 [0.332, 0.454], mean action: 71.500 [23.000, 86.000], mean observation: 3.163 [-0.924, 10.277], loss: 1.332947, mae: 5.010365, mean_q: 5.252422
 40221/100000: episode: 4106, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.386, mean reward: 0.439 [0.407, 0.512], mean action: 73.400 [28.000, 99.000], mean observation: 3.162 [-0.874, 10.231], loss: 1.391277, mae: 5.010890, mean_q: 5.253723
 40231/100000: episode: 4107, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.593, mean reward: 0.459 [0.403, 0.564], mean action: 66.800 [18.000, 85.000], mean observation: 3.144 [-1.290, 10.291], loss: 1.485273, mae: 5.011189, mean_q: 5.256212
 40241/100000: episode: 4108, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.004, mean reward: 0.400 [0.358, 0.459], mean action: 58.000 [16.000, 85.000], mean observation: 3.155 [-1.580, 10.434], loss: 1.293699, mae: 5.010228, mean_q: 5.254938
 40251/100000: episode: 4109, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.700, mean reward: 0.470 [0.371, 0.548], mean action: 59.300 [1.000, 92.000], mean observation: 3.158 [-1.518, 10.320], loss: 1.501964, mae: 5.011037, mean_q: 5.254695
 40261/100000: episode: 4110, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.874, mean reward: 0.387 [0.332, 0.455], mean action: 65.400 [22.000, 85.000], mean observation: 3.157 [-1.323, 10.224], loss: 1.266792, mae: 5.009946, mean_q: 5.254162
 40271/100000: episode: 4111, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.052, mean reward: 0.405 [0.402, 0.432], mean action: 80.600 [42.000, 85.000], mean observation: 3.171 [-0.977, 10.378], loss: 1.254508, mae: 5.009810, mean_q: 5.245840
 40281/100000: episode: 4112, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.761, mean reward: 0.376 [0.335, 0.435], mean action: 55.200 [3.000, 85.000], mean observation: 3.163 [-1.265, 10.274], loss: 1.360390, mae: 5.010194, mean_q: 5.240063
 40291/100000: episode: 4113, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.942, mean reward: 0.394 [0.338, 0.433], mean action: 62.700 [22.000, 85.000], mean observation: 3.173 [-0.911, 10.269], loss: 0.910051, mae: 5.008433, mean_q: 5.238974
 40301/100000: episode: 4114, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 3.798, mean reward: 0.380 [0.335, 0.420], mean action: 63.100 [4.000, 85.000], mean observation: 3.154 [-1.537, 10.291], loss: 1.087998, mae: 5.009417, mean_q: 5.241502
 40311/100000: episode: 4115, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.046, mean reward: 0.405 [0.390, 0.419], mean action: 65.800 [8.000, 85.000], mean observation: 3.148 [-1.460, 10.236], loss: 1.019865, mae: 5.009571, mean_q: 5.242883
 40321/100000: episode: 4116, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.916, mean reward: 0.392 [0.376, 0.410], mean action: 68.500 [14.000, 91.000], mean observation: 3.155 [-1.631, 10.388], loss: 1.217814, mae: 5.010737, mean_q: 5.243771
 40331/100000: episode: 4117, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.033, mean reward: 0.403 [0.389, 0.486], mean action: 65.300 [28.000, 100.000], mean observation: 3.154 [-1.961, 10.378], loss: 1.302610, mae: 5.011083, mean_q: 5.242311
 40341/100000: episode: 4118, duration: 0.126s, episode steps: 10, steps per second: 80, episode reward: 3.953, mean reward: 0.395 [0.342, 0.534], mean action: 65.200 [10.000, 100.000], mean observation: 3.161 [-1.434, 10.471], loss: 1.402266, mae: 5.011777, mean_q: 5.241693
 40351/100000: episode: 4119, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.523, mean reward: 0.352 [0.324, 0.515], mean action: 83.000 [66.000, 95.000], mean observation: 3.182 [-1.014, 10.351], loss: 1.601051, mae: 5.012523, mean_q: 5.242762
 40361/100000: episode: 4120, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.918, mean reward: 0.392 [0.357, 0.476], mean action: 70.000 [6.000, 88.000], mean observation: 3.153 [-0.949, 10.384], loss: 1.215603, mae: 5.010647, mean_q: 5.242410
 40371/100000: episode: 4121, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.634, mean reward: 0.463 [0.447, 0.530], mean action: 66.700 [11.000, 94.000], mean observation: 3.173 [-2.346, 10.297], loss: 1.159236, mae: 5.010182, mean_q: 5.244315
 40375/100000: episode: 4122, duration: 0.068s, episode steps: 4, steps per second: 59, episode reward: 10.997, mean reward: 2.749 [0.332, 10.000], mean action: 67.750 [16.000, 85.000], mean observation: 3.148 [-1.572, 10.220], loss: 1.080734, mae: 5.010011, mean_q: 5.246707
 40385/100000: episode: 4123, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.817, mean reward: 0.382 [0.309, 0.466], mean action: 70.800 [22.000, 98.000], mean observation: 3.158 [-1.615, 10.277], loss: 1.037796, mae: 5.010151, mean_q: 5.248322
 40395/100000: episode: 4124, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.772, mean reward: 0.377 [0.358, 0.464], mean action: 77.900 [16.000, 91.000], mean observation: 3.160 [-1.364, 10.334], loss: 1.255224, mae: 5.011302, mean_q: 5.249348
 40405/100000: episode: 4125, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.393, mean reward: 0.439 [0.439, 0.439], mean action: 76.000 [44.000, 97.000], mean observation: 3.148 [-0.934, 10.352], loss: 1.198786, mae: 5.011239, mean_q: 5.249751
 40415/100000: episode: 4126, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.056, mean reward: 0.406 [0.394, 0.514], mean action: 74.800 [31.000, 87.000], mean observation: 3.158 [-1.218, 10.298], loss: 1.177255, mae: 5.011387, mean_q: 5.250022
 40425/100000: episode: 4127, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.722, mean reward: 0.372 [0.359, 0.424], mean action: 59.900 [8.000, 96.000], mean observation: 3.170 [-1.296, 10.372], loss: 1.108580, mae: 5.011343, mean_q: 5.249286
 40435/100000: episode: 4128, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.008, mean reward: 0.401 [0.360, 0.431], mean action: 64.800 [4.000, 85.000], mean observation: 3.163 [-1.328, 10.299], loss: 1.251955, mae: 5.011912, mean_q: 5.251182
 40445/100000: episode: 4129, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.704, mean reward: 0.370 [0.356, 0.453], mean action: 57.200 [13.000, 85.000], mean observation: 3.146 [-1.171, 10.221], loss: 1.079125, mae: 5.011406, mean_q: 5.251811
 40455/100000: episode: 4130, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.821, mean reward: 0.382 [0.354, 0.401], mean action: 72.700 [15.000, 95.000], mean observation: 3.150 [-1.032, 10.246], loss: 1.143675, mae: 5.011928, mean_q: 5.253024
 40465/100000: episode: 4131, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.904, mean reward: 0.490 [0.403, 0.528], mean action: 80.900 [52.000, 89.000], mean observation: 3.153 [-0.947, 10.416], loss: 1.063512, mae: 5.011832, mean_q: 5.252658
 40475/100000: episode: 4132, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.285, mean reward: 0.429 [0.371, 0.466], mean action: 60.500 [4.000, 85.000], mean observation: 3.149 [-1.178, 10.324], loss: 1.560448, mae: 5.013919, mean_q: 5.252885
 40485/100000: episode: 4133, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.903, mean reward: 0.390 [0.341, 0.415], mean action: 75.400 [25.000, 86.000], mean observation: 3.162 [-1.558, 10.444], loss: 1.416978, mae: 5.013224, mean_q: 5.254512
 40495/100000: episode: 4134, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.365, mean reward: 0.437 [0.365, 0.564], mean action: 67.100 [18.000, 85.000], mean observation: 3.148 [-1.467, 10.284], loss: 1.285846, mae: 5.012689, mean_q: 5.255239
 40505/100000: episode: 4135, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.928, mean reward: 0.393 [0.351, 0.434], mean action: 76.500 [23.000, 100.000], mean observation: 3.160 [-1.356, 10.364], loss: 1.242299, mae: 5.012615, mean_q: 5.255094
 40515/100000: episode: 4136, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.843, mean reward: 0.384 [0.348, 0.399], mean action: 55.100 [9.000, 100.000], mean observation: 3.156 [-1.030, 10.317], loss: 0.946695, mae: 5.011480, mean_q: 5.256711
 40525/100000: episode: 4137, duration: 0.117s, episode steps: 10, steps per second: 86, episode reward: 4.274, mean reward: 0.427 [0.384, 0.455], mean action: 71.600 [19.000, 85.000], mean observation: 3.145 [-1.649, 10.246], loss: 1.326097, mae: 5.013185, mean_q: 5.259190
 40535/100000: episode: 4138, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.362, mean reward: 0.436 [0.386, 0.467], mean action: 72.300 [8.000, 96.000], mean observation: 3.143 [-1.406, 10.283], loss: 1.343487, mae: 5.013169, mean_q: 5.261186
 40545/100000: episode: 4139, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.832, mean reward: 0.383 [0.308, 0.505], mean action: 64.000 [7.000, 85.000], mean observation: 3.137 [-1.473, 10.253], loss: 1.035834, mae: 5.012138, mean_q: 5.259976
 40555/100000: episode: 4140, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 3.262, mean reward: 0.326 [0.310, 0.368], mean action: 83.100 [76.000, 85.000], mean observation: 3.173 [-0.670, 10.344], loss: 1.112589, mae: 5.012705, mean_q: 5.254490
 40565/100000: episode: 4141, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.555, mean reward: 0.355 [0.293, 0.411], mean action: 60.300 [18.000, 85.000], mean observation: 3.157 [-1.268, 10.359], loss: 1.853129, mae: 5.015455, mean_q: 5.251101
 40575/100000: episode: 4142, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 3.906, mean reward: 0.391 [0.350, 0.478], mean action: 68.600 [11.000, 100.000], mean observation: 3.155 [-1.905, 10.347], loss: 1.482567, mae: 5.013459, mean_q: 5.252127
 40585/100000: episode: 4143, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.812, mean reward: 0.381 [0.339, 0.436], mean action: 58.300 [22.000, 90.000], mean observation: 3.155 [-1.102, 10.253], loss: 1.507957, mae: 5.013227, mean_q: 5.250251
 40595/100000: episode: 4144, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.713, mean reward: 0.371 [0.350, 0.394], mean action: 79.900 [51.000, 88.000], mean observation: 3.154 [-1.534, 10.345], loss: 1.133752, mae: 5.011533, mean_q: 5.246409
 40605/100000: episode: 4145, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.399, mean reward: 0.440 [0.384, 0.563], mean action: 67.100 [25.000, 93.000], mean observation: 3.170 [-1.286, 10.528], loss: 1.253100, mae: 5.012197, mean_q: 5.243604
 40615/100000: episode: 4146, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.144, mean reward: 0.414 [0.334, 0.504], mean action: 69.600 [10.000, 85.000], mean observation: 3.152 [-1.658, 10.356], loss: 0.992491, mae: 5.011748, mean_q: 5.239241
 40625/100000: episode: 4147, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.231, mean reward: 0.423 [0.346, 0.506], mean action: 71.000 [28.000, 85.000], mean observation: 3.149 [-1.377, 10.316], loss: 1.245262, mae: 5.012891, mean_q: 5.237819
 40635/100000: episode: 4148, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.472, mean reward: 0.447 [0.361, 0.479], mean action: 74.800 [2.000, 101.000], mean observation: 3.144 [-1.575, 10.389], loss: 1.158552, mae: 5.012722, mean_q: 5.236767
 40645/100000: episode: 4149, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.020, mean reward: 0.402 [0.359, 0.441], mean action: 55.300 [10.000, 90.000], mean observation: 3.147 [-0.770, 10.378], loss: 1.171638, mae: 5.012938, mean_q: 5.235605
 40650/100000: episode: 4150, duration: 0.079s, episode steps: 5, steps per second: 63, episode reward: 11.534, mean reward: 2.307 [0.383, 10.000], mean action: 73.200 [38.000, 90.000], mean observation: 3.150 [-1.649, 10.197], loss: 1.464878, mae: 5.014197, mean_q: 5.235490
 40660/100000: episode: 4151, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.678, mean reward: 0.368 [0.284, 0.502], mean action: 71.000 [28.000, 85.000], mean observation: 3.154 [-1.019, 10.322], loss: 1.013533, mae: 5.012393, mean_q: 5.236604
 40663/100000: episode: 4152, duration: 0.064s, episode steps: 3, steps per second: 47, episode reward: 10.817, mean reward: 3.606 [0.408, 10.000], mean action: 61.667 [15.000, 85.000], mean observation: 3.147 [-1.231, 10.294], loss: 0.471683, mae: 5.010353, mean_q: 5.238115
 40673/100000: episode: 4153, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.566, mean reward: 0.457 [0.405, 0.510], mean action: 64.000 [22.000, 85.000], mean observation: 3.156 [-2.119, 10.407], loss: 1.218875, mae: 5.013430, mean_q: 5.239326
 40683/100000: episode: 4154, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.843, mean reward: 0.384 [0.349, 0.442], mean action: 68.400 [1.000, 85.000], mean observation: 3.150 [-1.397, 10.354], loss: 1.146995, mae: 5.013089, mean_q: 5.241624
 40693/100000: episode: 4155, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.624, mean reward: 0.462 [0.426, 0.543], mean action: 63.200 [9.000, 85.000], mean observation: 3.153 [-1.194, 10.356], loss: 1.130210, mae: 5.013403, mean_q: 5.241492
 40703/100000: episode: 4156, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.635, mean reward: 0.363 [0.318, 0.422], mean action: 59.000 [7.000, 97.000], mean observation: 3.150 [-1.441, 10.360], loss: 1.331119, mae: 5.014277, mean_q: 5.239748
 40713/100000: episode: 4157, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 3.770, mean reward: 0.377 [0.368, 0.438], mean action: 85.800 [72.000, 98.000], mean observation: 3.158 [-1.409, 10.366], loss: 1.276973, mae: 5.014192, mean_q: 5.240862
 40723/100000: episode: 4158, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.570, mean reward: 0.457 [0.457, 0.458], mean action: 65.000 [9.000, 87.000], mean observation: 3.146 [-1.234, 10.317], loss: 0.933599, mae: 5.013106, mean_q: 5.235723
 40733/100000: episode: 4159, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.076, mean reward: 0.408 [0.336, 0.443], mean action: 68.200 [23.000, 93.000], mean observation: 3.143 [-1.091, 10.299], loss: 1.116784, mae: 5.013753, mean_q: 5.232837
 40743/100000: episode: 4160, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 5.581, mean reward: 0.558 [0.558, 0.558], mean action: 69.200 [22.000, 86.000], mean observation: 3.152 [-0.973, 10.372], loss: 1.230311, mae: 5.014304, mean_q: 5.231927
 40753/100000: episode: 4161, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.177, mean reward: 0.418 [0.331, 0.437], mean action: 52.900 [0.000, 86.000], mean observation: 3.162 [-1.581, 10.293], loss: 1.419932, mae: 5.015063, mean_q: 5.231626
 40763/100000: episode: 4162, duration: 0.126s, episode steps: 10, steps per second: 80, episode reward: 3.616, mean reward: 0.362 [0.311, 0.497], mean action: 68.700 [3.000, 94.000], mean observation: 3.151 [-1.064, 10.269], loss: 0.845820, mae: 5.013110, mean_q: 5.229676
 40773/100000: episode: 4163, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.370, mean reward: 0.437 [0.398, 0.523], mean action: 55.200 [14.000, 85.000], mean observation: 3.159 [-1.331, 10.434], loss: 1.643680, mae: 5.016307, mean_q: 5.228322
 40783/100000: episode: 4164, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 3.688, mean reward: 0.369 [0.339, 0.467], mean action: 69.100 [14.000, 90.000], mean observation: 3.174 [-1.515, 10.302], loss: 0.852650, mae: 5.013194, mean_q: 5.229249
 40793/100000: episode: 4165, duration: 0.131s, episode steps: 10, steps per second: 77, episode reward: 3.928, mean reward: 0.393 [0.337, 0.494], mean action: 63.400 [20.000, 93.000], mean observation: 3.149 [-1.574, 10.252], loss: 1.084374, mae: 5.014409, mean_q: 5.228510
 40803/100000: episode: 4166, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.860, mean reward: 0.386 [0.355, 0.436], mean action: 69.300 [7.000, 85.000], mean observation: 3.155 [-1.517, 10.226], loss: 1.362636, mae: 5.015794, mean_q: 5.228368
 40813/100000: episode: 4167, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.415, mean reward: 0.342 [0.313, 0.399], mean action: 70.300 [27.000, 85.000], mean observation: 3.152 [-0.669, 10.317], loss: 0.843442, mae: 5.014020, mean_q: 5.231191
 40823/100000: episode: 4168, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.223, mean reward: 0.422 [0.383, 0.514], mean action: 72.400 [20.000, 94.000], mean observation: 3.146 [-0.926, 10.269], loss: 1.137904, mae: 5.015751, mean_q: 5.231687
 40833/100000: episode: 4169, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.177, mean reward: 0.418 [0.330, 0.508], mean action: 60.500 [19.000, 85.000], mean observation: 3.148 [-1.257, 10.344], loss: 1.082566, mae: 5.015753, mean_q: 5.231748
 40837/100000: episode: 4170, duration: 0.071s, episode steps: 4, steps per second: 57, episode reward: 11.293, mean reward: 2.823 [0.425, 10.000], mean action: 65.750 [39.000, 86.000], mean observation: 3.153 [-1.403, 10.272], loss: 1.134036, mae: 5.016475, mean_q: 5.231411
 40847/100000: episode: 4171, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 3.952, mean reward: 0.395 [0.366, 0.494], mean action: 79.500 [51.000, 99.000], mean observation: 3.160 [-1.183, 10.444], loss: 1.426045, mae: 5.017815, mean_q: 5.232067
 40857/100000: episode: 4172, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.766, mean reward: 0.377 [0.303, 0.432], mean action: 62.800 [8.000, 92.000], mean observation: 3.154 [-1.167, 10.421], loss: 1.403544, mae: 5.017717, mean_q: 5.232914
 40867/100000: episode: 4173, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.373, mean reward: 0.437 [0.334, 0.482], mean action: 58.500 [22.000, 86.000], mean observation: 3.175 [-1.340, 10.278], loss: 1.480788, mae: 5.018126, mean_q: 5.234995
 40877/100000: episode: 4174, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 4.069, mean reward: 0.407 [0.368, 0.507], mean action: 78.700 [26.000, 91.000], mean observation: 3.153 [-1.185, 10.241], loss: 1.326466, mae: 5.017596, mean_q: 5.236302
 40887/100000: episode: 4175, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.745, mean reward: 0.374 [0.308, 0.537], mean action: 69.900 [29.000, 100.000], mean observation: 3.144 [-1.213, 10.346], loss: 1.345596, mae: 5.017461, mean_q: 5.237808
 40897/100000: episode: 4176, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 3.929, mean reward: 0.393 [0.379, 0.450], mean action: 77.000 [20.000, 98.000], mean observation: 3.168 [-0.838, 10.262], loss: 1.629735, mae: 5.018424, mean_q: 5.239990
 40907/100000: episode: 4177, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 4.153, mean reward: 0.415 [0.412, 0.447], mean action: 85.300 [79.000, 86.000], mean observation: 3.172 [-1.082, 10.344], loss: 1.307676, mae: 5.016866, mean_q: 5.242245
 40917/100000: episode: 4178, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.751, mean reward: 0.375 [0.347, 0.469], mean action: 75.700 [15.000, 98.000], mean observation: 3.145 [-1.362, 10.381], loss: 1.241795, mae: 5.016437, mean_q: 5.243050
 40927/100000: episode: 4179, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.374, mean reward: 0.437 [0.430, 0.459], mean action: 64.800 [14.000, 89.000], mean observation: 3.172 [-1.663, 10.418], loss: 1.348138, mae: 5.017040, mean_q: 5.238978
 40937/100000: episode: 4180, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.502, mean reward: 0.450 [0.443, 0.455], mean action: 58.100 [7.000, 99.000], mean observation: 3.146 [-1.028, 10.312], loss: 0.877144, mae: 5.015193, mean_q: 5.237475
 40947/100000: episode: 4181, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.912, mean reward: 0.391 [0.304, 0.529], mean action: 60.500 [7.000, 87.000], mean observation: 3.160 [-0.849, 10.401], loss: 1.636313, mae: 5.018427, mean_q: 5.238042
 40957/100000: episode: 4182, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.973, mean reward: 0.397 [0.383, 0.453], mean action: 72.100 [26.000, 86.000], mean observation: 3.153 [-1.048, 10.236], loss: 1.420422, mae: 5.017272, mean_q: 5.239880
 40967/100000: episode: 4183, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 3.912, mean reward: 0.391 [0.367, 0.462], mean action: 80.200 [36.000, 98.000], mean observation: 3.154 [-0.927, 10.370], loss: 1.191384, mae: 5.016407, mean_q: 5.241446
 40977/100000: episode: 4184, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.078, mean reward: 0.408 [0.367, 0.437], mean action: 61.800 [10.000, 89.000], mean observation: 3.172 [-0.883, 10.271], loss: 1.004806, mae: 5.016136, mean_q: 5.242743
 40987/100000: episode: 4185, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.404, mean reward: 0.440 [0.433, 0.466], mean action: 52.000 [10.000, 86.000], mean observation: 3.153 [-2.470, 10.295], loss: 1.006691, mae: 5.016553, mean_q: 5.244546
 40997/100000: episode: 4186, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.626, mean reward: 0.463 [0.460, 0.486], mean action: 62.300 [19.000, 86.000], mean observation: 3.155 [-1.672, 10.328], loss: 1.111192, mae: 5.017338, mean_q: 5.246627
 41007/100000: episode: 4187, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.023, mean reward: 0.402 [0.358, 0.454], mean action: 37.800 [2.000, 97.000], mean observation: 3.152 [-1.158, 10.275], loss: 1.120194, mae: 5.017610, mean_q: 5.248387
 41017/100000: episode: 4188, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.802, mean reward: 0.380 [0.343, 0.406], mean action: 75.500 [3.000, 98.000], mean observation: 3.162 [-0.897, 10.332], loss: 1.250200, mae: 5.018419, mean_q: 5.249936
 41027/100000: episode: 4189, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 3.853, mean reward: 0.385 [0.351, 0.454], mean action: 80.600 [45.000, 101.000], mean observation: 3.142 [-1.680, 10.294], loss: 1.090902, mae: 5.017919, mean_q: 5.250886
 41037/100000: episode: 4190, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.037, mean reward: 0.404 [0.325, 0.489], mean action: 65.800 [20.000, 94.000], mean observation: 3.158 [-1.189, 10.350], loss: 0.919136, mae: 5.017988, mean_q: 5.251684
 41047/100000: episode: 4191, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.740, mean reward: 0.374 [0.354, 0.525], mean action: 75.100 [26.000, 87.000], mean observation: 3.165 [-0.632, 10.319], loss: 0.842375, mae: 5.018373, mean_q: 5.247716
 41057/100000: episode: 4192, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 3.876, mean reward: 0.388 [0.387, 0.389], mean action: 78.100 [27.000, 97.000], mean observation: 3.151 [-0.818, 10.309], loss: 1.324987, mae: 5.020828, mean_q: 5.245323
 41067/100000: episode: 4193, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 5.547, mean reward: 0.555 [0.555, 0.555], mean action: 81.000 [47.000, 97.000], mean observation: 3.173 [-2.005, 10.318], loss: 1.185683, mae: 5.020367, mean_q: 5.246052
 41077/100000: episode: 4194, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.112, mean reward: 0.411 [0.347, 0.432], mean action: 63.800 [11.000, 86.000], mean observation: 3.159 [-1.198, 10.270], loss: 1.185637, mae: 5.020671, mean_q: 5.248020
 41087/100000: episode: 4195, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 5.245, mean reward: 0.524 [0.524, 0.524], mean action: 79.300 [32.000, 89.000], mean observation: 3.147 [-1.174, 10.225], loss: 1.158202, mae: 5.020879, mean_q: 5.250713
 41097/100000: episode: 4196, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.790, mean reward: 0.379 [0.319, 0.486], mean action: 76.500 [27.000, 98.000], mean observation: 3.164 [-1.459, 10.209], loss: 1.331485, mae: 5.021935, mean_q: 5.253616
 41107/100000: episode: 4197, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.489, mean reward: 0.449 [0.378, 0.517], mean action: 48.900 [0.000, 86.000], mean observation: 3.153 [-1.455, 10.328], loss: 1.529439, mae: 5.022729, mean_q: 5.257065
 41117/100000: episode: 4198, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.705, mean reward: 0.471 [0.382, 0.507], mean action: 66.600 [13.000, 94.000], mean observation: 3.170 [-1.880, 10.296], loss: 1.387725, mae: 5.022508, mean_q: 5.258858
 41127/100000: episode: 4199, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 4.214, mean reward: 0.421 [0.394, 0.491], mean action: 75.100 [41.000, 86.000], mean observation: 3.141 [-1.458, 10.314], loss: 1.122578, mae: 5.021513, mean_q: 5.258724
 41137/100000: episode: 4200, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.285, mean reward: 0.428 [0.358, 0.532], mean action: 59.800 [0.000, 93.000], mean observation: 3.150 [-1.456, 10.321], loss: 0.997060, mae: 5.021237, mean_q: 5.251886
 41147/100000: episode: 4201, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.977, mean reward: 0.398 [0.351, 0.507], mean action: 72.500 [48.000, 86.000], mean observation: 3.169 [-1.085, 10.359], loss: 1.305149, mae: 5.022551, mean_q: 5.240415
 41157/100000: episode: 4202, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.839, mean reward: 0.384 [0.345, 0.532], mean action: 71.900 [14.000, 94.000], mean observation: 3.152 [-0.723, 10.341], loss: 1.568110, mae: 5.023465, mean_q: 5.233214
 41167/100000: episode: 4203, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.810, mean reward: 0.481 [0.471, 0.530], mean action: 68.600 [34.000, 86.000], mean observation: 3.146 [-1.411, 10.233], loss: 1.078096, mae: 5.021585, mean_q: 5.228987
 41168/100000: episode: 4204, duration: 0.026s, episode steps: 1, steps per second: 38, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 86.000 [86.000, 86.000], mean observation: 3.166 [-1.014, 10.631], loss: 1.810179, mae: 5.024764, mean_q: 5.228582
 41178/100000: episode: 4205, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.952, mean reward: 0.495 [0.495, 0.502], mean action: 83.300 [59.000, 86.000], mean observation: 3.170 [-0.981, 10.434], loss: 1.417743, mae: 5.022995, mean_q: 5.228697
 41188/100000: episode: 4206, duration: 0.107s, episode steps: 10, steps per second: 94, episode reward: 4.637, mean reward: 0.464 [0.459, 0.482], mean action: 71.700 [7.000, 91.000], mean observation: 3.159 [-1.763, 10.359], loss: 1.179966, mae: 5.022072, mean_q: 5.230543
 41198/100000: episode: 4207, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.498, mean reward: 0.450 [0.438, 0.451], mean action: 70.500 [2.000, 93.000], mean observation: 3.148 [-1.391, 10.345], loss: 1.358460, mae: 5.022860, mean_q: 5.233824
 41208/100000: episode: 4208, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 4.903, mean reward: 0.490 [0.357, 0.579], mean action: 79.300 [1.000, 99.000], mean observation: 3.152 [-1.325, 10.412], loss: 1.376501, mae: 5.023140, mean_q: 5.237015
 41218/100000: episode: 4209, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.537, mean reward: 0.354 [0.324, 0.401], mean action: 60.900 [1.000, 89.000], mean observation: 3.139 [-1.788, 10.384], loss: 1.086729, mae: 5.022046, mean_q: 5.239815
 41228/100000: episode: 4210, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.967, mean reward: 0.397 [0.352, 0.454], mean action: 81.000 [61.000, 86.000], mean observation: 3.149 [-0.887, 10.413], loss: 0.971696, mae: 5.021686, mean_q: 5.241632
 41238/100000: episode: 4211, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.476, mean reward: 0.348 [0.327, 0.385], mean action: 66.200 [25.000, 92.000], mean observation: 3.143 [-1.230, 10.227], loss: 1.316196, mae: 5.023155, mean_q: 5.243286
 41248/100000: episode: 4212, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.201, mean reward: 0.420 [0.408, 0.432], mean action: 71.300 [1.000, 97.000], mean observation: 3.154 [-1.388, 10.275], loss: 1.443278, mae: 5.023686, mean_q: 5.243863
 41258/100000: episode: 4213, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.625, mean reward: 0.463 [0.396, 0.499], mean action: 78.200 [32.000, 97.000], mean observation: 3.167 [-1.174, 10.444], loss: 1.040285, mae: 5.021993, mean_q: 5.240308
 41268/100000: episode: 4214, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.913, mean reward: 0.391 [0.368, 0.416], mean action: 65.100 [6.000, 88.000], mean observation: 3.143 [-0.930, 10.398], loss: 1.153827, mae: 5.022602, mean_q: 5.240948
 41278/100000: episode: 4215, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.905, mean reward: 0.391 [0.378, 0.416], mean action: 73.900 [12.000, 86.000], mean observation: 3.147 [-1.191, 10.239], loss: 1.310432, mae: 5.023141, mean_q: 5.243478
 41288/100000: episode: 4216, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.902, mean reward: 0.390 [0.373, 0.527], mean action: 82.000 [48.000, 91.000], mean observation: 3.157 [-0.993, 10.331], loss: 1.241464, mae: 5.022724, mean_q: 5.245803
 41298/100000: episode: 4217, duration: 0.117s, episode steps: 10, steps per second: 86, episode reward: 4.079, mean reward: 0.408 [0.361, 0.567], mean action: 71.800 [10.000, 86.000], mean observation: 3.153 [-0.986, 10.237], loss: 1.077761, mae: 5.022287, mean_q: 5.247192
 41308/100000: episode: 4218, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 4.425, mean reward: 0.442 [0.333, 0.474], mean action: 73.700 [13.000, 91.000], mean observation: 3.152 [-0.955, 10.293], loss: 1.114689, mae: 5.022761, mean_q: 5.248309
 41318/100000: episode: 4219, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.359, mean reward: 0.436 [0.420, 0.474], mean action: 62.600 [26.000, 86.000], mean observation: 3.157 [-1.230, 10.312], loss: 1.208793, mae: 5.023311, mean_q: 5.249326
 41328/100000: episode: 4220, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.103, mean reward: 0.410 [0.391, 0.445], mean action: 70.700 [22.000, 86.000], mean observation: 3.167 [-1.297, 10.284], loss: 1.089504, mae: 5.023175, mean_q: 5.251100
 41338/100000: episode: 4221, duration: 0.118s, episode steps: 10, steps per second: 84, episode reward: 4.321, mean reward: 0.432 [0.367, 0.552], mean action: 69.700 [14.000, 87.000], mean observation: 3.152 [-1.114, 10.293], loss: 1.346680, mae: 5.024225, mean_q: 5.252540
 41348/100000: episode: 4222, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.295, mean reward: 0.430 [0.376, 0.505], mean action: 64.500 [2.000, 90.000], mean observation: 3.166 [-1.113, 10.345], loss: 1.284189, mae: 5.024182, mean_q: 5.253784
 41358/100000: episode: 4223, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.588, mean reward: 0.459 [0.374, 0.503], mean action: 78.100 [26.000, 89.000], mean observation: 3.161 [-1.963, 10.247], loss: 1.390489, mae: 5.024890, mean_q: 5.255528
 41368/100000: episode: 4224, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.906, mean reward: 0.391 [0.323, 0.493], mean action: 75.400 [30.000, 86.000], mean observation: 3.157 [-0.942, 10.363], loss: 1.364947, mae: 5.024528, mean_q: 5.257236
 41378/100000: episode: 4225, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.080, mean reward: 0.408 [0.405, 0.432], mean action: 68.400 [37.000, 99.000], mean observation: 3.154 [-1.537, 10.279], loss: 1.223277, mae: 5.023890, mean_q: 5.256445
 41388/100000: episode: 4226, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.254, mean reward: 0.425 [0.309, 0.475], mean action: 67.900 [3.000, 101.000], mean observation: 3.166 [-1.506, 10.395], loss: 1.030152, mae: 5.023363, mean_q: 5.254502
 41398/100000: episode: 4227, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.516, mean reward: 0.452 [0.452, 0.452], mean action: 70.100 [23.000, 86.000], mean observation: 3.156 [-1.339, 10.296], loss: 0.939693, mae: 5.023349, mean_q: 5.256787
 41408/100000: episode: 4228, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.918, mean reward: 0.392 [0.326, 0.576], mean action: 73.000 [7.000, 92.000], mean observation: 3.154 [-0.853, 10.368], loss: 1.519371, mae: 5.025823, mean_q: 5.261043
 41418/100000: episode: 4229, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 3.733, mean reward: 0.373 [0.363, 0.467], mean action: 84.500 [70.000, 87.000], mean observation: 3.157 [-1.157, 10.370], loss: 1.227961, mae: 5.024735, mean_q: 5.264829
 41428/100000: episode: 4230, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 3.432, mean reward: 0.343 [0.315, 0.389], mean action: 71.100 [49.000, 86.000], mean observation: 3.175 [-1.247, 10.228], loss: 1.233623, mae: 5.024723, mean_q: 5.267514
 41438/100000: episode: 4231, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.575, mean reward: 0.458 [0.423, 0.464], mean action: 78.100 [23.000, 86.000], mean observation: 3.170 [-1.447, 10.300], loss: 1.222049, mae: 5.025148, mean_q: 5.270076
 41448/100000: episode: 4232, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 3.922, mean reward: 0.392 [0.359, 0.479], mean action: 73.900 [31.000, 92.000], mean observation: 3.160 [-0.757, 10.401], loss: 1.226028, mae: 5.025114, mean_q: 5.273042
 41458/100000: episode: 4233, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.654, mean reward: 0.365 [0.325, 0.441], mean action: 61.400 [10.000, 93.000], mean observation: 3.162 [-1.676, 10.446], loss: 0.975424, mae: 5.024649, mean_q: 5.277087
 41468/100000: episode: 4234, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.736, mean reward: 0.374 [0.359, 0.387], mean action: 52.000 [0.000, 86.000], mean observation: 3.145 [-1.326, 10.379], loss: 0.916018, mae: 5.025000, mean_q: 5.281320
 41478/100000: episode: 4235, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.989, mean reward: 0.399 [0.356, 0.464], mean action: 61.800 [13.000, 86.000], mean observation: 3.157 [-1.163, 10.338], loss: 1.131429, mae: 5.026347, mean_q: 5.281189
 41488/100000: episode: 4236, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.091, mean reward: 0.409 [0.379, 0.446], mean action: 53.600 [19.000, 86.000], mean observation: 3.166 [-0.937, 10.477], loss: 1.426481, mae: 5.027642, mean_q: 5.277170
 41498/100000: episode: 4237, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.235, mean reward: 0.424 [0.335, 0.543], mean action: 48.500 [8.000, 86.000], mean observation: 3.148 [-1.630, 10.317], loss: 1.325258, mae: 5.027195, mean_q: 5.269413
 41508/100000: episode: 4238, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.889, mean reward: 0.389 [0.338, 0.447], mean action: 68.300 [23.000, 86.000], mean observation: 3.178 [-1.350, 10.314], loss: 1.352531, mae: 5.026896, mean_q: 5.267495
 41518/100000: episode: 4239, duration: 0.113s, episode steps: 10, steps per second: 88, episode reward: 3.794, mean reward: 0.379 [0.343, 0.424], mean action: 74.700 [35.000, 86.000], mean observation: 3.157 [-1.267, 10.291], loss: 1.246568, mae: 5.026351, mean_q: 5.267315
 41528/100000: episode: 4240, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.587, mean reward: 0.459 [0.355, 0.480], mean action: 66.300 [5.000, 86.000], mean observation: 3.149 [-0.885, 10.245], loss: 1.267604, mae: 5.026391, mean_q: 5.266935
 41538/100000: episode: 4241, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 3.883, mean reward: 0.388 [0.358, 0.483], mean action: 77.900 [19.000, 97.000], mean observation: 3.137 [-1.664, 10.473], loss: 1.303349, mae: 5.026675, mean_q: 5.264044
 41543/100000: episode: 4242, duration: 0.076s, episode steps: 5, steps per second: 66, episode reward: 11.822, mean reward: 2.364 [0.439, 10.000], mean action: 71.400 [13.000, 86.000], mean observation: 3.148 [-1.349, 10.414], loss: 1.171503, mae: 5.026796, mean_q: 5.263780
 41553/100000: episode: 4243, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.332, mean reward: 0.433 [0.400, 0.493], mean action: 78.700 [24.000, 86.000], mean observation: 3.143 [-1.880, 10.382], loss: 1.023398, mae: 5.026423, mean_q: 5.264132
 41563/100000: episode: 4244, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.584, mean reward: 0.458 [0.406, 0.531], mean action: 73.000 [37.000, 97.000], mean observation: 3.158 [-1.072, 10.342], loss: 1.162063, mae: 5.027389, mean_q: 5.263683
 41573/100000: episode: 4245, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.147, mean reward: 0.415 [0.366, 0.489], mean action: 71.300 [17.000, 86.000], mean observation: 3.163 [-1.040, 10.422], loss: 1.262605, mae: 5.028221, mean_q: 5.260308
 41583/100000: episode: 4246, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.601, mean reward: 0.360 [0.343, 0.399], mean action: 62.700 [30.000, 86.000], mean observation: 3.155 [-1.599, 10.230], loss: 1.337210, mae: 5.028725, mean_q: 5.251027
 41593/100000: episode: 4247, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 3.457, mean reward: 0.346 [0.295, 0.440], mean action: 77.200 [30.000, 86.000], mean observation: 3.142 [-1.552, 10.254], loss: 1.377301, mae: 5.029047, mean_q: 5.246375
 41603/100000: episode: 4248, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.285, mean reward: 0.428 [0.324, 0.463], mean action: 56.100 [11.000, 86.000], mean observation: 3.158 [-1.224, 10.243], loss: 1.297575, mae: 5.028470, mean_q: 5.242260
 41613/100000: episode: 4249, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.996, mean reward: 0.400 [0.335, 0.463], mean action: 56.400 [14.000, 101.000], mean observation: 3.158 [-1.744, 10.255], loss: 1.029036, mae: 5.027435, mean_q: 5.237178
 41623/100000: episode: 4250, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.630, mean reward: 0.463 [0.410, 0.546], mean action: 71.100 [35.000, 92.000], mean observation: 3.140 [-1.498, 10.210], loss: 1.367738, mae: 5.028997, mean_q: 5.237575
 41630/100000: episode: 4251, duration: 0.089s, episode steps: 7, steps per second: 78, episode reward: 12.118, mean reward: 1.731 [0.339, 10.000], mean action: 72.571 [31.000, 86.000], mean observation: 3.159 [-0.855, 10.318], loss: 1.182243, mae: 5.028617, mean_q: 5.240211
 41640/100000: episode: 4252, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.678, mean reward: 0.468 [0.379, 0.496], mean action: 69.100 [11.000, 98.000], mean observation: 3.149 [-1.261, 10.227], loss: 0.977621, mae: 5.027822, mean_q: 5.243291
 41650/100000: episode: 4253, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.978, mean reward: 0.398 [0.290, 0.556], mean action: 65.000 [7.000, 86.000], mean observation: 3.158 [-0.731, 10.338], loss: 1.056836, mae: 5.028420, mean_q: 5.246115
 41660/100000: episode: 4254, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.226, mean reward: 0.423 [0.340, 0.530], mean action: 67.100 [21.000, 86.000], mean observation: 3.156 [-1.308, 10.265], loss: 1.415330, mae: 5.029899, mean_q: 5.250040
 41670/100000: episode: 4255, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.223, mean reward: 0.422 [0.391, 0.452], mean action: 61.700 [5.000, 92.000], mean observation: 3.166 [-1.184, 10.343], loss: 0.811245, mae: 5.027585, mean_q: 5.252281
 41680/100000: episode: 4256, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.009, mean reward: 0.401 [0.333, 0.453], mean action: 68.900 [5.000, 99.000], mean observation: 3.151 [-1.279, 10.327], loss: 1.156023, mae: 5.029075, mean_q: 5.246728
 41690/100000: episode: 4257, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 4.638, mean reward: 0.464 [0.407, 0.478], mean action: 78.500 [26.000, 101.000], mean observation: 3.150 [-0.832, 10.310], loss: 1.110981, mae: 5.029267, mean_q: 5.244525
 41700/100000: episode: 4258, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 3.825, mean reward: 0.383 [0.323, 0.393], mean action: 79.400 [39.000, 99.000], mean observation: 3.165 [-1.501, 10.356], loss: 1.515921, mae: 5.030771, mean_q: 5.244542
 41710/100000: episode: 4259, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.477, mean reward: 0.348 [0.304, 0.440], mean action: 60.300 [12.000, 91.000], mean observation: 3.155 [-1.244, 10.345], loss: 1.215276, mae: 5.029553, mean_q: 5.245764
 41720/100000: episode: 4260, duration: 0.109s, episode steps: 10, steps per second: 91, episode reward: 4.240, mean reward: 0.424 [0.415, 0.475], mean action: 75.200 [43.000, 99.000], mean observation: 3.161 [-2.197, 10.312], loss: 1.282356, mae: 5.029930, mean_q: 5.247951
 41730/100000: episode: 4261, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.222, mean reward: 0.422 [0.385, 0.473], mean action: 61.000 [17.000, 91.000], mean observation: 3.151 [-2.012, 10.280], loss: 0.982877, mae: 5.028722, mean_q: 5.250503
 41740/100000: episode: 4262, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.294, mean reward: 0.429 [0.344, 0.557], mean action: 71.800 [12.000, 86.000], mean observation: 3.148 [-1.589, 10.301], loss: 1.428163, mae: 5.030450, mean_q: 5.252236
 41750/100000: episode: 4263, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.549, mean reward: 0.455 [0.404, 0.468], mean action: 73.200 [17.000, 86.000], mean observation: 3.185 [-1.135, 10.385], loss: 1.345745, mae: 5.029989, mean_q: 5.251063
 41760/100000: episode: 4264, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.564, mean reward: 0.456 [0.383, 0.582], mean action: 68.700 [14.000, 92.000], mean observation: 3.151 [-1.110, 10.367], loss: 1.150880, mae: 5.029415, mean_q: 5.249069
 41770/100000: episode: 4265, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 4.660, mean reward: 0.466 [0.455, 0.512], mean action: 53.300 [7.000, 86.000], mean observation: 3.131 [-1.056, 10.430], loss: 1.215960, mae: 5.029454, mean_q: 5.248466
 41780/100000: episode: 4266, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.075, mean reward: 0.408 [0.377, 0.552], mean action: 77.600 [57.000, 86.000], mean observation: 3.149 [-1.147, 10.254], loss: 0.978876, mae: 5.029155, mean_q: 5.250873
 41790/100000: episode: 4267, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 3.876, mean reward: 0.388 [0.364, 0.427], mean action: 65.600 [12.000, 86.000], mean observation: 3.149 [-1.872, 10.271], loss: 1.667919, mae: 5.031737, mean_q: 5.252744
 41800/100000: episode: 4268, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.096, mean reward: 0.410 [0.347, 0.568], mean action: 62.500 [24.000, 86.000], mean observation: 3.147 [-1.954, 10.318], loss: 1.099399, mae: 5.029408, mean_q: 5.251065
 41810/100000: episode: 4269, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.420, mean reward: 0.442 [0.336, 0.558], mean action: 71.100 [20.000, 86.000], mean observation: 3.161 [-1.623, 10.291], loss: 1.273564, mae: 5.030263, mean_q: 5.250516
 41820/100000: episode: 4270, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.311, mean reward: 0.431 [0.387, 0.486], mean action: 61.900 [18.000, 86.000], mean observation: 3.151 [-2.116, 10.384], loss: 1.450183, mae: 5.030810, mean_q: 5.251777
 41830/100000: episode: 4271, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.402, mean reward: 0.440 [0.394, 0.470], mean action: 77.500 [32.000, 87.000], mean observation: 3.145 [-1.239, 10.224], loss: 1.283184, mae: 5.029948, mean_q: 5.251930
 41840/100000: episode: 4272, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 3.677, mean reward: 0.368 [0.358, 0.435], mean action: 80.000 [20.000, 92.000], mean observation: 3.155 [-1.331, 10.246], loss: 1.249370, mae: 5.029699, mean_q: 5.248085
 41850/100000: episode: 4273, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.672, mean reward: 0.367 [0.308, 0.422], mean action: 60.200 [19.000, 86.000], mean observation: 3.156 [-1.116, 10.306], loss: 1.394509, mae: 5.030182, mean_q: 5.243716
 41860/100000: episode: 4274, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 3.995, mean reward: 0.400 [0.307, 0.443], mean action: 58.400 [26.000, 82.000], mean observation: 3.165 [-1.902, 10.429], loss: 1.139433, mae: 5.029099, mean_q: 5.245503
 41870/100000: episode: 4275, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.793, mean reward: 0.379 [0.343, 0.425], mean action: 49.600 [3.000, 70.000], mean observation: 3.162 [-1.022, 10.372], loss: 1.142021, mae: 5.029150, mean_q: 5.247273
 41880/100000: episode: 4276, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.898, mean reward: 0.390 [0.345, 0.454], mean action: 43.300 [3.000, 79.000], mean observation: 3.161 [-1.472, 10.279], loss: 1.483366, mae: 5.030624, mean_q: 5.249253
 41890/100000: episode: 4277, duration: 0.150s, episode steps: 10, steps per second: 66, episode reward: 3.847, mean reward: 0.385 [0.380, 0.425], mean action: 56.700 [52.000, 91.000], mean observation: 3.158 [-1.296, 10.199], loss: 1.257852, mae: 5.029762, mean_q: 5.250838
 41900/100000: episode: 4278, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.800, mean reward: 0.380 [0.359, 0.422], mean action: 58.900 [22.000, 95.000], mean observation: 3.163 [-1.020, 10.301], loss: 1.493994, mae: 5.030459, mean_q: 5.253074
 41910/100000: episode: 4279, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.001, mean reward: 0.400 [0.352, 0.471], mean action: 50.300 [21.000, 97.000], mean observation: 3.162 [-1.104, 10.302], loss: 0.967663, mae: 5.028185, mean_q: 5.255874
 41920/100000: episode: 4280, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.725, mean reward: 0.372 [0.322, 0.478], mean action: 54.200 [18.000, 82.000], mean observation: 3.162 [-1.593, 10.209], loss: 1.353857, mae: 5.029853, mean_q: 5.259013
 41930/100000: episode: 4281, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.655, mean reward: 0.365 [0.308, 0.466], mean action: 57.300 [23.000, 94.000], mean observation: 3.153 [-1.308, 10.246], loss: 1.244274, mae: 5.029605, mean_q: 5.261119
 41940/100000: episode: 4282, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.624, mean reward: 0.462 [0.404, 0.536], mean action: 50.500 [23.000, 97.000], mean observation: 3.165 [-1.505, 10.320], loss: 1.297446, mae: 5.029720, mean_q: 5.262846
 41950/100000: episode: 4283, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.912, mean reward: 0.391 [0.386, 0.424], mean action: 56.500 [35.000, 82.000], mean observation: 3.160 [-1.767, 10.279], loss: 1.434392, mae: 5.030347, mean_q: 5.261456
 41960/100000: episode: 4284, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.377, mean reward: 0.438 [0.437, 0.447], mean action: 49.900 [19.000, 79.000], mean observation: 3.166 [-1.914, 10.376], loss: 1.414053, mae: 5.030382, mean_q: 5.259146
 41970/100000: episode: 4285, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.023, mean reward: 0.402 [0.379, 0.485], mean action: 55.000 [22.000, 87.000], mean observation: 3.158 [-1.457, 10.369], loss: 1.431564, mae: 5.030379, mean_q: 5.259248
 41980/100000: episode: 4286, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.956, mean reward: 0.396 [0.278, 0.492], mean action: 41.200 [2.000, 74.000], mean observation: 3.155 [-1.035, 10.351], loss: 1.256472, mae: 5.029873, mean_q: 5.254308
 41990/100000: episode: 4287, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.574, mean reward: 0.457 [0.454, 0.486], mean action: 50.000 [43.000, 53.000], mean observation: 3.163 [-1.110, 10.352], loss: 1.464047, mae: 5.030391, mean_q: 5.248342
 42000/100000: episode: 4288, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.871, mean reward: 0.387 [0.349, 0.490], mean action: 49.400 [33.000, 101.000], mean observation: 3.158 [-1.071, 10.346], loss: 1.179656, mae: 5.029120, mean_q: 5.245231
 42010/100000: episode: 4289, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.059, mean reward: 0.406 [0.344, 0.450], mean action: 46.300 [17.000, 74.000], mean observation: 3.165 [-2.008, 10.395], loss: 1.110759, mae: 5.028839, mean_q: 5.245505
 42020/100000: episode: 4290, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.077, mean reward: 0.408 [0.339, 0.486], mean action: 48.600 [28.000, 97.000], mean observation: 3.159 [-1.132, 10.407], loss: 1.211650, mae: 5.029395, mean_q: 5.246537
 42030/100000: episode: 4291, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.369, mean reward: 0.437 [0.333, 0.581], mean action: 45.600 [30.000, 76.000], mean observation: 3.172 [-2.071, 10.375], loss: 1.267193, mae: 5.029823, mean_q: 5.247136
 42040/100000: episode: 4292, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.121, mean reward: 0.412 [0.345, 0.506], mean action: 26.900 [0.000, 43.000], mean observation: 3.158 [-1.465, 10.376], loss: 1.485839, mae: 5.030335, mean_q: 5.247876
 42050/100000: episode: 4293, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.988, mean reward: 0.399 [0.330, 0.456], mean action: 49.400 [3.000, 88.000], mean observation: 3.162 [-1.930, 10.395], loss: 1.292506, mae: 5.029563, mean_q: 5.249015
 42060/100000: episode: 4294, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.147, mean reward: 0.415 [0.405, 0.465], mean action: 47.900 [43.000, 66.000], mean observation: 3.162 [-1.892, 10.530], loss: 1.039470, mae: 5.028494, mean_q: 5.249768
 42070/100000: episode: 4295, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.233, mean reward: 0.423 [0.396, 0.561], mean action: 51.700 [43.000, 78.000], mean observation: 3.157 [-1.512, 10.439], loss: 1.517751, mae: 5.030152, mean_q: 5.250789
 42080/100000: episode: 4296, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.235, mean reward: 0.423 [0.417, 0.468], mean action: 42.400 [8.000, 69.000], mean observation: 3.144 [-1.060, 10.268], loss: 1.042608, mae: 5.028411, mean_q: 5.247945
 42090/100000: episode: 4297, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.224, mean reward: 0.422 [0.333, 0.515], mean action: 47.300 [4.000, 95.000], mean observation: 3.157 [-1.165, 10.358], loss: 1.259264, mae: 5.028989, mean_q: 5.247170
 42100/100000: episode: 4298, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.381, mean reward: 0.438 [0.303, 0.550], mean action: 38.600 [14.000, 66.000], mean observation: 3.160 [-1.597, 10.394], loss: 1.014097, mae: 5.028529, mean_q: 5.247575
 42110/100000: episode: 4299, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.767, mean reward: 0.377 [0.318, 0.487], mean action: 42.200 [7.000, 94.000], mean observation: 3.161 [-1.569, 10.536], loss: 1.101349, mae: 5.029202, mean_q: 5.243722
 42120/100000: episode: 4300, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.135, mean reward: 0.414 [0.351, 0.526], mean action: 38.900 [3.000, 94.000], mean observation: 3.145 [-1.863, 10.393], loss: 1.393684, mae: 5.030690, mean_q: 5.242649
 42130/100000: episode: 4301, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.354, mean reward: 0.435 [0.307, 0.587], mean action: 47.400 [10.000, 92.000], mean observation: 3.162 [-1.955, 10.299], loss: 1.360055, mae: 5.031005, mean_q: 5.243942
 42140/100000: episode: 4302, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.136, mean reward: 0.414 [0.301, 0.559], mean action: 41.900 [18.000, 92.000], mean observation: 3.146 [-1.442, 10.290], loss: 1.021178, mae: 5.029897, mean_q: 5.246009
 42150/100000: episode: 4303, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.893, mean reward: 0.389 [0.339, 0.412], mean action: 55.800 [1.000, 99.000], mean observation: 3.157 [-1.248, 10.272], loss: 1.005745, mae: 5.030373, mean_q: 5.247343
 42160/100000: episode: 4304, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.915, mean reward: 0.391 [0.359, 0.474], mean action: 53.300 [43.000, 99.000], mean observation: 3.151 [-1.175, 10.226], loss: 1.405549, mae: 5.032363, mean_q: 5.248415
 42170/100000: episode: 4305, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.159, mean reward: 0.416 [0.368, 0.537], mean action: 51.500 [9.000, 99.000], mean observation: 3.164 [-1.671, 10.366], loss: 1.213768, mae: 5.031807, mean_q: 5.249376
 42180/100000: episode: 4306, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.940, mean reward: 0.394 [0.368, 0.528], mean action: 56.800 [43.000, 89.000], mean observation: 3.150 [-1.508, 10.270], loss: 1.421583, mae: 5.032804, mean_q: 5.250250
 42190/100000: episode: 4307, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.003, mean reward: 0.400 [0.325, 0.463], mean action: 43.900 [18.000, 96.000], mean observation: 3.158 [-1.368, 10.421], loss: 1.194025, mae: 5.031762, mean_q: 5.251272
 42200/100000: episode: 4308, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.922, mean reward: 0.392 [0.317, 0.466], mean action: 42.700 [10.000, 96.000], mean observation: 3.158 [-1.343, 10.337], loss: 1.372911, mae: 5.032442, mean_q: 5.252810
 42210/100000: episode: 4309, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.140, mean reward: 0.414 [0.325, 0.498], mean action: 42.800 [21.000, 73.000], mean observation: 3.160 [-1.122, 10.473], loss: 1.266541, mae: 5.032144, mean_q: 5.254325
 42220/100000: episode: 4310, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.709, mean reward: 0.371 [0.333, 0.431], mean action: 44.600 [4.000, 88.000], mean observation: 3.155 [-1.500, 10.297], loss: 1.264870, mae: 5.032023, mean_q: 5.256256
 42230/100000: episode: 4311, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.766, mean reward: 0.377 [0.325, 0.503], mean action: 51.400 [14.000, 95.000], mean observation: 3.157 [-1.208, 10.199], loss: 1.081689, mae: 5.031275, mean_q: 5.257999
 42240/100000: episode: 4312, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.848, mean reward: 0.385 [0.371, 0.430], mean action: 58.600 [25.000, 101.000], mean observation: 3.153 [-2.116, 10.232], loss: 1.105801, mae: 5.031637, mean_q: 5.259540
 42250/100000: episode: 4313, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.739, mean reward: 0.374 [0.331, 0.427], mean action: 51.800 [12.000, 100.000], mean observation: 3.151 [-1.176, 10.423], loss: 1.213420, mae: 5.032328, mean_q: 5.260574
 42260/100000: episode: 4314, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.095, mean reward: 0.409 [0.338, 0.493], mean action: 47.000 [9.000, 95.000], mean observation: 3.164 [-1.617, 10.277], loss: 1.483814, mae: 5.033521, mean_q: 5.261807
 42270/100000: episode: 4315, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.961, mean reward: 0.396 [0.375, 0.467], mean action: 50.700 [43.000, 100.000], mean observation: 3.167 [-0.952, 10.269], loss: 1.217320, mae: 5.032588, mean_q: 5.262795
 42280/100000: episode: 4316, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.173, mean reward: 0.417 [0.384, 0.558], mean action: 41.800 [8.000, 90.000], mean observation: 3.151 [-1.174, 10.291], loss: 1.130644, mae: 5.032426, mean_q: 5.263947
 42290/100000: episode: 4317, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.584, mean reward: 0.358 [0.336, 0.448], mean action: 56.500 [43.000, 93.000], mean observation: 3.157 [-1.249, 10.280], loss: 1.176644, mae: 5.032833, mean_q: 5.263201
 42300/100000: episode: 4318, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.288, mean reward: 0.429 [0.340, 0.479], mean action: 62.500 [27.000, 100.000], mean observation: 3.164 [-1.168, 10.241], loss: 1.250744, mae: 5.032964, mean_q: 5.259519
 42303/100000: episode: 4319, duration: 0.078s, episode steps: 3, steps per second: 38, episode reward: 10.942, mean reward: 3.647 [0.401, 10.000], mean action: 30.667 [6.000, 43.000], mean observation: 3.149 [-2.667, 10.110], loss: 0.830082, mae: 5.031500, mean_q: 5.256346
 42313/100000: episode: 4320, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.190, mean reward: 0.419 [0.337, 0.567], mean action: 50.300 [6.000, 91.000], mean observation: 3.150 [-1.523, 10.315], loss: 1.253338, mae: 5.033099, mean_q: 5.253358
 42323/100000: episode: 4321, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.916, mean reward: 0.392 [0.323, 0.535], mean action: 50.300 [43.000, 78.000], mean observation: 3.156 [-1.067, 10.186], loss: 1.057698, mae: 5.032279, mean_q: 5.252207
 42333/100000: episode: 4322, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.235, mean reward: 0.424 [0.317, 0.473], mean action: 34.400 [12.000, 43.000], mean observation: 3.157 [-1.116, 10.274], loss: 1.158673, mae: 5.033095, mean_q: 5.253088
 42343/100000: episode: 4323, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.826, mean reward: 0.383 [0.317, 0.416], mean action: 47.500 [1.000, 86.000], mean observation: 3.160 [-1.379, 10.220], loss: 1.159164, mae: 5.033223, mean_q: 5.252763
 42353/100000: episode: 4324, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.494, mean reward: 0.349 [0.299, 0.476], mean action: 66.700 [16.000, 90.000], mean observation: 3.165 [-1.217, 10.352], loss: 1.750072, mae: 5.035635, mean_q: 5.253997
 42363/100000: episode: 4325, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.958, mean reward: 0.396 [0.357, 0.506], mean action: 73.900 [19.000, 96.000], mean observation: 3.152 [-0.933, 10.331], loss: 1.034309, mae: 5.032492, mean_q: 5.257065
 42373/100000: episode: 4326, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.120, mean reward: 0.412 [0.392, 0.468], mean action: 62.500 [7.000, 86.000], mean observation: 3.145 [-1.068, 10.268], loss: 1.016467, mae: 5.032372, mean_q: 5.260018
 42383/100000: episode: 4327, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.746, mean reward: 0.475 [0.378, 0.581], mean action: 59.900 [17.000, 86.000], mean observation: 3.152 [-1.594, 10.268], loss: 1.129593, mae: 5.033240, mean_q: 5.262467
 42393/100000: episode: 4328, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.897, mean reward: 0.390 [0.356, 0.452], mean action: 65.900 [1.000, 87.000], mean observation: 3.156 [-1.283, 10.205], loss: 0.950634, mae: 5.033137, mean_q: 5.264244
 42403/100000: episode: 4329, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.392, mean reward: 0.439 [0.404, 0.525], mean action: 67.000 [3.000, 86.000], mean observation: 3.155 [-1.990, 10.230], loss: 1.495072, mae: 5.035518, mean_q: 5.264480
 42413/100000: episode: 4330, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.169, mean reward: 0.417 [0.403, 0.453], mean action: 64.300 [2.000, 87.000], mean observation: 3.139 [-1.178, 10.258], loss: 1.117801, mae: 5.034222, mean_q: 5.263686
 42423/100000: episode: 4331, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.137, mean reward: 0.414 [0.386, 0.497], mean action: 75.300 [21.000, 91.000], mean observation: 3.161 [-0.873, 10.389], loss: 1.269025, mae: 5.035105, mean_q: 5.260993
 42433/100000: episode: 4332, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 5.304, mean reward: 0.530 [0.354, 0.571], mean action: 76.800 [21.000, 90.000], mean observation: 3.145 [-0.877, 10.246], loss: 1.081374, mae: 5.034493, mean_q: 5.260049
 42443/100000: episode: 4333, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.851, mean reward: 0.385 [0.337, 0.409], mean action: 64.800 [5.000, 86.000], mean observation: 3.168 [-1.391, 10.326], loss: 1.315812, mae: 5.035790, mean_q: 5.260613
 42453/100000: episode: 4334, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 3.643, mean reward: 0.364 [0.338, 0.432], mean action: 80.700 [54.000, 86.000], mean observation: 3.159 [-0.702, 10.371], loss: 1.154535, mae: 5.035345, mean_q: 5.261844
 42463/100000: episode: 4335, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.987, mean reward: 0.399 [0.376, 0.500], mean action: 75.200 [31.000, 99.000], mean observation: 3.134 [-1.202, 10.253], loss: 1.577626, mae: 5.036824, mean_q: 5.261476
 42473/100000: episode: 4336, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 3.615, mean reward: 0.362 [0.322, 0.425], mean action: 75.800 [0.000, 99.000], mean observation: 3.155 [-0.949, 10.375], loss: 0.766925, mae: 5.033475, mean_q: 5.258265
 42483/100000: episode: 4337, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 4.022, mean reward: 0.402 [0.328, 0.486], mean action: 78.100 [9.000, 100.000], mean observation: 3.168 [-1.141, 10.437], loss: 1.235688, mae: 5.035613, mean_q: 5.260691
 42493/100000: episode: 4338, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.127, mean reward: 0.413 [0.328, 0.463], mean action: 72.100 [19.000, 86.000], mean observation: 3.174 [-1.311, 10.379], loss: 0.995090, mae: 5.035016, mean_q: 5.263931
 42503/100000: episode: 4339, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.904, mean reward: 0.390 [0.317, 0.512], mean action: 56.500 [6.000, 86.000], mean observation: 3.159 [-1.788, 10.317], loss: 0.923586, mae: 5.035310, mean_q: 5.267604
 42513/100000: episode: 4340, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.964, mean reward: 0.396 [0.378, 0.474], mean action: 77.100 [0.000, 88.000], mean observation: 3.146 [-1.418, 10.299], loss: 1.472831, mae: 5.037781, mean_q: 5.270549
 42523/100000: episode: 4341, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.235, mean reward: 0.424 [0.387, 0.590], mean action: 69.800 [26.000, 97.000], mean observation: 3.152 [-1.943, 10.291], loss: 1.356958, mae: 5.037357, mean_q: 5.272756
 42533/100000: episode: 4342, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.009, mean reward: 0.401 [0.368, 0.505], mean action: 75.400 [16.000, 93.000], mean observation: 3.147 [-1.106, 10.230], loss: 1.259305, mae: 5.036990, mean_q: 5.271598
 42543/100000: episode: 4343, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.254, mean reward: 0.325 [0.298, 0.408], mean action: 67.700 [32.000, 89.000], mean observation: 3.177 [-1.437, 10.333], loss: 1.331066, mae: 5.037344, mean_q: 5.264037
 42553/100000: episode: 4344, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.577, mean reward: 0.458 [0.358, 0.503], mean action: 76.300 [31.000, 94.000], mean observation: 3.170 [-1.198, 10.351], loss: 1.262318, mae: 5.037227, mean_q: 5.261189
 42563/100000: episode: 4345, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.925, mean reward: 0.393 [0.372, 0.439], mean action: 69.100 [27.000, 86.000], mean observation: 3.160 [-2.375, 10.366], loss: 1.191071, mae: 5.037101, mean_q: 5.259412
 42573/100000: episode: 4346, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.072, mean reward: 0.407 [0.361, 0.472], mean action: 65.000 [18.000, 86.000], mean observation: 3.142 [-1.109, 10.281], loss: 1.235779, mae: 5.037245, mean_q: 5.257928
 42583/100000: episode: 4347, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.325, mean reward: 0.433 [0.433, 0.433], mean action: 74.700 [23.000, 97.000], mean observation: 3.145 [-1.779, 10.317], loss: 0.806067, mae: 5.035990, mean_q: 5.255683
 42593/100000: episode: 4348, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 4.405, mean reward: 0.440 [0.381, 0.480], mean action: 68.000 [6.000, 101.000], mean observation: 3.152 [-1.719, 10.318], loss: 1.360339, mae: 5.038737, mean_q: 5.254973
 42603/100000: episode: 4349, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.782, mean reward: 0.378 [0.272, 0.502], mean action: 64.900 [5.000, 99.000], mean observation: 3.170 [-1.700, 10.208], loss: 1.394828, mae: 5.039165, mean_q: 5.251520
 42613/100000: episode: 4350, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.077, mean reward: 0.408 [0.307, 0.486], mean action: 41.800 [6.000, 53.000], mean observation: 3.159 [-1.206, 10.311], loss: 1.342764, mae: 5.038722, mean_q: 5.247381
 42623/100000: episode: 4351, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 5.112, mean reward: 0.511 [0.511, 0.511], mean action: 63.300 [35.000, 96.000], mean observation: 3.159 [-2.010, 10.396], loss: 1.221186, mae: 5.038486, mean_q: 5.248282
 42633/100000: episode: 4352, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.033, mean reward: 0.403 [0.396, 0.448], mean action: 57.900 [21.000, 95.000], mean observation: 3.168 [-1.201, 10.252], loss: 1.451896, mae: 5.039385, mean_q: 5.250630
 42643/100000: episode: 4353, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.096, mean reward: 0.410 [0.331, 0.456], mean action: 47.200 [4.000, 96.000], mean observation: 3.158 [-1.563, 10.467], loss: 1.316534, mae: 5.038814, mean_q: 5.254695
 42653/100000: episode: 4354, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.157, mean reward: 0.416 [0.378, 0.468], mean action: 51.800 [22.000, 74.000], mean observation: 3.172 [-2.052, 10.390], loss: 1.460672, mae: 5.039165, mean_q: 5.259564
 42663/100000: episode: 4355, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.296, mean reward: 0.430 [0.425, 0.451], mean action: 55.300 [4.000, 99.000], mean observation: 3.165 [-1.462, 10.354], loss: 1.033068, mae: 5.037410, mean_q: 5.261600
 42673/100000: episode: 4356, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.099, mean reward: 0.410 [0.375, 0.499], mean action: 50.400 [7.000, 90.000], mean observation: 3.158 [-1.543, 10.335], loss: 1.462925, mae: 5.039356, mean_q: 5.261930
 42683/100000: episode: 4357, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 5.576, mean reward: 0.558 [0.558, 0.558], mean action: 59.700 [49.000, 99.000], mean observation: 3.156 [-1.532, 10.343], loss: 1.051340, mae: 5.037854, mean_q: 5.263217
 42693/100000: episode: 4358, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.102, mean reward: 0.410 [0.312, 0.575], mean action: 46.900 [16.000, 100.000], mean observation: 3.147 [-1.366, 10.389], loss: 1.237062, mae: 5.038691, mean_q: 5.260641
 42703/100000: episode: 4359, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.958, mean reward: 0.396 [0.296, 0.510], mean action: 41.800 [9.000, 62.000], mean observation: 3.142 [-1.318, 10.344], loss: 1.366193, mae: 5.039050, mean_q: 5.257994
 42713/100000: episode: 4360, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.766, mean reward: 0.377 [0.323, 0.411], mean action: 49.700 [11.000, 82.000], mean observation: 3.162 [-1.199, 10.251], loss: 0.976157, mae: 5.037801, mean_q: 5.256172
 42723/100000: episode: 4361, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.336, mean reward: 0.434 [0.321, 0.504], mean action: 48.800 [0.000, 84.000], mean observation: 3.146 [-1.368, 10.277], loss: 1.124833, mae: 5.038738, mean_q: 5.253006
 42733/100000: episode: 4362, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.013, mean reward: 0.401 [0.320, 0.425], mean action: 49.000 [3.000, 78.000], mean observation: 3.148 [-1.076, 10.348], loss: 1.469507, mae: 5.040374, mean_q: 5.251239
 42743/100000: episode: 4363, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.340, mean reward: 0.434 [0.316, 0.587], mean action: 56.200 [11.000, 85.000], mean observation: 3.153 [-1.267, 10.324], loss: 1.123004, mae: 5.038818, mean_q: 5.245716
 42753/100000: episode: 4364, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.979, mean reward: 0.398 [0.275, 0.472], mean action: 49.400 [15.000, 69.000], mean observation: 3.149 [-1.207, 10.303], loss: 1.245199, mae: 5.039313, mean_q: 5.243424
 42763/100000: episode: 4365, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.812, mean reward: 0.381 [0.381, 0.381], mean action: 62.200 [38.000, 90.000], mean observation: 3.157 [-1.014, 10.301], loss: 1.542738, mae: 5.040244, mean_q: 5.238974
 42773/100000: episode: 4366, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 3.427, mean reward: 0.343 [0.294, 0.447], mean action: 87.500 [70.000, 90.000], mean observation: 3.174 [-0.851, 10.380], loss: 1.304883, mae: 5.039136, mean_q: 5.239566
 42783/100000: episode: 4367, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 4.357, mean reward: 0.436 [0.362, 0.454], mean action: 82.800 [45.000, 90.000], mean observation: 3.142 [-1.400, 10.366], loss: 1.264473, mae: 5.038768, mean_q: 5.241580
 42793/100000: episode: 4368, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 4.296, mean reward: 0.430 [0.377, 0.551], mean action: 71.300 [15.000, 90.000], mean observation: 3.161 [-1.381, 10.430], loss: 0.940011, mae: 5.037507, mean_q: 5.243070
 42803/100000: episode: 4369, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.121, mean reward: 0.412 [0.358, 0.468], mean action: 49.500 [1.000, 90.000], mean observation: 3.162 [-1.451, 10.338], loss: 1.384054, mae: 5.039481, mean_q: 5.243892
 42813/100000: episode: 4370, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 4.023, mean reward: 0.402 [0.360, 0.522], mean action: 72.800 [14.000, 90.000], mean observation: 3.166 [-0.745, 10.331], loss: 1.109782, mae: 5.038289, mean_q: 5.245191
 42823/100000: episode: 4371, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.298, mean reward: 0.430 [0.330, 0.502], mean action: 71.300 [20.000, 90.000], mean observation: 3.159 [-0.954, 10.388], loss: 1.107126, mae: 5.038231, mean_q: 5.242312
 42833/100000: episode: 4372, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 3.664, mean reward: 0.366 [0.347, 0.399], mean action: 83.400 [38.000, 95.000], mean observation: 3.154 [-0.865, 10.262], loss: 1.054523, mae: 5.038247, mean_q: 5.241390
 42843/100000: episode: 4373, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.936, mean reward: 0.394 [0.355, 0.417], mean action: 49.500 [1.000, 90.000], mean observation: 3.162 [-0.793, 10.388], loss: 1.175605, mae: 5.039021, mean_q: 5.241862
 42853/100000: episode: 4374, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 3.884, mean reward: 0.388 [0.361, 0.424], mean action: 54.000 [13.000, 90.000], mean observation: 3.168 [-1.051, 10.338], loss: 1.104539, mae: 5.039067, mean_q: 5.242349
 42863/100000: episode: 4375, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.035, mean reward: 0.404 [0.401, 0.424], mean action: 73.500 [47.000, 94.000], mean observation: 3.167 [-1.213, 10.219], loss: 1.143485, mae: 5.039568, mean_q: 5.242918
 42873/100000: episode: 4376, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.601, mean reward: 0.460 [0.356, 0.499], mean action: 66.200 [13.000, 101.000], mean observation: 3.166 [-1.269, 10.406], loss: 1.297323, mae: 5.040416, mean_q: 5.242265
 42883/100000: episode: 4377, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.541, mean reward: 0.354 [0.329, 0.435], mean action: 66.600 [8.000, 86.000], mean observation: 3.159 [-1.156, 10.357], loss: 1.073060, mae: 5.039406, mean_q: 5.242886
 42893/100000: episode: 4378, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.755, mean reward: 0.476 [0.458, 0.489], mean action: 63.400 [14.000, 86.000], mean observation: 3.166 [-0.912, 10.338], loss: 1.352985, mae: 5.040782, mean_q: 5.239510
 42903/100000: episode: 4379, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 3.909, mean reward: 0.391 [0.386, 0.435], mean action: 80.800 [26.000, 94.000], mean observation: 3.135 [-1.280, 10.371], loss: 1.444711, mae: 5.041217, mean_q: 5.239580
 42913/100000: episode: 4380, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.955, mean reward: 0.395 [0.317, 0.437], mean action: 73.000 [14.000, 101.000], mean observation: 3.133 [-1.364, 10.261], loss: 1.086548, mae: 5.039350, mean_q: 5.241763
 42923/100000: episode: 4381, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.517, mean reward: 0.452 [0.333, 0.610], mean action: 54.200 [10.000, 92.000], mean observation: 3.160 [-1.567, 10.373], loss: 1.624120, mae: 5.041484, mean_q: 5.244050
 42933/100000: episode: 4382, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.833, mean reward: 0.383 [0.301, 0.459], mean action: 70.100 [32.000, 86.000], mean observation: 3.165 [-1.761, 10.347], loss: 1.058579, mae: 5.039083, mean_q: 5.246478
 42943/100000: episode: 4383, duration: 0.121s, episode steps: 10, steps per second: 82, episode reward: 3.535, mean reward: 0.353 [0.330, 0.367], mean action: 70.100 [25.000, 86.000], mean observation: 3.173 [-0.634, 10.278], loss: 1.241827, mae: 5.039711, mean_q: 5.246602
 42953/100000: episode: 4384, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.929, mean reward: 0.393 [0.293, 0.489], mean action: 49.100 [12.000, 86.000], mean observation: 3.145 [-1.128, 10.287], loss: 1.272986, mae: 5.039956, mean_q: 5.243997
 42963/100000: episode: 4385, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.043, mean reward: 0.404 [0.380, 0.498], mean action: 58.600 [15.000, 99.000], mean observation: 3.140 [-1.198, 10.299], loss: 0.961984, mae: 5.038633, mean_q: 5.246269
 42973/100000: episode: 4386, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.921, mean reward: 0.392 [0.338, 0.486], mean action: 45.900 [7.000, 89.000], mean observation: 3.151 [-1.559, 10.352], loss: 1.656011, mae: 5.041505, mean_q: 5.248250
 42983/100000: episode: 4387, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.838, mean reward: 0.484 [0.365, 0.552], mean action: 38.800 [11.000, 66.000], mean observation: 3.145 [-1.335, 10.306], loss: 1.347234, mae: 5.039969, mean_q: 5.249834
 42993/100000: episode: 4388, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.815, mean reward: 0.382 [0.310, 0.457], mean action: 44.600 [43.000, 55.000], mean observation: 3.162 [-1.322, 10.428], loss: 1.371259, mae: 5.039758, mean_q: 5.251030
 43003/100000: episode: 4389, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.974, mean reward: 0.497 [0.414, 0.569], mean action: 51.100 [3.000, 101.000], mean observation: 3.159 [-1.310, 10.352], loss: 1.253772, mae: 5.038838, mean_q: 5.250926
 43013/100000: episode: 4390, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.794, mean reward: 0.379 [0.368, 0.408], mean action: 68.000 [36.000, 86.000], mean observation: 3.149 [-1.206, 10.359], loss: 1.362589, mae: 5.039050, mean_q: 5.249263
 43023/100000: episode: 4391, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.199, mean reward: 0.420 [0.364, 0.456], mean action: 60.000 [0.000, 86.000], mean observation: 3.152 [-1.517, 10.298], loss: 1.185068, mae: 5.038197, mean_q: 5.249547
 43033/100000: episode: 4392, duration: 0.123s, episode steps: 10, steps per second: 82, episode reward: 4.219, mean reward: 0.422 [0.378, 0.468], mean action: 69.700 [20.000, 92.000], mean observation: 3.143 [-1.862, 10.476], loss: 1.357682, mae: 5.038993, mean_q: 5.250981
 43043/100000: episode: 4393, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.031, mean reward: 0.403 [0.291, 0.537], mean action: 47.500 [12.000, 73.000], mean observation: 3.152 [-1.194, 10.349], loss: 1.027796, mae: 5.037716, mean_q: 5.252580
 43053/100000: episode: 4394, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 13.451, mean reward: 1.345 [0.324, 10.000], mean action: 63.700 [19.000, 92.000], mean observation: 3.151 [-1.339, 10.408], loss: 1.561023, mae: 5.040029, mean_q: 5.251472
 43063/100000: episode: 4395, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.954, mean reward: 0.395 [0.376, 0.465], mean action: 68.100 [6.000, 86.000], mean observation: 3.148 [-1.211, 10.492], loss: 0.708989, mae: 5.036687, mean_q: 5.253261
 43073/100000: episode: 4396, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.430, mean reward: 0.443 [0.333, 0.482], mean action: 74.200 [13.000, 100.000], mean observation: 3.163 [-0.809, 10.244], loss: 1.393631, mae: 5.039621, mean_q: 5.256192
 43083/100000: episode: 4397, duration: 0.113s, episode steps: 10, steps per second: 88, episode reward: 3.423, mean reward: 0.342 [0.287, 0.363], mean action: 76.200 [21.000, 86.000], mean observation: 3.164 [-1.013, 10.358], loss: 1.025259, mae: 5.038329, mean_q: 5.258586
 43093/100000: episode: 4398, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.530, mean reward: 0.453 [0.451, 0.461], mean action: 76.800 [25.000, 100.000], mean observation: 3.149 [-1.505, 10.238], loss: 1.138013, mae: 5.038530, mean_q: 5.260924
 43103/100000: episode: 4399, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.282, mean reward: 0.428 [0.428, 0.428], mean action: 63.600 [30.000, 86.000], mean observation: 3.164 [-1.567, 10.221], loss: 1.232687, mae: 5.039048, mean_q: 5.262280
 43113/100000: episode: 4400, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.445, mean reward: 0.445 [0.293, 0.558], mean action: 61.100 [14.000, 86.000], mean observation: 3.148 [-1.189, 10.388], loss: 0.997437, mae: 5.038339, mean_q: 5.256794
 43123/100000: episode: 4401, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.032, mean reward: 0.403 [0.361, 0.575], mean action: 75.100 [8.000, 97.000], mean observation: 3.151 [-2.075, 10.452], loss: 1.077324, mae: 5.038617, mean_q: 5.255328
 43133/100000: episode: 4402, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.014, mean reward: 0.401 [0.381, 0.553], mean action: 74.900 [39.000, 100.000], mean observation: 3.134 [-1.201, 10.324], loss: 1.453017, mae: 5.040079, mean_q: 5.256022
 43143/100000: episode: 4403, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.990, mean reward: 0.399 [0.359, 0.447], mean action: 65.000 [3.000, 86.000], mean observation: 3.145 [-1.473, 10.341], loss: 1.086254, mae: 5.038759, mean_q: 5.256871
 43153/100000: episode: 4404, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.069, mean reward: 0.407 [0.369, 0.476], mean action: 58.400 [13.000, 99.000], mean observation: 3.160 [-1.392, 10.344], loss: 0.878505, mae: 5.037932, mean_q: 5.258717
 43163/100000: episode: 4405, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.163, mean reward: 0.416 [0.378, 0.559], mean action: 57.100 [12.000, 87.000], mean observation: 3.159 [-1.899, 10.219], loss: 1.314420, mae: 5.039796, mean_q: 5.257972
 43173/100000: episode: 4406, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 4.052, mean reward: 0.405 [0.373, 0.458], mean action: 75.900 [15.000, 98.000], mean observation: 3.145 [-1.419, 10.246], loss: 1.180323, mae: 5.039409, mean_q: 5.253015
 43183/100000: episode: 4407, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.929, mean reward: 0.393 [0.387, 0.444], mean action: 74.900 [23.000, 91.000], mean observation: 3.164 [-0.745, 10.376], loss: 1.285363, mae: 5.040055, mean_q: 5.248996
 43193/100000: episode: 4408, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.930, mean reward: 0.393 [0.367, 0.444], mean action: 77.400 [15.000, 86.000], mean observation: 3.166 [-1.083, 10.357], loss: 1.348290, mae: 5.040315, mean_q: 5.244248
 43203/100000: episode: 4409, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.075, mean reward: 0.408 [0.408, 0.408], mean action: 79.800 [46.000, 86.000], mean observation: 3.158 [-1.632, 10.403], loss: 1.565295, mae: 5.040841, mean_q: 5.242510
 43213/100000: episode: 4410, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 5.269, mean reward: 0.527 [0.444, 0.536], mean action: 74.800 [1.000, 93.000], mean observation: 3.144 [-0.877, 10.293], loss: 1.334031, mae: 5.040023, mean_q: 5.242176
 43223/100000: episode: 4411, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 3.930, mean reward: 0.393 [0.368, 0.532], mean action: 71.300 [28.000, 86.000], mean observation: 3.158 [-0.910, 10.307], loss: 1.251235, mae: 5.039893, mean_q: 5.241137
 43233/100000: episode: 4412, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.964, mean reward: 0.396 [0.324, 0.472], mean action: 47.100 [1.000, 86.000], mean observation: 3.163 [-2.095, 10.197], loss: 1.670222, mae: 5.041298, mean_q: 5.242148
 43243/100000: episode: 4413, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.651, mean reward: 0.365 [0.308, 0.417], mean action: 53.500 [4.000, 86.000], mean observation: 3.147 [-0.745, 10.275], loss: 1.345540, mae: 5.039895, mean_q: 5.243743
 43253/100000: episode: 4414, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.235, mean reward: 0.423 [0.418, 0.443], mean action: 63.700 [18.000, 86.000], mean observation: 3.161 [-1.562, 10.391], loss: 1.049002, mae: 5.038551, mean_q: 5.245698
 43263/100000: episode: 4415, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.110, mean reward: 0.411 [0.368, 0.456], mean action: 50.900 [3.000, 100.000], mean observation: 3.156 [-0.957, 10.365], loss: 1.378508, mae: 5.039925, mean_q: 5.249030
 43273/100000: episode: 4416, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.703, mean reward: 0.370 [0.345, 0.465], mean action: 65.700 [0.000, 90.000], mean observation: 3.146 [-1.301, 10.346], loss: 1.033438, mae: 5.038579, mean_q: 5.251676
 43283/100000: episode: 4417, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.133, mean reward: 0.413 [0.365, 0.510], mean action: 73.400 [21.000, 95.000], mean observation: 3.140 [-1.789, 10.194], loss: 1.121912, mae: 5.038935, mean_q: 5.253961
 43293/100000: episode: 4418, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 4.484, mean reward: 0.448 [0.443, 0.473], mean action: 74.500 [12.000, 86.000], mean observation: 3.156 [-0.974, 10.373], loss: 1.052100, mae: 5.038590, mean_q: 5.256142
 43303/100000: episode: 4419, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.423, mean reward: 0.442 [0.394, 0.511], mean action: 77.600 [6.000, 86.000], mean observation: 3.146 [-0.973, 10.281], loss: 1.221501, mae: 5.039359, mean_q: 5.258751
 43313/100000: episode: 4420, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.779, mean reward: 0.378 [0.330, 0.436], mean action: 76.500 [6.000, 101.000], mean observation: 3.145 [-1.329, 10.250], loss: 1.010271, mae: 5.038857, mean_q: 5.261665
 43323/100000: episode: 4421, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.009, mean reward: 0.401 [0.354, 0.461], mean action: 58.900 [6.000, 86.000], mean observation: 3.166 [-0.818, 10.369], loss: 1.081083, mae: 5.039461, mean_q: 5.265934
 43333/100000: episode: 4422, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.058, mean reward: 0.406 [0.332, 0.487], mean action: 72.600 [34.000, 97.000], mean observation: 3.147 [-1.715, 10.417], loss: 1.334560, mae: 5.040749, mean_q: 5.269750
 43343/100000: episode: 4423, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.123, mean reward: 0.412 [0.385, 0.499], mean action: 63.500 [1.000, 88.000], mean observation: 3.143 [-1.547, 10.343], loss: 1.194659, mae: 5.040126, mean_q: 5.273041
 43353/100000: episode: 4424, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.928, mean reward: 0.393 [0.353, 0.460], mean action: 50.900 [2.000, 94.000], mean observation: 3.161 [-1.746, 10.437], loss: 1.354895, mae: 5.040946, mean_q: 5.275620
 43363/100000: episode: 4425, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.510, mean reward: 0.351 [0.306, 0.476], mean action: 74.400 [45.000, 86.000], mean observation: 3.153 [-2.157, 10.226], loss: 1.332400, mae: 5.040739, mean_q: 5.277671
 43373/100000: episode: 4426, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.839, mean reward: 0.384 [0.327, 0.468], mean action: 70.400 [25.000, 100.000], mean observation: 3.137 [-1.787, 10.284], loss: 1.288932, mae: 5.040723, mean_q: 5.276411
 43383/100000: episode: 4427, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.577, mean reward: 0.458 [0.329, 0.580], mean action: 57.700 [5.000, 100.000], mean observation: 3.144 [-1.440, 10.379], loss: 1.255906, mae: 5.040414, mean_q: 5.277143
 43393/100000: episode: 4428, duration: 0.107s, episode steps: 10, steps per second: 94, episode reward: 4.297, mean reward: 0.430 [0.430, 0.430], mean action: 87.500 [86.000, 101.000], mean observation: 3.164 [-1.418, 10.246], loss: 0.955848, mae: 5.039541, mean_q: 5.275021
 43403/100000: episode: 4429, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 3.778, mean reward: 0.378 [0.367, 0.412], mean action: 70.000 [36.000, 98.000], mean observation: 3.164 [-1.776, 10.310], loss: 1.055808, mae: 5.040030, mean_q: 5.274378
 43413/100000: episode: 4430, duration: 0.108s, episode steps: 10, steps per second: 92, episode reward: 3.985, mean reward: 0.399 [0.398, 0.400], mean action: 81.100 [19.000, 95.000], mean observation: 3.171 [-1.459, 10.319], loss: 0.988530, mae: 5.040189, mean_q: 5.272220
 43423/100000: episode: 4431, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.124, mean reward: 0.412 [0.319, 0.521], mean action: 52.600 [1.000, 86.000], mean observation: 3.136 [-1.614, 10.379], loss: 1.318867, mae: 5.041806, mean_q: 5.269834
 43433/100000: episode: 4432, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 3.971, mean reward: 0.397 [0.327, 0.542], mean action: 69.900 [25.000, 100.000], mean observation: 3.160 [-1.576, 10.321], loss: 0.988733, mae: 5.040891, mean_q: 5.268224
 43443/100000: episode: 4433, duration: 0.099s, episode steps: 10, steps per second: 101, episode reward: 4.539, mean reward: 0.454 [0.450, 0.488], mean action: 81.400 [65.000, 89.000], mean observation: 3.152 [-0.961, 10.311], loss: 1.232439, mae: 5.041993, mean_q: 5.269219
 43453/100000: episode: 4434, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.159, mean reward: 0.416 [0.300, 0.499], mean action: 73.400 [17.000, 90.000], mean observation: 3.162 [-1.092, 10.357], loss: 0.949747, mae: 5.040900, mean_q: 5.271825
 43463/100000: episode: 4435, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.147, mean reward: 0.415 [0.329, 0.577], mean action: 78.200 [4.000, 92.000], mean observation: 3.155 [-1.650, 10.521], loss: 1.469406, mae: 5.043183, mean_q: 5.275094
 43473/100000: episode: 4436, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.498, mean reward: 0.450 [0.418, 0.539], mean action: 64.100 [15.000, 86.000], mean observation: 3.157 [-1.033, 10.320], loss: 1.604749, mae: 5.043466, mean_q: 5.278063
 43483/100000: episode: 4437, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.115, mean reward: 0.411 [0.355, 0.488], mean action: 54.000 [3.000, 86.000], mean observation: 3.167 [-1.034, 10.418], loss: 1.391182, mae: 5.042625, mean_q: 5.281058
 43493/100000: episode: 4438, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.185, mean reward: 0.418 [0.373, 0.455], mean action: 60.500 [11.000, 97.000], mean observation: 3.153 [-1.029, 10.304], loss: 1.160231, mae: 5.041664, mean_q: 5.283347
 43503/100000: episode: 4439, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 3.840, mean reward: 0.384 [0.349, 0.446], mean action: 81.000 [51.000, 86.000], mean observation: 3.172 [-1.199, 10.329], loss: 1.061434, mae: 5.041393, mean_q: 5.282031
 43513/100000: episode: 4440, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.140, mean reward: 0.414 [0.404, 0.421], mean action: 64.700 [1.000, 86.000], mean observation: 3.153 [-1.505, 10.371], loss: 1.224187, mae: 5.042020, mean_q: 5.274780
 43523/100000: episode: 4441, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.993, mean reward: 0.399 [0.358, 0.466], mean action: 62.300 [0.000, 96.000], mean observation: 3.146 [-1.205, 10.302], loss: 1.140826, mae: 5.042013, mean_q: 5.270678
 43533/100000: episode: 4442, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.502, mean reward: 0.450 [0.316, 0.563], mean action: 66.100 [0.000, 95.000], mean observation: 3.174 [-1.156, 10.330], loss: 0.784574, mae: 5.041101, mean_q: 5.263159
 43543/100000: episode: 4443, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.200, mean reward: 0.420 [0.329, 0.599], mean action: 37.800 [0.000, 86.000], mean observation: 3.149 [-1.066, 10.357], loss: 1.485970, mae: 5.044094, mean_q: 5.261963
 43553/100000: episode: 4444, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.740, mean reward: 0.474 [0.355, 0.510], mean action: 70.500 [8.000, 100.000], mean observation: 3.157 [-1.344, 10.324], loss: 1.394061, mae: 5.043702, mean_q: 5.263597
 43563/100000: episode: 4445, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.150, mean reward: 0.415 [0.337, 0.533], mean action: 45.300 [4.000, 93.000], mean observation: 3.152 [-2.853, 10.322], loss: 1.105662, mae: 5.042341, mean_q: 5.263329
 43573/100000: episode: 4446, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.984, mean reward: 0.398 [0.365, 0.468], mean action: 67.000 [25.000, 101.000], mean observation: 3.148 [-1.014, 10.386], loss: 1.051976, mae: 5.042334, mean_q: 5.262812
 43583/100000: episode: 4447, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.950, mean reward: 0.395 [0.291, 0.475], mean action: 42.600 [1.000, 86.000], mean observation: 3.160 [-1.177, 10.353], loss: 1.374619, mae: 5.043649, mean_q: 5.264596
 43593/100000: episode: 4448, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.279, mean reward: 0.428 [0.340, 0.525], mean action: 55.600 [5.000, 86.000], mean observation: 3.154 [-1.492, 10.241], loss: 1.248674, mae: 5.042873, mean_q: 5.264369
 43603/100000: episode: 4449, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.620, mean reward: 0.362 [0.348, 0.395], mean action: 65.300 [4.000, 86.000], mean observation: 3.152 [-1.775, 10.433], loss: 0.874491, mae: 5.041462, mean_q: 5.265882
 43613/100000: episode: 4450, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.421, mean reward: 0.442 [0.341, 0.499], mean action: 68.900 [13.000, 86.000], mean observation: 3.135 [-1.144, 10.261], loss: 1.260160, mae: 5.043175, mean_q: 5.268917
 43623/100000: episode: 4451, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.064, mean reward: 0.406 [0.318, 0.447], mean action: 67.100 [22.000, 101.000], mean observation: 3.163 [-1.622, 10.461], loss: 1.303412, mae: 5.043671, mean_q: 5.271812
 43633/100000: episode: 4452, duration: 0.110s, episode steps: 10, steps per second: 90, episode reward: 4.284, mean reward: 0.428 [0.331, 0.531], mean action: 85.700 [77.000, 97.000], mean observation: 3.147 [-1.242, 10.291], loss: 1.411415, mae: 5.044351, mean_q: 5.272155
 43643/100000: episode: 4453, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.208, mean reward: 0.421 [0.331, 0.465], mean action: 61.800 [6.000, 86.000], mean observation: 3.160 [-1.582, 10.431], loss: 1.235272, mae: 5.043715, mean_q: 5.268701
 43653/100000: episode: 4454, duration: 0.126s, episode steps: 10, steps per second: 80, episode reward: 4.293, mean reward: 0.429 [0.398, 0.480], mean action: 66.500 [1.000, 86.000], mean observation: 3.133 [-1.208, 10.336], loss: 1.012569, mae: 5.043120, mean_q: 5.267516
 43663/100000: episode: 4455, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.077, mean reward: 0.408 [0.323, 0.502], mean action: 66.100 [10.000, 86.000], mean observation: 3.143 [-1.138, 10.372], loss: 1.199945, mae: 5.043984, mean_q: 5.266691
 43673/100000: episode: 4456, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.672, mean reward: 0.367 [0.299, 0.444], mean action: 65.700 [25.000, 86.000], mean observation: 3.165 [-2.102, 10.288], loss: 1.392798, mae: 5.044902, mean_q: 5.267752
 43683/100000: episode: 4457, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.347, mean reward: 0.435 [0.414, 0.465], mean action: 69.900 [23.000, 92.000], mean observation: 3.150 [-1.325, 10.320], loss: 1.059567, mae: 5.043493, mean_q: 5.269373
 43693/100000: episode: 4458, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 3.563, mean reward: 0.356 [0.351, 0.359], mean action: 79.100 [11.000, 97.000], mean observation: 3.160 [-1.532, 10.362], loss: 1.314363, mae: 5.044599, mean_q: 5.272630
 43703/100000: episode: 4459, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.400, mean reward: 0.340 [0.308, 0.419], mean action: 63.900 [15.000, 86.000], mean observation: 3.152 [-1.583, 10.316], loss: 1.686191, mae: 5.046304, mean_q: 5.275134
 43713/100000: episode: 4460, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.214, mean reward: 0.421 [0.382, 0.444], mean action: 64.500 [8.000, 86.000], mean observation: 3.165 [-1.109, 10.397], loss: 1.218584, mae: 5.044607, mean_q: 5.273744
 43723/100000: episode: 4461, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.869, mean reward: 0.387 [0.323, 0.449], mean action: 50.900 [1.000, 99.000], mean observation: 3.149 [-1.182, 10.381], loss: 1.398497, mae: 5.045305, mean_q: 5.275504
 43733/100000: episode: 4462, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.138, mean reward: 0.414 [0.385, 0.502], mean action: 83.000 [64.000, 94.000], mean observation: 3.150 [-1.406, 10.353], loss: 1.431647, mae: 5.045609, mean_q: 5.275587
 43743/100000: episode: 4463, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.200, mean reward: 0.420 [0.411, 0.461], mean action: 62.200 [2.000, 97.000], mean observation: 3.159 [-1.388, 10.339], loss: 1.250183, mae: 5.044595, mean_q: 5.275919
 43753/100000: episode: 4464, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.625, mean reward: 0.362 [0.313, 0.458], mean action: 71.600 [11.000, 92.000], mean observation: 3.154 [-1.073, 10.271], loss: 1.424781, mae: 5.045124, mean_q: 5.273627
 43763/100000: episode: 4465, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.233, mean reward: 0.423 [0.359, 0.476], mean action: 43.300 [2.000, 99.000], mean observation: 3.157 [-1.460, 10.282], loss: 1.321042, mae: 5.044287, mean_q: 5.273841
 43773/100000: episode: 4466, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.493, mean reward: 0.449 [0.400, 0.583], mean action: 64.100 [15.000, 100.000], mean observation: 3.164 [-1.542, 10.368], loss: 0.862792, mae: 5.042611, mean_q: 5.272973
 43783/100000: episode: 4467, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.127, mean reward: 0.413 [0.381, 0.431], mean action: 65.500 [30.000, 98.000], mean observation: 3.159 [-0.735, 10.236], loss: 1.611801, mae: 5.045600, mean_q: 5.273984
 43784/100000: episode: 4468, duration: 0.022s, episode steps: 1, steps per second: 45, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 88.000 [88.000, 88.000], mean observation: 3.121 [-1.433, 10.100], loss: 1.867154, mae: 5.046339, mean_q: 5.273766
 43794/100000: episode: 4469, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.320, mean reward: 0.432 [0.377, 0.515], mean action: 72.600 [21.000, 95.000], mean observation: 3.151 [-1.551, 10.338], loss: 1.323887, mae: 5.044416, mean_q: 5.274051
 43804/100000: episode: 4470, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 3.897, mean reward: 0.390 [0.373, 0.425], mean action: 69.400 [2.000, 96.000], mean observation: 3.155 [-0.804, 10.288], loss: 1.200841, mae: 5.043898, mean_q: 5.275955
 43807/100000: episode: 4471, duration: 0.060s, episode steps: 3, steps per second: 50, episode reward: 10.724, mean reward: 3.575 [0.362, 10.000], mean action: 68.000 [32.000, 86.000], mean observation: 3.170 [-0.904, 10.254], loss: 1.255535, mae: 5.043884, mean_q: 5.275874
 43817/100000: episode: 4472, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.710, mean reward: 0.371 [0.329, 0.423], mean action: 66.500 [3.000, 91.000], mean observation: 3.152 [-1.371, 10.282], loss: 1.518292, mae: 5.045044, mean_q: 5.275274
 43827/100000: episode: 4473, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.996, mean reward: 0.400 [0.399, 0.403], mean action: 72.600 [44.000, 93.000], mean observation: 3.142 [-1.450, 10.313], loss: 1.473801, mae: 5.044831, mean_q: 5.277043
 43837/100000: episode: 4474, duration: 0.099s, episode steps: 10, steps per second: 101, episode reward: 3.707, mean reward: 0.371 [0.363, 0.393], mean action: 79.900 [54.000, 98.000], mean observation: 3.156 [-1.694, 10.238], loss: 1.326226, mae: 5.044190, mean_q: 5.279941
 43847/100000: episode: 4475, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 4.306, mean reward: 0.431 [0.389, 0.501], mean action: 66.100 [16.000, 86.000], mean observation: 3.156 [-1.197, 10.299], loss: 1.136531, mae: 5.043370, mean_q: 5.280348
 43857/100000: episode: 4476, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.960, mean reward: 0.396 [0.338, 0.574], mean action: 40.200 [6.000, 86.000], mean observation: 3.159 [-1.383, 10.409], loss: 1.023850, mae: 5.043253, mean_q: 5.279871
 43867/100000: episode: 4477, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.796, mean reward: 0.380 [0.349, 0.434], mean action: 65.200 [13.000, 86.000], mean observation: 3.151 [-0.971, 10.422], loss: 0.947612, mae: 5.043134, mean_q: 5.280603
 43877/100000: episode: 4478, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.413, mean reward: 0.441 [0.440, 0.442], mean action: 58.600 [3.000, 86.000], mean observation: 3.171 [-1.151, 10.343], loss: 1.178728, mae: 5.043947, mean_q: 5.280542
 43887/100000: episode: 4479, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.703, mean reward: 0.370 [0.328, 0.443], mean action: 52.500 [12.000, 86.000], mean observation: 3.140 [-1.635, 10.410], loss: 1.552353, mae: 5.045470, mean_q: 5.280147
 43897/100000: episode: 4480, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.764, mean reward: 0.376 [0.294, 0.463], mean action: 60.400 [9.000, 86.000], mean observation: 3.165 [-1.570, 10.299], loss: 1.379467, mae: 5.044679, mean_q: 5.279595
 43907/100000: episode: 4481, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 3.955, mean reward: 0.395 [0.386, 0.430], mean action: 82.100 [42.000, 92.000], mean observation: 3.154 [-0.770, 10.328], loss: 1.053157, mae: 5.043601, mean_q: 5.277516
 43917/100000: episode: 4482, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 4.183, mean reward: 0.418 [0.417, 0.429], mean action: 72.400 [43.000, 86.000], mean observation: 3.156 [-1.294, 10.206], loss: 0.876321, mae: 5.042895, mean_q: 5.278098
 43927/100000: episode: 4483, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.419, mean reward: 0.442 [0.416, 0.447], mean action: 67.100 [19.000, 93.000], mean observation: 3.159 [-0.824, 10.434], loss: 0.860064, mae: 5.043380, mean_q: 5.279615
 43937/100000: episode: 4484, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 3.971, mean reward: 0.397 [0.371, 0.515], mean action: 72.000 [6.000, 96.000], mean observation: 3.150 [-1.236, 10.247], loss: 1.087616, mae: 5.044932, mean_q: 5.280056
 43947/100000: episode: 4485, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.022, mean reward: 0.402 [0.359, 0.453], mean action: 69.900 [3.000, 86.000], mean observation: 3.162 [-1.532, 10.304], loss: 1.084563, mae: 5.045173, mean_q: 5.281826
 43957/100000: episode: 4486, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 3.821, mean reward: 0.382 [0.371, 0.421], mean action: 73.100 [35.000, 89.000], mean observation: 3.146 [-1.389, 10.267], loss: 1.435732, mae: 5.046655, mean_q: 5.279629
 43967/100000: episode: 4487, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.869, mean reward: 0.387 [0.301, 0.512], mean action: 58.300 [4.000, 91.000], mean observation: 3.151 [-1.254, 10.229], loss: 1.405547, mae: 5.046683, mean_q: 5.277029
 43977/100000: episode: 4488, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 4.352, mean reward: 0.435 [0.409, 0.465], mean action: 73.700 [25.000, 86.000], mean observation: 3.167 [-0.991, 10.359], loss: 1.310128, mae: 5.046172, mean_q: 5.275529
 43987/100000: episode: 4489, duration: 0.117s, episode steps: 10, steps per second: 86, episode reward: 3.110, mean reward: 0.311 [0.302, 0.386], mean action: 86.000 [86.000, 86.000], mean observation: 3.181 [-0.797, 10.336], loss: 0.929118, mae: 5.044858, mean_q: 5.273377
 43997/100000: episode: 4490, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.747, mean reward: 0.375 [0.315, 0.447], mean action: 66.800 [22.000, 101.000], mean observation: 3.166 [-1.866, 10.283], loss: 1.052289, mae: 5.045468, mean_q: 5.272016
 44007/100000: episode: 4491, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 3.496, mean reward: 0.350 [0.288, 0.441], mean action: 82.500 [46.000, 96.000], mean observation: 3.143 [-1.430, 10.333], loss: 1.606127, mae: 5.047725, mean_q: 5.272295
 44017/100000: episode: 4492, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.189, mean reward: 0.419 [0.412, 0.447], mean action: 70.600 [31.000, 86.000], mean observation: 3.149 [-1.516, 10.380], loss: 1.540590, mae: 5.047132, mean_q: 5.272035
 44027/100000: episode: 4493, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.145, mean reward: 0.414 [0.303, 0.462], mean action: 57.800 [13.000, 90.000], mean observation: 3.169 [-1.547, 10.474], loss: 1.015877, mae: 5.045074, mean_q: 5.271844
 44037/100000: episode: 4494, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.578, mean reward: 0.358 [0.287, 0.443], mean action: 71.800 [7.000, 91.000], mean observation: 3.166 [-1.219, 10.406], loss: 1.257527, mae: 5.046070, mean_q: 5.273834
 44047/100000: episode: 4495, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.701, mean reward: 0.470 [0.423, 0.504], mean action: 72.400 [2.000, 96.000], mean observation: 3.146 [-1.150, 10.332], loss: 1.165706, mae: 5.046011, mean_q: 5.277162
 44057/100000: episode: 4496, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 3.789, mean reward: 0.379 [0.377, 0.396], mean action: 86.600 [85.000, 93.000], mean observation: 3.173 [-1.161, 10.385], loss: 1.567345, mae: 5.047839, mean_q: 5.279441
 44067/100000: episode: 4497, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.381, mean reward: 0.438 [0.438, 0.438], mean action: 77.900 [26.000, 101.000], mean observation: 3.149 [-0.634, 10.394], loss: 1.099745, mae: 5.046070, mean_q: 5.281422
 44077/100000: episode: 4498, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.350, mean reward: 0.435 [0.372, 0.480], mean action: 50.500 [4.000, 100.000], mean observation: 3.145 [-1.410, 10.332], loss: 1.250114, mae: 5.046929, mean_q: 5.279312
 44087/100000: episode: 4499, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.024, mean reward: 0.402 [0.345, 0.489], mean action: 68.600 [3.000, 94.000], mean observation: 3.159 [-1.638, 10.311], loss: 0.893126, mae: 5.045651, mean_q: 5.278803
 44097/100000: episode: 4500, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.120, mean reward: 0.412 [0.363, 0.583], mean action: 59.600 [11.000, 89.000], mean observation: 3.154 [-1.517, 10.418], loss: 1.186655, mae: 5.047128, mean_q: 5.281114
 44107/100000: episode: 4501, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.078, mean reward: 0.408 [0.368, 0.502], mean action: 53.400 [5.000, 89.000], mean observation: 3.147 [-1.690, 10.376], loss: 1.366933, mae: 5.047956, mean_q: 5.283055
 44117/100000: episode: 4502, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.549, mean reward: 0.455 [0.415, 0.600], mean action: 68.300 [3.000, 86.000], mean observation: 3.163 [-1.036, 10.305], loss: 1.620554, mae: 5.048965, mean_q: 5.284728
 44127/100000: episode: 4503, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 4.614, mean reward: 0.461 [0.458, 0.476], mean action: 77.100 [13.000, 101.000], mean observation: 3.164 [-0.992, 10.356], loss: 1.161745, mae: 5.047108, mean_q: 5.287916
 44137/100000: episode: 4504, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.506, mean reward: 0.351 [0.324, 0.436], mean action: 63.400 [5.000, 86.000], mean observation: 3.157 [-1.097, 10.341], loss: 1.168761, mae: 5.047291, mean_q: 5.290794
 44147/100000: episode: 4505, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 5.067, mean reward: 0.507 [0.493, 0.538], mean action: 75.800 [6.000, 97.000], mean observation: 3.153 [-0.927, 10.381], loss: 1.056450, mae: 5.047153, mean_q: 5.289583
 44157/100000: episode: 4506, duration: 0.131s, episode steps: 10, steps per second: 77, episode reward: 3.557, mean reward: 0.356 [0.308, 0.409], mean action: 70.800 [23.000, 89.000], mean observation: 3.163 [-1.355, 10.260], loss: 1.297495, mae: 5.048026, mean_q: 5.288908
 44167/100000: episode: 4507, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.435, mean reward: 0.443 [0.402, 0.473], mean action: 75.000 [35.000, 86.000], mean observation: 3.141 [-1.076, 10.262], loss: 1.031291, mae: 5.047427, mean_q: 5.288482
 44177/100000: episode: 4508, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.969, mean reward: 0.397 [0.309, 0.451], mean action: 68.700 [5.000, 86.000], mean observation: 3.163 [-1.177, 10.309], loss: 1.304857, mae: 5.048905, mean_q: 5.285399
 44187/100000: episode: 4509, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.246, mean reward: 0.425 [0.390, 0.582], mean action: 72.900 [18.000, 93.000], mean observation: 3.153 [-2.168, 10.173], loss: 1.098306, mae: 5.048131, mean_q: 5.280675
 44197/100000: episode: 4510, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.937, mean reward: 0.394 [0.326, 0.499], mean action: 63.700 [6.000, 93.000], mean observation: 3.157 [-1.718, 10.307], loss: 1.339804, mae: 5.049392, mean_q: 5.276208
 44207/100000: episode: 4511, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.332, mean reward: 0.433 [0.418, 0.482], mean action: 69.400 [17.000, 86.000], mean observation: 3.152 [-1.176, 10.345], loss: 1.063316, mae: 5.048517, mean_q: 5.274015
 44217/100000: episode: 4512, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.541, mean reward: 0.454 [0.420, 0.485], mean action: 67.500 [3.000, 86.000], mean observation: 3.154 [-1.390, 10.328], loss: 1.296533, mae: 5.049619, mean_q: 5.272903
 44227/100000: episode: 4513, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 3.880, mean reward: 0.388 [0.353, 0.518], mean action: 69.000 [24.000, 86.000], mean observation: 3.157 [-1.828, 10.347], loss: 1.510098, mae: 5.050485, mean_q: 5.267120
 44237/100000: episode: 4514, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.343, mean reward: 0.434 [0.347, 0.477], mean action: 55.600 [0.000, 86.000], mean observation: 3.141 [-0.841, 10.273], loss: 1.367493, mae: 5.049723, mean_q: 5.264570
 44243/100000: episode: 4515, duration: 0.075s, episode steps: 6, steps per second: 80, episode reward: 11.840, mean reward: 1.973 [0.324, 10.000], mean action: 68.333 [14.000, 86.000], mean observation: 3.161 [-0.912, 10.272], loss: 1.305452, mae: 5.049157, mean_q: 5.265270
 44253/100000: episode: 4516, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.054, mean reward: 0.405 [0.381, 0.439], mean action: 72.300 [31.000, 101.000], mean observation: 3.151 [-1.421, 10.280], loss: 1.272326, mae: 5.048920, mean_q: 5.267428
 44263/100000: episode: 4517, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.358, mean reward: 0.436 [0.386, 0.511], mean action: 55.100 [5.000, 86.000], mean observation: 3.149 [-1.406, 10.297], loss: 0.957550, mae: 5.047734, mean_q: 5.269894
 44273/100000: episode: 4518, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 3.353, mean reward: 0.335 [0.267, 0.492], mean action: 78.900 [13.000, 99.000], mean observation: 3.165 [-1.787, 10.291], loss: 1.123253, mae: 5.048774, mean_q: 5.270917
 44283/100000: episode: 4519, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.517, mean reward: 0.452 [0.430, 0.492], mean action: 67.400 [0.000, 86.000], mean observation: 3.155 [-1.421, 10.407], loss: 1.222926, mae: 5.049696, mean_q: 5.274949
 44284/100000: episode: 4520, duration: 0.038s, episode steps: 1, steps per second: 27, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 86.000 [86.000, 86.000], mean observation: 3.152 [-1.265, 10.100], loss: 0.821945, mae: 5.048049, mean_q: 5.277757
 44294/100000: episode: 4521, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 3.831, mean reward: 0.383 [0.364, 0.508], mean action: 77.100 [7.000, 97.000], mean observation: 3.173 [-2.168, 10.320], loss: 1.050337, mae: 5.049274, mean_q: 5.279941
 44304/100000: episode: 4522, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 5.009, mean reward: 0.501 [0.441, 0.540], mean action: 63.700 [1.000, 86.000], mean observation: 3.163 [-1.511, 10.275], loss: 1.351958, mae: 5.050645, mean_q: 5.283647
 44314/100000: episode: 4523, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.291, mean reward: 0.429 [0.346, 0.503], mean action: 65.600 [14.000, 93.000], mean observation: 3.149 [-1.085, 10.330], loss: 1.426923, mae: 5.050637, mean_q: 5.287605
 44324/100000: episode: 4524, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.713, mean reward: 0.471 [0.401, 0.528], mean action: 67.300 [12.000, 92.000], mean observation: 3.167 [-1.027, 10.325], loss: 0.989922, mae: 5.048892, mean_q: 5.289674
 44334/100000: episode: 4525, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 3.923, mean reward: 0.392 [0.387, 0.413], mean action: 74.600 [16.000, 100.000], mean observation: 3.149 [-1.730, 10.277], loss: 1.159348, mae: 5.049566, mean_q: 5.292882
 44344/100000: episode: 4526, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.992, mean reward: 0.399 [0.322, 0.497], mean action: 61.800 [7.000, 86.000], mean observation: 3.156 [-2.059, 10.370], loss: 1.055034, mae: 5.049213, mean_q: 5.296445
 44349/100000: episode: 4527, duration: 0.071s, episode steps: 5, steps per second: 70, episode reward: 11.770, mean reward: 2.354 [0.406, 10.000], mean action: 66.600 [11.000, 88.000], mean observation: 3.155 [-1.262, 10.317], loss: 1.852467, mae: 5.052150, mean_q: 5.297261
 44359/100000: episode: 4528, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 3.946, mean reward: 0.395 [0.373, 0.544], mean action: 80.400 [48.000, 86.000], mean observation: 3.118 [-1.099, 10.322], loss: 1.106702, mae: 5.049078, mean_q: 5.298532
 44369/100000: episode: 4529, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.321, mean reward: 0.432 [0.354, 0.499], mean action: 50.800 [7.000, 86.000], mean observation: 3.170 [-1.538, 10.372], loss: 1.101487, mae: 5.049159, mean_q: 5.298476
 44379/100000: episode: 4530, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.209, mean reward: 0.421 [0.409, 0.425], mean action: 62.000 [15.000, 101.000], mean observation: 3.149 [-1.184, 10.194], loss: 1.365519, mae: 5.050376, mean_q: 5.297662
 44389/100000: episode: 4531, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.601, mean reward: 0.360 [0.322, 0.391], mean action: 76.800 [48.000, 86.000], mean observation: 3.153 [-0.622, 10.282], loss: 1.345022, mae: 5.050131, mean_q: 5.294670
 44399/100000: episode: 4532, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 4.498, mean reward: 0.450 [0.384, 0.524], mean action: 72.200 [37.000, 89.000], mean observation: 3.145 [-0.816, 10.200], loss: 1.273542, mae: 5.049842, mean_q: 5.293224
 44409/100000: episode: 4533, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.719, mean reward: 0.372 [0.339, 0.453], mean action: 72.000 [20.000, 86.000], mean observation: 3.159 [-1.402, 10.370], loss: 1.363556, mae: 5.050049, mean_q: 5.289321
 44419/100000: episode: 4534, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 5.029, mean reward: 0.503 [0.450, 0.583], mean action: 70.400 [18.000, 86.000], mean observation: 3.180 [-0.966, 10.264], loss: 1.307158, mae: 5.049860, mean_q: 5.286756
 44429/100000: episode: 4535, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.142, mean reward: 0.414 [0.286, 0.539], mean action: 60.600 [22.000, 86.000], mean observation: 3.148 [-1.477, 10.289], loss: 1.262984, mae: 5.049584, mean_q: 5.286355
 44439/100000: episode: 4536, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 4.419, mean reward: 0.442 [0.390, 0.476], mean action: 81.100 [27.000, 96.000], mean observation: 3.158 [-1.597, 10.316], loss: 1.170992, mae: 5.049329, mean_q: 5.287668
 44449/100000: episode: 4537, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 5.026, mean reward: 0.503 [0.334, 0.568], mean action: 63.400 [6.000, 98.000], mean observation: 3.163 [-1.300, 10.449], loss: 1.194228, mae: 5.049687, mean_q: 5.290206
 44459/100000: episode: 4538, duration: 0.108s, episode steps: 10, steps per second: 92, episode reward: 4.130, mean reward: 0.413 [0.354, 0.450], mean action: 71.800 [19.000, 88.000], mean observation: 3.165 [-1.593, 10.262], loss: 1.377750, mae: 5.050492, mean_q: 5.292614
 44469/100000: episode: 4539, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.000, mean reward: 0.400 [0.303, 0.512], mean action: 54.600 [0.000, 86.000], mean observation: 3.155 [-1.651, 10.333], loss: 1.074299, mae: 5.049402, mean_q: 5.294271
 44479/100000: episode: 4540, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.637, mean reward: 0.464 [0.334, 0.478], mean action: 77.800 [5.000, 86.000], mean observation: 3.142 [-1.040, 10.316], loss: 1.189697, mae: 5.050325, mean_q: 5.297637
 44489/100000: episode: 4541, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.791, mean reward: 0.479 [0.430, 0.507], mean action: 71.700 [16.000, 95.000], mean observation: 3.168 [-1.517, 10.270], loss: 1.405980, mae: 5.051339, mean_q: 5.301408
 44499/100000: episode: 4542, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.934, mean reward: 0.393 [0.367, 0.405], mean action: 64.200 [12.000, 96.000], mean observation: 3.148 [-1.775, 10.410], loss: 1.272467, mae: 5.050540, mean_q: 5.305413
 44509/100000: episode: 4543, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.102, mean reward: 0.410 [0.386, 0.460], mean action: 71.300 [0.000, 101.000], mean observation: 3.164 [-1.273, 10.339], loss: 1.517037, mae: 5.051351, mean_q: 5.307548
 44519/100000: episode: 4544, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.012, mean reward: 0.401 [0.353, 0.480], mean action: 64.400 [32.000, 91.000], mean observation: 3.161 [-1.455, 10.307], loss: 1.449013, mae: 5.051314, mean_q: 5.308181
 44529/100000: episode: 4545, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.716, mean reward: 0.372 [0.299, 0.462], mean action: 75.400 [35.000, 86.000], mean observation: 3.160 [-0.888, 10.325], loss: 1.178959, mae: 5.050263, mean_q: 5.306454
 44539/100000: episode: 4546, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.854, mean reward: 0.385 [0.338, 0.505], mean action: 60.400 [12.000, 86.000], mean observation: 3.160 [-2.351, 10.334], loss: 1.436969, mae: 5.051364, mean_q: 5.302907
 44549/100000: episode: 4547, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.154, mean reward: 0.415 [0.382, 0.436], mean action: 61.900 [9.000, 86.000], mean observation: 3.159 [-0.862, 10.241], loss: 1.066707, mae: 5.049788, mean_q: 5.303055
 44559/100000: episode: 4548, duration: 0.126s, episode steps: 10, steps per second: 80, episode reward: 3.558, mean reward: 0.356 [0.330, 0.466], mean action: 72.200 [28.000, 92.000], mean observation: 3.160 [-1.460, 10.340], loss: 1.219184, mae: 5.050473, mean_q: 5.305434
 44569/100000: episode: 4549, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.204, mean reward: 0.420 [0.367, 0.443], mean action: 59.100 [0.000, 100.000], mean observation: 3.162 [-0.983, 10.207], loss: 1.309327, mae: 5.050974, mean_q: 5.306519
 44579/100000: episode: 4550, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.207, mean reward: 0.421 [0.341, 0.439], mean action: 65.000 [11.000, 86.000], mean observation: 3.146 [-0.819, 10.476], loss: 1.087619, mae: 5.050221, mean_q: 5.307007
 44589/100000: episode: 4551, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 4.479, mean reward: 0.448 [0.378, 0.489], mean action: 70.200 [11.000, 100.000], mean observation: 3.153 [-0.966, 10.255], loss: 1.378474, mae: 5.051518, mean_q: 5.309866
 44599/100000: episode: 4552, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.237, mean reward: 0.424 [0.389, 0.513], mean action: 73.000 [27.000, 90.000], mean observation: 3.169 [-1.631, 10.294], loss: 0.914317, mae: 5.049685, mean_q: 5.309601
 44609/100000: episode: 4553, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.310, mean reward: 0.431 [0.358, 0.539], mean action: 51.200 [2.000, 86.000], mean observation: 3.151 [-1.281, 10.370], loss: 1.134241, mae: 5.050796, mean_q: 5.311267
 44619/100000: episode: 4554, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.548, mean reward: 0.355 [0.313, 0.468], mean action: 73.900 [33.000, 91.000], mean observation: 3.167 [-1.390, 10.290], loss: 1.363823, mae: 5.052071, mean_q: 5.310666
 44629/100000: episode: 4555, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.221, mean reward: 0.422 [0.403, 0.466], mean action: 68.900 [6.000, 86.000], mean observation: 3.161 [-1.412, 10.392], loss: 1.477750, mae: 5.052780, mean_q: 5.305482
 44639/100000: episode: 4556, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 3.416, mean reward: 0.342 [0.335, 0.353], mean action: 70.300 [17.000, 86.000], mean observation: 3.158 [-1.572, 10.363], loss: 1.718301, mae: 5.053458, mean_q: 5.302335
 44649/100000: episode: 4557, duration: 0.121s, episode steps: 10, steps per second: 82, episode reward: 4.773, mean reward: 0.477 [0.441, 0.518], mean action: 68.700 [25.000, 94.000], mean observation: 3.156 [-0.893, 10.355], loss: 1.356100, mae: 5.051660, mean_q: 5.302410
 44659/100000: episode: 4558, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.642, mean reward: 0.464 [0.422, 0.477], mean action: 65.700 [16.000, 95.000], mean observation: 3.157 [-1.060, 10.292], loss: 1.492377, mae: 5.051942, mean_q: 5.300720
 44669/100000: episode: 4559, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.998, mean reward: 0.400 [0.377, 0.506], mean action: 73.200 [34.000, 86.000], mean observation: 3.153 [-0.930, 10.322], loss: 1.355574, mae: 5.051275, mean_q: 5.301052
 44679/100000: episode: 4560, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 3.542, mean reward: 0.354 [0.302, 0.377], mean action: 77.200 [11.000, 97.000], mean observation: 3.152 [-1.381, 10.324], loss: 1.380553, mae: 5.051331, mean_q: 5.303283
 44689/100000: episode: 4561, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.081, mean reward: 0.408 [0.278, 0.500], mean action: 63.000 [4.000, 86.000], mean observation: 3.149 [-1.294, 10.510], loss: 1.518734, mae: 5.051972, mean_q: 5.304631
 44699/100000: episode: 4562, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.771, mean reward: 0.477 [0.435, 0.482], mean action: 69.600 [8.000, 86.000], mean observation: 3.163 [-1.276, 10.285], loss: 1.062344, mae: 5.050221, mean_q: 5.306818
 44709/100000: episode: 4563, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.124, mean reward: 0.412 [0.323, 0.454], mean action: 73.700 [12.000, 97.000], mean observation: 3.154 [-1.403, 10.462], loss: 0.924204, mae: 5.050014, mean_q: 5.307694
 44719/100000: episode: 4564, duration: 0.118s, episode steps: 10, steps per second: 84, episode reward: 3.690, mean reward: 0.369 [0.354, 0.449], mean action: 82.800 [71.000, 86.000], mean observation: 3.168 [-0.875, 10.237], loss: 1.490149, mae: 5.052351, mean_q: 5.308057
 44729/100000: episode: 4565, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 3.848, mean reward: 0.385 [0.371, 0.456], mean action: 75.300 [11.000, 86.000], mean observation: 3.171 [-1.275, 10.291], loss: 0.866536, mae: 5.050105, mean_q: 5.307012
 44739/100000: episode: 4566, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.013, mean reward: 0.401 [0.345, 0.451], mean action: 72.200 [42.000, 95.000], mean observation: 3.156 [-1.315, 10.263], loss: 1.401695, mae: 5.052626, mean_q: 5.305852
 44749/100000: episode: 4567, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.269, mean reward: 0.427 [0.316, 0.567], mean action: 65.100 [5.000, 101.000], mean observation: 3.159 [-1.616, 10.424], loss: 1.142494, mae: 5.051842, mean_q: 5.301787
 44759/100000: episode: 4568, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 4.684, mean reward: 0.468 [0.349, 0.520], mean action: 77.900 [14.000, 98.000], mean observation: 3.169 [-0.796, 10.386], loss: 1.388545, mae: 5.053033, mean_q: 5.299143
 44769/100000: episode: 4569, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 3.988, mean reward: 0.399 [0.364, 0.454], mean action: 66.900 [21.000, 88.000], mean observation: 3.143 [-1.284, 10.257], loss: 1.382253, mae: 5.053424, mean_q: 5.295624
 44779/100000: episode: 4570, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.022, mean reward: 0.402 [0.325, 0.426], mean action: 73.700 [31.000, 86.000], mean observation: 3.158 [-1.442, 10.376], loss: 0.954553, mae: 5.051736, mean_q: 5.297430
 44789/100000: episode: 4571, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.520, mean reward: 0.452 [0.411, 0.521], mean action: 65.100 [30.000, 89.000], mean observation: 3.153 [-1.467, 10.372], loss: 0.917891, mae: 5.051845, mean_q: 5.300621
 44799/100000: episode: 4572, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.624, mean reward: 0.362 [0.323, 0.382], mean action: 63.800 [3.000, 86.000], mean observation: 3.141 [-1.342, 10.406], loss: 1.405114, mae: 5.054123, mean_q: 5.305535
 44809/100000: episode: 4573, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.989, mean reward: 0.399 [0.334, 0.481], mean action: 61.000 [6.000, 86.000], mean observation: 3.147 [-0.961, 10.278], loss: 1.301263, mae: 5.053979, mean_q: 5.309096
 44819/100000: episode: 4574, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.047, mean reward: 0.405 [0.386, 0.437], mean action: 66.900 [9.000, 86.000], mean observation: 3.159 [-1.959, 10.249], loss: 0.876357, mae: 5.052730, mean_q: 5.307170
 44829/100000: episode: 4575, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.776, mean reward: 0.378 [0.352, 0.417], mean action: 63.900 [13.000, 96.000], mean observation: 3.160 [-1.165, 10.299], loss: 1.045414, mae: 5.053763, mean_q: 5.307884
 44839/100000: episode: 4576, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 3.738, mean reward: 0.374 [0.368, 0.415], mean action: 78.500 [11.000, 86.000], mean observation: 3.160 [-1.343, 10.377], loss: 1.105406, mae: 5.054860, mean_q: 5.308276
 44849/100000: episode: 4577, duration: 0.112s, episode steps: 10, steps per second: 90, episode reward: 4.651, mean reward: 0.465 [0.465, 0.469], mean action: 83.700 [67.000, 96.000], mean observation: 3.145 [-2.169, 10.347], loss: 1.381486, mae: 5.056224, mean_q: 5.305932
 44859/100000: episode: 4578, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.190, mean reward: 0.419 [0.341, 0.447], mean action: 68.800 [14.000, 86.000], mean observation: 3.169 [-1.388, 10.355], loss: 1.559466, mae: 5.056657, mean_q: 5.301290
 44869/100000: episode: 4579, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 3.969, mean reward: 0.397 [0.383, 0.420], mean action: 74.100 [19.000, 100.000], mean observation: 3.152 [-1.148, 10.356], loss: 1.193216, mae: 5.055360, mean_q: 5.295146
 44879/100000: episode: 4580, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.861, mean reward: 0.386 [0.358, 0.454], mean action: 64.100 [10.000, 95.000], mean observation: 3.154 [-1.941, 10.313], loss: 0.988297, mae: 5.054843, mean_q: 5.293624
 44889/100000: episode: 4581, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 4.541, mean reward: 0.454 [0.428, 0.516], mean action: 82.000 [37.000, 92.000], mean observation: 3.172 [-1.181, 10.331], loss: 1.211366, mae: 5.056063, mean_q: 5.293929
 44890/100000: episode: 4582, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 77.000 [77.000, 77.000], mean observation: 3.175 [-1.005, 10.462], loss: 1.138901, mae: 5.056094, mean_q: 5.293966
 44900/100000: episode: 4583, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 4.084, mean reward: 0.408 [0.399, 0.424], mean action: 82.200 [58.000, 95.000], mean observation: 3.154 [-0.853, 10.365], loss: 1.229903, mae: 5.056409, mean_q: 5.294389
 44910/100000: episode: 4584, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.421, mean reward: 0.442 [0.429, 0.474], mean action: 65.400 [7.000, 87.000], mean observation: 3.153 [-1.511, 10.207], loss: 1.511646, mae: 5.057280, mean_q: 5.296834
 44920/100000: episode: 4585, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 4.370, mean reward: 0.437 [0.377, 0.477], mean action: 70.000 [6.000, 101.000], mean observation: 3.149 [-1.180, 10.378], loss: 1.327921, mae: 5.056334, mean_q: 5.296856
 44930/100000: episode: 4586, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 4.487, mean reward: 0.449 [0.382, 0.479], mean action: 65.000 [2.000, 100.000], mean observation: 3.168 [-1.309, 10.381], loss: 1.020673, mae: 5.054980, mean_q: 5.294641
 44940/100000: episode: 4587, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.120, mean reward: 0.412 [0.384, 0.503], mean action: 77.400 [10.000, 100.000], mean observation: 3.163 [-1.020, 10.336], loss: 1.342404, mae: 5.056186, mean_q: 5.294428
 44950/100000: episode: 4588, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 5.057, mean reward: 0.506 [0.506, 0.506], mean action: 70.100 [9.000, 101.000], mean observation: 3.166 [-1.058, 10.422], loss: 1.212004, mae: 5.055791, mean_q: 5.292181
 44960/100000: episode: 4589, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.496, mean reward: 0.350 [0.325, 0.426], mean action: 78.200 [33.000, 101.000], mean observation: 3.153 [-0.921, 10.276], loss: 1.447883, mae: 5.056796, mean_q: 5.285675
 44970/100000: episode: 4590, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 3.994, mean reward: 0.399 [0.376, 0.418], mean action: 67.000 [25.000, 86.000], mean observation: 3.152 [-1.346, 10.322], loss: 1.261797, mae: 5.056199, mean_q: 5.282739
 44980/100000: episode: 4591, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.075, mean reward: 0.407 [0.389, 0.509], mean action: 64.500 [9.000, 98.000], mean observation: 3.156 [-1.343, 10.532], loss: 1.061074, mae: 5.055506, mean_q: 5.283282
 44990/100000: episode: 4592, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.207, mean reward: 0.421 [0.325, 0.501], mean action: 56.900 [7.000, 86.000], mean observation: 3.163 [-1.044, 10.550], loss: 1.241634, mae: 5.056678, mean_q: 5.285062
 45000/100000: episode: 4593, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.370, mean reward: 0.437 [0.413, 0.525], mean action: 59.900 [18.000, 86.000], mean observation: 3.155 [-1.457, 10.329], loss: 1.107410, mae: 5.056372, mean_q: 5.284314
 45010/100000: episode: 4594, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.857, mean reward: 0.386 [0.337, 0.482], mean action: 64.200 [18.000, 97.000], mean observation: 3.164 [-1.424, 10.267], loss: 1.238697, mae: 5.057394, mean_q: 5.281504
 45020/100000: episode: 4595, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.935, mean reward: 0.393 [0.312, 0.497], mean action: 65.900 [14.000, 100.000], mean observation: 3.151 [-1.752, 10.321], loss: 1.117378, mae: 5.056925, mean_q: 5.276964
 45030/100000: episode: 4596, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.428, mean reward: 0.443 [0.397, 0.563], mean action: 66.400 [1.000, 95.000], mean observation: 3.152 [-1.678, 10.265], loss: 1.143478, mae: 5.056956, mean_q: 5.272069
 45040/100000: episode: 4597, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.132, mean reward: 0.413 [0.364, 0.423], mean action: 71.300 [9.000, 96.000], mean observation: 3.141 [-1.406, 10.351], loss: 1.062126, mae: 5.057072, mean_q: 5.267982
 45050/100000: episode: 4598, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 4.632, mean reward: 0.463 [0.402, 0.470], mean action: 79.100 [8.000, 95.000], mean observation: 3.170 [-1.656, 10.303], loss: 1.220389, mae: 5.057945, mean_q: 5.268018
 45060/100000: episode: 4599, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.557, mean reward: 0.456 [0.306, 0.483], mean action: 85.000 [75.000, 87.000], mean observation: 3.151 [-1.058, 10.371], loss: 1.548340, mae: 5.058928, mean_q: 5.268135
 45070/100000: episode: 4600, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 3.621, mean reward: 0.362 [0.301, 0.510], mean action: 63.100 [0.000, 86.000], mean observation: 3.148 [-1.223, 10.595], loss: 1.307486, mae: 5.058185, mean_q: 5.264537
 45080/100000: episode: 4601, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.533, mean reward: 0.453 [0.366, 0.475], mean action: 80.800 [50.000, 98.000], mean observation: 3.165 [-1.083, 10.318], loss: 1.528450, mae: 5.058921, mean_q: 5.262293
 45090/100000: episode: 4602, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.165, mean reward: 0.416 [0.350, 0.495], mean action: 58.600 [3.000, 86.000], mean observation: 3.147 [-1.380, 10.286], loss: 1.402574, mae: 5.058216, mean_q: 5.261621
 45100/100000: episode: 4603, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.762, mean reward: 0.376 [0.368, 0.391], mean action: 68.700 [14.000, 99.000], mean observation: 3.153 [-0.983, 10.349], loss: 1.017200, mae: 5.056844, mean_q: 5.259930
 45110/100000: episode: 4604, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.174, mean reward: 0.417 [0.381, 0.459], mean action: 58.800 [3.000, 86.000], mean observation: 3.148 [-1.616, 10.308], loss: 1.251261, mae: 5.057615, mean_q: 5.257258
 45120/100000: episode: 4605, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.593, mean reward: 0.459 [0.458, 0.460], mean action: 66.800 [8.000, 91.000], mean observation: 3.147 [-1.110, 10.374], loss: 1.356640, mae: 5.057912, mean_q: 5.254793
 45130/100000: episode: 4606, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.268, mean reward: 0.427 [0.426, 0.433], mean action: 68.800 [34.000, 86.000], mean observation: 3.173 [-1.680, 10.310], loss: 1.376553, mae: 5.058096, mean_q: 5.253671
 45140/100000: episode: 4607, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.954, mean reward: 0.395 [0.391, 0.434], mean action: 71.900 [16.000, 100.000], mean observation: 3.170 [-1.662, 10.254], loss: 1.063539, mae: 5.056820, mean_q: 5.254752
 45150/100000: episode: 4608, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 4.187, mean reward: 0.419 [0.388, 0.582], mean action: 65.800 [35.000, 101.000], mean observation: 3.172 [-0.832, 10.371], loss: 1.278168, mae: 5.057847, mean_q: 5.255189
 45160/100000: episode: 4609, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.812, mean reward: 0.381 [0.353, 0.396], mean action: 77.100 [46.000, 94.000], mean observation: 3.149 [-1.258, 10.334], loss: 1.204532, mae: 5.057745, mean_q: 5.251209
 45170/100000: episode: 4610, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.992, mean reward: 0.399 [0.360, 0.464], mean action: 51.600 [0.000, 99.000], mean observation: 3.168 [-0.741, 10.307], loss: 1.162831, mae: 5.057437, mean_q: 5.249367
 45180/100000: episode: 4611, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.156, mean reward: 0.416 [0.312, 0.477], mean action: 68.600 [5.000, 94.000], mean observation: 3.164 [-1.020, 10.348], loss: 1.569331, mae: 5.058948, mean_q: 5.249307
 45190/100000: episode: 4612, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 4.045, mean reward: 0.405 [0.343, 0.480], mean action: 71.900 [0.000, 95.000], mean observation: 3.167 [-1.098, 10.271], loss: 1.324187, mae: 5.057886, mean_q: 5.248356
 45200/100000: episode: 4613, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.008, mean reward: 0.401 [0.333, 0.514], mean action: 59.600 [2.000, 86.000], mean observation: 3.148 [-1.605, 10.324], loss: 1.288826, mae: 5.057708, mean_q: 5.247193
 45210/100000: episode: 4614, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.552, mean reward: 0.455 [0.395, 0.546], mean action: 41.500 [6.000, 101.000], mean observation: 3.156 [-1.253, 10.276], loss: 1.172622, mae: 5.057207, mean_q: 5.245931
 45220/100000: episode: 4615, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.925, mean reward: 0.393 [0.353, 0.462], mean action: 62.600 [6.000, 94.000], mean observation: 3.154 [-1.968, 10.314], loss: 1.249041, mae: 5.057066, mean_q: 5.246286
 45230/100000: episode: 4616, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.697, mean reward: 0.470 [0.356, 0.490], mean action: 46.300 [21.000, 93.000], mean observation: 3.158 [-1.122, 10.429], loss: 1.067884, mae: 5.056563, mean_q: 5.247373
 45240/100000: episode: 4617, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.103, mean reward: 0.410 [0.342, 0.462], mean action: 16.600 [6.000, 34.000], mean observation: 3.157 [-1.324, 10.301], loss: 1.096247, mae: 5.056931, mean_q: 5.247465
 45250/100000: episode: 4618, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.342, mean reward: 0.434 [0.329, 0.629], mean action: 25.800 [4.000, 81.000], mean observation: 3.153 [-1.857, 10.359], loss: 1.397094, mae: 5.058122, mean_q: 5.246800
 45260/100000: episode: 4619, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.017, mean reward: 0.402 [0.329, 0.485], mean action: 34.700 [6.000, 91.000], mean observation: 3.148 [-1.418, 10.270], loss: 1.445019, mae: 5.058159, mean_q: 5.247828
 45270/100000: episode: 4620, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.085, mean reward: 0.408 [0.338, 0.461], mean action: 38.600 [4.000, 96.000], mean observation: 3.159 [-1.353, 10.460], loss: 1.406386, mae: 5.057575, mean_q: 5.250522
 45280/100000: episode: 4621, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.185, mean reward: 0.418 [0.322, 0.500], mean action: 42.800 [25.000, 80.000], mean observation: 3.152 [-0.962, 10.353], loss: 1.445849, mae: 5.057515, mean_q: 5.252674
 45290/100000: episode: 4622, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.004, mean reward: 0.400 [0.365, 0.450], mean action: 53.700 [32.000, 95.000], mean observation: 3.162 [-0.925, 10.352], loss: 0.902571, mae: 5.054772, mean_q: 5.254734
 45300/100000: episode: 4623, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.754, mean reward: 0.375 [0.337, 0.417], mean action: 45.400 [21.000, 81.000], mean observation: 3.154 [-1.639, 10.246], loss: 0.898601, mae: 5.054892, mean_q: 5.253558
 45310/100000: episode: 4624, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.217, mean reward: 0.422 [0.284, 0.503], mean action: 27.800 [0.000, 32.000], mean observation: 3.167 [-1.288, 10.327], loss: 1.254615, mae: 5.056491, mean_q: 5.251153
 45320/100000: episode: 4625, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.883, mean reward: 0.388 [0.343, 0.452], mean action: 46.900 [20.000, 86.000], mean observation: 3.155 [-2.039, 10.257], loss: 1.109574, mae: 5.056126, mean_q: 5.249193
 45330/100000: episode: 4626, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.930, mean reward: 0.393 [0.354, 0.444], mean action: 44.000 [18.000, 96.000], mean observation: 3.147 [-1.518, 10.323], loss: 1.244008, mae: 5.056563, mean_q: 5.250047
 45340/100000: episode: 4627, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.140, mean reward: 0.414 [0.299, 0.537], mean action: 38.000 [1.000, 98.000], mean observation: 3.158 [-1.137, 10.289], loss: 1.020721, mae: 5.055686, mean_q: 5.252705
 45350/100000: episode: 4628, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.141, mean reward: 0.414 [0.338, 0.487], mean action: 38.700 [32.000, 52.000], mean observation: 3.154 [-1.425, 10.338], loss: 1.482860, mae: 5.057817, mean_q: 5.254249
 45359/100000: episode: 4629, duration: 0.163s, episode steps: 9, steps per second: 55, episode reward: 13.294, mean reward: 1.477 [0.352, 10.000], mean action: 33.889 [6.000, 101.000], mean observation: 3.165 [-1.787, 10.668], loss: 1.344738, mae: 5.056634, mean_q: 5.251345
 45360/100000: episode: 4630, duration: 0.037s, episode steps: 1, steps per second: 27, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 6.000 [6.000, 6.000], mean observation: 3.150 [-0.663, 10.545], loss: 1.150489, mae: 5.055365, mean_q: 5.250653
 45370/100000: episode: 4631, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 4.082, mean reward: 0.408 [0.368, 0.455], mean action: 18.100 [6.000, 59.000], mean observation: 3.154 [-1.682, 10.393], loss: 1.519021, mae: 5.057081, mean_q: 5.251516
 45380/100000: episode: 4632, duration: 0.227s, episode steps: 10, steps per second: 44, episode reward: 3.912, mean reward: 0.391 [0.326, 0.462], mean action: 15.300 [6.000, 82.000], mean observation: 3.157 [-1.345, 10.305], loss: 1.273714, mae: 5.055849, mean_q: 5.249579
 45390/100000: episode: 4633, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.266, mean reward: 0.427 [0.414, 0.540], mean action: 33.300 [6.000, 88.000], mean observation: 3.154 [-1.354, 10.208], loss: 1.306249, mae: 5.055567, mean_q: 5.246505
 45400/100000: episode: 4634, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.384, mean reward: 0.438 [0.388, 0.496], mean action: 36.700 [6.000, 98.000], mean observation: 3.150 [-1.722, 10.337], loss: 1.437088, mae: 5.055641, mean_q: 5.246258
 45401/100000: episode: 4635, duration: 0.025s, episode steps: 1, steps per second: 40, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 58.000 [58.000, 58.000], mean observation: 3.165 [-0.978, 10.587], loss: 1.486444, mae: 5.055447, mean_q: 5.246278
 45411/100000: episode: 4636, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.427, mean reward: 0.443 [0.372, 0.537], mean action: 35.500 [6.000, 88.000], mean observation: 3.157 [-1.623, 10.279], loss: 1.522428, mae: 5.055150, mean_q: 5.246483
 45421/100000: episode: 4637, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.940, mean reward: 0.394 [0.361, 0.423], mean action: 47.300 [25.000, 99.000], mean observation: 3.172 [-1.863, 10.435], loss: 1.460496, mae: 5.054701, mean_q: 5.247078
 45431/100000: episode: 4638, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.822, mean reward: 0.382 [0.356, 0.414], mean action: 48.800 [34.000, 95.000], mean observation: 3.148 [-0.850, 10.258], loss: 1.009877, mae: 5.052765, mean_q: 5.248200
 45441/100000: episode: 4639, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.672, mean reward: 0.367 [0.299, 0.468], mean action: 31.900 [0.000, 55.000], mean observation: 3.167 [-1.323, 10.508], loss: 1.017031, mae: 5.052617, mean_q: 5.248758
 45451/100000: episode: 4640, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.140, mean reward: 0.414 [0.369, 0.451], mean action: 39.600 [2.000, 95.000], mean observation: 3.162 [-1.004, 10.291], loss: 0.962755, mae: 5.052804, mean_q: 5.250194
 45461/100000: episode: 4641, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.701, mean reward: 0.370 [0.343, 0.429], mean action: 50.800 [25.000, 96.000], mean observation: 3.158 [-1.853, 10.496], loss: 1.148937, mae: 5.053722, mean_q: 5.251265
 45471/100000: episode: 4642, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.112, mean reward: 0.411 [0.342, 0.500], mean action: 33.500 [8.000, 61.000], mean observation: 3.160 [-1.317, 10.430], loss: 1.249762, mae: 5.054376, mean_q: 5.248133
 45481/100000: episode: 4643, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.610, mean reward: 0.361 [0.314, 0.419], mean action: 67.700 [6.000, 99.000], mean observation: 3.167 [-1.028, 10.222], loss: 1.010612, mae: 5.053685, mean_q: 5.247617
 45491/100000: episode: 4644, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.888, mean reward: 0.389 [0.333, 0.466], mean action: 75.000 [14.000, 99.000], mean observation: 3.153 [-1.111, 10.264], loss: 1.105922, mae: 5.054307, mean_q: 5.249076
 45501/100000: episode: 4645, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.572, mean reward: 0.457 [0.406, 0.523], mean action: 79.000 [24.000, 95.000], mean observation: 3.142 [-1.255, 10.467], loss: 1.107240, mae: 5.054807, mean_q: 5.250455
 45511/100000: episode: 4646, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.743, mean reward: 0.474 [0.391, 0.509], mean action: 75.300 [18.000, 95.000], mean observation: 3.156 [-1.541, 10.270], loss: 1.174228, mae: 5.055105, mean_q: 5.251667
 45521/100000: episode: 4647, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.819, mean reward: 0.382 [0.336, 0.439], mean action: 57.000 [5.000, 95.000], mean observation: 3.152 [-1.663, 10.355], loss: 1.268251, mae: 5.055847, mean_q: 5.252499
 45531/100000: episode: 4648, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.863, mean reward: 0.386 [0.343, 0.462], mean action: 70.000 [4.000, 95.000], mean observation: 3.166 [-1.147, 10.450], loss: 1.411669, mae: 5.056367, mean_q: 5.253231
 45541/100000: episode: 4649, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.755, mean reward: 0.376 [0.369, 0.400], mean action: 73.300 [5.000, 95.000], mean observation: 3.151 [-1.526, 10.290], loss: 1.269871, mae: 5.055462, mean_q: 5.254370
 45551/100000: episode: 4650, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.098, mean reward: 0.410 [0.352, 0.493], mean action: 75.300 [16.000, 95.000], mean observation: 3.165 [-0.819, 10.292], loss: 1.282017, mae: 5.055655, mean_q: 5.255631
 45561/100000: episode: 4651, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.063, mean reward: 0.406 [0.366, 0.456], mean action: 58.400 [5.000, 95.000], mean observation: 3.168 [-0.655, 10.421], loss: 1.016127, mae: 5.054269, mean_q: 5.257827
 45571/100000: episode: 4652, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.477, mean reward: 0.448 [0.352, 0.505], mean action: 76.900 [8.000, 95.000], mean observation: 3.147 [-0.952, 10.262], loss: 1.039734, mae: 5.054526, mean_q: 5.259565
 45581/100000: episode: 4653, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.744, mean reward: 0.374 [0.296, 0.428], mean action: 60.400 [16.000, 95.000], mean observation: 3.165 [-1.705, 10.387], loss: 1.345917, mae: 5.055792, mean_q: 5.259086
 45591/100000: episode: 4654, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.240, mean reward: 0.424 [0.416, 0.448], mean action: 71.700 [15.000, 95.000], mean observation: 3.166 [-0.880, 10.351], loss: 1.296000, mae: 5.055524, mean_q: 5.259793
 45601/100000: episode: 4655, duration: 0.120s, episode steps: 10, steps per second: 84, episode reward: 4.406, mean reward: 0.441 [0.414, 0.536], mean action: 81.000 [48.000, 95.000], mean observation: 3.149 [-1.221, 10.143], loss: 1.224289, mae: 5.055235, mean_q: 5.258656
 45611/100000: episode: 4656, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.593, mean reward: 0.359 [0.354, 0.411], mean action: 79.700 [1.000, 95.000], mean observation: 3.166 [-1.036, 10.205], loss: 1.180691, mae: 5.054922, mean_q: 5.257504
 45621/100000: episode: 4657, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 13.067, mean reward: 1.307 [0.282, 10.000], mean action: 83.400 [52.000, 95.000], mean observation: 3.170 [-1.215, 10.211], loss: 1.250101, mae: 5.055026, mean_q: 5.256488
 45631/100000: episode: 4658, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.274, mean reward: 0.427 [0.427, 0.427], mean action: 58.400 [19.000, 95.000], mean observation: 3.158 [-1.325, 10.336], loss: 1.044430, mae: 5.054368, mean_q: 5.257854
 45641/100000: episode: 4659, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 3.444, mean reward: 0.344 [0.331, 0.391], mean action: 93.300 [78.000, 95.000], mean observation: 3.153 [-1.176, 10.327], loss: 0.841785, mae: 5.053667, mean_q: 5.259721
 45651/100000: episode: 4660, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.793, mean reward: 0.379 [0.360, 0.407], mean action: 85.500 [0.000, 95.000], mean observation: 3.144 [-0.732, 10.409], loss: 0.687770, mae: 5.053463, mean_q: 5.262068
 45661/100000: episode: 4661, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 4.770, mean reward: 0.477 [0.398, 0.530], mean action: 76.200 [10.000, 95.000], mean observation: 3.153 [-1.645, 10.406], loss: 1.158859, mae: 5.055711, mean_q: 5.261027
 45671/100000: episode: 4662, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.896, mean reward: 0.490 [0.490, 0.490], mean action: 72.000 [11.000, 95.000], mean observation: 3.159 [-1.235, 10.264], loss: 1.434787, mae: 5.057129, mean_q: 5.258546
 45681/100000: episode: 4663, duration: 0.106s, episode steps: 10, steps per second: 95, episode reward: 3.815, mean reward: 0.382 [0.376, 0.390], mean action: 87.600 [37.000, 97.000], mean observation: 3.152 [-1.627, 10.378], loss: 1.470064, mae: 5.057101, mean_q: 5.256180
 45691/100000: episode: 4664, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.546, mean reward: 0.355 [0.330, 0.405], mean action: 76.700 [24.000, 95.000], mean observation: 3.162 [-1.052, 10.287], loss: 1.466536, mae: 5.056643, mean_q: 5.256541
 45701/100000: episode: 4665, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.562, mean reward: 0.456 [0.408, 0.485], mean action: 65.600 [3.000, 95.000], mean observation: 3.137 [-0.953, 10.375], loss: 1.105107, mae: 5.055062, mean_q: 5.258464
 45711/100000: episode: 4666, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.998, mean reward: 0.400 [0.357, 0.454], mean action: 74.300 [19.000, 95.000], mean observation: 3.157 [-1.522, 10.496], loss: 1.250744, mae: 5.055390, mean_q: 5.257495
 45721/100000: episode: 4667, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 3.823, mean reward: 0.382 [0.345, 0.432], mean action: 79.400 [27.000, 95.000], mean observation: 3.147 [-2.059, 10.342], loss: 1.185428, mae: 5.055412, mean_q: 5.257929
 45731/100000: episode: 4668, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.340, mean reward: 0.434 [0.433, 0.439], mean action: 56.800 [32.000, 95.000], mean observation: 3.164 [-1.396, 10.384], loss: 1.111520, mae: 5.055249, mean_q: 5.260303
 45741/100000: episode: 4669, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 5.019, mean reward: 0.502 [0.474, 0.546], mean action: 51.300 [1.000, 95.000], mean observation: 3.167 [-1.392, 10.385], loss: 1.210724, mae: 5.055634, mean_q: 5.262718
 45751/100000: episode: 4670, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.476, mean reward: 0.448 [0.431, 0.498], mean action: 38.800 [20.000, 64.000], mean observation: 3.164 [-1.754, 10.289], loss: 1.356461, mae: 5.056511, mean_q: 5.264398
 45761/100000: episode: 4671, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 3.947, mean reward: 0.395 [0.353, 0.453], mean action: 29.800 [14.000, 39.000], mean observation: 3.155 [-1.416, 10.373], loss: 1.188491, mae: 5.055900, mean_q: 5.262761
 45771/100000: episode: 4672, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.979, mean reward: 0.398 [0.358, 0.464], mean action: 43.300 [24.000, 96.000], mean observation: 3.148 [-1.351, 10.336], loss: 1.144736, mae: 5.055791, mean_q: 5.262967
 45781/100000: episode: 4673, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.117, mean reward: 0.412 [0.356, 0.468], mean action: 43.700 [6.000, 86.000], mean observation: 3.159 [-1.123, 10.334], loss: 1.127412, mae: 5.056105, mean_q: 5.263856
 45791/100000: episode: 4674, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.736, mean reward: 0.374 [0.331, 0.408], mean action: 57.100 [32.000, 95.000], mean observation: 3.165 [-1.036, 10.373], loss: 1.262935, mae: 5.056683, mean_q: 5.261979
 45801/100000: episode: 4675, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 5.272, mean reward: 0.527 [0.527, 0.527], mean action: 81.100 [2.000, 97.000], mean observation: 3.180 [-1.288, 10.403], loss: 1.073185, mae: 5.056121, mean_q: 5.259757
 45802/100000: episode: 4676, duration: 0.024s, episode steps: 1, steps per second: 41, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 80.000 [80.000, 80.000], mean observation: 3.151 [-1.216, 10.134], loss: 0.786105, mae: 5.055348, mean_q: 5.258435
 45812/100000: episode: 4677, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.038, mean reward: 0.404 [0.364, 0.503], mean action: 68.200 [5.000, 95.000], mean observation: 3.159 [-1.108, 10.340], loss: 1.392949, mae: 5.057368, mean_q: 5.258431
 45822/100000: episode: 4678, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.844, mean reward: 0.384 [0.319, 0.473], mean action: 63.900 [24.000, 95.000], mean observation: 3.165 [-1.065, 10.309], loss: 1.410050, mae: 5.057492, mean_q: 5.256351
 45832/100000: episode: 4679, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.728, mean reward: 0.373 [0.355, 0.427], mean action: 41.000 [26.000, 79.000], mean observation: 3.157 [-1.167, 10.420], loss: 1.213882, mae: 5.056428, mean_q: 5.256006
 45842/100000: episode: 4680, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.052, mean reward: 0.405 [0.318, 0.528], mean action: 42.800 [34.000, 90.000], mean observation: 3.160 [-1.873, 10.350], loss: 1.751415, mae: 5.058302, mean_q: 5.257048
 45852/100000: episode: 4681, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.166, mean reward: 0.417 [0.382, 0.517], mean action: 58.000 [27.000, 99.000], mean observation: 3.165 [-1.008, 10.282], loss: 1.568750, mae: 5.057612, mean_q: 5.257602
 45862/100000: episode: 4682, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.845, mean reward: 0.385 [0.325, 0.546], mean action: 48.500 [11.000, 97.000], mean observation: 3.158 [-1.621, 10.322], loss: 1.047535, mae: 5.055641, mean_q: 5.258520
 45872/100000: episode: 4683, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.349, mean reward: 0.435 [0.339, 0.449], mean action: 64.300 [6.000, 96.000], mean observation: 3.162 [-1.180, 10.288], loss: 1.012735, mae: 5.056014, mean_q: 5.260010
 45882/100000: episode: 4684, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.210, mean reward: 0.421 [0.341, 0.474], mean action: 66.000 [4.000, 95.000], mean observation: 3.139 [-1.649, 10.361], loss: 1.395406, mae: 5.057339, mean_q: 5.262574
 45892/100000: episode: 4685, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.014, mean reward: 0.401 [0.370, 0.448], mean action: 73.800 [34.000, 95.000], mean observation: 3.171 [-1.546, 10.383], loss: 1.009039, mae: 5.055963, mean_q: 5.262334
 45902/100000: episode: 4686, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.817, mean reward: 0.382 [0.339, 0.460], mean action: 52.800 [7.000, 95.000], mean observation: 3.161 [-1.249, 10.412], loss: 1.149925, mae: 5.056983, mean_q: 5.259725
 45912/100000: episode: 4687, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 5.019, mean reward: 0.502 [0.424, 0.554], mean action: 80.300 [47.000, 95.000], mean observation: 3.147 [-0.936, 10.458], loss: 1.197049, mae: 5.056972, mean_q: 5.260159
 45922/100000: episode: 4688, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 4.440, mean reward: 0.444 [0.338, 0.508], mean action: 75.800 [34.000, 95.000], mean observation: 3.152 [-1.099, 10.253], loss: 1.020328, mae: 5.056567, mean_q: 5.261336
 45932/100000: episode: 4689, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.801, mean reward: 0.380 [0.362, 0.458], mean action: 44.800 [6.000, 95.000], mean observation: 3.170 [-1.358, 10.265], loss: 1.390183, mae: 5.057961, mean_q: 5.260475
 45942/100000: episode: 4690, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.460, mean reward: 0.446 [0.446, 0.446], mean action: 60.000 [32.000, 95.000], mean observation: 3.164 [-1.501, 10.414], loss: 1.320107, mae: 5.057560, mean_q: 5.259214
 45952/100000: episode: 4691, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.212, mean reward: 0.421 [0.324, 0.512], mean action: 52.600 [2.000, 98.000], mean observation: 3.158 [-1.156, 10.442], loss: 1.337709, mae: 5.057418, mean_q: 5.260155
 45962/100000: episode: 4692, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.361, mean reward: 0.436 [0.436, 0.436], mean action: 48.900 [23.000, 92.000], mean observation: 3.152 [-1.246, 10.240], loss: 1.388178, mae: 5.057146, mean_q: 5.259439
 45972/100000: episode: 4693, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.284, mean reward: 0.428 [0.371, 0.470], mean action: 37.300 [5.000, 81.000], mean observation: 3.149 [-1.665, 10.285], loss: 1.018179, mae: 5.055481, mean_q: 5.258750
 45982/100000: episode: 4694, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.802, mean reward: 0.380 [0.351, 0.428], mean action: 47.600 [16.000, 85.000], mean observation: 3.152 [-1.135, 10.389], loss: 0.958849, mae: 5.055476, mean_q: 5.257901
 45992/100000: episode: 4695, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.996, mean reward: 0.400 [0.335, 0.552], mean action: 41.500 [9.000, 78.000], mean observation: 3.140 [-1.373, 10.296], loss: 1.352215, mae: 5.057201, mean_q: 5.259225
 46002/100000: episode: 4696, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.758, mean reward: 0.376 [0.299, 0.450], mean action: 40.100 [13.000, 93.000], mean observation: 3.160 [-1.883, 10.344], loss: 1.091676, mae: 5.056363, mean_q: 5.262252
 46012/100000: episode: 4697, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.664, mean reward: 0.366 [0.316, 0.472], mean action: 39.000 [18.000, 52.000], mean observation: 3.158 [-1.266, 10.425], loss: 1.207023, mae: 5.056679, mean_q: 5.264943
 46022/100000: episode: 4698, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.229, mean reward: 0.423 [0.380, 0.574], mean action: 42.700 [18.000, 79.000], mean observation: 3.160 [-1.682, 10.223], loss: 0.657952, mae: 5.054665, mean_q: 5.264421
 46031/100000: episode: 4699, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 13.268, mean reward: 1.474 [0.295, 10.000], mean action: 40.222 [0.000, 99.000], mean observation: 3.163 [-2.770, 10.355], loss: 0.961203, mae: 5.056397, mean_q: 5.265066
 46041/100000: episode: 4700, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.346, mean reward: 0.435 [0.420, 0.519], mean action: 42.000 [7.000, 97.000], mean observation: 3.149 [-1.490, 10.291], loss: 1.196860, mae: 5.057600, mean_q: 5.266844
 46051/100000: episode: 4701, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.054, mean reward: 0.405 [0.334, 0.465], mean action: 32.300 [1.000, 40.000], mean observation: 3.155 [-2.067, 10.472], loss: 1.603852, mae: 5.059301, mean_q: 5.267406
 46061/100000: episode: 4702, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.079, mean reward: 0.408 [0.345, 0.508], mean action: 46.100 [6.000, 81.000], mean observation: 3.170 [-1.353, 10.309], loss: 1.182284, mae: 5.057834, mean_q: 5.264625
 46071/100000: episode: 4703, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.051, mean reward: 0.405 [0.357, 0.523], mean action: 62.700 [23.000, 90.000], mean observation: 3.161 [-1.557, 10.321], loss: 1.368388, mae: 5.058498, mean_q: 5.262182
 46081/100000: episode: 4704, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.084, mean reward: 0.408 [0.333, 0.478], mean action: 46.900 [0.000, 98.000], mean observation: 3.159 [-1.061, 10.327], loss: 1.171190, mae: 5.057462, mean_q: 5.259469
 46089/100000: episode: 4705, duration: 0.139s, episode steps: 8, steps per second: 58, episode reward: 12.857, mean reward: 1.607 [0.299, 10.000], mean action: 43.375 [0.000, 100.000], mean observation: 3.166 [-1.394, 10.260], loss: 1.197311, mae: 5.057499, mean_q: 5.259118
 46099/100000: episode: 4706, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.964, mean reward: 0.396 [0.330, 0.455], mean action: 49.000 [39.000, 97.000], mean observation: 3.164 [-1.440, 10.396], loss: 1.129975, mae: 5.057372, mean_q: 5.259512
 46109/100000: episode: 4707, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.041, mean reward: 0.404 [0.321, 0.491], mean action: 37.500 [0.000, 101.000], mean observation: 3.160 [-1.672, 10.447], loss: 1.205610, mae: 5.058034, mean_q: 5.260592
 46119/100000: episode: 4708, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.099, mean reward: 0.410 [0.395, 0.502], mean action: 51.100 [33.000, 90.000], mean observation: 3.157 [-1.630, 10.366], loss: 1.111650, mae: 5.057848, mean_q: 5.258989
 46129/100000: episode: 4709, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.975, mean reward: 0.397 [0.309, 0.497], mean action: 38.500 [8.000, 100.000], mean observation: 3.163 [-1.642, 10.394], loss: 1.550047, mae: 5.059127, mean_q: 5.259293
 46139/100000: episode: 4710, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.671, mean reward: 0.467 [0.461, 0.513], mean action: 36.400 [11.000, 61.000], mean observation: 3.164 [-1.189, 10.218], loss: 1.506276, mae: 5.058755, mean_q: 5.260049
 46149/100000: episode: 4711, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.917, mean reward: 0.392 [0.306, 0.437], mean action: 41.300 [7.000, 81.000], mean observation: 3.172 [-1.197, 10.392], loss: 1.327818, mae: 5.057804, mean_q: 5.261395
 46159/100000: episode: 4712, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 5.028, mean reward: 0.503 [0.503, 0.503], mean action: 62.900 [38.000, 98.000], mean observation: 3.138 [-1.008, 10.332], loss: 1.442460, mae: 5.057734, mean_q: 5.262355
 46169/100000: episode: 4713, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.139, mean reward: 0.414 [0.397, 0.442], mean action: 49.700 [32.000, 96.000], mean observation: 3.164 [-1.064, 10.444], loss: 1.623236, mae: 5.057937, mean_q: 5.260058
 46179/100000: episode: 4714, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.381, mean reward: 0.438 [0.417, 0.514], mean action: 45.900 [7.000, 92.000], mean observation: 3.161 [-1.788, 10.357], loss: 1.057132, mae: 5.055415, mean_q: 5.260835
 46189/100000: episode: 4715, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.387, mean reward: 0.439 [0.322, 0.491], mean action: 57.000 [30.000, 97.000], mean observation: 3.167 [-0.794, 10.236], loss: 1.154827, mae: 5.055963, mean_q: 5.263009
 46199/100000: episode: 4716, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.024, mean reward: 0.402 [0.345, 0.535], mean action: 35.400 [8.000, 63.000], mean observation: 3.150 [-1.370, 10.286], loss: 1.173458, mae: 5.055749, mean_q: 5.264860
 46209/100000: episode: 4717, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.954, mean reward: 0.395 [0.342, 0.525], mean action: 38.500 [20.000, 81.000], mean observation: 3.158 [-1.443, 10.390], loss: 1.119537, mae: 5.055485, mean_q: 5.264223
 46219/100000: episode: 4718, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.619, mean reward: 0.462 [0.457, 0.508], mean action: 41.200 [19.000, 93.000], mean observation: 3.170 [-1.314, 10.427], loss: 1.467250, mae: 5.056498, mean_q: 5.261686
 46229/100000: episode: 4719, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.397, mean reward: 0.440 [0.375, 0.519], mean action: 40.500 [36.000, 57.000], mean observation: 3.156 [-1.031, 10.319], loss: 1.288483, mae: 5.055796, mean_q: 5.261956
 46239/100000: episode: 4720, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.002, mean reward: 0.400 [0.347, 0.489], mean action: 46.900 [9.000, 84.000], mean observation: 3.147 [-1.803, 10.263], loss: 1.025145, mae: 5.054580, mean_q: 5.260878
 46249/100000: episode: 4721, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.166, mean reward: 0.417 [0.384, 0.440], mean action: 35.700 [15.000, 55.000], mean observation: 3.163 [-1.519, 10.245], loss: 1.338281, mae: 5.055930, mean_q: 5.259591
 46259/100000: episode: 4722, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.050, mean reward: 0.405 [0.345, 0.549], mean action: 45.300 [29.000, 99.000], mean observation: 3.151 [-1.694, 10.277], loss: 1.447012, mae: 5.056467, mean_q: 5.258533
 46269/100000: episode: 4723, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.997, mean reward: 0.400 [0.348, 0.503], mean action: 48.900 [22.000, 98.000], mean observation: 3.156 [-1.006, 10.400], loss: 0.933668, mae: 5.054514, mean_q: 5.256232
 46279/100000: episode: 4724, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.757, mean reward: 0.376 [0.313, 0.473], mean action: 50.000 [28.000, 72.000], mean observation: 3.166 [-1.439, 10.317], loss: 1.147784, mae: 5.055533, mean_q: 5.256219
 46289/100000: episode: 4725, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.095, mean reward: 0.410 [0.361, 0.532], mean action: 58.400 [32.000, 99.000], mean observation: 3.154 [-1.219, 10.295], loss: 1.094405, mae: 5.055482, mean_q: 5.256062
 46299/100000: episode: 4726, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.057, mean reward: 0.406 [0.341, 0.572], mean action: 24.200 [4.000, 32.000], mean observation: 3.164 [-1.702, 10.361], loss: 1.250563, mae: 5.056338, mean_q: 5.257562
 46309/100000: episode: 4727, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.955, mean reward: 0.396 [0.299, 0.465], mean action: 42.600 [8.000, 97.000], mean observation: 3.148 [-1.716, 10.365], loss: 1.181960, mae: 5.056064, mean_q: 5.258899
 46315/100000: episode: 4728, duration: 0.108s, episode steps: 6, steps per second: 55, episode reward: 11.927, mean reward: 1.988 [0.267, 10.000], mean action: 34.667 [32.000, 48.000], mean observation: 3.152 [-2.452, 10.504], loss: 1.315049, mae: 5.056690, mean_q: 5.259737
 46325/100000: episode: 4729, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.203, mean reward: 0.420 [0.334, 0.526], mean action: 34.100 [13.000, 79.000], mean observation: 3.155 [-1.889, 10.387], loss: 1.377236, mae: 5.057115, mean_q: 5.258566
 46335/100000: episode: 4730, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.131, mean reward: 0.413 [0.365, 0.456], mean action: 32.000 [12.000, 58.000], mean observation: 3.153 [-1.610, 10.351], loss: 1.018695, mae: 5.055855, mean_q: 5.253482
 46345/100000: episode: 4731, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.193, mean reward: 0.419 [0.334, 0.482], mean action: 57.200 [18.000, 97.000], mean observation: 3.162 [-1.042, 10.228], loss: 1.348308, mae: 5.057181, mean_q: 5.253812
 46355/100000: episode: 4732, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.662, mean reward: 0.466 [0.402, 0.506], mean action: 51.400 [0.000, 69.000], mean observation: 3.161 [-1.680, 10.421], loss: 1.190130, mae: 5.056518, mean_q: 5.253695
 46365/100000: episode: 4733, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.099, mean reward: 0.410 [0.346, 0.557], mean action: 63.300 [11.000, 92.000], mean observation: 3.154 [-0.613, 10.308], loss: 1.183259, mae: 5.056578, mean_q: 5.254846
 46375/100000: episode: 4734, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.218, mean reward: 0.422 [0.345, 0.560], mean action: 37.800 [0.000, 100.000], mean observation: 3.147 [-1.689, 10.279], loss: 1.309076, mae: 5.057078, mean_q: 5.254281
 46385/100000: episode: 4735, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.261, mean reward: 0.426 [0.341, 0.471], mean action: 45.100 [2.000, 76.000], mean observation: 3.149 [-0.955, 10.448], loss: 1.092505, mae: 5.056241, mean_q: 5.255395
 46395/100000: episode: 4736, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.259, mean reward: 0.426 [0.394, 0.456], mean action: 40.700 [8.000, 74.000], mean observation: 3.160 [-1.456, 10.440], loss: 1.052635, mae: 5.056088, mean_q: 5.256749
 46405/100000: episode: 4737, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.853, mean reward: 0.385 [0.344, 0.471], mean action: 48.800 [36.000, 87.000], mean observation: 3.168 [-1.246, 10.259], loss: 1.142671, mae: 5.056449, mean_q: 5.258068
 46415/100000: episode: 4738, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.264, mean reward: 0.426 [0.331, 0.567], mean action: 47.000 [19.000, 84.000], mean observation: 3.163 [-1.726, 10.359], loss: 1.135367, mae: 5.056752, mean_q: 5.259341
 46425/100000: episode: 4739, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.719, mean reward: 0.372 [0.323, 0.430], mean action: 49.400 [10.000, 93.000], mean observation: 3.156 [-1.163, 10.274], loss: 1.639282, mae: 5.058678, mean_q: 5.258904
 46435/100000: episode: 4740, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.647, mean reward: 0.365 [0.319, 0.430], mean action: 60.900 [26.000, 100.000], mean observation: 3.164 [-1.063, 10.310], loss: 1.149885, mae: 5.056543, mean_q: 5.259212
 46444/100000: episode: 4741, duration: 0.162s, episode steps: 9, steps per second: 56, episode reward: 13.186, mean reward: 1.465 [0.389, 10.000], mean action: 44.889 [5.000, 78.000], mean observation: 3.162 [-1.282, 10.327], loss: 1.160515, mae: 5.056657, mean_q: 5.258478
 46454/100000: episode: 4742, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.862, mean reward: 0.386 [0.324, 0.517], mean action: 48.300 [28.000, 90.000], mean observation: 3.153 [-1.233, 10.207], loss: 1.230720, mae: 5.057028, mean_q: 5.254589
 46464/100000: episode: 4743, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 5.143, mean reward: 0.514 [0.365, 0.586], mean action: 47.700 [0.000, 94.000], mean observation: 3.155 [-2.051, 10.326], loss: 1.191578, mae: 5.057138, mean_q: 5.251427
 46474/100000: episode: 4744, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 3.820, mean reward: 0.382 [0.360, 0.452], mean action: 51.600 [18.000, 97.000], mean observation: 3.163 [-1.385, 10.309], loss: 1.065503, mae: 5.056556, mean_q: 5.250175
 46484/100000: episode: 4745, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.435, mean reward: 0.443 [0.405, 0.533], mean action: 38.700 [3.000, 96.000], mean observation: 3.155 [-1.256, 10.253], loss: 1.222755, mae: 5.057199, mean_q: 5.249338
 46494/100000: episode: 4746, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.937, mean reward: 0.394 [0.332, 0.452], mean action: 51.500 [21.000, 80.000], mean observation: 3.156 [-1.925, 10.300], loss: 1.099699, mae: 5.056849, mean_q: 5.248960
 46504/100000: episode: 4747, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.047, mean reward: 0.405 [0.396, 0.445], mean action: 42.700 [7.000, 80.000], mean observation: 3.156 [-1.536, 10.239], loss: 1.545107, mae: 5.058325, mean_q: 5.248657
 46514/100000: episode: 4748, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.630, mean reward: 0.463 [0.412, 0.484], mean action: 38.300 [4.000, 76.000], mean observation: 3.179 [-1.588, 10.327], loss: 1.333979, mae: 5.057191, mean_q: 5.249524
 46524/100000: episode: 4749, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 5.609, mean reward: 0.561 [0.561, 0.561], mean action: 53.700 [40.000, 99.000], mean observation: 3.167 [-1.201, 10.223], loss: 1.427594, mae: 5.057014, mean_q: 5.251035
 46534/100000: episode: 4750, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.668, mean reward: 0.367 [0.319, 0.418], mean action: 46.100 [0.000, 95.000], mean observation: 3.160 [-1.926, 10.460], loss: 1.276323, mae: 5.056130, mean_q: 5.248223
 46544/100000: episode: 4751, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.693, mean reward: 0.369 [0.321, 0.430], mean action: 48.300 [8.000, 94.000], mean observation: 3.152 [-1.453, 10.268], loss: 1.013790, mae: 5.055382, mean_q: 5.244899
 46554/100000: episode: 4752, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.166, mean reward: 0.417 [0.403, 0.440], mean action: 63.200 [1.000, 98.000], mean observation: 3.161 [-0.902, 10.391], loss: 1.657086, mae: 5.057594, mean_q: 5.241727
 46564/100000: episode: 4753, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.708, mean reward: 0.371 [0.305, 0.476], mean action: 34.800 [6.000, 56.000], mean observation: 3.151 [-1.616, 10.215], loss: 1.200024, mae: 5.055474, mean_q: 5.237218
 46574/100000: episode: 4754, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.035, mean reward: 0.404 [0.388, 0.440], mean action: 55.100 [24.000, 88.000], mean observation: 3.151 [-1.516, 10.212], loss: 1.103231, mae: 5.055275, mean_q: 5.236290
 46584/100000: episode: 4755, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.844, mean reward: 0.384 [0.277, 0.472], mean action: 34.700 [11.000, 76.000], mean observation: 3.158 [-0.893, 10.338], loss: 1.050758, mae: 5.054923, mean_q: 5.234441
 46594/100000: episode: 4756, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.371, mean reward: 0.437 [0.383, 0.488], mean action: 38.800 [15.000, 77.000], mean observation: 3.155 [-0.936, 10.261], loss: 1.333821, mae: 5.056132, mean_q: 5.235069
 46604/100000: episode: 4757, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 5.006, mean reward: 0.501 [0.341, 0.540], mean action: 42.000 [7.000, 78.000], mean observation: 3.159 [-1.960, 10.366], loss: 1.294131, mae: 5.056045, mean_q: 5.235065
 46614/100000: episode: 4758, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.001, mean reward: 0.400 [0.324, 0.478], mean action: 44.300 [15.000, 78.000], mean observation: 3.164 [-1.533, 10.405], loss: 1.075605, mae: 5.055265, mean_q: 5.237132
 46624/100000: episode: 4759, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.277, mean reward: 0.428 [0.389, 0.504], mean action: 50.100 [38.000, 77.000], mean observation: 3.139 [-1.193, 10.259], loss: 1.466822, mae: 5.056910, mean_q: 5.237965
 46634/100000: episode: 4760, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.718, mean reward: 0.372 [0.285, 0.452], mean action: 43.600 [10.000, 76.000], mean observation: 3.159 [-1.208, 10.367], loss: 1.112545, mae: 5.055427, mean_q: 5.238228
 46644/100000: episode: 4761, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.109, mean reward: 0.411 [0.349, 0.470], mean action: 44.600 [9.000, 84.000], mean observation: 3.162 [-1.416, 10.349], loss: 1.146890, mae: 5.055647, mean_q: 5.238549
 46654/100000: episode: 4762, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.684, mean reward: 0.368 [0.315, 0.478], mean action: 48.000 [17.000, 101.000], mean observation: 3.158 [-1.781, 10.375], loss: 1.589001, mae: 5.057411, mean_q: 5.236318
 46664/100000: episode: 4763, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.168, mean reward: 0.417 [0.315, 0.534], mean action: 50.700 [5.000, 73.000], mean observation: 3.155 [-1.110, 10.296], loss: 1.366471, mae: 5.056463, mean_q: 5.235583
 46674/100000: episode: 4764, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.698, mean reward: 0.370 [0.307, 0.442], mean action: 54.900 [25.000, 88.000], mean observation: 3.151 [-1.236, 10.241], loss: 0.897865, mae: 5.054128, mean_q: 5.236397
 46684/100000: episode: 4765, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.568, mean reward: 0.457 [0.337, 0.486], mean action: 42.600 [3.000, 95.000], mean observation: 3.159 [-1.232, 10.478], loss: 0.956166, mae: 5.054286, mean_q: 5.238324
 46694/100000: episode: 4766, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.359, mean reward: 0.436 [0.358, 0.569], mean action: 40.800 [2.000, 58.000], mean observation: 3.149 [-1.287, 10.349], loss: 1.175666, mae: 5.055284, mean_q: 5.239978
 46704/100000: episode: 4767, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.819, mean reward: 0.382 [0.327, 0.484], mean action: 43.900 [0.000, 83.000], mean observation: 3.156 [-1.747, 10.274], loss: 1.419591, mae: 5.056346, mean_q: 5.241059
 46714/100000: episode: 4768, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.463, mean reward: 0.446 [0.388, 0.523], mean action: 56.100 [10.000, 95.000], mean observation: 3.159 [-1.768, 10.277], loss: 1.123348, mae: 5.055008, mean_q: 5.236197
 46724/100000: episode: 4769, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.776, mean reward: 0.378 [0.306, 0.494], mean action: 61.400 [31.000, 95.000], mean observation: 3.159 [-1.846, 10.283], loss: 0.859382, mae: 5.053949, mean_q: 5.233675
 46734/100000: episode: 4770, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.500, mean reward: 0.450 [0.441, 0.489], mean action: 48.900 [26.000, 92.000], mean observation: 3.173 [-1.364, 10.474], loss: 1.526517, mae: 5.056333, mean_q: 5.233767
 46744/100000: episode: 4771, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 5.046, mean reward: 0.505 [0.496, 0.538], mean action: 47.000 [12.000, 89.000], mean observation: 3.160 [-1.125, 10.340], loss: 1.368573, mae: 5.055501, mean_q: 5.234365
 46754/100000: episode: 4772, duration: 0.153s, episode steps: 10, steps per second: 66, episode reward: 3.963, mean reward: 0.396 [0.338, 0.487], mean action: 42.500 [0.000, 90.000], mean observation: 3.160 [-1.102, 10.245], loss: 1.479914, mae: 5.055673, mean_q: 5.235079
 46764/100000: episode: 4773, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.147, mean reward: 0.415 [0.349, 0.481], mean action: 47.800 [10.000, 98.000], mean observation: 3.157 [-1.179, 10.279], loss: 1.151866, mae: 5.053991, mean_q: 5.235846
 46774/100000: episode: 4774, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.456, mean reward: 0.346 [0.267, 0.414], mean action: 59.300 [24.000, 101.000], mean observation: 3.164 [-1.208, 10.423], loss: 1.050530, mae: 5.053777, mean_q: 5.236779
 46784/100000: episode: 4775, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.190, mean reward: 0.419 [0.391, 0.456], mean action: 48.200 [37.000, 67.000], mean observation: 3.138 [-0.904, 10.281], loss: 1.282109, mae: 5.054631, mean_q: 5.237733
 46794/100000: episode: 4776, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.488, mean reward: 0.449 [0.342, 0.535], mean action: 41.600 [10.000, 89.000], mean observation: 3.149 [-0.959, 10.248], loss: 1.181623, mae: 5.054526, mean_q: 5.239011
 46804/100000: episode: 4777, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.101, mean reward: 0.410 [0.355, 0.517], mean action: 44.700 [12.000, 79.000], mean observation: 3.173 [-1.281, 10.392], loss: 1.171680, mae: 5.054642, mean_q: 5.239007
 46814/100000: episode: 4778, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.085, mean reward: 0.408 [0.378, 0.567], mean action: 35.500 [8.000, 43.000], mean observation: 3.152 [-0.749, 10.174], loss: 1.174478, mae: 5.054351, mean_q: 5.235932
 46824/100000: episode: 4779, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.279, mean reward: 0.428 [0.329, 0.452], mean action: 52.400 [2.000, 100.000], mean observation: 3.164 [-0.976, 10.334], loss: 1.495813, mae: 5.055648, mean_q: 5.232922
 46834/100000: episode: 4780, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.895, mean reward: 0.389 [0.345, 0.526], mean action: 44.300 [4.000, 93.000], mean observation: 3.149 [-2.149, 10.424], loss: 0.806897, mae: 5.052890, mean_q: 5.228897
 46844/100000: episode: 4781, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 3.969, mean reward: 0.397 [0.380, 0.463], mean action: 58.800 [23.000, 100.000], mean observation: 3.159 [-0.916, 10.332], loss: 1.423668, mae: 5.055711, mean_q: 5.227396
 46854/100000: episode: 4782, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.869, mean reward: 0.387 [0.348, 0.467], mean action: 48.700 [6.000, 91.000], mean observation: 3.159 [-1.371, 10.382], loss: 1.235570, mae: 5.054880, mean_q: 5.228156
 46864/100000: episode: 4783, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.095, mean reward: 0.410 [0.282, 0.556], mean action: 42.400 [4.000, 86.000], mean observation: 3.143 [-1.531, 10.373], loss: 0.905890, mae: 5.053393, mean_q: 5.229248
 46874/100000: episode: 4784, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.838, mean reward: 0.384 [0.341, 0.461], mean action: 49.700 [43.000, 71.000], mean observation: 3.163 [-0.930, 10.252], loss: 1.353498, mae: 5.055137, mean_q: 5.230489
 46884/100000: episode: 4785, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.158, mean reward: 0.416 [0.338, 0.550], mean action: 41.600 [7.000, 55.000], mean observation: 3.152 [-0.874, 10.437], loss: 1.211913, mae: 5.054077, mean_q: 5.229561
 46894/100000: episode: 4786, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.073, mean reward: 0.407 [0.383, 0.475], mean action: 47.700 [34.000, 85.000], mean observation: 3.154 [-1.136, 10.255], loss: 1.386506, mae: 5.054665, mean_q: 5.228343
 46904/100000: episode: 4787, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.199, mean reward: 0.420 [0.323, 0.477], mean action: 34.300 [3.000, 82.000], mean observation: 3.159 [-1.017, 10.381], loss: 1.205477, mae: 5.053679, mean_q: 5.228765
 46914/100000: episode: 4788, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 5.004, mean reward: 0.500 [0.418, 0.521], mean action: 67.100 [4.000, 95.000], mean observation: 3.158 [-1.226, 10.415], loss: 1.412006, mae: 5.054606, mean_q: 5.229493
 46924/100000: episode: 4789, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.020, mean reward: 0.402 [0.371, 0.444], mean action: 68.600 [26.000, 95.000], mean observation: 3.157 [-1.081, 10.240], loss: 1.235207, mae: 5.053667, mean_q: 5.230533
 46934/100000: episode: 4790, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.817, mean reward: 0.382 [0.341, 0.450], mean action: 64.400 [30.000, 95.000], mean observation: 3.145 [-1.009, 10.414], loss: 1.489737, mae: 5.054584, mean_q: 5.231104
 46944/100000: episode: 4791, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 4.903, mean reward: 0.490 [0.435, 0.560], mean action: 56.600 [41.000, 101.000], mean observation: 3.152 [-2.032, 10.449], loss: 1.540590, mae: 5.054341, mean_q: 5.231373
 46945/100000: episode: 4792, duration: 0.033s, episode steps: 1, steps per second: 31, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 34.000 [34.000, 34.000], mean observation: 3.186 [-1.066, 10.565], loss: 2.202047, mae: 5.056586, mean_q: 5.232419
 46946/100000: episode: 4793, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 101.000 [101.000, 101.000], mean observation: 3.155 [-0.699, 10.108], loss: 1.160329, mae: 5.052563, mean_q: 5.233016
 46956/100000: episode: 4794, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.069, mean reward: 0.407 [0.314, 0.569], mean action: 54.600 [41.000, 91.000], mean observation: 3.148 [-1.496, 10.362], loss: 1.286540, mae: 5.052782, mean_q: 5.234661
 46966/100000: episode: 4795, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.050, mean reward: 0.405 [0.356, 0.476], mean action: 50.100 [8.000, 97.000], mean observation: 3.154 [-1.746, 10.312], loss: 1.504336, mae: 5.053279, mean_q: 5.236876
 46976/100000: episode: 4796, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.463, mean reward: 0.446 [0.357, 0.555], mean action: 43.400 [6.000, 96.000], mean observation: 3.156 [-1.326, 10.293], loss: 1.313405, mae: 5.052201, mean_q: 5.238205
 46986/100000: episode: 4797, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.011, mean reward: 0.401 [0.298, 0.528], mean action: 47.200 [1.000, 83.000], mean observation: 3.147 [-1.339, 10.259], loss: 1.179961, mae: 5.051349, mean_q: 5.238922
 46996/100000: episode: 4798, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.445, mean reward: 0.445 [0.314, 0.506], mean action: 46.800 [15.000, 97.000], mean observation: 3.171 [-1.970, 10.261], loss: 1.267214, mae: 5.051244, mean_q: 5.239731
 47006/100000: episode: 4799, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.935, mean reward: 0.393 [0.325, 0.461], mean action: 43.900 [23.000, 86.000], mean observation: 3.156 [-1.117, 10.292], loss: 1.255004, mae: 5.051164, mean_q: 5.240799
 47010/100000: episode: 4800, duration: 0.070s, episode steps: 4, steps per second: 57, episode reward: 11.342, mean reward: 2.836 [0.429, 10.000], mean action: 40.750 [40.000, 41.000], mean observation: 3.161 [-1.601, 10.231], loss: 0.988760, mae: 5.049942, mean_q: 5.238765
 47020/100000: episode: 4801, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.139, mean reward: 0.414 [0.357, 0.461], mean action: 35.900 [3.000, 74.000], mean observation: 3.154 [-1.522, 10.175], loss: 1.239061, mae: 5.050995, mean_q: 5.236491
 47030/100000: episode: 4802, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.412, mean reward: 0.441 [0.334, 0.540], mean action: 48.100 [22.000, 101.000], mean observation: 3.163 [-1.118, 10.491], loss: 1.391349, mae: 5.051694, mean_q: 5.236291
 47040/100000: episode: 4803, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.280, mean reward: 0.428 [0.345, 0.530], mean action: 35.300 [6.000, 83.000], mean observation: 3.167 [-1.586, 10.350], loss: 1.037663, mae: 5.050366, mean_q: 5.238155
 47050/100000: episode: 4804, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.386, mean reward: 0.439 [0.383, 0.594], mean action: 44.200 [8.000, 88.000], mean observation: 3.158 [-1.671, 10.339], loss: 1.278959, mae: 5.051148, mean_q: 5.240378
 47060/100000: episode: 4805, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.783, mean reward: 0.378 [0.320, 0.509], mean action: 41.800 [41.000, 49.000], mean observation: 3.171 [-1.688, 10.305], loss: 1.247907, mae: 5.050897, mean_q: 5.241650
 47070/100000: episode: 4806, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.198, mean reward: 0.420 [0.370, 0.467], mean action: 42.700 [10.000, 74.000], mean observation: 3.168 [-2.099, 10.420], loss: 1.144391, mae: 5.050550, mean_q: 5.238091
 47080/100000: episode: 4807, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.430, mean reward: 0.443 [0.417, 0.538], mean action: 46.200 [11.000, 94.000], mean observation: 3.133 [-1.456, 10.279], loss: 1.262812, mae: 5.050990, mean_q: 5.236629
 47090/100000: episode: 4808, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.124, mean reward: 0.412 [0.335, 0.542], mean action: 53.400 [27.000, 93.000], mean observation: 3.156 [-1.912, 10.270], loss: 1.321837, mae: 5.051166, mean_q: 5.237712
 47100/100000: episode: 4809, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.085, mean reward: 0.408 [0.375, 0.457], mean action: 56.200 [11.000, 95.000], mean observation: 3.144 [-1.357, 10.292], loss: 1.134157, mae: 5.049928, mean_q: 5.238887
 47110/100000: episode: 4810, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.168, mean reward: 0.417 [0.360, 0.552], mean action: 39.600 [9.000, 59.000], mean observation: 3.167 [-1.259, 10.522], loss: 1.248266, mae: 5.049906, mean_q: 5.240013
 47120/100000: episode: 4811, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.378, mean reward: 0.438 [0.386, 0.548], mean action: 53.800 [2.000, 100.000], mean observation: 3.154 [-1.181, 10.318], loss: 1.305009, mae: 5.050440, mean_q: 5.241718
 47130/100000: episode: 4812, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.922, mean reward: 0.492 [0.422, 0.568], mean action: 42.200 [7.000, 75.000], mean observation: 3.176 [-1.188, 10.364], loss: 1.045753, mae: 5.049398, mean_q: 5.243824
 47140/100000: episode: 4813, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.618, mean reward: 0.362 [0.299, 0.431], mean action: 53.800 [41.000, 95.000], mean observation: 3.151 [-0.967, 10.398], loss: 1.391319, mae: 5.050768, mean_q: 5.241770
 47150/100000: episode: 4814, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.066, mean reward: 0.407 [0.312, 0.515], mean action: 54.100 [31.000, 97.000], mean observation: 3.161 [-1.192, 10.308], loss: 1.197028, mae: 5.049947, mean_q: 5.240374
 47160/100000: episode: 4815, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.711, mean reward: 0.371 [0.326, 0.471], mean action: 51.700 [31.000, 85.000], mean observation: 3.150 [-1.128, 10.222], loss: 1.180608, mae: 5.050103, mean_q: 5.241355
 47170/100000: episode: 4816, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.309, mean reward: 0.431 [0.392, 0.457], mean action: 40.400 [12.000, 69.000], mean observation: 3.171 [-1.760, 10.305], loss: 1.289476, mae: 5.050608, mean_q: 5.243242
 47180/100000: episode: 4817, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.903, mean reward: 0.390 [0.294, 0.541], mean action: 41.100 [14.000, 82.000], mean observation: 3.155 [-1.822, 10.220], loss: 1.312352, mae: 5.050790, mean_q: 5.244856
 47190/100000: episode: 4818, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.337, mean reward: 0.434 [0.351, 0.489], mean action: 45.400 [0.000, 91.000], mean observation: 3.158 [-1.047, 10.295], loss: 1.487553, mae: 5.051315, mean_q: 5.246043
 47200/100000: episode: 4819, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 5.092, mean reward: 0.509 [0.509, 0.509], mean action: 37.800 [15.000, 55.000], mean observation: 3.161 [-1.238, 10.388], loss: 1.338538, mae: 5.050692, mean_q: 5.247634
 47210/100000: episode: 4820, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.270, mean reward: 0.427 [0.367, 0.501], mean action: 37.200 [1.000, 74.000], mean observation: 3.159 [-1.298, 10.318], loss: 1.325318, mae: 5.050265, mean_q: 5.248918
 47220/100000: episode: 4821, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.069, mean reward: 0.407 [0.353, 0.426], mean action: 35.600 [9.000, 67.000], mean observation: 3.162 [-1.583, 10.287], loss: 1.196965, mae: 5.049393, mean_q: 5.249859
 47230/100000: episode: 4822, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.980, mean reward: 0.398 [0.311, 0.537], mean action: 50.800 [2.000, 100.000], mean observation: 3.162 [-1.284, 10.365], loss: 1.240271, mae: 5.049652, mean_q: 5.251708
 47240/100000: episode: 4823, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.013, mean reward: 0.401 [0.353, 0.424], mean action: 27.900 [0.000, 62.000], mean observation: 3.156 [-1.298, 10.425], loss: 1.138407, mae: 5.049353, mean_q: 5.254079
 47250/100000: episode: 4824, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.284, mean reward: 0.428 [0.413, 0.454], mean action: 45.400 [25.000, 101.000], mean observation: 3.155 [-1.363, 10.272], loss: 1.050084, mae: 5.049002, mean_q: 5.251634
 47260/100000: episode: 4825, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.107, mean reward: 0.411 [0.349, 0.549], mean action: 50.000 [24.000, 89.000], mean observation: 3.155 [-1.704, 10.413], loss: 1.489419, mae: 5.050886, mean_q: 5.249297
 47270/100000: episode: 4826, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.510, mean reward: 0.351 [0.313, 0.496], mean action: 44.000 [26.000, 84.000], mean observation: 3.157 [-1.046, 10.342], loss: 0.971841, mae: 5.048509, mean_q: 5.247203
 47280/100000: episode: 4827, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.944, mean reward: 0.394 [0.315, 0.468], mean action: 44.000 [41.000, 71.000], mean observation: 3.165 [-1.824, 10.512], loss: 1.128650, mae: 5.049334, mean_q: 5.246509
 47290/100000: episode: 4828, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.544, mean reward: 0.454 [0.450, 0.495], mean action: 41.400 [27.000, 62.000], mean observation: 3.149 [-1.046, 10.467], loss: 1.400796, mae: 5.050301, mean_q: 5.248153
 47300/100000: episode: 4829, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.849, mean reward: 0.385 [0.332, 0.431], mean action: 66.600 [35.000, 90.000], mean observation: 3.152 [-1.502, 10.319], loss: 1.214033, mae: 5.049493, mean_q: 5.250135
 47310/100000: episode: 4830, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.202, mean reward: 0.420 [0.288, 0.491], mean action: 35.000 [7.000, 45.000], mean observation: 3.155 [-1.040, 10.339], loss: 1.355542, mae: 5.049940, mean_q: 5.251435
 47320/100000: episode: 4831, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.903, mean reward: 0.390 [0.313, 0.493], mean action: 33.900 [4.000, 67.000], mean observation: 3.148 [-1.671, 10.312], loss: 1.285998, mae: 5.049483, mean_q: 5.253650
 47330/100000: episode: 4832, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.054, mean reward: 0.405 [0.357, 0.491], mean action: 49.100 [28.000, 84.000], mean observation: 3.165 [-1.544, 10.319], loss: 1.673707, mae: 5.050677, mean_q: 5.257443
 47340/100000: episode: 4833, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.850, mean reward: 0.385 [0.326, 0.460], mean action: 48.600 [33.000, 81.000], mean observation: 3.161 [-0.971, 10.249], loss: 1.283720, mae: 5.048528, mean_q: 5.259430
 47350/100000: episode: 4834, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.139, mean reward: 0.414 [0.370, 0.482], mean action: 54.000 [26.000, 101.000], mean observation: 3.173 [-1.467, 10.349], loss: 1.123573, mae: 5.047864, mean_q: 5.260882
 47360/100000: episode: 4835, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.267, mean reward: 0.427 [0.318, 0.497], mean action: 39.400 [5.000, 69.000], mean observation: 3.156 [-1.328, 10.264], loss: 1.157368, mae: 5.048074, mean_q: 5.257554
 47370/100000: episode: 4836, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.057, mean reward: 0.406 [0.335, 0.538], mean action: 46.100 [22.000, 91.000], mean observation: 3.163 [-1.235, 10.305], loss: 1.356025, mae: 5.048957, mean_q: 5.255642
 47380/100000: episode: 4837, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.511, mean reward: 0.351 [0.311, 0.406], mean action: 39.800 [0.000, 92.000], mean observation: 3.154 [-1.260, 10.391], loss: 1.517733, mae: 5.049458, mean_q: 5.253692
 47390/100000: episode: 4838, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.242, mean reward: 0.424 [0.319, 0.593], mean action: 47.000 [28.000, 74.000], mean observation: 3.166 [-1.566, 10.281], loss: 1.207086, mae: 5.048088, mean_q: 5.245957
 47400/100000: episode: 4839, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.964, mean reward: 0.396 [0.308, 0.486], mean action: 45.500 [12.000, 89.000], mean observation: 3.166 [-1.433, 10.448], loss: 1.559171, mae: 5.049165, mean_q: 5.247206
 47410/100000: episode: 4840, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.328, mean reward: 0.433 [0.433, 0.433], mean action: 51.600 [43.000, 98.000], mean observation: 3.164 [-1.263, 10.296], loss: 1.461013, mae: 5.048410, mean_q: 5.250050
 47420/100000: episode: 4841, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.667, mean reward: 0.367 [0.301, 0.446], mean action: 45.000 [16.000, 62.000], mean observation: 3.166 [-1.383, 10.273], loss: 1.136501, mae: 5.046695, mean_q: 5.251840
 47430/100000: episode: 4842, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.286, mean reward: 0.429 [0.411, 0.475], mean action: 43.000 [43.000, 43.000], mean observation: 3.140 [-1.519, 10.333], loss: 1.435601, mae: 5.047565, mean_q: 5.251593
 47440/100000: episode: 4843, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.142, mean reward: 0.414 [0.351, 0.512], mean action: 35.900 [11.000, 96.000], mean observation: 3.144 [-1.381, 10.266], loss: 1.149838, mae: 5.046248, mean_q: 5.250090
 47450/100000: episode: 4844, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.243, mean reward: 0.324 [0.267, 0.355], mean action: 51.300 [43.000, 89.000], mean observation: 3.157 [-1.759, 10.376], loss: 1.209481, mae: 5.046498, mean_q: 5.251432
 47460/100000: episode: 4845, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.490, mean reward: 0.449 [0.361, 0.519], mean action: 42.400 [7.000, 96.000], mean observation: 3.146 [-1.788, 10.298], loss: 1.177873, mae: 5.046288, mean_q: 5.253713
 47470/100000: episode: 4846, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.858, mean reward: 0.486 [0.486, 0.486], mean action: 54.100 [30.000, 94.000], mean observation: 3.156 [-1.373, 10.487], loss: 1.244795, mae: 5.046645, mean_q: 5.256761
 47480/100000: episode: 4847, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 3.954, mean reward: 0.395 [0.344, 0.475], mean action: 42.700 [22.000, 61.000], mean observation: 3.159 [-1.769, 10.319], loss: 1.099647, mae: 5.045961, mean_q: 5.259483
 47490/100000: episode: 4848, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.145, mean reward: 0.414 [0.380, 0.484], mean action: 43.900 [10.000, 68.000], mean observation: 3.163 [-1.840, 10.253], loss: 1.360719, mae: 5.047437, mean_q: 5.261916
 47500/100000: episode: 4849, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.176, mean reward: 0.418 [0.394, 0.446], mean action: 61.600 [17.000, 100.000], mean observation: 3.157 [-1.731, 10.174], loss: 0.979444, mae: 5.046196, mean_q: 5.264409
 47504/100000: episode: 4850, duration: 0.066s, episode steps: 4, steps per second: 61, episode reward: 11.350, mean reward: 2.838 [0.405, 10.000], mean action: 59.000 [43.000, 83.000], mean observation: 3.162 [-1.282, 10.166], loss: 1.277043, mae: 5.047583, mean_q: 5.266104
 47514/100000: episode: 4851, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.734, mean reward: 0.373 [0.338, 0.418], mean action: 48.200 [9.000, 94.000], mean observation: 3.151 [-1.393, 10.177], loss: 1.468039, mae: 5.048385, mean_q: 5.268007
 47524/100000: episode: 4852, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.078, mean reward: 0.408 [0.366, 0.521], mean action: 56.100 [23.000, 95.000], mean observation: 3.165 [-1.363, 10.233], loss: 1.243868, mae: 5.047303, mean_q: 5.270789
 47534/100000: episode: 4853, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.797, mean reward: 0.380 [0.296, 0.426], mean action: 59.400 [40.000, 101.000], mean observation: 3.163 [-0.900, 10.321], loss: 1.164256, mae: 5.047050, mean_q: 5.273260
 47544/100000: episode: 4854, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.786, mean reward: 0.379 [0.280, 0.509], mean action: 53.000 [31.000, 93.000], mean observation: 3.157 [-1.466, 10.296], loss: 1.223095, mae: 5.047390, mean_q: 5.274819
 47554/100000: episode: 4855, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.637, mean reward: 0.364 [0.289, 0.475], mean action: 41.200 [5.000, 83.000], mean observation: 3.151 [-2.151, 10.270], loss: 1.181581, mae: 5.047332, mean_q: 5.272354
 47564/100000: episode: 4856, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.011, mean reward: 0.401 [0.361, 0.462], mean action: 32.400 [0.000, 48.000], mean observation: 3.149 [-1.605, 10.416], loss: 1.384775, mae: 5.048045, mean_q: 5.271650
 47574/100000: episode: 4857, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.328, mean reward: 0.433 [0.372, 0.489], mean action: 41.200 [16.000, 63.000], mean observation: 3.170 [-1.362, 10.524], loss: 1.235797, mae: 5.047421, mean_q: 5.272930
 47584/100000: episode: 4858, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.025, mean reward: 0.402 [0.322, 0.556], mean action: 42.700 [7.000, 83.000], mean observation: 3.166 [-1.465, 10.272], loss: 1.358954, mae: 5.047803, mean_q: 5.275159
 47594/100000: episode: 4859, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.863, mean reward: 0.386 [0.301, 0.453], mean action: 41.700 [3.000, 79.000], mean observation: 3.158 [-1.388, 10.299], loss: 1.628511, mae: 5.048739, mean_q: 5.276597
 47604/100000: episode: 4860, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.532, mean reward: 0.453 [0.400, 0.541], mean action: 44.800 [7.000, 79.000], mean observation: 3.140 [-1.900, 10.300], loss: 1.128036, mae: 5.046560, mean_q: 5.277980
 47614/100000: episode: 4861, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.911, mean reward: 0.391 [0.344, 0.411], mean action: 50.900 [1.000, 99.000], mean observation: 3.173 [-1.794, 10.416], loss: 1.178491, mae: 5.046386, mean_q: 5.280097
 47624/100000: episode: 4862, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.563, mean reward: 0.456 [0.454, 0.469], mean action: 37.200 [11.000, 52.000], mean observation: 3.178 [-1.639, 10.338], loss: 1.161773, mae: 5.046794, mean_q: 5.282311
 47634/100000: episode: 4863, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.622, mean reward: 0.462 [0.371, 0.507], mean action: 48.400 [30.000, 73.000], mean observation: 3.165 [-1.819, 10.407], loss: 1.204395, mae: 5.047202, mean_q: 5.284540
 47644/100000: episode: 4864, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.038, mean reward: 0.404 [0.349, 0.490], mean action: 42.100 [2.000, 72.000], mean observation: 3.155 [-1.195, 10.333], loss: 1.048438, mae: 5.046661, mean_q: 5.287320
 47654/100000: episode: 4865, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.277, mean reward: 0.428 [0.398, 0.549], mean action: 54.100 [41.000, 97.000], mean observation: 3.162 [-1.753, 10.362], loss: 1.263004, mae: 5.047629, mean_q: 5.289905
 47664/100000: episode: 4866, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.227, mean reward: 0.423 [0.326, 0.483], mean action: 47.700 [18.000, 87.000], mean observation: 3.154 [-1.272, 10.398], loss: 1.075544, mae: 5.047217, mean_q: 5.286292
 47674/100000: episode: 4867, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 6.100, mean reward: 0.610 [0.610, 0.610], mean action: 52.100 [41.000, 100.000], mean observation: 3.164 [-2.042, 10.243], loss: 1.119284, mae: 5.047297, mean_q: 5.281447
 47680/100000: episode: 4868, duration: 0.110s, episode steps: 6, steps per second: 55, episode reward: 11.822, mean reward: 1.970 [0.325, 10.000], mean action: 50.000 [41.000, 95.000], mean observation: 3.162 [-1.888, 10.236], loss: 1.334174, mae: 5.048472, mean_q: 5.281974
 47690/100000: episode: 4869, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.213, mean reward: 0.421 [0.357, 0.562], mean action: 45.300 [24.000, 70.000], mean observation: 3.164 [-0.998, 10.341], loss: 1.310599, mae: 5.048351, mean_q: 5.283667
 47700/100000: episode: 4870, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.628, mean reward: 0.363 [0.274, 0.443], mean action: 42.700 [2.000, 87.000], mean observation: 3.156 [-0.755, 10.263], loss: 1.446308, mae: 5.048759, mean_q: 5.281654
 47710/100000: episode: 4871, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.131, mean reward: 0.413 [0.390, 0.494], mean action: 53.700 [41.000, 86.000], mean observation: 3.174 [-1.540, 10.288], loss: 1.408771, mae: 5.048499, mean_q: 5.280510
 47720/100000: episode: 4872, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.455, mean reward: 0.446 [0.404, 0.484], mean action: 42.500 [6.000, 84.000], mean observation: 3.142 [-1.329, 10.254], loss: 1.194783, mae: 5.047793, mean_q: 5.281940
 47730/100000: episode: 4873, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.292, mean reward: 0.429 [0.342, 0.528], mean action: 42.500 [12.000, 62.000], mean observation: 3.151 [-1.448, 10.268], loss: 1.524736, mae: 5.048998, mean_q: 5.284011
 47740/100000: episode: 4874, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 5.276, mean reward: 0.528 [0.528, 0.528], mean action: 39.100 [5.000, 58.000], mean observation: 3.138 [-0.946, 10.260], loss: 0.666078, mae: 5.046027, mean_q: 5.285924
 47750/100000: episode: 4875, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.232, mean reward: 0.423 [0.358, 0.510], mean action: 50.300 [21.000, 92.000], mean observation: 3.149 [-1.004, 10.361], loss: 1.384567, mae: 5.049163, mean_q: 5.286962
 47760/100000: episode: 4876, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.095, mean reward: 0.410 [0.342, 0.541], mean action: 46.000 [1.000, 96.000], mean observation: 3.154 [-1.467, 10.412], loss: 1.179149, mae: 5.048548, mean_q: 5.284491
 47770/100000: episode: 4877, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.157, mean reward: 0.416 [0.376, 0.484], mean action: 42.800 [6.000, 68.000], mean observation: 3.153 [-1.946, 10.291], loss: 1.027830, mae: 5.048298, mean_q: 5.283890
 47780/100000: episode: 4878, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.773, mean reward: 0.377 [0.355, 0.423], mean action: 45.900 [22.000, 101.000], mean observation: 3.157 [-1.260, 10.297], loss: 1.222433, mae: 5.049567, mean_q: 5.280822
 47790/100000: episode: 4879, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.968, mean reward: 0.397 [0.327, 0.521], mean action: 45.700 [3.000, 93.000], mean observation: 3.149 [-1.259, 10.298], loss: 1.177378, mae: 5.049788, mean_q: 5.280218
 47800/100000: episode: 4880, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.617, mean reward: 0.362 [0.332, 0.399], mean action: 46.300 [23.000, 82.000], mean observation: 3.147 [-1.367, 10.245], loss: 1.263953, mae: 5.050377, mean_q: 5.282739
 47810/100000: episode: 4881, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.878, mean reward: 0.388 [0.373, 0.441], mean action: 46.100 [35.000, 76.000], mean observation: 3.145 [-1.242, 10.282], loss: 1.299214, mae: 5.050630, mean_q: 5.286853
 47820/100000: episode: 4882, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.755, mean reward: 0.476 [0.476, 0.476], mean action: 72.400 [34.000, 99.000], mean observation: 3.172 [-1.114, 10.354], loss: 1.296312, mae: 5.050693, mean_q: 5.288043
 47830/100000: episode: 4883, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.714, mean reward: 0.371 [0.329, 0.417], mean action: 54.000 [29.000, 99.000], mean observation: 3.157 [-0.811, 10.406], loss: 1.171738, mae: 5.050347, mean_q: 5.287815
 47840/100000: episode: 4884, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.083, mean reward: 0.408 [0.400, 0.470], mean action: 49.400 [7.000, 96.000], mean observation: 3.167 [-1.332, 10.288], loss: 1.252221, mae: 5.050713, mean_q: 5.288672
 47850/100000: episode: 4885, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.109, mean reward: 0.411 [0.379, 0.539], mean action: 46.600 [38.000, 100.000], mean observation: 3.143 [-1.424, 10.469], loss: 1.008175, mae: 5.050042, mean_q: 5.286728
 47860/100000: episode: 4886, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.081, mean reward: 0.408 [0.338, 0.470], mean action: 46.500 [3.000, 78.000], mean observation: 3.161 [-2.061, 10.315], loss: 1.293767, mae: 5.051423, mean_q: 5.281470
 47870/100000: episode: 4887, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.376, mean reward: 0.438 [0.377, 0.535], mean action: 34.300 [10.000, 41.000], mean observation: 3.139 [-1.431, 10.381], loss: 1.137948, mae: 5.051383, mean_q: 5.276860
 47880/100000: episode: 4888, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.297, mean reward: 0.430 [0.388, 0.531], mean action: 42.200 [20.000, 74.000], mean observation: 3.147 [-1.203, 10.211], loss: 1.463774, mae: 5.052659, mean_q: 5.274329
 47890/100000: episode: 4889, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.270, mean reward: 0.427 [0.334, 0.561], mean action: 51.700 [2.000, 94.000], mean observation: 3.146 [-1.729, 10.261], loss: 1.176862, mae: 5.051383, mean_q: 5.267083
 47900/100000: episode: 4890, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.423, mean reward: 0.442 [0.428, 0.523], mean action: 41.100 [13.000, 65.000], mean observation: 3.148 [-1.271, 10.191], loss: 1.510474, mae: 5.052360, mean_q: 5.261899
 47910/100000: episode: 4891, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.018, mean reward: 0.402 [0.366, 0.563], mean action: 52.800 [30.000, 100.000], mean observation: 3.145 [-1.115, 10.328], loss: 1.301138, mae: 5.051247, mean_q: 5.253926
 47920/100000: episode: 4892, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.589, mean reward: 0.359 [0.317, 0.432], mean action: 41.900 [9.000, 94.000], mean observation: 3.163 [-1.734, 10.387], loss: 1.411083, mae: 5.051265, mean_q: 5.251331
 47930/100000: episode: 4893, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.672, mean reward: 0.367 [0.314, 0.429], mean action: 48.100 [41.000, 98.000], mean observation: 3.161 [-1.389, 10.354], loss: 1.158251, mae: 5.050279, mean_q: 5.252185
 47940/100000: episode: 4894, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.088, mean reward: 0.409 [0.328, 0.531], mean action: 41.400 [0.000, 98.000], mean observation: 3.161 [-0.942, 10.289], loss: 1.223592, mae: 5.050468, mean_q: 5.254348
 47950/100000: episode: 4895, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.187, mean reward: 0.419 [0.372, 0.501], mean action: 46.300 [9.000, 101.000], mean observation: 3.166 [-1.420, 10.362], loss: 1.366402, mae: 5.050756, mean_q: 5.257598
 47960/100000: episode: 4896, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.688, mean reward: 0.369 [0.294, 0.472], mean action: 53.100 [38.000, 90.000], mean observation: 3.159 [-1.087, 10.301], loss: 1.358315, mae: 5.050912, mean_q: 5.258223
 47970/100000: episode: 4897, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.111, mean reward: 0.411 [0.332, 0.523], mean action: 43.400 [2.000, 97.000], mean observation: 3.157 [-1.100, 10.332], loss: 1.021592, mae: 5.049295, mean_q: 5.254120
 47980/100000: episode: 4898, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.810, mean reward: 0.381 [0.333, 0.443], mean action: 43.400 [17.000, 97.000], mean observation: 3.154 [-1.676, 10.300], loss: 1.576733, mae: 5.051665, mean_q: 5.254150
 47990/100000: episode: 4899, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.645, mean reward: 0.364 [0.292, 0.425], mean action: 23.400 [1.000, 41.000], mean observation: 3.154 [-1.122, 10.311], loss: 1.280231, mae: 5.050178, mean_q: 5.255824
 48000/100000: episode: 4900, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.228, mean reward: 0.423 [0.327, 0.530], mean action: 33.000 [8.000, 72.000], mean observation: 3.162 [-1.623, 10.286], loss: 1.506063, mae: 5.050576, mean_q: 5.256808
 48010/100000: episode: 4901, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.483, mean reward: 0.448 [0.338, 0.587], mean action: 44.300 [8.000, 98.000], mean observation: 3.152 [-2.064, 10.563], loss: 1.245297, mae: 5.049173, mean_q: 5.258800
 48020/100000: episode: 4902, duration: 0.215s, episode steps: 10, steps per second: 47, episode reward: 4.725, mean reward: 0.473 [0.415, 0.571], mean action: 21.600 [1.000, 101.000], mean observation: 3.166 [-1.935, 10.459], loss: 1.246899, mae: 5.049024, mean_q: 5.258771
 48030/100000: episode: 4903, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.330, mean reward: 0.433 [0.392, 0.495], mean action: 16.600 [4.000, 46.000], mean observation: 3.166 [-2.187, 10.582], loss: 1.517299, mae: 5.050114, mean_q: 5.255924
 48040/100000: episode: 4904, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.257, mean reward: 0.426 [0.379, 0.489], mean action: 41.500 [8.000, 90.000], mean observation: 3.144 [-1.119, 10.372], loss: 1.523870, mae: 5.049710, mean_q: 5.254706
 48050/100000: episode: 4905, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.089, mean reward: 0.409 [0.311, 0.480], mean action: 26.900 [8.000, 99.000], mean observation: 3.162 [-1.443, 10.418], loss: 1.162715, mae: 5.048356, mean_q: 5.253451
 48060/100000: episode: 4906, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.117, mean reward: 0.412 [0.378, 0.459], mean action: 34.400 [3.000, 87.000], mean observation: 3.152 [-1.073, 10.287], loss: 1.123411, mae: 5.048389, mean_q: 5.252380
 48070/100000: episode: 4907, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.332, mean reward: 0.433 [0.425, 0.478], mean action: 39.300 [30.000, 41.000], mean observation: 3.171 [-1.483, 10.441], loss: 1.420878, mae: 5.049604, mean_q: 5.250802
 48080/100000: episode: 4908, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.394, mean reward: 0.439 [0.399, 0.514], mean action: 41.100 [4.000, 101.000], mean observation: 3.153 [-1.099, 10.269], loss: 1.147884, mae: 5.048624, mean_q: 5.251139
 48090/100000: episode: 4909, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.993, mean reward: 0.399 [0.327, 0.508], mean action: 49.100 [27.000, 81.000], mean observation: 3.153 [-1.453, 10.281], loss: 1.562490, mae: 5.050046, mean_q: 5.252795
 48100/100000: episode: 4910, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.108, mean reward: 0.411 [0.377, 0.458], mean action: 42.400 [40.000, 56.000], mean observation: 3.163 [-1.337, 10.300], loss: 1.361292, mae: 5.049109, mean_q: 5.254996
 48110/100000: episode: 4911, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.130, mean reward: 0.413 [0.373, 0.543], mean action: 32.600 [6.000, 52.000], mean observation: 3.155 [-1.969, 10.299], loss: 1.424755, mae: 5.049481, mean_q: 5.256701
 48120/100000: episode: 4912, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.139, mean reward: 0.414 [0.321, 0.454], mean action: 42.500 [12.000, 89.000], mean observation: 3.150 [-1.529, 10.273], loss: 1.242069, mae: 5.048578, mean_q: 5.259120
 48130/100000: episode: 4913, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.955, mean reward: 0.395 [0.318, 0.503], mean action: 46.700 [28.000, 98.000], mean observation: 3.152 [-1.476, 10.367], loss: 1.139332, mae: 5.048132, mean_q: 5.261732
 48140/100000: episode: 4914, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.174, mean reward: 0.417 [0.356, 0.470], mean action: 40.800 [33.000, 54.000], mean observation: 3.158 [-1.168, 10.345], loss: 1.319762, mae: 5.048957, mean_q: 5.264038
 48150/100000: episode: 4915, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.821, mean reward: 0.382 [0.342, 0.467], mean action: 50.300 [41.000, 100.000], mean observation: 3.164 [-1.515, 10.531], loss: 1.113752, mae: 5.048455, mean_q: 5.266349
 48160/100000: episode: 4916, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.960, mean reward: 0.396 [0.367, 0.466], mean action: 33.400 [5.000, 73.000], mean observation: 3.147 [-1.356, 10.328], loss: 1.160937, mae: 5.048658, mean_q: 5.267846
 48170/100000: episode: 4917, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.970, mean reward: 0.397 [0.324, 0.506], mean action: 37.000 [14.000, 67.000], mean observation: 3.166 [-1.505, 10.312], loss: 1.140903, mae: 5.048749, mean_q: 5.268336
 48180/100000: episode: 4918, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.962, mean reward: 0.396 [0.326, 0.495], mean action: 48.400 [11.000, 93.000], mean observation: 3.162 [-1.826, 10.264], loss: 1.326339, mae: 5.049853, mean_q: 5.268246
 48190/100000: episode: 4919, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.089, mean reward: 0.409 [0.336, 0.497], mean action: 30.900 [5.000, 41.000], mean observation: 3.162 [-1.132, 10.252], loss: 0.977292, mae: 5.049033, mean_q: 5.271561
 48200/100000: episode: 4920, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.726, mean reward: 0.473 [0.459, 0.543], mean action: 59.100 [41.000, 94.000], mean observation: 3.149 [-0.993, 10.314], loss: 1.287509, mae: 5.050527, mean_q: 5.275431
 48210/100000: episode: 4921, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.958, mean reward: 0.396 [0.336, 0.566], mean action: 50.800 [22.000, 88.000], mean observation: 3.168 [-1.642, 10.257], loss: 1.422892, mae: 5.051311, mean_q: 5.275096
 48220/100000: episode: 4922, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.954, mean reward: 0.395 [0.306, 0.561], mean action: 50.100 [13.000, 78.000], mean observation: 3.159 [-1.162, 10.272], loss: 1.252971, mae: 5.050771, mean_q: 5.275438
 48230/100000: episode: 4923, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.306, mean reward: 0.431 [0.398, 0.514], mean action: 34.700 [3.000, 41.000], mean observation: 3.165 [-1.124, 10.395], loss: 1.493208, mae: 5.051423, mean_q: 5.277197
 48240/100000: episode: 4924, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.240, mean reward: 0.424 [0.362, 0.538], mean action: 33.200 [2.000, 53.000], mean observation: 3.156 [-1.543, 10.328], loss: 1.361828, mae: 5.050813, mean_q: 5.279482
 48250/100000: episode: 4925, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.790, mean reward: 0.379 [0.339, 0.466], mean action: 47.100 [7.000, 98.000], mean observation: 3.154 [-1.564, 10.402], loss: 1.238052, mae: 5.050103, mean_q: 5.279265
 48260/100000: episode: 4926, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.597, mean reward: 0.360 [0.327, 0.431], mean action: 38.300 [0.000, 57.000], mean observation: 3.162 [-1.782, 10.251], loss: 1.275856, mae: 5.050465, mean_q: 5.277513
 48270/100000: episode: 4927, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.585, mean reward: 0.458 [0.454, 0.497], mean action: 40.000 [18.000, 81.000], mean observation: 3.148 [-1.384, 10.292], loss: 1.200634, mae: 5.050229, mean_q: 5.277689
 48280/100000: episode: 4928, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.199, mean reward: 0.420 [0.399, 0.527], mean action: 47.700 [17.000, 99.000], mean observation: 3.163 [-2.205, 10.409], loss: 1.611757, mae: 5.051964, mean_q: 5.274138
 48290/100000: episode: 4929, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.869, mean reward: 0.487 [0.427, 0.498], mean action: 48.700 [5.000, 99.000], mean observation: 3.163 [-1.009, 10.258], loss: 1.222500, mae: 5.050405, mean_q: 5.270701
 48300/100000: episode: 4930, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.623, mean reward: 0.362 [0.304, 0.452], mean action: 47.900 [41.000, 76.000], mean observation: 3.164 [-0.957, 10.270], loss: 1.017699, mae: 5.049584, mean_q: 5.266198
 48310/100000: episode: 4931, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.787, mean reward: 0.379 [0.310, 0.489], mean action: 45.100 [3.000, 91.000], mean observation: 3.158 [-1.691, 10.404], loss: 1.159684, mae: 5.050366, mean_q: 5.263583
 48320/100000: episode: 4932, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.043, mean reward: 0.404 [0.279, 0.515], mean action: 44.400 [10.000, 87.000], mean observation: 3.154 [-1.196, 10.430], loss: 1.555816, mae: 5.052166, mean_q: 5.264332
 48330/100000: episode: 4933, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.239, mean reward: 0.424 [0.329, 0.499], mean action: 38.200 [21.000, 65.000], mean observation: 3.160 [-1.253, 10.146], loss: 1.472084, mae: 5.051500, mean_q: 5.266666
 48340/100000: episode: 4934, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 5.791, mean reward: 0.579 [0.579, 0.579], mean action: 55.300 [32.000, 91.000], mean observation: 3.150 [-1.190, 10.304], loss: 1.109207, mae: 5.049894, mean_q: 5.269511
 48350/100000: episode: 4935, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.076, mean reward: 0.408 [0.297, 0.463], mean action: 55.100 [41.000, 90.000], mean observation: 3.157 [-1.506, 10.391], loss: 1.593926, mae: 5.051659, mean_q: 5.271661
 48360/100000: episode: 4936, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.034, mean reward: 0.403 [0.311, 0.454], mean action: 45.000 [31.000, 65.000], mean observation: 3.153 [-1.849, 10.339], loss: 1.233066, mae: 5.050572, mean_q: 5.273040
 48370/100000: episode: 4937, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.906, mean reward: 0.391 [0.359, 0.423], mean action: 46.600 [17.000, 87.000], mean observation: 3.169 [-1.506, 10.349], loss: 1.350652, mae: 5.051000, mean_q: 5.275311
 48380/100000: episode: 4938, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.677, mean reward: 0.368 [0.329, 0.421], mean action: 50.100 [41.000, 61.000], mean observation: 3.152 [-1.503, 10.177], loss: 1.198513, mae: 5.050454, mean_q: 5.277757
 48390/100000: episode: 4939, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.353, mean reward: 0.435 [0.326, 0.544], mean action: 57.000 [18.000, 100.000], mean observation: 3.149 [-1.382, 10.242], loss: 1.112967, mae: 5.050204, mean_q: 5.280656
 48400/100000: episode: 4940, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.998, mean reward: 0.400 [0.338, 0.438], mean action: 40.900 [1.000, 99.000], mean observation: 3.161 [-1.316, 10.295], loss: 1.498286, mae: 5.051570, mean_q: 5.282495
 48410/100000: episode: 4941, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.959, mean reward: 0.396 [0.301, 0.550], mean action: 40.700 [2.000, 70.000], mean observation: 3.156 [-1.269, 10.370], loss: 1.134671, mae: 5.050295, mean_q: 5.279109
 48420/100000: episode: 4942, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.844, mean reward: 0.384 [0.304, 0.532], mean action: 49.200 [34.000, 98.000], mean observation: 3.154 [-1.330, 10.261], loss: 1.328631, mae: 5.051053, mean_q: 5.279116
 48430/100000: episode: 4943, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.887, mean reward: 0.389 [0.304, 0.545], mean action: 41.000 [17.000, 76.000], mean observation: 3.155 [-1.034, 10.327], loss: 1.082426, mae: 5.050363, mean_q: 5.276935
 48433/100000: episode: 4944, duration: 0.067s, episode steps: 3, steps per second: 45, episode reward: 10.717, mean reward: 3.572 [0.347, 10.000], mean action: 36.667 [28.000, 41.000], mean observation: 3.148 [-1.057, 10.193], loss: 1.383783, mae: 5.051423, mean_q: 5.274444
 48443/100000: episode: 4945, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.972, mean reward: 0.397 [0.366, 0.555], mean action: 40.000 [3.000, 71.000], mean observation: 3.159 [-1.050, 10.431], loss: 1.360486, mae: 5.051617, mean_q: 5.274085
 48453/100000: episode: 4946, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.225, mean reward: 0.423 [0.367, 0.444], mean action: 41.100 [5.000, 84.000], mean observation: 3.171 [-1.482, 10.278], loss: 1.158555, mae: 5.050896, mean_q: 5.274283
 48463/100000: episode: 4947, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.873, mean reward: 0.387 [0.363, 0.461], mean action: 40.500 [14.000, 71.000], mean observation: 3.152 [-1.258, 10.297], loss: 1.252243, mae: 5.051259, mean_q: 5.273549
 48473/100000: episode: 4948, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.624, mean reward: 0.362 [0.306, 0.438], mean action: 43.600 [16.000, 78.000], mean observation: 3.163 [-1.337, 10.255], loss: 1.375405, mae: 5.051604, mean_q: 5.274228
 48483/100000: episode: 4949, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.325, mean reward: 0.433 [0.373, 0.573], mean action: 35.800 [12.000, 62.000], mean observation: 3.162 [-1.112, 10.260], loss: 1.472050, mae: 5.051808, mean_q: 5.271306
 48493/100000: episode: 4950, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.026, mean reward: 0.403 [0.366, 0.478], mean action: 51.700 [35.000, 90.000], mean observation: 3.156 [-1.133, 10.228], loss: 1.276726, mae: 5.050775, mean_q: 5.272312
 48503/100000: episode: 4951, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.772, mean reward: 0.377 [0.355, 0.428], mean action: 48.100 [41.000, 99.000], mean observation: 3.147 [-1.079, 10.342], loss: 1.124406, mae: 5.050109, mean_q: 5.274924
 48513/100000: episode: 4952, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.290, mean reward: 0.429 [0.346, 0.519], mean action: 48.900 [5.000, 92.000], mean observation: 3.156 [-1.536, 10.291], loss: 1.331014, mae: 5.051055, mean_q: 5.274755
 48523/100000: episode: 4953, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.374, mean reward: 0.437 [0.328, 0.511], mean action: 48.700 [18.000, 96.000], mean observation: 3.166 [-1.936, 10.250], loss: 1.182901, mae: 5.050352, mean_q: 5.271263
 48533/100000: episode: 4954, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.892, mean reward: 0.389 [0.295, 0.526], mean action: 48.500 [15.000, 84.000], mean observation: 3.154 [-1.625, 10.402], loss: 1.622313, mae: 5.051942, mean_q: 5.270071
 48543/100000: episode: 4955, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.535, mean reward: 0.454 [0.287, 0.520], mean action: 35.600 [1.000, 52.000], mean observation: 3.151 [-1.406, 10.298], loss: 1.130947, mae: 5.050128, mean_q: 5.266747
 48553/100000: episode: 4956, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.227, mean reward: 0.423 [0.360, 0.524], mean action: 52.300 [23.000, 96.000], mean observation: 3.154 [-1.452, 10.308], loss: 1.456346, mae: 5.051386, mean_q: 5.265338
 48563/100000: episode: 4957, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.219, mean reward: 0.422 [0.418, 0.457], mean action: 48.500 [18.000, 93.000], mean observation: 3.148 [-1.955, 10.474], loss: 1.388475, mae: 5.050940, mean_q: 5.267623
 48573/100000: episode: 4958, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.279, mean reward: 0.428 [0.384, 0.532], mean action: 49.300 [41.000, 78.000], mean observation: 3.156 [-1.503, 10.362], loss: 1.165695, mae: 5.050075, mean_q: 5.269544
 48583/100000: episode: 4959, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.141, mean reward: 0.414 [0.317, 0.569], mean action: 33.500 [2.000, 50.000], mean observation: 3.165 [-1.158, 10.345], loss: 1.251574, mae: 5.050685, mean_q: 5.269774
 48593/100000: episode: 4960, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.480, mean reward: 0.448 [0.448, 0.448], mean action: 57.100 [22.000, 90.000], mean observation: 3.140 [-1.617, 10.348], loss: 1.293349, mae: 5.050919, mean_q: 5.272422
 48603/100000: episode: 4961, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.015, mean reward: 0.401 [0.321, 0.478], mean action: 44.600 [11.000, 99.000], mean observation: 3.142 [-1.523, 10.235], loss: 0.990897, mae: 5.049801, mean_q: 5.273259
 48613/100000: episode: 4962, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.131, mean reward: 0.413 [0.379, 0.523], mean action: 29.400 [0.000, 96.000], mean observation: 3.153 [-1.163, 10.399], loss: 1.127588, mae: 5.050634, mean_q: 5.273234
 48623/100000: episode: 4963, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.296, mean reward: 0.430 [0.364, 0.466], mean action: 42.100 [5.000, 90.000], mean observation: 3.167 [-1.508, 10.276], loss: 1.289234, mae: 5.051537, mean_q: 5.272727
 48630/100000: episode: 4964, duration: 0.127s, episode steps: 7, steps per second: 55, episode reward: 12.954, mean reward: 1.851 [0.492, 10.000], mean action: 44.429 [9.000, 79.000], mean observation: 3.158 [-1.246, 10.393], loss: 0.915084, mae: 5.050441, mean_q: 5.272018
 48631/100000: episode: 4965, duration: 0.024s, episode steps: 1, steps per second: 41, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 41.000 [41.000, 41.000], mean observation: 3.141 [-0.662, 10.100], loss: 1.515829, mae: 5.052650, mean_q: 5.270877
 48641/100000: episode: 4966, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.938, mean reward: 0.394 [0.346, 0.457], mean action: 37.600 [9.000, 59.000], mean observation: 3.156 [-1.139, 10.358], loss: 1.222486, mae: 5.051810, mean_q: 5.268842
 48651/100000: episode: 4967, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.092, mean reward: 0.409 [0.399, 0.449], mean action: 54.100 [38.000, 94.000], mean observation: 3.152 [-0.974, 10.333], loss: 1.537919, mae: 5.052941, mean_q: 5.267179
 48661/100000: episode: 4968, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.414, mean reward: 0.441 [0.320, 0.590], mean action: 44.100 [2.000, 73.000], mean observation: 3.149 [-1.349, 10.390], loss: 1.082142, mae: 5.051005, mean_q: 5.263445
 48671/100000: episode: 4969, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.294, mean reward: 0.429 [0.366, 0.529], mean action: 46.100 [22.000, 74.000], mean observation: 3.161 [-1.023, 10.370], loss: 1.197128, mae: 5.051340, mean_q: 5.264303
 48681/100000: episode: 4970, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 5.105, mean reward: 0.510 [0.477, 0.514], mean action: 50.400 [11.000, 93.000], mean observation: 3.172 [-1.567, 10.364], loss: 0.935921, mae: 5.050544, mean_q: 5.266604
 48691/100000: episode: 4971, duration: 0.221s, episode steps: 10, steps per second: 45, episode reward: 4.315, mean reward: 0.432 [0.343, 0.592], mean action: 48.900 [41.000, 82.000], mean observation: 3.135 [-1.561, 10.279], loss: 1.206172, mae: 5.051877, mean_q: 5.267383
 48701/100000: episode: 4972, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.227, mean reward: 0.423 [0.375, 0.464], mean action: 55.700 [11.000, 87.000], mean observation: 3.153 [-1.434, 10.383], loss: 0.988701, mae: 5.051296, mean_q: 5.266298
 48711/100000: episode: 4973, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.660, mean reward: 0.466 [0.466, 0.466], mean action: 44.000 [33.000, 64.000], mean observation: 3.148 [-1.876, 10.342], loss: 1.216179, mae: 5.052310, mean_q: 5.263276
 48721/100000: episode: 4974, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.989, mean reward: 0.499 [0.366, 0.539], mean action: 51.200 [3.000, 89.000], mean observation: 3.180 [-1.295, 10.453], loss: 1.047549, mae: 5.052480, mean_q: 5.260944
 48731/100000: episode: 4975, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.785, mean reward: 0.379 [0.323, 0.404], mean action: 35.200 [7.000, 41.000], mean observation: 3.146 [-1.544, 10.356], loss: 1.258110, mae: 5.053310, mean_q: 5.257207
 48741/100000: episode: 4976, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.544, mean reward: 0.454 [0.398, 0.502], mean action: 53.300 [10.000, 96.000], mean observation: 3.147 [-0.955, 10.437], loss: 1.086349, mae: 5.052836, mean_q: 5.251506
 48751/100000: episode: 4977, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.046, mean reward: 0.405 [0.373, 0.434], mean action: 38.500 [14.000, 87.000], mean observation: 3.140 [-1.503, 10.277], loss: 1.202266, mae: 5.053360, mean_q: 5.248661
 48761/100000: episode: 4978, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 5.177, mean reward: 0.518 [0.515, 0.530], mean action: 44.000 [25.000, 86.000], mean observation: 3.137 [-1.342, 10.387], loss: 1.122899, mae: 5.053117, mean_q: 5.250151
 48771/100000: episode: 4979, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.044, mean reward: 0.404 [0.329, 0.534], mean action: 42.600 [5.000, 99.000], mean observation: 3.158 [-1.305, 10.295], loss: 1.396273, mae: 5.054462, mean_q: 5.252977
 48781/100000: episode: 4980, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.047, mean reward: 0.405 [0.366, 0.524], mean action: 40.900 [9.000, 98.000], mean observation: 3.168 [-1.825, 10.362], loss: 1.122273, mae: 5.053368, mean_q: 5.255034
 48791/100000: episode: 4981, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.497, mean reward: 0.450 [0.426, 0.524], mean action: 45.300 [15.000, 87.000], mean observation: 3.162 [-1.437, 10.355], loss: 1.159114, mae: 5.053576, mean_q: 5.253757
 48801/100000: episode: 4982, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.002, mean reward: 0.400 [0.332, 0.517], mean action: 48.400 [12.000, 82.000], mean observation: 3.155 [-0.798, 10.237], loss: 1.251303, mae: 5.054104, mean_q: 5.255040
 48811/100000: episode: 4983, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.687, mean reward: 0.369 [0.301, 0.506], mean action: 49.400 [0.000, 84.000], mean observation: 3.159 [-0.913, 10.328], loss: 1.020024, mae: 5.053701, mean_q: 5.255810
 48817/100000: episode: 4984, duration: 0.130s, episode steps: 6, steps per second: 46, episode reward: 12.825, mean reward: 2.137 [0.565, 10.000], mean action: 50.833 [33.000, 66.000], mean observation: 3.149 [-1.076, 10.315], loss: 1.256978, mae: 5.054371, mean_q: 5.256440
 48821/100000: episode: 4985, duration: 0.083s, episode steps: 4, steps per second: 48, episode reward: 11.231, mean reward: 2.808 [0.391, 10.000], mean action: 31.000 [6.000, 51.000], mean observation: 3.172 [-1.801, 10.216], loss: 1.400087, mae: 5.055136, mean_q: 5.257165
 48831/100000: episode: 4986, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.610, mean reward: 0.461 [0.349, 0.670], mean action: 48.600 [0.000, 93.000], mean observation: 3.155 [-1.809, 10.273], loss: 0.936634, mae: 5.053511, mean_q: 5.258447
 48841/100000: episode: 4987, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.608, mean reward: 0.461 [0.415, 0.548], mean action: 54.200 [20.000, 90.000], mean observation: 3.156 [-1.692, 10.355], loss: 1.387258, mae: 5.055406, mean_q: 5.260030
 48851/100000: episode: 4988, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.739, mean reward: 0.374 [0.337, 0.507], mean action: 58.000 [27.000, 94.000], mean observation: 3.167 [-0.999, 10.275], loss: 1.357159, mae: 5.055015, mean_q: 5.261569
 48861/100000: episode: 4989, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.347, mean reward: 0.435 [0.369, 0.558], mean action: 55.300 [10.000, 95.000], mean observation: 3.166 [-1.378, 10.317], loss: 1.315298, mae: 5.054828, mean_q: 5.263700
 48871/100000: episode: 4990, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.128, mean reward: 0.413 [0.323, 0.467], mean action: 39.100 [5.000, 75.000], mean observation: 3.157 [-1.076, 10.380], loss: 1.195987, mae: 5.054213, mean_q: 5.265023
 48881/100000: episode: 4991, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 5.264, mean reward: 0.526 [0.526, 0.526], mean action: 42.900 [17.000, 51.000], mean observation: 3.154 [-1.870, 10.335], loss: 1.317744, mae: 5.054623, mean_q: 5.260152
 48891/100000: episode: 4992, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.800, mean reward: 0.380 [0.331, 0.450], mean action: 44.200 [35.000, 57.000], mean observation: 3.159 [-1.754, 10.360], loss: 1.257116, mae: 5.054238, mean_q: 5.259283
 48901/100000: episode: 4993, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.274, mean reward: 0.427 [0.414, 0.519], mean action: 42.300 [32.000, 84.000], mean observation: 3.154 [-1.342, 10.211], loss: 1.269465, mae: 5.054376, mean_q: 5.260276
 48911/100000: episode: 4994, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.792, mean reward: 0.379 [0.347, 0.456], mean action: 48.100 [32.000, 99.000], mean observation: 3.151 [-1.015, 10.380], loss: 1.635998, mae: 5.055833, mean_q: 5.262328
 48921/100000: episode: 4995, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.484, mean reward: 0.448 [0.348, 0.538], mean action: 41.600 [14.000, 92.000], mean observation: 3.146 [-1.457, 10.284], loss: 0.880180, mae: 5.052760, mean_q: 5.262691
 48931/100000: episode: 4996, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.742, mean reward: 0.374 [0.300, 0.444], mean action: 44.000 [14.000, 77.000], mean observation: 3.157 [-1.287, 10.259], loss: 0.982903, mae: 5.053053, mean_q: 5.262361
 48941/100000: episode: 4997, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.150, mean reward: 0.415 [0.355, 0.469], mean action: 29.800 [6.000, 40.000], mean observation: 3.153 [-1.508, 10.233], loss: 1.456363, mae: 5.054987, mean_q: 5.262956
 48951/100000: episode: 4998, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.917, mean reward: 0.392 [0.326, 0.486], mean action: 34.100 [6.000, 92.000], mean observation: 3.168 [-0.842, 10.477], loss: 1.284422, mae: 5.054316, mean_q: 5.264414
 48961/100000: episode: 4999, duration: 0.153s, episode steps: 10, steps per second: 66, episode reward: 4.345, mean reward: 0.434 [0.393, 0.484], mean action: 44.800 [15.000, 95.000], mean observation: 3.144 [-1.295, 10.301], loss: 1.347266, mae: 5.054748, mean_q: 5.266301
 48971/100000: episode: 5000, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 3.580, mean reward: 0.358 [0.256, 0.472], mean action: 62.600 [18.000, 100.000], mean observation: 3.165 [-1.290, 10.467], loss: 1.404636, mae: 5.054603, mean_q: 5.265903
 48981/100000: episode: 5001, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.190, mean reward: 0.419 [0.354, 0.533], mean action: 40.000 [6.000, 96.000], mean observation: 3.151 [-1.969, 10.405], loss: 1.580449, mae: 5.054975, mean_q: 5.263040
 48991/100000: episode: 5002, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.772, mean reward: 0.377 [0.330, 0.452], mean action: 43.500 [6.000, 92.000], mean observation: 3.145 [-1.580, 10.347], loss: 1.609730, mae: 5.054599, mean_q: 5.260760
 49001/100000: episode: 5003, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.797, mean reward: 0.480 [0.473, 0.505], mean action: 50.200 [3.000, 101.000], mean observation: 3.143 [-1.639, 10.237], loss: 1.471190, mae: 5.053529, mean_q: 5.255372
 49011/100000: episode: 5004, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.325, mean reward: 0.433 [0.367, 0.470], mean action: 39.500 [0.000, 87.000], mean observation: 3.144 [-1.116, 10.285], loss: 1.353992, mae: 5.052678, mean_q: 5.252877
 49021/100000: episode: 5005, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.855, mean reward: 0.486 [0.483, 0.505], mean action: 45.400 [19.000, 62.000], mean observation: 3.156 [-1.451, 10.176], loss: 1.161308, mae: 5.051625, mean_q: 5.254200
 49031/100000: episode: 5006, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.694, mean reward: 0.369 [0.316, 0.432], mean action: 46.700 [15.000, 101.000], mean observation: 3.157 [-1.016, 10.434], loss: 1.086431, mae: 5.051312, mean_q: 5.255591
 49041/100000: episode: 5007, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.892, mean reward: 0.389 [0.282, 0.471], mean action: 53.000 [38.000, 98.000], mean observation: 3.140 [-1.257, 10.272], loss: 1.079980, mae: 5.051480, mean_q: 5.257611
 49051/100000: episode: 5008, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.958, mean reward: 0.396 [0.321, 0.458], mean action: 38.400 [12.000, 61.000], mean observation: 3.151 [-1.559, 10.477], loss: 1.216196, mae: 5.052239, mean_q: 5.259510
 49061/100000: episode: 5009, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 3.788, mean reward: 0.379 [0.276, 0.477], mean action: 32.900 [0.000, 43.000], mean observation: 3.159 [-2.284, 10.318], loss: 0.946585, mae: 5.051296, mean_q: 5.261349
 49071/100000: episode: 5010, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.596, mean reward: 0.360 [0.295, 0.449], mean action: 36.400 [10.000, 45.000], mean observation: 3.158 [-1.584, 10.305], loss: 1.588343, mae: 5.053819, mean_q: 5.261486
 49081/100000: episode: 5011, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.159, mean reward: 0.416 [0.331, 0.511], mean action: 56.700 [10.000, 101.000], mean observation: 3.166 [-2.123, 10.427], loss: 1.462045, mae: 5.053068, mean_q: 5.258508
 49082/100000: episode: 5012, duration: 0.027s, episode steps: 1, steps per second: 36, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 43.000 [43.000, 43.000], mean observation: 3.141 [-1.353, 10.194], loss: 1.172795, mae: 5.051557, mean_q: 5.256610
 49092/100000: episode: 5013, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.631, mean reward: 0.463 [0.395, 0.478], mean action: 40.200 [18.000, 63.000], mean observation: 3.155 [-1.684, 10.187], loss: 1.084348, mae: 5.051412, mean_q: 5.257203
 49102/100000: episode: 5014, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.016, mean reward: 0.402 [0.321, 0.468], mean action: 35.600 [3.000, 58.000], mean observation: 3.163 [-1.202, 10.393], loss: 1.374619, mae: 5.052663, mean_q: 5.257041
 49112/100000: episode: 5015, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.207, mean reward: 0.421 [0.336, 0.559], mean action: 43.200 [1.000, 88.000], mean observation: 3.157 [-1.229, 10.348], loss: 1.525160, mae: 5.053034, mean_q: 5.255826
 49122/100000: episode: 5016, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.085, mean reward: 0.409 [0.340, 0.469], mean action: 52.400 [35.000, 100.000], mean observation: 3.169 [-0.825, 10.439], loss: 1.277683, mae: 5.051886, mean_q: 5.254795
 49132/100000: episode: 5017, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.774, mean reward: 0.377 [0.327, 0.464], mean action: 55.000 [4.000, 90.000], mean observation: 3.163 [-1.260, 10.337], loss: 1.390249, mae: 5.052128, mean_q: 5.255315
 49142/100000: episode: 5018, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.860, mean reward: 0.386 [0.321, 0.418], mean action: 61.200 [38.000, 100.000], mean observation: 3.135 [-1.025, 10.283], loss: 0.910302, mae: 5.050572, mean_q: 5.254154
 49152/100000: episode: 5019, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.716, mean reward: 0.472 [0.392, 0.515], mean action: 44.700 [1.000, 94.000], mean observation: 3.154 [-1.836, 10.289], loss: 1.124704, mae: 5.051885, mean_q: 5.254908
 49162/100000: episode: 5020, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.731, mean reward: 0.373 [0.338, 0.457], mean action: 45.700 [34.000, 83.000], mean observation: 3.169 [-1.194, 10.303], loss: 1.304730, mae: 5.052888, mean_q: 5.255984
 49172/100000: episode: 5021, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.418, mean reward: 0.442 [0.364, 0.482], mean action: 49.900 [15.000, 82.000], mean observation: 3.160 [-0.921, 10.227], loss: 1.069164, mae: 5.052191, mean_q: 5.255098
 49182/100000: episode: 5022, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.849, mean reward: 0.385 [0.315, 0.538], mean action: 53.400 [13.000, 101.000], mean observation: 3.161 [-1.645, 10.258], loss: 1.145750, mae: 5.052763, mean_q: 5.254548
 49192/100000: episode: 5023, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.925, mean reward: 0.393 [0.351, 0.488], mean action: 42.100 [4.000, 66.000], mean observation: 3.159 [-1.093, 10.374], loss: 1.147591, mae: 5.052954, mean_q: 5.256307
 49202/100000: episode: 5024, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.173, mean reward: 0.417 [0.402, 0.474], mean action: 57.600 [43.000, 95.000], mean observation: 3.158 [-1.049, 10.302], loss: 1.071364, mae: 5.052939, mean_q: 5.259025
 49212/100000: episode: 5025, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.733, mean reward: 0.473 [0.371, 0.553], mean action: 41.800 [15.000, 56.000], mean observation: 3.156 [-1.512, 10.314], loss: 1.495928, mae: 5.055000, mean_q: 5.258454
 49222/100000: episode: 5026, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.047, mean reward: 0.405 [0.365, 0.436], mean action: 43.800 [2.000, 82.000], mean observation: 3.158 [-1.776, 10.285], loss: 1.678694, mae: 5.055526, mean_q: 5.256179
 49232/100000: episode: 5027, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.161, mean reward: 0.416 [0.322, 0.495], mean action: 38.100 [2.000, 71.000], mean observation: 3.159 [-2.118, 10.305], loss: 1.335969, mae: 5.054182, mean_q: 5.253129
 49239/100000: episode: 5028, duration: 0.133s, episode steps: 7, steps per second: 53, episode reward: 12.348, mean reward: 1.764 [0.363, 10.000], mean action: 46.429 [24.000, 78.000], mean observation: 3.162 [-1.256, 10.285], loss: 1.042799, mae: 5.052712, mean_q: 5.252241
 49249/100000: episode: 5029, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.939, mean reward: 0.394 [0.331, 0.471], mean action: 55.700 [42.000, 92.000], mean observation: 3.166 [-1.282, 10.253], loss: 1.402528, mae: 5.054013, mean_q: 5.253233
 49259/100000: episode: 5030, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.955, mean reward: 0.395 [0.305, 0.470], mean action: 50.000 [43.000, 90.000], mean observation: 3.152 [-1.326, 10.369], loss: 1.194052, mae: 5.053117, mean_q: 5.254287
 49269/100000: episode: 5031, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.217, mean reward: 0.422 [0.391, 0.523], mean action: 43.100 [6.000, 88.000], mean observation: 3.164 [-1.329, 10.330], loss: 1.423187, mae: 5.053570, mean_q: 5.250140
 49279/100000: episode: 5032, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.935, mean reward: 0.393 [0.300, 0.577], mean action: 40.200 [10.000, 79.000], mean observation: 3.143 [-1.280, 10.166], loss: 1.056782, mae: 5.051827, mean_q: 5.248861
 49289/100000: episode: 5033, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.079, mean reward: 0.408 [0.287, 0.512], mean action: 44.300 [22.000, 71.000], mean observation: 3.157 [-1.101, 10.251], loss: 1.328078, mae: 5.052877, mean_q: 5.248301
 49299/100000: episode: 5034, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.060, mean reward: 0.406 [0.396, 0.473], mean action: 39.100 [3.000, 70.000], mean observation: 3.142 [-1.396, 10.338], loss: 1.354377, mae: 5.052884, mean_q: 5.246049
 49309/100000: episode: 5035, duration: 0.114s, episode steps: 10, steps per second: 87, episode reward: 3.613, mean reward: 0.361 [0.327, 0.442], mean action: 71.700 [42.000, 95.000], mean observation: 3.163 [-0.614, 10.331], loss: 1.432948, mae: 5.052939, mean_q: 5.244154
 49319/100000: episode: 5036, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.913, mean reward: 0.391 [0.358, 0.415], mean action: 84.800 [40.000, 95.000], mean observation: 3.158 [-1.041, 10.205], loss: 1.393209, mae: 5.052450, mean_q: 5.243330
 49329/100000: episode: 5037, duration: 0.086s, episode steps: 10, steps per second: 116, episode reward: 4.097, mean reward: 0.410 [0.400, 0.451], mean action: 95.000 [95.000, 95.000], mean observation: 3.164 [-0.924, 10.251], loss: 1.015340, mae: 5.050675, mean_q: 5.242864
 49339/100000: episode: 5038, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.845, mean reward: 0.385 [0.340, 0.403], mean action: 59.400 [0.000, 95.000], mean observation: 3.168 [-1.623, 10.303], loss: 1.119848, mae: 5.051124, mean_q: 5.240823
 49349/100000: episode: 5039, duration: 0.117s, episode steps: 10, steps per second: 86, episode reward: 4.151, mean reward: 0.415 [0.338, 0.479], mean action: 79.300 [5.000, 95.000], mean observation: 3.151 [-1.191, 10.247], loss: 1.385594, mae: 5.052396, mean_q: 5.238187
 49359/100000: episode: 5040, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.123, mean reward: 0.412 [0.353, 0.445], mean action: 79.600 [36.000, 96.000], mean observation: 3.154 [-1.263, 10.263], loss: 1.153098, mae: 5.051635, mean_q: 5.235506
 49369/100000: episode: 5041, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 3.137, mean reward: 0.314 [0.307, 0.363], mean action: 92.400 [76.000, 95.000], mean observation: 3.162 [-2.017, 10.392], loss: 1.350487, mae: 5.052590, mean_q: 5.235682
 49379/100000: episode: 5042, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.438, mean reward: 0.444 [0.415, 0.470], mean action: 58.400 [3.000, 95.000], mean observation: 3.158 [-1.699, 10.482], loss: 1.182190, mae: 5.052144, mean_q: 5.236825
 49389/100000: episode: 5043, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.047, mean reward: 0.405 [0.405, 0.405], mean action: 70.400 [26.000, 99.000], mean observation: 3.159 [-1.719, 10.285], loss: 0.971286, mae: 5.051328, mean_q: 5.238447
 49399/100000: episode: 5044, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 4.092, mean reward: 0.409 [0.409, 0.409], mean action: 86.800 [60.000, 95.000], mean observation: 3.136 [-1.192, 10.240], loss: 1.104263, mae: 5.052086, mean_q: 5.241492
 49409/100000: episode: 5045, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 4.822, mean reward: 0.482 [0.482, 0.482], mean action: 85.000 [27.000, 95.000], mean observation: 3.148 [-1.080, 10.308], loss: 1.250085, mae: 5.052787, mean_q: 5.244137
 49419/100000: episode: 5046, duration: 0.120s, episode steps: 10, steps per second: 84, episode reward: 5.044, mean reward: 0.504 [0.504, 0.504], mean action: 80.200 [26.000, 95.000], mean observation: 3.179 [-1.848, 10.254], loss: 1.340671, mae: 5.053371, mean_q: 5.244029
 49429/100000: episode: 5047, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 3.777, mean reward: 0.378 [0.368, 0.400], mean action: 73.600 [10.000, 100.000], mean observation: 3.155 [-1.109, 10.475], loss: 1.496477, mae: 5.053893, mean_q: 5.240911
 49439/100000: episode: 5048, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.244, mean reward: 0.424 [0.377, 0.485], mean action: 70.100 [21.000, 95.000], mean observation: 3.164 [-1.660, 10.522], loss: 1.180970, mae: 5.052765, mean_q: 5.240094
 49449/100000: episode: 5049, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.004, mean reward: 0.400 [0.301, 0.453], mean action: 54.600 [2.000, 92.000], mean observation: 3.162 [-1.024, 10.382], loss: 1.070563, mae: 5.052495, mean_q: 5.240234
 49459/100000: episode: 5050, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.643, mean reward: 0.364 [0.310, 0.439], mean action: 61.800 [16.000, 94.000], mean observation: 3.155 [-1.761, 10.413], loss: 1.101901, mae: 5.052758, mean_q: 5.242000
 49469/100000: episode: 5051, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.932, mean reward: 0.393 [0.386, 0.437], mean action: 63.700 [57.000, 87.000], mean observation: 3.153 [-1.586, 10.373], loss: 1.179927, mae: 5.053199, mean_q: 5.243645
 49479/100000: episode: 5052, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.173, mean reward: 0.417 [0.364, 0.439], mean action: 49.600 [3.000, 86.000], mean observation: 3.150 [-1.106, 10.430], loss: 1.563047, mae: 5.054671, mean_q: 5.244786
 49489/100000: episode: 5053, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.172, mean reward: 0.417 [0.328, 0.522], mean action: 47.100 [7.000, 91.000], mean observation: 3.153 [-1.481, 10.274], loss: 1.237855, mae: 5.053346, mean_q: 5.245938
 49499/100000: episode: 5054, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.777, mean reward: 0.378 [0.298, 0.432], mean action: 51.200 [8.000, 72.000], mean observation: 3.163 [-1.440, 10.258], loss: 1.210938, mae: 5.052975, mean_q: 5.247594
 49509/100000: episode: 5055, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.832, mean reward: 0.483 [0.457, 0.510], mean action: 64.600 [23.000, 95.000], mean observation: 3.157 [-1.155, 10.407], loss: 1.227206, mae: 5.053163, mean_q: 5.249207
 49519/100000: episode: 5056, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.173, mean reward: 0.417 [0.351, 0.460], mean action: 50.000 [6.000, 97.000], mean observation: 3.151 [-1.299, 10.419], loss: 1.113171, mae: 5.052598, mean_q: 5.250663
 49529/100000: episode: 5057, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 3.727, mean reward: 0.373 [0.320, 0.420], mean action: 61.900 [3.000, 96.000], mean observation: 3.150 [-1.293, 10.278], loss: 1.486672, mae: 5.054016, mean_q: 5.251668
 49539/100000: episode: 5058, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 4.132, mean reward: 0.413 [0.375, 0.445], mean action: 59.700 [42.000, 72.000], mean observation: 3.153 [-1.210, 10.222], loss: 1.466348, mae: 5.053864, mean_q: 5.252784
 49549/100000: episode: 5059, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.768, mean reward: 0.377 [0.279, 0.471], mean action: 50.900 [1.000, 62.000], mean observation: 3.156 [-1.629, 10.289], loss: 1.203392, mae: 5.052289, mean_q: 5.254817
 49559/100000: episode: 5060, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.923, mean reward: 0.392 [0.343, 0.489], mean action: 50.000 [6.000, 78.000], mean observation: 3.142 [-1.778, 10.238], loss: 1.419279, mae: 5.053004, mean_q: 5.257277
 49569/100000: episode: 5061, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.377, mean reward: 0.438 [0.389, 0.496], mean action: 51.700 [18.000, 83.000], mean observation: 3.166 [-1.384, 10.288], loss: 1.016387, mae: 5.051435, mean_q: 5.259145
 49579/100000: episode: 5062, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.992, mean reward: 0.399 [0.313, 0.481], mean action: 57.300 [36.000, 74.000], mean observation: 3.144 [-1.848, 10.199], loss: 1.251624, mae: 5.052412, mean_q: 5.260394
 49589/100000: episode: 5063, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.790, mean reward: 0.379 [0.328, 0.442], mean action: 52.900 [5.000, 79.000], mean observation: 3.147 [-2.116, 10.269], loss: 1.039074, mae: 5.051659, mean_q: 5.261609
 49599/100000: episode: 5064, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.302, mean reward: 0.430 [0.383, 0.571], mean action: 44.300 [0.000, 78.000], mean observation: 3.163 [-2.007, 10.324], loss: 1.128952, mae: 5.052305, mean_q: 5.262971
 49609/100000: episode: 5065, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.166, mean reward: 0.417 [0.382, 0.516], mean action: 46.500 [8.000, 100.000], mean observation: 3.152 [-1.103, 10.352], loss: 1.446656, mae: 5.053680, mean_q: 5.264755
 49619/100000: episode: 5066, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.124, mean reward: 0.412 [0.380, 0.512], mean action: 44.100 [0.000, 77.000], mean observation: 3.164 [-2.832, 10.314], loss: 1.130522, mae: 5.052390, mean_q: 5.267119
 49629/100000: episode: 5067, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.579, mean reward: 0.458 [0.458, 0.458], mean action: 52.700 [10.000, 80.000], mean observation: 3.160 [-1.002, 10.354], loss: 1.163375, mae: 5.052538, mean_q: 5.269053
 49639/100000: episode: 5068, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.053, mean reward: 0.405 [0.333, 0.492], mean action: 43.000 [4.000, 84.000], mean observation: 3.158 [-1.667, 10.318], loss: 0.947589, mae: 5.052007, mean_q: 5.270074
 49649/100000: episode: 5069, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.583, mean reward: 0.358 [0.311, 0.382], mean action: 60.000 [23.000, 98.000], mean observation: 3.159 [-1.409, 10.260], loss: 1.311412, mae: 5.053775, mean_q: 5.272137
 49659/100000: episode: 5070, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.423, mean reward: 0.442 [0.315, 0.573], mean action: 53.600 [38.000, 72.000], mean observation: 3.159 [-1.630, 10.311], loss: 1.149936, mae: 5.052989, mean_q: 5.273947
 49669/100000: episode: 5071, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.989, mean reward: 0.399 [0.345, 0.474], mean action: 56.900 [35.000, 95.000], mean observation: 3.174 [-1.451, 10.304], loss: 0.995597, mae: 5.052618, mean_q: 5.275332
 49679/100000: episode: 5072, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.947, mean reward: 0.395 [0.365, 0.478], mean action: 49.500 [1.000, 93.000], mean observation: 3.145 [-0.876, 10.274], loss: 1.550302, mae: 5.055143, mean_q: 5.277217
 49689/100000: episode: 5073, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.920, mean reward: 0.392 [0.355, 0.454], mean action: 51.800 [3.000, 92.000], mean observation: 3.157 [-1.019, 10.217], loss: 0.985782, mae: 5.052886, mean_q: 5.278625
 49699/100000: episode: 5074, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.117, mean reward: 0.412 [0.370, 0.460], mean action: 51.500 [4.000, 96.000], mean observation: 3.140 [-1.181, 10.338], loss: 1.066740, mae: 5.053355, mean_q: 5.280231
 49709/100000: episode: 5075, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.457, mean reward: 0.446 [0.384, 0.478], mean action: 49.500 [0.000, 91.000], mean observation: 3.152 [-1.493, 10.461], loss: 1.140769, mae: 5.053834, mean_q: 5.281766
 49719/100000: episode: 5076, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 4.333, mean reward: 0.433 [0.423, 0.518], mean action: 67.200 [57.000, 96.000], mean observation: 3.164 [-1.448, 10.320], loss: 1.220446, mae: 5.054224, mean_q: 5.282891
 49729/100000: episode: 5077, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.221, mean reward: 0.422 [0.353, 0.554], mean action: 52.400 [8.000, 64.000], mean observation: 3.166 [-1.499, 10.325], loss: 1.289602, mae: 5.054831, mean_q: 5.284414
 49739/100000: episode: 5078, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.190, mean reward: 0.419 [0.413, 0.450], mean action: 56.900 [20.000, 96.000], mean observation: 3.168 [-1.607, 10.241], loss: 0.954595, mae: 5.053772, mean_q: 5.285814
 49749/100000: episode: 5079, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.825, mean reward: 0.483 [0.443, 0.509], mean action: 53.300 [17.000, 84.000], mean observation: 3.142 [-1.251, 10.349], loss: 1.217257, mae: 5.054910, mean_q: 5.285921
 49759/100000: episode: 5080, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.099, mean reward: 0.410 [0.373, 0.489], mean action: 46.500 [5.000, 75.000], mean observation: 3.149 [-1.655, 10.240], loss: 1.376314, mae: 5.055576, mean_q: 5.282046
 49769/100000: episode: 5081, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.864, mean reward: 0.386 [0.320, 0.496], mean action: 47.100 [17.000, 57.000], mean observation: 3.152 [-1.001, 10.358], loss: 1.055560, mae: 5.054733, mean_q: 5.281684
 49779/100000: episode: 5082, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.093, mean reward: 0.409 [0.345, 0.419], mean action: 60.800 [14.000, 89.000], mean observation: 3.159 [-1.590, 10.340], loss: 1.499526, mae: 5.056565, mean_q: 5.282417
 49789/100000: episode: 5083, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.670, mean reward: 0.367 [0.322, 0.416], mean action: 64.600 [57.000, 94.000], mean observation: 3.156 [-1.061, 10.480], loss: 1.265041, mae: 5.055800, mean_q: 5.281541
 49799/100000: episode: 5084, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 5.776, mean reward: 0.578 [0.387, 0.656], mean action: 45.900 [3.000, 97.000], mean observation: 3.145 [-1.712, 10.247], loss: 1.155685, mae: 5.055756, mean_q: 5.283372
 49809/100000: episode: 5085, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.041, mean reward: 0.404 [0.388, 0.431], mean action: 58.000 [7.000, 86.000], mean observation: 3.165 [-0.749, 10.287], loss: 1.331216, mae: 5.056482, mean_q: 5.286834
 49819/100000: episode: 5086, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.146, mean reward: 0.415 [0.331, 0.424], mean action: 55.000 [11.000, 74.000], mean observation: 3.168 [-1.140, 10.317], loss: 1.131453, mae: 5.055797, mean_q: 5.288741
 49829/100000: episode: 5087, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.256, mean reward: 0.426 [0.406, 0.464], mean action: 55.100 [18.000, 98.000], mean observation: 3.136 [-1.533, 10.359], loss: 1.201126, mae: 5.056166, mean_q: 5.290931
 49839/100000: episode: 5088, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.151, mean reward: 0.415 [0.406, 0.439], mean action: 58.100 [19.000, 95.000], mean observation: 3.166 [-1.516, 10.313], loss: 1.230667, mae: 5.056461, mean_q: 5.293040
 49849/100000: episode: 5089, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.802, mean reward: 0.380 [0.309, 0.559], mean action: 48.100 [4.000, 76.000], mean observation: 3.156 [-1.038, 10.456], loss: 1.153440, mae: 5.056012, mean_q: 5.294862
 49859/100000: episode: 5090, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.365, mean reward: 0.436 [0.345, 0.546], mean action: 59.300 [19.000, 93.000], mean observation: 3.161 [-1.311, 10.318], loss: 1.422347, mae: 5.057158, mean_q: 5.295730
 49869/100000: episode: 5091, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.866, mean reward: 0.387 [0.357, 0.443], mean action: 56.000 [16.000, 99.000], mean observation: 3.151 [-1.677, 10.270], loss: 1.479576, mae: 5.057448, mean_q: 5.291131
 49879/100000: episode: 5092, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.818, mean reward: 0.382 [0.322, 0.474], mean action: 47.400 [4.000, 57.000], mean observation: 3.145 [-1.734, 10.349], loss: 1.313674, mae: 5.056400, mean_q: 5.289532
 49889/100000: episode: 5093, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.083, mean reward: 0.408 [0.323, 0.454], mean action: 50.500 [17.000, 90.000], mean observation: 3.164 [-0.959, 10.186], loss: 1.242130, mae: 5.056097, mean_q: 5.289314
 49899/100000: episode: 5094, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.311, mean reward: 0.431 [0.309, 0.487], mean action: 55.400 [36.000, 83.000], mean observation: 3.147 [-1.501, 10.240], loss: 1.110296, mae: 5.055669, mean_q: 5.283886
 49909/100000: episode: 5095, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.584, mean reward: 0.458 [0.421, 0.501], mean action: 67.500 [22.000, 96.000], mean observation: 3.161 [-1.120, 10.375], loss: 1.163744, mae: 5.056515, mean_q: 5.276705
 49919/100000: episode: 5096, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.803, mean reward: 0.380 [0.355, 0.515], mean action: 65.700 [19.000, 100.000], mean observation: 3.142 [-1.515, 10.360], loss: 1.369005, mae: 5.057318, mean_q: 5.276399
 49929/100000: episode: 5097, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.870, mean reward: 0.387 [0.323, 0.505], mean action: 48.600 [3.000, 82.000], mean observation: 3.149 [-1.915, 10.188], loss: 1.573282, mae: 5.058068, mean_q: 5.277482
 49939/100000: episode: 5098, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.900, mean reward: 0.390 [0.342, 0.442], mean action: 50.500 [14.000, 57.000], mean observation: 3.167 [-2.325, 10.291], loss: 1.429267, mae: 5.057377, mean_q: 5.278330
 49949/100000: episode: 5099, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.495, mean reward: 0.449 [0.367, 0.595], mean action: 59.600 [34.000, 92.000], mean observation: 3.156 [-1.463, 10.243], loss: 1.235523, mae: 5.056550, mean_q: 5.278007
 49959/100000: episode: 5100, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.255, mean reward: 0.425 [0.375, 0.475], mean action: 50.000 [11.000, 57.000], mean observation: 3.153 [-1.212, 10.212], loss: 1.168482, mae: 5.056167, mean_q: 5.275049
 49969/100000: episode: 5101, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.905, mean reward: 0.391 [0.366, 0.435], mean action: 58.200 [14.000, 94.000], mean observation: 3.162 [-2.323, 10.453], loss: 1.163406, mae: 5.056308, mean_q: 5.275890
 49979/100000: episode: 5102, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.236, mean reward: 0.424 [0.413, 0.465], mean action: 62.500 [39.000, 98.000], mean observation: 3.140 [-0.919, 10.388], loss: 1.064398, mae: 5.056352, mean_q: 5.278984
 49989/100000: episode: 5103, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.686, mean reward: 0.469 [0.445, 0.488], mean action: 56.700 [23.000, 88.000], mean observation: 3.164 [-1.457, 10.424], loss: 1.089663, mae: 5.056435, mean_q: 5.281959
 49999/100000: episode: 5104, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.725, mean reward: 0.372 [0.333, 0.446], mean action: 56.300 [32.000, 85.000], mean observation: 3.156 [-1.762, 10.335], loss: 1.429688, mae: 5.058080, mean_q: 5.284100
 50009/100000: episode: 5105, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.167, mean reward: 0.417 [0.348, 0.529], mean action: 61.400 [47.000, 93.000], mean observation: 3.149 [-1.358, 10.428], loss: 1.339379, mae: 5.057842, mean_q: 5.285703
 50019/100000: episode: 5106, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.154, mean reward: 0.415 [0.350, 0.471], mean action: 41.000 [6.000, 57.000], mean observation: 3.148 [-2.193, 10.363], loss: 1.365998, mae: 5.057901, mean_q: 5.283143
 50029/100000: episode: 5107, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.973, mean reward: 0.397 [0.384, 0.431], mean action: 52.000 [10.000, 77.000], mean observation: 3.167 [-1.164, 10.317], loss: 1.056891, mae: 5.056572, mean_q: 5.282630
 50039/100000: episode: 5108, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.184, mean reward: 0.418 [0.384, 0.511], mean action: 59.400 [12.000, 90.000], mean observation: 3.153 [-1.677, 10.394], loss: 1.154371, mae: 5.057049, mean_q: 5.282475
 50049/100000: episode: 5109, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.197, mean reward: 0.420 [0.363, 0.462], mean action: 54.000 [20.000, 87.000], mean observation: 3.152 [-1.473, 10.248], loss: 1.274448, mae: 5.057584, mean_q: 5.278750
 50059/100000: episode: 5110, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.232, mean reward: 0.423 [0.335, 0.462], mean action: 58.800 [9.000, 98.000], mean observation: 3.144 [-1.104, 10.318], loss: 1.412540, mae: 5.058029, mean_q: 5.274314
 50069/100000: episode: 5111, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.130, mean reward: 0.413 [0.396, 0.481], mean action: 43.700 [8.000, 70.000], mean observation: 3.147 [-1.105, 10.327], loss: 1.530214, mae: 5.058398, mean_q: 5.274078
 50079/100000: episode: 5112, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.145, mean reward: 0.415 [0.370, 0.521], mean action: 61.900 [20.000, 95.000], mean observation: 3.141 [-1.424, 10.457], loss: 1.124641, mae: 5.056474, mean_q: 5.271924
 50089/100000: episode: 5113, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 4.116, mean reward: 0.412 [0.377, 0.509], mean action: 57.000 [57.000, 57.000], mean observation: 3.167 [-1.161, 10.245], loss: 1.541975, mae: 5.057849, mean_q: 5.270179
 50099/100000: episode: 5114, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.319, mean reward: 0.432 [0.355, 0.494], mean action: 44.100 [4.000, 72.000], mean observation: 3.153 [-1.498, 10.398], loss: 1.549744, mae: 5.057798, mean_q: 5.271621
 50109/100000: episode: 5115, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.905, mean reward: 0.390 [0.310, 0.545], mean action: 35.000 [6.000, 57.000], mean observation: 3.168 [-1.575, 10.471], loss: 0.987557, mae: 5.055624, mean_q: 5.270428
 50119/100000: episode: 5116, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.747, mean reward: 0.375 [0.311, 0.464], mean action: 44.500 [4.000, 60.000], mean observation: 3.160 [-1.145, 10.462], loss: 1.196184, mae: 5.056562, mean_q: 5.269710
 50129/100000: episode: 5117, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.991, mean reward: 0.399 [0.350, 0.451], mean action: 48.000 [1.000, 79.000], mean observation: 3.163 [-1.583, 10.343], loss: 0.988193, mae: 5.055896, mean_q: 5.266465
 50139/100000: episode: 5118, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.744, mean reward: 0.374 [0.313, 0.413], mean action: 47.000 [22.000, 86.000], mean observation: 3.168 [-1.785, 10.406], loss: 1.279961, mae: 5.057022, mean_q: 5.266237
 50149/100000: episode: 5119, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.182, mean reward: 0.418 [0.337, 0.506], mean action: 41.700 [11.000, 57.000], mean observation: 3.153 [-1.714, 10.358], loss: 1.470803, mae: 5.057852, mean_q: 5.265270
 50159/100000: episode: 5120, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.489, mean reward: 0.449 [0.440, 0.503], mean action: 55.200 [40.000, 83.000], mean observation: 3.145 [-1.737, 10.298], loss: 1.279908, mae: 5.056746, mean_q: 5.265829
 50169/100000: episode: 5121, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.913, mean reward: 0.391 [0.330, 0.439], mean action: 63.800 [57.000, 88.000], mean observation: 3.149 [-1.378, 10.344], loss: 1.193514, mae: 5.056774, mean_q: 5.267783
 50179/100000: episode: 5122, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.488, mean reward: 0.449 [0.446, 0.460], mean action: 61.400 [42.000, 97.000], mean observation: 3.159 [-1.869, 10.395], loss: 1.314364, mae: 5.057319, mean_q: 5.269765
 50189/100000: episode: 5123, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.056, mean reward: 0.406 [0.288, 0.519], mean action: 51.800 [18.000, 82.000], mean observation: 3.154 [-1.511, 10.367], loss: 1.358813, mae: 5.057727, mean_q: 5.273338
 50199/100000: episode: 5124, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.600, mean reward: 0.360 [0.333, 0.398], mean action: 63.600 [26.000, 101.000], mean observation: 3.150 [-1.177, 10.354], loss: 1.374347, mae: 5.057673, mean_q: 5.271651
 50209/100000: episode: 5125, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.356, mean reward: 0.436 [0.327, 0.593], mean action: 54.100 [40.000, 67.000], mean observation: 3.162 [-1.562, 10.282], loss: 1.364678, mae: 5.057639, mean_q: 5.267133
 50219/100000: episode: 5126, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.914, mean reward: 0.391 [0.305, 0.459], mean action: 36.100 [1.000, 57.000], mean observation: 3.153 [-1.370, 10.292], loss: 1.474829, mae: 5.058015, mean_q: 5.267689
 50229/100000: episode: 5127, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.347, mean reward: 0.335 [0.299, 0.389], mean action: 52.300 [1.000, 88.000], mean observation: 3.160 [-1.454, 10.335], loss: 1.369090, mae: 5.057252, mean_q: 5.268590
 50239/100000: episode: 5128, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.070, mean reward: 0.407 [0.323, 0.490], mean action: 39.200 [1.000, 75.000], mean observation: 3.154 [-2.134, 10.365], loss: 1.114752, mae: 5.056046, mean_q: 5.269870
 50249/100000: episode: 5129, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 5.298, mean reward: 0.530 [0.530, 0.530], mean action: 52.800 [3.000, 93.000], mean observation: 3.156 [-1.422, 10.272], loss: 1.305578, mae: 5.056788, mean_q: 5.269186
 50259/100000: episode: 5130, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.698, mean reward: 0.470 [0.392, 0.478], mean action: 47.900 [8.000, 77.000], mean observation: 3.158 [-1.154, 10.388], loss: 1.058673, mae: 5.055856, mean_q: 5.269057
 50260/100000: episode: 5131, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 24.000 [24.000, 24.000], mean observation: 3.130 [-1.451, 10.100], loss: 0.795180, mae: 5.055260, mean_q: 5.270499
 50270/100000: episode: 5132, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.386, mean reward: 0.439 [0.439, 0.439], mean action: 60.000 [34.000, 92.000], mean observation: 3.147 [-1.385, 10.270], loss: 1.598718, mae: 5.058195, mean_q: 5.271978
 50280/100000: episode: 5133, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.079, mean reward: 0.408 [0.358, 0.505], mean action: 51.100 [10.000, 84.000], mean observation: 3.168 [-1.502, 10.248], loss: 1.320282, mae: 5.057115, mean_q: 5.273160
 50290/100000: episode: 5134, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.953, mean reward: 0.395 [0.328, 0.476], mean action: 65.800 [38.000, 92.000], mean observation: 3.141 [-1.477, 10.356], loss: 1.216805, mae: 5.056717, mean_q: 5.269949
 50300/100000: episode: 5135, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.200, mean reward: 0.420 [0.340, 0.443], mean action: 53.900 [6.000, 96.000], mean observation: 3.167 [-1.837, 10.385], loss: 1.300313, mae: 5.056952, mean_q: 5.259057
 50310/100000: episode: 5136, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.022, mean reward: 0.402 [0.380, 0.457], mean action: 63.400 [7.000, 99.000], mean observation: 3.146 [-1.620, 10.269], loss: 1.206469, mae: 5.056656, mean_q: 5.254626
 50320/100000: episode: 5137, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.019, mean reward: 0.402 [0.391, 0.428], mean action: 56.100 [30.000, 88.000], mean observation: 3.144 [-1.239, 10.345], loss: 1.773824, mae: 5.058719, mean_q: 5.253322
 50330/100000: episode: 5138, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.997, mean reward: 0.400 [0.325, 0.480], mean action: 63.200 [20.000, 89.000], mean observation: 3.151 [-1.635, 10.259], loss: 1.148475, mae: 5.056036, mean_q: 5.250219
 50340/100000: episode: 5139, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 3.768, mean reward: 0.377 [0.326, 0.466], mean action: 75.000 [40.000, 91.000], mean observation: 3.169 [-1.455, 10.344], loss: 0.802438, mae: 5.054568, mean_q: 5.250944
 50350/100000: episode: 5140, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.328, mean reward: 0.433 [0.417, 0.496], mean action: 69.400 [19.000, 85.000], mean observation: 3.149 [-1.321, 10.392], loss: 1.734982, mae: 5.058279, mean_q: 5.251417
 50360/100000: episode: 5141, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.693, mean reward: 0.369 [0.344, 0.423], mean action: 60.900 [4.000, 86.000], mean observation: 3.143 [-0.989, 10.238], loss: 1.254988, mae: 5.056211, mean_q: 5.252224
 50370/100000: episode: 5142, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 3.887, mean reward: 0.389 [0.341, 0.490], mean action: 60.500 [13.000, 83.000], mean observation: 3.148 [-1.115, 10.335], loss: 1.896075, mae: 5.058545, mean_q: 5.253439
 50380/100000: episode: 5143, duration: 0.131s, episode steps: 10, steps per second: 77, episode reward: 4.177, mean reward: 0.418 [0.418, 0.418], mean action: 75.400 [35.000, 93.000], mean observation: 3.172 [-0.766, 10.429], loss: 1.265304, mae: 5.055265, mean_q: 5.251892
 50390/100000: episode: 5144, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.622, mean reward: 0.362 [0.311, 0.397], mean action: 68.500 [19.000, 90.000], mean observation: 3.163 [-0.690, 10.316], loss: 1.062165, mae: 5.054259, mean_q: 5.253281
 50400/100000: episode: 5145, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 3.729, mean reward: 0.373 [0.318, 0.430], mean action: 71.700 [24.000, 96.000], mean observation: 3.170 [-1.334, 10.286], loss: 1.319319, mae: 5.055327, mean_q: 5.254336
 50410/100000: episode: 5146, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.296, mean reward: 0.430 [0.425, 0.441], mean action: 72.500 [39.000, 93.000], mean observation: 3.162 [-0.790, 10.299], loss: 1.232685, mae: 5.054928, mean_q: 5.256222
 50420/100000: episode: 5147, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.114, mean reward: 0.411 [0.396, 0.475], mean action: 63.000 [17.000, 75.000], mean observation: 3.135 [-1.539, 10.296], loss: 1.260996, mae: 5.055225, mean_q: 5.258351
 50430/100000: episode: 5148, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.176, mean reward: 0.418 [0.383, 0.467], mean action: 53.700 [13.000, 98.000], mean observation: 3.156 [-0.807, 10.447], loss: 1.082568, mae: 5.054708, mean_q: 5.260689
 50440/100000: episode: 5149, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.427, mean reward: 0.443 [0.409, 0.513], mean action: 60.400 [26.000, 87.000], mean observation: 3.153 [-0.840, 10.270], loss: 1.389831, mae: 5.055869, mean_q: 5.262648
 50450/100000: episode: 5150, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.480, mean reward: 0.448 [0.448, 0.448], mean action: 66.900 [42.000, 75.000], mean observation: 3.165 [-1.104, 10.327], loss: 1.180699, mae: 5.054965, mean_q: 5.261716
 50460/100000: episode: 5151, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.610, mean reward: 0.461 [0.383, 0.548], mean action: 58.700 [3.000, 75.000], mean observation: 3.149 [-1.966, 10.336], loss: 1.252786, mae: 5.054962, mean_q: 5.247137
 50470/100000: episode: 5152, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.425, mean reward: 0.343 [0.284, 0.444], mean action: 61.800 [28.000, 94.000], mean observation: 3.168 [-1.259, 10.436], loss: 1.298939, mae: 5.054997, mean_q: 5.239594
 50480/100000: episode: 5153, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.651, mean reward: 0.365 [0.280, 0.463], mean action: 42.200 [2.000, 75.000], mean observation: 3.154 [-1.503, 10.356], loss: 1.228788, mae: 5.054508, mean_q: 5.237621
 50490/100000: episode: 5154, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.767, mean reward: 0.377 [0.359, 0.427], mean action: 46.200 [18.000, 74.000], mean observation: 3.156 [-1.130, 10.194], loss: 1.107782, mae: 5.053870, mean_q: 5.238897
 50500/100000: episode: 5155, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.579, mean reward: 0.358 [0.293, 0.403], mean action: 38.000 [1.000, 75.000], mean observation: 3.150 [-0.948, 10.306], loss: 0.979534, mae: 5.053621, mean_q: 5.240600
 50510/100000: episode: 5156, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.320, mean reward: 0.432 [0.334, 0.481], mean action: 49.000 [1.000, 93.000], mean observation: 3.147 [-0.966, 10.309], loss: 1.052279, mae: 5.054132, mean_q: 5.238601
 50520/100000: episode: 5157, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.896, mean reward: 0.390 [0.329, 0.424], mean action: 33.200 [1.000, 96.000], mean observation: 3.147 [-1.205, 10.343], loss: 1.087076, mae: 5.054477, mean_q: 5.238912
 50530/100000: episode: 5158, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.791, mean reward: 0.379 [0.324, 0.448], mean action: 39.100 [25.000, 81.000], mean observation: 3.143 [-2.438, 10.412], loss: 0.883984, mae: 5.053982, mean_q: 5.239522
 50540/100000: episode: 5159, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.892, mean reward: 0.389 [0.296, 0.497], mean action: 51.000 [1.000, 100.000], mean observation: 3.162 [-1.337, 10.373], loss: 1.453547, mae: 5.056392, mean_q: 5.238302
 50550/100000: episode: 5160, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.844, mean reward: 0.384 [0.340, 0.463], mean action: 45.300 [12.000, 96.000], mean observation: 3.144 [-1.058, 10.230], loss: 1.597944, mae: 5.056590, mean_q: 5.239472
 50560/100000: episode: 5161, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.271, mean reward: 0.427 [0.417, 0.484], mean action: 39.700 [12.000, 75.000], mean observation: 3.148 [-1.539, 10.238], loss: 1.029243, mae: 5.054244, mean_q: 5.241971
 50570/100000: episode: 5162, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.794, mean reward: 0.479 [0.380, 0.575], mean action: 34.400 [5.000, 92.000], mean observation: 3.165 [-1.631, 10.350], loss: 1.627516, mae: 5.056463, mean_q: 5.242874
 50580/100000: episode: 5163, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.851, mean reward: 0.385 [0.308, 0.465], mean action: 34.800 [2.000, 82.000], mean observation: 3.149 [-1.761, 10.488], loss: 1.508168, mae: 5.055583, mean_q: 5.242240
 50590/100000: episode: 5164, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.293, mean reward: 0.429 [0.343, 0.498], mean action: 36.200 [15.000, 94.000], mean observation: 3.147 [-1.784, 10.299], loss: 0.991998, mae: 5.053356, mean_q: 5.240176
 50600/100000: episode: 5165, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.939, mean reward: 0.394 [0.355, 0.449], mean action: 42.000 [25.000, 83.000], mean observation: 3.165 [-1.333, 10.449], loss: 1.162803, mae: 5.054050, mean_q: 5.239466
 50610/100000: episode: 5166, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.158, mean reward: 0.416 [0.373, 0.507], mean action: 40.200 [25.000, 84.000], mean observation: 3.158 [-0.817, 10.377], loss: 1.427562, mae: 5.054824, mean_q: 5.240134
 50620/100000: episode: 5167, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.132, mean reward: 0.413 [0.377, 0.515], mean action: 46.300 [7.000, 98.000], mean observation: 3.154 [-1.603, 10.444], loss: 1.202689, mae: 5.053910, mean_q: 5.241217
 50630/100000: episode: 5168, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.095, mean reward: 0.409 [0.331, 0.513], mean action: 27.600 [21.000, 55.000], mean observation: 3.158 [-1.555, 10.517], loss: 1.238552, mae: 5.053930, mean_q: 5.241088
 50640/100000: episode: 5169, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.093, mean reward: 0.409 [0.367, 0.515], mean action: 39.700 [25.000, 88.000], mean observation: 3.143 [-1.535, 10.292], loss: 1.456206, mae: 5.054512, mean_q: 5.238318
 50650/100000: episode: 5170, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.563, mean reward: 0.456 [0.409, 0.519], mean action: 27.900 [5.000, 79.000], mean observation: 3.151 [-1.585, 10.347], loss: 1.386754, mae: 5.053845, mean_q: 5.232909
 50660/100000: episode: 5171, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.255, mean reward: 0.425 [0.310, 0.539], mean action: 29.700 [3.000, 84.000], mean observation: 3.154 [-1.578, 10.319], loss: 1.408654, mae: 5.053630, mean_q: 5.231588
 50670/100000: episode: 5172, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.671, mean reward: 0.467 [0.357, 0.572], mean action: 22.700 [3.000, 100.000], mean observation: 3.155 [-1.929, 10.436], loss: 1.695339, mae: 5.054403, mean_q: 5.231938
 50680/100000: episode: 5173, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.055, mean reward: 0.405 [0.314, 0.584], mean action: 26.100 [3.000, 94.000], mean observation: 3.160 [-1.831, 10.356], loss: 1.049901, mae: 5.051674, mean_q: 5.233190
 50690/100000: episode: 5174, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.501, mean reward: 0.450 [0.356, 0.582], mean action: 29.800 [2.000, 88.000], mean observation: 3.156 [-1.607, 10.419], loss: 1.343646, mae: 5.052705, mean_q: 5.234634
 50700/100000: episode: 5175, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.317, mean reward: 0.432 [0.375, 0.474], mean action: 24.900 [0.000, 98.000], mean observation: 3.165 [-1.885, 10.415], loss: 0.875801, mae: 5.051068, mean_q: 5.235863
 50710/100000: episode: 5176, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.117, mean reward: 0.412 [0.321, 0.578], mean action: 23.500 [3.000, 81.000], mean observation: 3.153 [-1.362, 10.369], loss: 1.562239, mae: 5.053848, mean_q: 5.236566
 50720/100000: episode: 5177, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.264, mean reward: 0.426 [0.346, 0.494], mean action: 27.500 [3.000, 64.000], mean observation: 3.162 [-2.220, 10.178], loss: 1.112766, mae: 5.052210, mean_q: 5.232167
 50730/100000: episode: 5178, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.985, mean reward: 0.399 [0.331, 0.475], mean action: 28.100 [3.000, 72.000], mean observation: 3.155 [-1.516, 10.300], loss: 1.292618, mae: 5.053026, mean_q: 5.229825
 50740/100000: episode: 5179, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.431, mean reward: 0.443 [0.399, 0.510], mean action: 39.300 [3.000, 96.000], mean observation: 3.152 [-1.158, 10.276], loss: 0.876023, mae: 5.051433, mean_q: 5.225608
 50750/100000: episode: 5180, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.417, mean reward: 0.442 [0.434, 0.447], mean action: 42.500 [19.000, 82.000], mean observation: 3.162 [-1.727, 10.336], loss: 1.065598, mae: 5.052419, mean_q: 5.225637
 50760/100000: episode: 5181, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.919, mean reward: 0.392 [0.318, 0.531], mean action: 26.900 [3.000, 69.000], mean observation: 3.154 [-0.948, 10.307], loss: 1.297422, mae: 5.053493, mean_q: 5.223235
 50770/100000: episode: 5182, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.387, mean reward: 0.439 [0.335, 0.552], mean action: 37.100 [3.000, 98.000], mean observation: 3.166 [-2.073, 10.341], loss: 1.247263, mae: 5.053300, mean_q: 5.223684
 50780/100000: episode: 5183, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.018, mean reward: 0.402 [0.377, 0.447], mean action: 25.500 [3.000, 89.000], mean observation: 3.166 [-1.717, 10.373], loss: 1.412750, mae: 5.053885, mean_q: 5.224592
 50790/100000: episode: 5184, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 3.937, mean reward: 0.394 [0.301, 0.467], mean action: 21.600 [3.000, 90.000], mean observation: 3.158 [-1.415, 10.251], loss: 1.166168, mae: 5.052755, mean_q: 5.225692
 50793/100000: episode: 5185, duration: 0.063s, episode steps: 3, steps per second: 48, episode reward: 10.920, mean reward: 3.640 [0.460, 10.000], mean action: 46.667 [3.000, 97.000], mean observation: 3.136 [-1.859, 10.245], loss: 1.014952, mae: 5.052442, mean_q: 5.226168
 50803/100000: episode: 5186, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.086, mean reward: 0.409 [0.328, 0.460], mean action: 19.200 [3.000, 95.000], mean observation: 3.153 [-1.161, 10.406], loss: 1.238009, mae: 5.052842, mean_q: 5.226521
 50813/100000: episode: 5187, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.884, mean reward: 0.388 [0.317, 0.484], mean action: 18.400 [3.000, 34.000], mean observation: 3.159 [-2.129, 10.328], loss: 1.166915, mae: 5.052580, mean_q: 5.225329
 50823/100000: episode: 5188, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.983, mean reward: 0.398 [0.333, 0.598], mean action: 39.500 [10.000, 94.000], mean observation: 3.161 [-1.679, 10.332], loss: 1.021901, mae: 5.051932, mean_q: 5.225890
 50833/100000: episode: 5189, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.914, mean reward: 0.391 [0.325, 0.440], mean action: 36.000 [7.000, 93.000], mean observation: 3.160 [-1.538, 10.370], loss: 1.178733, mae: 5.052526, mean_q: 5.228153
 50843/100000: episode: 5190, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.083, mean reward: 0.408 [0.395, 0.435], mean action: 38.100 [6.000, 98.000], mean observation: 3.151 [-1.772, 10.253], loss: 1.245370, mae: 5.052824, mean_q: 5.231069
 50853/100000: episode: 5191, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.160, mean reward: 0.416 [0.369, 0.517], mean action: 38.700 [25.000, 90.000], mean observation: 3.162 [-1.320, 10.309], loss: 1.139729, mae: 5.052454, mean_q: 5.232715
 50863/100000: episode: 5192, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.179, mean reward: 0.418 [0.387, 0.511], mean action: 28.600 [25.000, 61.000], mean observation: 3.155 [-1.496, 10.299], loss: 0.906887, mae: 5.051717, mean_q: 5.234752
 50873/100000: episode: 5193, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.360, mean reward: 0.436 [0.373, 0.499], mean action: 38.500 [2.000, 101.000], mean observation: 3.161 [-1.503, 10.209], loss: 1.108072, mae: 5.052661, mean_q: 5.236457
 50883/100000: episode: 5194, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.763, mean reward: 0.376 [0.325, 0.419], mean action: 35.100 [11.000, 80.000], mean observation: 3.141 [-1.327, 10.386], loss: 1.102556, mae: 5.052651, mean_q: 5.237911
 50893/100000: episode: 5195, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.981, mean reward: 0.398 [0.350, 0.446], mean action: 31.200 [4.000, 80.000], mean observation: 3.148 [-1.152, 10.299], loss: 1.251604, mae: 5.053144, mean_q: 5.234776
 50903/100000: episode: 5196, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.935, mean reward: 0.394 [0.285, 0.489], mean action: 53.000 [21.000, 97.000], mean observation: 3.170 [-1.885, 10.394], loss: 1.339414, mae: 5.053500, mean_q: 5.231586
 50913/100000: episode: 5197, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.838, mean reward: 0.384 [0.284, 0.437], mean action: 25.900 [6.000, 63.000], mean observation: 3.165 [-1.848, 10.282], loss: 1.292933, mae: 5.053232, mean_q: 5.232107
 50923/100000: episode: 5198, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.316, mean reward: 0.432 [0.379, 0.494], mean action: 64.400 [25.000, 95.000], mean observation: 3.162 [-1.262, 10.451], loss: 1.181630, mae: 5.052588, mean_q: 5.233432
 50933/100000: episode: 5199, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.918, mean reward: 0.392 [0.348, 0.465], mean action: 55.500 [6.000, 96.000], mean observation: 3.153 [-1.129, 10.421], loss: 1.410195, mae: 5.053435, mean_q: 5.234300
 50943/100000: episode: 5200, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 3.882, mean reward: 0.388 [0.271, 0.553], mean action: 33.600 [25.000, 68.000], mean observation: 3.157 [-1.361, 10.386], loss: 1.097726, mae: 5.052022, mean_q: 5.235007
 50953/100000: episode: 5201, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.859, mean reward: 0.386 [0.357, 0.474], mean action: 33.200 [25.000, 70.000], mean observation: 3.156 [-1.548, 10.277], loss: 1.469936, mae: 5.053561, mean_q: 5.236124
 50963/100000: episode: 5202, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.962, mean reward: 0.396 [0.358, 0.452], mean action: 37.100 [14.000, 94.000], mean observation: 3.140 [-1.446, 10.323], loss: 1.112339, mae: 5.052129, mean_q: 5.238610
 50973/100000: episode: 5203, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.178, mean reward: 0.418 [0.366, 0.594], mean action: 26.700 [1.000, 54.000], mean observation: 3.161 [-2.071, 10.389], loss: 1.180905, mae: 5.052442, mean_q: 5.238231
 50983/100000: episode: 5204, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.344, mean reward: 0.434 [0.422, 0.460], mean action: 36.600 [25.000, 81.000], mean observation: 3.168 [-1.205, 10.268], loss: 1.311388, mae: 5.052798, mean_q: 5.238880
 50993/100000: episode: 5205, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.911, mean reward: 0.391 [0.333, 0.436], mean action: 33.400 [0.000, 85.000], mean observation: 3.156 [-1.291, 10.367], loss: 1.084884, mae: 5.051716, mean_q: 5.239779
 51003/100000: episode: 5206, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.679, mean reward: 0.468 [0.387, 0.554], mean action: 23.200 [1.000, 61.000], mean observation: 3.170 [-1.534, 10.480], loss: 0.838087, mae: 5.051074, mean_q: 5.240218
 51013/100000: episode: 5207, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.399, mean reward: 0.440 [0.353, 0.564], mean action: 40.900 [25.000, 82.000], mean observation: 3.154 [-1.321, 10.365], loss: 1.266474, mae: 5.053087, mean_q: 5.234845
 51023/100000: episode: 5208, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.057, mean reward: 0.406 [0.311, 0.464], mean action: 33.800 [21.000, 70.000], mean observation: 3.154 [-1.349, 10.259], loss: 0.968862, mae: 5.052115, mean_q: 5.230665
 51033/100000: episode: 5209, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.040, mean reward: 0.404 [0.344, 0.452], mean action: 21.200 [0.000, 86.000], mean observation: 3.158 [-1.151, 10.430], loss: 1.314191, mae: 5.053496, mean_q: 5.231554
 51043/100000: episode: 5210, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.349, mean reward: 0.435 [0.332, 0.529], mean action: 14.600 [0.000, 79.000], mean observation: 3.164 [-1.857, 10.428], loss: 1.102462, mae: 5.052621, mean_q: 5.232230
 51053/100000: episode: 5211, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 3.962, mean reward: 0.396 [0.353, 0.453], mean action: 11.300 [0.000, 79.000], mean observation: 3.154 [-1.674, 10.390], loss: 1.014396, mae: 5.052449, mean_q: 5.231175
 51063/100000: episode: 5212, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.408, mean reward: 0.441 [0.376, 0.493], mean action: 24.700 [0.000, 80.000], mean observation: 3.154 [-1.165, 10.368], loss: 1.198748, mae: 5.053596, mean_q: 5.228597
 51073/100000: episode: 5213, duration: 0.226s, episode steps: 10, steps per second: 44, episode reward: 4.034, mean reward: 0.403 [0.346, 0.456], mean action: 16.200 [0.000, 90.000], mean observation: 3.169 [-1.461, 10.353], loss: 1.557581, mae: 5.054653, mean_q: 5.226596
 51082/100000: episode: 5214, duration: 0.163s, episode steps: 9, steps per second: 55, episode reward: 13.225, mean reward: 1.469 [0.368, 10.000], mean action: 19.889 [0.000, 55.000], mean observation: 3.155 [-1.333, 10.234], loss: 1.140739, mae: 5.052786, mean_q: 5.227102
 51083/100000: episode: 5215, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.144 [-1.127, 10.100], loss: 1.090160, mae: 5.052517, mean_q: 5.226475
 51093/100000: episode: 5216, duration: 0.217s, episode steps: 10, steps per second: 46, episode reward: 3.935, mean reward: 0.393 [0.328, 0.474], mean action: 17.400 [0.000, 82.000], mean observation: 3.157 [-1.070, 10.332], loss: 1.478722, mae: 5.053924, mean_q: 5.225684
 51103/100000: episode: 5217, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.383, mean reward: 0.438 [0.345, 0.503], mean action: 33.300 [0.000, 101.000], mean observation: 3.160 [-1.837, 10.325], loss: 1.175784, mae: 5.052357, mean_q: 5.225476
 51113/100000: episode: 5218, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 3.822, mean reward: 0.382 [0.321, 0.469], mean action: 16.500 [0.000, 65.000], mean observation: 3.147 [-1.169, 10.332], loss: 1.129906, mae: 5.051998, mean_q: 5.226738
 51123/100000: episode: 5219, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.213, mean reward: 0.421 [0.337, 0.534], mean action: 12.600 [0.000, 64.000], mean observation: 3.153 [-1.248, 10.371], loss: 1.277314, mae: 5.052496, mean_q: 5.228040
 51133/100000: episode: 5220, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.826, mean reward: 0.383 [0.300, 0.460], mean action: 30.800 [0.000, 92.000], mean observation: 3.141 [-1.614, 10.241], loss: 1.158828, mae: 5.051765, mean_q: 5.225362
 51143/100000: episode: 5221, duration: 0.238s, episode steps: 10, steps per second: 42, episode reward: 4.021, mean reward: 0.402 [0.347, 0.452], mean action: 10.000 [0.000, 69.000], mean observation: 3.152 [-1.703, 10.292], loss: 0.908367, mae: 5.050704, mean_q: 5.225549
 51153/100000: episode: 5222, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.314, mean reward: 0.431 [0.348, 0.529], mean action: 10.800 [0.000, 63.000], mean observation: 3.160 [-1.911, 10.492], loss: 1.341697, mae: 5.052563, mean_q: 5.225318
 51163/100000: episode: 5223, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.945, mean reward: 0.394 [0.333, 0.479], mean action: 34.000 [0.000, 95.000], mean observation: 3.159 [-1.578, 10.252], loss: 1.208388, mae: 5.052027, mean_q: 5.225489
 51173/100000: episode: 5224, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 3.976, mean reward: 0.398 [0.333, 0.500], mean action: 14.900 [0.000, 60.000], mean observation: 3.155 [-0.954, 10.290], loss: 1.481627, mae: 5.052623, mean_q: 5.226040
 51183/100000: episode: 5225, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.208, mean reward: 0.421 [0.377, 0.552], mean action: 29.500 [0.000, 72.000], mean observation: 3.158 [-1.833, 10.374], loss: 1.334910, mae: 5.051871, mean_q: 5.225032
 51193/100000: episode: 5226, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.329, mean reward: 0.433 [0.335, 0.524], mean action: 27.100 [0.000, 91.000], mean observation: 3.156 [-1.763, 10.460], loss: 1.138492, mae: 5.050751, mean_q: 5.226211
 51203/100000: episode: 5227, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.053, mean reward: 0.405 [0.346, 0.493], mean action: 41.200 [0.000, 101.000], mean observation: 3.154 [-1.418, 10.247], loss: 1.559523, mae: 5.052047, mean_q: 5.228489
 51213/100000: episode: 5228, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.419, mean reward: 0.442 [0.300, 0.589], mean action: 22.300 [0.000, 75.000], mean observation: 3.163 [-1.094, 10.425], loss: 0.926170, mae: 5.049532, mean_q: 5.228547
 51223/100000: episode: 5229, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.295, mean reward: 0.429 [0.305, 0.536], mean action: 22.000 [0.000, 96.000], mean observation: 3.159 [-1.055, 10.363], loss: 0.785159, mae: 5.048936, mean_q: 5.228929
 51233/100000: episode: 5230, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.279, mean reward: 0.428 [0.370, 0.507], mean action: 24.300 [0.000, 89.000], mean observation: 3.160 [-1.272, 10.332], loss: 1.083182, mae: 5.050438, mean_q: 5.228551
 51243/100000: episode: 5231, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 3.946, mean reward: 0.395 [0.309, 0.459], mean action: 24.500 [0.000, 62.000], mean observation: 3.160 [-1.401, 10.273], loss: 1.604777, mae: 5.052434, mean_q: 5.229159
 51253/100000: episode: 5232, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.364, mean reward: 0.436 [0.345, 0.528], mean action: 29.500 [0.000, 96.000], mean observation: 3.164 [-0.956, 10.367], loss: 1.197000, mae: 5.050648, mean_q: 5.230849
 51263/100000: episode: 5233, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.984, mean reward: 0.398 [0.316, 0.478], mean action: 46.400 [0.000, 94.000], mean observation: 3.152 [-1.279, 10.185], loss: 1.036760, mae: 5.050016, mean_q: 5.232524
 51273/100000: episode: 5234, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 3.882, mean reward: 0.388 [0.305, 0.459], mean action: 15.600 [0.000, 77.000], mean observation: 3.159 [-1.423, 10.419], loss: 1.064489, mae: 5.049909, mean_q: 5.232913
 51283/100000: episode: 5235, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.155, mean reward: 0.416 [0.305, 0.490], mean action: 38.500 [0.000, 100.000], mean observation: 3.162 [-1.268, 10.275], loss: 1.420976, mae: 5.051448, mean_q: 5.233977
 51293/100000: episode: 5236, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.127, mean reward: 0.413 [0.343, 0.539], mean action: 32.200 [3.000, 90.000], mean observation: 3.147 [-1.141, 10.372], loss: 1.110306, mae: 5.050105, mean_q: 5.234935
 51303/100000: episode: 5237, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.878, mean reward: 0.388 [0.312, 0.572], mean action: 27.200 [3.000, 96.000], mean observation: 3.155 [-1.469, 10.313], loss: 1.172429, mae: 5.050126, mean_q: 5.236228
 51313/100000: episode: 5238, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.463, mean reward: 0.446 [0.325, 0.555], mean action: 40.200 [3.000, 96.000], mean observation: 3.150 [-1.912, 10.304], loss: 1.325876, mae: 5.050743, mean_q: 5.237688
 51323/100000: episode: 5239, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.056, mean reward: 0.406 [0.336, 0.577], mean action: 14.800 [3.000, 67.000], mean observation: 3.152 [-1.342, 10.347], loss: 0.939947, mae: 5.049221, mean_q: 5.239348
 51333/100000: episode: 5240, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.823, mean reward: 0.382 [0.366, 0.402], mean action: 40.400 [3.000, 97.000], mean observation: 3.158 [-1.284, 10.351], loss: 1.048115, mae: 5.049791, mean_q: 5.240707
 51343/100000: episode: 5241, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.875, mean reward: 0.387 [0.310, 0.447], mean action: 44.100 [3.000, 99.000], mean observation: 3.153 [-1.406, 10.291], loss: 1.114124, mae: 5.050287, mean_q: 5.241325
 51353/100000: episode: 5242, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.340, mean reward: 0.434 [0.365, 0.583], mean action: 31.500 [3.000, 101.000], mean observation: 3.155 [-1.833, 10.316], loss: 1.028834, mae: 5.050216, mean_q: 5.241664
 51363/100000: episode: 5243, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.937, mean reward: 0.394 [0.348, 0.434], mean action: 31.900 [3.000, 84.000], mean observation: 3.151 [-1.319, 10.426], loss: 1.281185, mae: 5.051553, mean_q: 5.242608
 51373/100000: episode: 5244, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.354, mean reward: 0.435 [0.353, 0.576], mean action: 44.400 [3.000, 98.000], mean observation: 3.149 [-1.351, 10.302], loss: 1.495387, mae: 5.052321, mean_q: 5.239711
 51383/100000: episode: 5245, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.082, mean reward: 0.408 [0.367, 0.444], mean action: 23.000 [3.000, 80.000], mean observation: 3.157 [-1.982, 10.312], loss: 1.393879, mae: 5.052032, mean_q: 5.238788
 51393/100000: episode: 5246, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 3.690, mean reward: 0.369 [0.282, 0.470], mean action: 12.600 [2.000, 43.000], mean observation: 3.153 [-1.868, 10.407], loss: 1.002499, mae: 5.050461, mean_q: 5.239974
 51403/100000: episode: 5247, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.898, mean reward: 0.390 [0.327, 0.472], mean action: 44.400 [0.000, 96.000], mean observation: 3.162 [-1.669, 10.465], loss: 1.001630, mae: 5.050576, mean_q: 5.241192
 51413/100000: episode: 5248, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.765, mean reward: 0.377 [0.340, 0.458], mean action: 32.500 [3.000, 88.000], mean observation: 3.158 [-1.046, 10.280], loss: 1.422648, mae: 5.052256, mean_q: 5.238426
 51423/100000: episode: 5249, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.413, mean reward: 0.441 [0.358, 0.593], mean action: 31.700 [3.000, 67.000], mean observation: 3.157 [-2.077, 10.390], loss: 1.349076, mae: 5.051876, mean_q: 5.237662
 51433/100000: episode: 5250, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.549, mean reward: 0.455 [0.422, 0.508], mean action: 32.000 [3.000, 86.000], mean observation: 3.152 [-1.511, 10.299], loss: 1.125252, mae: 5.050870, mean_q: 5.238214
 51443/100000: episode: 5251, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.171, mean reward: 0.417 [0.319, 0.563], mean action: 32.900 [3.000, 76.000], mean observation: 3.153 [-1.241, 10.266], loss: 1.039631, mae: 5.050814, mean_q: 5.235788
 51453/100000: episode: 5252, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 4.123, mean reward: 0.412 [0.331, 0.530], mean action: 18.000 [3.000, 90.000], mean observation: 3.158 [-1.653, 10.470], loss: 1.387217, mae: 5.052186, mean_q: 5.233237
 51463/100000: episode: 5253, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.154, mean reward: 0.415 [0.331, 0.497], mean action: 30.100 [0.000, 76.000], mean observation: 3.150 [-1.523, 10.339], loss: 1.717759, mae: 5.053337, mean_q: 5.233048
 51473/100000: episode: 5254, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 4.146, mean reward: 0.415 [0.375, 0.455], mean action: 17.100 [0.000, 88.000], mean observation: 3.151 [-1.343, 10.385], loss: 1.063168, mae: 5.050227, mean_q: 5.232201
 51483/100000: episode: 5255, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.181, mean reward: 0.418 [0.328, 0.495], mean action: 20.700 [0.000, 87.000], mean observation: 3.163 [-1.498, 10.402], loss: 0.887556, mae: 5.049361, mean_q: 5.231338
 51493/100000: episode: 5256, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.543, mean reward: 0.454 [0.350, 0.549], mean action: 41.600 [0.000, 99.000], mean observation: 3.154 [-1.459, 10.466], loss: 1.207364, mae: 5.050911, mean_q: 5.232319
 51503/100000: episode: 5257, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.331, mean reward: 0.433 [0.294, 0.545], mean action: 21.000 [0.000, 101.000], mean observation: 3.154 [-1.777, 10.310], loss: 1.310572, mae: 5.051398, mean_q: 5.234601
 51505/100000: episode: 5258, duration: 0.054s, episode steps: 2, steps per second: 37, episode reward: 10.378, mean reward: 5.189 [0.378, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.140 [-0.713, 10.319], loss: 1.321552, mae: 5.051046, mean_q: 5.234739
 51515/100000: episode: 5259, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 3.898, mean reward: 0.390 [0.331, 0.460], mean action: 19.000 [0.000, 84.000], mean observation: 3.151 [-1.186, 10.411], loss: 0.900945, mae: 5.049811, mean_q: 5.233182
 51525/100000: episode: 5260, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.213, mean reward: 0.421 [0.408, 0.475], mean action: 24.800 [3.000, 94.000], mean observation: 3.158 [-2.242, 10.398], loss: 1.419044, mae: 5.051763, mean_q: 5.234605
 51535/100000: episode: 5261, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.462, mean reward: 0.446 [0.313, 0.590], mean action: 37.100 [0.000, 77.000], mean observation: 3.160 [-1.307, 10.274], loss: 1.156899, mae: 5.050619, mean_q: 5.232794
 51545/100000: episode: 5262, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.158, mean reward: 0.416 [0.339, 0.492], mean action: 22.100 [0.000, 80.000], mean observation: 3.160 [-1.198, 10.282], loss: 1.278003, mae: 5.051144, mean_q: 5.232800
 51555/100000: episode: 5263, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.927, mean reward: 0.393 [0.323, 0.450], mean action: 39.100 [0.000, 99.000], mean observation: 3.154 [-1.290, 10.359], loss: 1.107605, mae: 5.050411, mean_q: 5.230732
 51565/100000: episode: 5264, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.461, mean reward: 0.446 [0.388, 0.508], mean action: 19.000 [0.000, 97.000], mean observation: 3.152 [-1.678, 10.375], loss: 0.996397, mae: 5.049799, mean_q: 5.231235
 51575/100000: episode: 5265, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.131, mean reward: 0.413 [0.369, 0.470], mean action: 16.700 [0.000, 96.000], mean observation: 3.155 [-1.283, 10.376], loss: 0.895674, mae: 5.049567, mean_q: 5.232193
 51585/100000: episode: 5266, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.211, mean reward: 0.421 [0.329, 0.467], mean action: 31.100 [0.000, 98.000], mean observation: 3.152 [-1.393, 10.256], loss: 1.585339, mae: 5.052320, mean_q: 5.226861
 51586/100000: episode: 5267, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 51.000 [51.000, 51.000], mean observation: 3.163 [-1.419, 10.100], loss: 0.792001, mae: 5.048828, mean_q: 5.224173
 51596/100000: episode: 5268, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.148, mean reward: 0.415 [0.348, 0.466], mean action: 10.000 [3.000, 39.000], mean observation: 3.157 [-1.726, 10.409], loss: 1.031891, mae: 5.050061, mean_q: 5.224994
 51606/100000: episode: 5269, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.211, mean reward: 0.421 [0.342, 0.482], mean action: 55.200 [3.000, 99.000], mean observation: 3.161 [-1.993, 10.362], loss: 1.296093, mae: 5.051348, mean_q: 5.225712
 51616/100000: episode: 5270, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.152, mean reward: 0.415 [0.333, 0.466], mean action: 19.700 [3.000, 69.000], mean observation: 3.159 [-1.365, 10.358], loss: 1.820661, mae: 5.053175, mean_q: 5.226763
 51622/100000: episode: 5271, duration: 0.114s, episode steps: 6, steps per second: 53, episode reward: 11.827, mean reward: 1.971 [0.289, 10.000], mean action: 28.167 [3.000, 93.000], mean observation: 3.151 [-1.694, 10.374], loss: 1.114193, mae: 5.050000, mean_q: 5.227713
 51632/100000: episode: 5272, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.413, mean reward: 0.441 [0.350, 0.566], mean action: 16.400 [3.000, 71.000], mean observation: 3.155 [-1.710, 10.415], loss: 1.138580, mae: 5.049807, mean_q: 5.229315
 51642/100000: episode: 5273, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.663, mean reward: 0.366 [0.323, 0.448], mean action: 32.600 [3.000, 100.000], mean observation: 3.154 [-1.347, 10.393], loss: 1.131069, mae: 5.049422, mean_q: 5.231677
 51650/100000: episode: 5274, duration: 0.165s, episode steps: 8, steps per second: 48, episode reward: 12.990, mean reward: 1.624 [0.319, 10.000], mean action: 19.375 [3.000, 72.000], mean observation: 3.163 [-1.217, 10.362], loss: 1.176335, mae: 5.049654, mean_q: 5.233908
 51660/100000: episode: 5275, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.839, mean reward: 0.384 [0.356, 0.421], mean action: 36.000 [3.000, 101.000], mean observation: 3.154 [-0.861, 10.224], loss: 1.234582, mae: 5.049745, mean_q: 5.237517
 51670/100000: episode: 5276, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.450, mean reward: 0.445 [0.381, 0.510], mean action: 47.400 [1.000, 97.000], mean observation: 3.155 [-1.493, 10.490], loss: 0.789796, mae: 5.048004, mean_q: 5.241675
 51680/100000: episode: 5277, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.429, mean reward: 0.443 [0.375, 0.518], mean action: 21.100 [3.000, 81.000], mean observation: 3.155 [-1.216, 10.271], loss: 1.220745, mae: 5.049946, mean_q: 5.245381
 51690/100000: episode: 5278, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.051, mean reward: 0.405 [0.326, 0.496], mean action: 24.400 [3.000, 63.000], mean observation: 3.159 [-1.590, 10.419], loss: 1.064578, mae: 5.049683, mean_q: 5.248463
 51700/100000: episode: 5279, duration: 0.226s, episode steps: 10, steps per second: 44, episode reward: 4.065, mean reward: 0.406 [0.339, 0.515], mean action: 21.900 [3.000, 60.000], mean observation: 3.163 [-1.266, 10.302], loss: 0.901045, mae: 5.049068, mean_q: 5.250552
 51710/100000: episode: 5280, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.319, mean reward: 0.432 [0.347, 0.563], mean action: 22.100 [3.000, 73.000], mean observation: 3.159 [-1.676, 10.353], loss: 1.343983, mae: 5.050886, mean_q: 5.252199
 51720/100000: episode: 5281, duration: 0.215s, episode steps: 10, steps per second: 46, episode reward: 4.432, mean reward: 0.443 [0.406, 0.487], mean action: 20.800 [3.000, 79.000], mean observation: 3.160 [-1.662, 10.343], loss: 1.361501, mae: 5.051070, mean_q: 5.254441
 51730/100000: episode: 5282, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.011, mean reward: 0.401 [0.327, 0.455], mean action: 37.500 [3.000, 98.000], mean observation: 3.163 [-1.119, 10.268], loss: 1.056980, mae: 5.049655, mean_q: 5.254690
 51740/100000: episode: 5283, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 4.248, mean reward: 0.425 [0.369, 0.464], mean action: 14.700 [3.000, 91.000], mean observation: 3.155 [-1.283, 10.360], loss: 1.215153, mae: 5.050455, mean_q: 5.252828
 51750/100000: episode: 5284, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 4.087, mean reward: 0.409 [0.317, 0.512], mean action: 28.900 [3.000, 87.000], mean observation: 3.156 [-1.285, 10.501], loss: 1.187768, mae: 5.050400, mean_q: 5.250367
 51760/100000: episode: 5285, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 4.171, mean reward: 0.417 [0.354, 0.527], mean action: 13.000 [3.000, 54.000], mean observation: 3.157 [-1.171, 10.526], loss: 1.483503, mae: 5.051498, mean_q: 5.246622
 51770/100000: episode: 5286, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.008, mean reward: 0.401 [0.340, 0.565], mean action: 25.000 [3.000, 101.000], mean observation: 3.156 [-1.985, 10.426], loss: 0.870348, mae: 5.049319, mean_q: 5.245645
 51780/100000: episode: 5287, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 4.108, mean reward: 0.411 [0.350, 0.530], mean action: 19.200 [3.000, 59.000], mean observation: 3.162 [-1.660, 10.593], loss: 1.383204, mae: 5.051313, mean_q: 5.246262
 51790/100000: episode: 5288, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.968, mean reward: 0.397 [0.330, 0.491], mean action: 26.100 [3.000, 68.000], mean observation: 3.142 [-1.289, 10.283], loss: 1.460251, mae: 5.051501, mean_q: 5.247715
 51800/100000: episode: 5289, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.987, mean reward: 0.399 [0.318, 0.490], mean action: 46.800 [3.000, 98.000], mean observation: 3.152 [-0.719, 10.240], loss: 1.292141, mae: 5.050367, mean_q: 5.247319
 51810/100000: episode: 5290, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 14.021, mean reward: 1.402 [0.441, 10.000], mean action: 38.000 [3.000, 91.000], mean observation: 3.151 [-1.498, 10.302], loss: 1.599831, mae: 5.051719, mean_q: 5.246408
 51820/100000: episode: 5291, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.145, mean reward: 0.415 [0.319, 0.486], mean action: 23.800 [3.000, 83.000], mean observation: 3.156 [-1.209, 10.246], loss: 1.282809, mae: 5.050014, mean_q: 5.245783
 51830/100000: episode: 5292, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 3.958, mean reward: 0.396 [0.336, 0.528], mean action: 22.100 [1.000, 67.000], mean observation: 3.159 [-1.678, 10.425], loss: 1.316921, mae: 5.050171, mean_q: 5.243335
 51840/100000: episode: 5293, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.271, mean reward: 0.427 [0.366, 0.526], mean action: 11.300 [3.000, 53.000], mean observation: 3.164 [-2.118, 10.310], loss: 1.008892, mae: 5.048895, mean_q: 5.243677
 51850/100000: episode: 5294, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.019, mean reward: 0.402 [0.330, 0.542], mean action: 34.300 [3.000, 71.000], mean observation: 3.148 [-1.650, 10.220], loss: 1.190641, mae: 5.049820, mean_q: 5.245510
 51860/100000: episode: 5295, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.960, mean reward: 0.396 [0.323, 0.434], mean action: 23.400 [3.000, 80.000], mean observation: 3.161 [-1.473, 10.301], loss: 1.106663, mae: 5.049407, mean_q: 5.243207
 51869/100000: episode: 5296, duration: 0.164s, episode steps: 9, steps per second: 55, episode reward: 13.230, mean reward: 1.470 [0.341, 10.000], mean action: 36.889 [3.000, 90.000], mean observation: 3.159 [-1.403, 10.383], loss: 1.633671, mae: 5.051593, mean_q: 5.242908
 51879/100000: episode: 5297, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.273, mean reward: 0.427 [0.301, 0.640], mean action: 23.300 [3.000, 61.000], mean observation: 3.154 [-2.325, 10.317], loss: 1.445251, mae: 5.050233, mean_q: 5.244636
 51889/100000: episode: 5298, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.308, mean reward: 0.431 [0.373, 0.479], mean action: 22.900 [3.000, 101.000], mean observation: 3.157 [-1.002, 10.340], loss: 1.189561, mae: 5.048976, mean_q: 5.247283
 51899/100000: episode: 5299, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.915, mean reward: 0.492 [0.390, 0.521], mean action: 47.100 [2.000, 99.000], mean observation: 3.158 [-1.486, 10.257], loss: 1.609962, mae: 5.050511, mean_q: 5.246082
 51909/100000: episode: 5300, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.025, mean reward: 0.403 [0.316, 0.440], mean action: 22.800 [2.000, 87.000], mean observation: 3.165 [-0.969, 10.458], loss: 0.985214, mae: 5.048042, mean_q: 5.244311
 51919/100000: episode: 5301, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.961, mean reward: 0.396 [0.332, 0.441], mean action: 34.500 [2.000, 91.000], mean observation: 3.154 [-1.326, 10.256], loss: 1.142051, mae: 5.048616, mean_q: 5.241632
 51929/100000: episode: 5302, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.371, mean reward: 0.437 [0.424, 0.511], mean action: 24.500 [3.000, 79.000], mean observation: 3.152 [-1.949, 10.496], loss: 1.111711, mae: 5.048665, mean_q: 5.230414
 51939/100000: episode: 5303, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.245, mean reward: 0.425 [0.352, 0.503], mean action: 71.300 [16.000, 100.000], mean observation: 3.163 [-1.037, 10.385], loss: 1.589699, mae: 5.050188, mean_q: 5.225889
 51949/100000: episode: 5304, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.518, mean reward: 0.352 [0.335, 0.408], mean action: 66.700 [12.000, 79.000], mean observation: 3.163 [-1.675, 10.260], loss: 1.200803, mae: 5.048381, mean_q: 5.227234
 51959/100000: episode: 5305, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.701, mean reward: 0.470 [0.374, 0.516], mean action: 53.300 [27.000, 79.000], mean observation: 3.148 [-1.665, 10.235], loss: 0.988694, mae: 5.047231, mean_q: 5.228383
 51969/100000: episode: 5306, duration: 0.108s, episode steps: 10, steps per second: 92, episode reward: 4.357, mean reward: 0.436 [0.414, 0.516], mean action: 76.600 [39.000, 98.000], mean observation: 3.145 [-1.184, 10.341], loss: 1.102787, mae: 5.047699, mean_q: 5.231307
 51979/100000: episode: 5307, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.073, mean reward: 0.407 [0.349, 0.533], mean action: 67.300 [4.000, 100.000], mean observation: 3.158 [-1.193, 10.306], loss: 1.517966, mae: 5.049056, mean_q: 5.233634
 51989/100000: episode: 5308, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.179, mean reward: 0.418 [0.409, 0.452], mean action: 66.300 [19.000, 101.000], mean observation: 3.160 [-1.461, 10.377], loss: 1.194703, mae: 5.047318, mean_q: 5.235608
 51999/100000: episode: 5309, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 3.841, mean reward: 0.384 [0.298, 0.447], mean action: 57.300 [13.000, 100.000], mean observation: 3.155 [-1.226, 10.187], loss: 0.997179, mae: 5.046401, mean_q: 5.236951
 52009/100000: episode: 5310, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.596, mean reward: 0.460 [0.390, 0.477], mean action: 67.800 [3.000, 98.000], mean observation: 3.155 [-1.093, 10.266], loss: 1.029239, mae: 5.046504, mean_q: 5.235180
 52019/100000: episode: 5311, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.969, mean reward: 0.397 [0.382, 0.495], mean action: 71.400 [20.000, 93.000], mean observation: 3.173 [-1.025, 10.240], loss: 1.204666, mae: 5.047266, mean_q: 5.231909
 52029/100000: episode: 5312, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.380, mean reward: 0.438 [0.412, 0.537], mean action: 58.600 [17.000, 97.000], mean observation: 3.148 [-1.652, 10.294], loss: 1.215597, mae: 5.047541, mean_q: 5.232589
 52039/100000: episode: 5313, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.781, mean reward: 0.378 [0.331, 0.459], mean action: 57.500 [2.000, 81.000], mean observation: 3.156 [-1.190, 10.273], loss: 1.366755, mae: 5.048299, mean_q: 5.229976
 52049/100000: episode: 5314, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.586, mean reward: 0.459 [0.368, 0.506], mean action: 54.700 [15.000, 97.000], mean observation: 3.168 [-0.982, 10.462], loss: 1.287221, mae: 5.047540, mean_q: 5.228125
 52059/100000: episode: 5315, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 5.417, mean reward: 0.542 [0.542, 0.542], mean action: 68.300 [21.000, 79.000], mean observation: 3.161 [-1.262, 10.278], loss: 1.567968, mae: 5.048514, mean_q: 5.228838
 52069/100000: episode: 5316, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.103, mean reward: 0.410 [0.350, 0.450], mean action: 65.300 [34.000, 85.000], mean observation: 3.150 [-1.231, 10.247], loss: 0.971906, mae: 5.046194, mean_q: 5.230563
 52079/100000: episode: 5317, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.621, mean reward: 0.462 [0.435, 0.474], mean action: 64.400 [16.000, 101.000], mean observation: 3.150 [-1.941, 10.292], loss: 1.155294, mae: 5.047127, mean_q: 5.232517
 52089/100000: episode: 5318, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.095, mean reward: 0.409 [0.351, 0.463], mean action: 47.300 [9.000, 100.000], mean observation: 3.166 [-1.464, 10.270], loss: 1.033944, mae: 5.046618, mean_q: 5.233230
 52099/100000: episode: 5319, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.429, mean reward: 0.443 [0.355, 0.531], mean action: 40.200 [0.000, 71.000], mean observation: 3.154 [-1.474, 10.332], loss: 1.208317, mae: 5.047544, mean_q: 5.235356
 52109/100000: episode: 5320, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.729, mean reward: 0.373 [0.321, 0.430], mean action: 46.300 [17.000, 95.000], mean observation: 3.154 [-1.406, 10.181], loss: 1.103299, mae: 5.047232, mean_q: 5.235185
 52119/100000: episode: 5321, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.014, mean reward: 0.401 [0.316, 0.554], mean action: 35.700 [1.000, 98.000], mean observation: 3.151 [-2.581, 10.269], loss: 1.160567, mae: 5.047458, mean_q: 5.232038
 52129/100000: episode: 5322, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.174, mean reward: 0.417 [0.349, 0.564], mean action: 26.800 [4.000, 38.000], mean observation: 3.151 [-1.653, 10.331], loss: 1.227567, mae: 5.047895, mean_q: 5.228908
 52139/100000: episode: 5323, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.070, mean reward: 0.407 [0.360, 0.514], mean action: 68.900 [26.000, 96.000], mean observation: 3.147 [-2.093, 10.352], loss: 1.342934, mae: 5.048462, mean_q: 5.225294
 52149/100000: episode: 5324, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.359, mean reward: 0.436 [0.403, 0.490], mean action: 54.600 [11.000, 80.000], mean observation: 3.155 [-1.828, 10.348], loss: 1.376076, mae: 5.048417, mean_q: 5.222110
 52159/100000: episode: 5325, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.721, mean reward: 0.372 [0.324, 0.485], mean action: 46.400 [8.000, 96.000], mean observation: 3.158 [-1.342, 10.412], loss: 1.235656, mae: 5.047541, mean_q: 5.219908
 52161/100000: episode: 5326, duration: 0.049s, episode steps: 2, steps per second: 41, episode reward: 10.451, mean reward: 5.225 [0.451, 10.000], mean action: 47.500 [38.000, 57.000], mean observation: 3.162 [-1.104, 10.100], loss: 1.300214, mae: 5.047560, mean_q: 5.220121
 52171/100000: episode: 5327, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.004, mean reward: 0.400 [0.385, 0.476], mean action: 36.800 [9.000, 63.000], mean observation: 3.162 [-1.680, 10.347], loss: 0.983362, mae: 5.046431, mean_q: 5.220293
 52181/100000: episode: 5328, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.311, mean reward: 0.431 [0.398, 0.507], mean action: 42.500 [38.000, 67.000], mean observation: 3.167 [-1.065, 10.255], loss: 1.446365, mae: 5.047941, mean_q: 5.219278
 52191/100000: episode: 5329, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.989, mean reward: 0.399 [0.313, 0.530], mean action: 33.600 [0.000, 79.000], mean observation: 3.153 [-1.496, 10.394], loss: 1.168002, mae: 5.046520, mean_q: 5.215968
 52201/100000: episode: 5330, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.042, mean reward: 0.404 [0.336, 0.455], mean action: 25.500 [0.000, 99.000], mean observation: 3.163 [-1.993, 10.301], loss: 1.211887, mae: 5.046558, mean_q: 5.218642
 52211/100000: episode: 5331, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 3.983, mean reward: 0.398 [0.369, 0.508], mean action: 48.900 [0.000, 89.000], mean observation: 3.158 [-1.799, 10.388], loss: 1.214570, mae: 5.046498, mean_q: 5.220325
 52221/100000: episode: 5332, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.238, mean reward: 0.424 [0.357, 0.551], mean action: 18.400 [0.000, 83.000], mean observation: 3.150 [-1.240, 10.394], loss: 1.409359, mae: 5.047077, mean_q: 5.221511
 52231/100000: episode: 5333, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.034, mean reward: 0.403 [0.301, 0.492], mean action: 15.000 [0.000, 80.000], mean observation: 3.165 [-1.482, 10.283], loss: 1.362168, mae: 5.046740, mean_q: 5.223750
 52241/100000: episode: 5334, duration: 0.221s, episode steps: 10, steps per second: 45, episode reward: 3.769, mean reward: 0.377 [0.319, 0.454], mean action: 19.000 [0.000, 84.000], mean observation: 3.159 [-1.648, 10.316], loss: 0.960963, mae: 5.044969, mean_q: 5.225828
 52251/100000: episode: 5335, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.520, mean reward: 0.452 [0.328, 0.633], mean action: 29.800 [0.000, 79.000], mean observation: 3.159 [-1.590, 10.287], loss: 1.318776, mae: 5.046289, mean_q: 5.227222
 52261/100000: episode: 5336, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.158, mean reward: 0.416 [0.307, 0.516], mean action: 46.400 [0.000, 99.000], mean observation: 3.174 [-1.364, 10.402], loss: 0.993959, mae: 5.044961, mean_q: 5.226636
 52271/100000: episode: 5337, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.068, mean reward: 0.407 [0.358, 0.479], mean action: 15.200 [0.000, 86.000], mean observation: 3.156 [-1.589, 10.298], loss: 1.126050, mae: 5.045645, mean_q: 5.227782
 52281/100000: episode: 5338, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.934, mean reward: 0.393 [0.320, 0.478], mean action: 29.500 [0.000, 99.000], mean observation: 3.156 [-1.765, 10.421], loss: 1.350237, mae: 5.046446, mean_q: 5.230473
 52291/100000: episode: 5339, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 3.902, mean reward: 0.390 [0.308, 0.506], mean action: 22.700 [0.000, 81.000], mean observation: 3.144 [-1.707, 10.367], loss: 0.785929, mae: 5.044028, mean_q: 5.233071
 52301/100000: episode: 5340, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.182, mean reward: 0.418 [0.352, 0.525], mean action: 10.200 [0.000, 80.000], mean observation: 3.155 [-1.363, 10.419], loss: 1.001862, mae: 5.045143, mean_q: 5.236398
 52311/100000: episode: 5341, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 4.080, mean reward: 0.408 [0.353, 0.451], mean action: 25.000 [0.000, 71.000], mean observation: 3.152 [-2.061, 10.242], loss: 1.400474, mae: 5.047105, mean_q: 5.239345
 52321/100000: episode: 5342, duration: 0.227s, episode steps: 10, steps per second: 44, episode reward: 4.179, mean reward: 0.418 [0.365, 0.460], mean action: 21.800 [0.000, 85.000], mean observation: 3.149 [-1.836, 10.418], loss: 1.472940, mae: 5.047486, mean_q: 5.237083
 52331/100000: episode: 5343, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.067, mean reward: 0.407 [0.318, 0.483], mean action: 35.400 [0.000, 98.000], mean observation: 3.151 [-1.298, 10.349], loss: 1.023530, mae: 5.045412, mean_q: 5.236357
 52341/100000: episode: 5344, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 3.968, mean reward: 0.397 [0.340, 0.556], mean action: 27.500 [0.000, 85.000], mean observation: 3.156 [-1.822, 10.356], loss: 1.315554, mae: 5.046581, mean_q: 5.237981
 52351/100000: episode: 5345, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.134, mean reward: 0.413 [0.287, 0.508], mean action: 32.600 [0.000, 100.000], mean observation: 3.152 [-1.731, 10.308], loss: 1.297539, mae: 5.046415, mean_q: 5.237713
 52361/100000: episode: 5346, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.161, mean reward: 0.416 [0.331, 0.463], mean action: 18.100 [0.000, 85.000], mean observation: 3.163 [-1.860, 10.559], loss: 1.252974, mae: 5.046018, mean_q: 5.237349
 52371/100000: episode: 5347, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.978, mean reward: 0.398 [0.319, 0.561], mean action: 25.400 [0.000, 72.000], mean observation: 3.166 [-1.359, 10.317], loss: 1.330600, mae: 5.046002, mean_q: 5.233003
 52381/100000: episode: 5348, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.057, mean reward: 0.406 [0.337, 0.484], mean action: 25.800 [0.000, 91.000], mean observation: 3.159 [-1.057, 10.382], loss: 1.308702, mae: 5.045943, mean_q: 5.231195
 52391/100000: episode: 5349, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.037, mean reward: 0.404 [0.351, 0.584], mean action: 25.100 [0.000, 74.000], mean observation: 3.161 [-1.629, 10.544], loss: 1.339714, mae: 5.045897, mean_q: 5.233686
 52401/100000: episode: 5350, duration: 0.236s, episode steps: 10, steps per second: 42, episode reward: 3.900, mean reward: 0.390 [0.326, 0.436], mean action: 23.200 [0.000, 97.000], mean observation: 3.152 [-1.186, 10.275], loss: 1.644908, mae: 5.046598, mean_q: 5.236610
 52411/100000: episode: 5351, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.237, mean reward: 0.424 [0.330, 0.529], mean action: 35.700 [0.000, 89.000], mean observation: 3.165 [-1.948, 10.384], loss: 1.137875, mae: 5.044008, mean_q: 5.235856
 52421/100000: episode: 5352, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.952, mean reward: 0.395 [0.312, 0.509], mean action: 33.400 [0.000, 74.000], mean observation: 3.158 [-1.527, 10.313], loss: 1.285561, mae: 5.044392, mean_q: 5.235848
 52431/100000: episode: 5353, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.064, mean reward: 0.406 [0.346, 0.557], mean action: 24.300 [0.000, 81.000], mean observation: 3.162 [-1.491, 10.324], loss: 1.076407, mae: 5.043335, mean_q: 5.236306
 52441/100000: episode: 5354, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.005, mean reward: 0.401 [0.304, 0.500], mean action: 21.500 [0.000, 90.000], mean observation: 3.159 [-2.009, 10.382], loss: 1.292585, mae: 5.044412, mean_q: 5.233874
 52451/100000: episode: 5355, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.109, mean reward: 0.411 [0.323, 0.481], mean action: 43.800 [0.000, 97.000], mean observation: 3.163 [-1.678, 10.467], loss: 1.108233, mae: 5.043434, mean_q: 5.230314
 52461/100000: episode: 5356, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.391, mean reward: 0.439 [0.357, 0.490], mean action: 40.800 [0.000, 90.000], mean observation: 3.150 [-1.213, 10.302], loss: 1.212004, mae: 5.044101, mean_q: 5.225145
 52471/100000: episode: 5357, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.233, mean reward: 0.423 [0.408, 0.456], mean action: 46.700 [0.000, 93.000], mean observation: 3.145 [-0.810, 10.371], loss: 1.655509, mae: 5.045589, mean_q: 5.222581
 52481/100000: episode: 5358, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.382, mean reward: 0.438 [0.350, 0.499], mean action: 44.800 [9.000, 84.000], mean observation: 3.160 [-1.099, 10.342], loss: 1.217412, mae: 5.043557, mean_q: 5.223576
 52491/100000: episode: 5359, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.174, mean reward: 0.417 [0.374, 0.428], mean action: 70.600 [2.000, 101.000], mean observation: 3.151 [-0.887, 10.420], loss: 1.164258, mae: 5.043132, mean_q: 5.224689
 52501/100000: episode: 5360, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.096, mean reward: 0.410 [0.363, 0.558], mean action: 75.000 [44.000, 79.000], mean observation: 3.168 [-1.790, 10.258], loss: 1.464829, mae: 5.043992, mean_q: 5.226118
 52511/100000: episode: 5361, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.987, mean reward: 0.399 [0.342, 0.477], mean action: 69.100 [0.000, 98.000], mean observation: 3.140 [-1.229, 10.574], loss: 1.590415, mae: 5.044302, mean_q: 5.226733
 52521/100000: episode: 5362, duration: 0.126s, episode steps: 10, steps per second: 80, episode reward: 4.121, mean reward: 0.412 [0.313, 0.468], mean action: 71.000 [15.000, 93.000], mean observation: 3.156 [-0.937, 10.280], loss: 1.137158, mae: 5.042071, mean_q: 5.220369
 52531/100000: episode: 5363, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.914, mean reward: 0.391 [0.351, 0.468], mean action: 51.300 [7.000, 90.000], mean observation: 3.152 [-1.179, 10.286], loss: 1.408591, mae: 5.042943, mean_q: 5.218286
 52541/100000: episode: 5364, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.030, mean reward: 0.403 [0.361, 0.547], mean action: 63.400 [20.000, 93.000], mean observation: 3.159 [-1.279, 10.300], loss: 1.683722, mae: 5.043834, mean_q: 5.218547
 52551/100000: episode: 5365, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 5.906, mean reward: 0.591 [0.387, 0.640], mean action: 58.300 [4.000, 79.000], mean observation: 3.153 [-1.335, 10.271], loss: 1.460084, mae: 5.042568, mean_q: 5.219246
 52561/100000: episode: 5366, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.010, mean reward: 0.401 [0.369, 0.452], mean action: 67.400 [16.000, 88.000], mean observation: 3.160 [-1.330, 10.292], loss: 1.380669, mae: 5.041430, mean_q: 5.220466
 52571/100000: episode: 5367, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.684, mean reward: 0.468 [0.429, 0.528], mean action: 53.000 [0.000, 79.000], mean observation: 3.152 [-1.292, 10.351], loss: 1.272684, mae: 5.040746, mean_q: 5.221456
 52574/100000: episode: 5368, duration: 0.075s, episode steps: 3, steps per second: 40, episode reward: 10.826, mean reward: 3.609 [0.351, 10.000], mean action: 15.000 [0.000, 45.000], mean observation: 3.164 [-1.192, 10.392], loss: 0.816145, mae: 5.038705, mean_q: 5.221385
 52584/100000: episode: 5369, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.343, mean reward: 0.434 [0.348, 0.494], mean action: 22.500 [0.000, 79.000], mean observation: 3.159 [-2.019, 10.351], loss: 1.139143, mae: 5.039910, mean_q: 5.221409
 52594/100000: episode: 5370, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.585, mean reward: 0.359 [0.339, 0.398], mean action: 60.700 [4.000, 79.000], mean observation: 3.152 [-1.877, 10.344], loss: 1.340187, mae: 5.040936, mean_q: 5.217813
 52604/100000: episode: 5371, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.691, mean reward: 0.369 [0.351, 0.377], mean action: 65.700 [18.000, 88.000], mean observation: 3.171 [-1.321, 10.319], loss: 1.067298, mae: 5.039937, mean_q: 5.219422
 52614/100000: episode: 5372, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.201, mean reward: 0.420 [0.383, 0.480], mean action: 69.300 [40.000, 79.000], mean observation: 3.149 [-1.811, 10.275], loss: 1.221846, mae: 5.040577, mean_q: 5.221081
 52624/100000: episode: 5373, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.621, mean reward: 0.462 [0.460, 0.477], mean action: 72.500 [35.000, 91.000], mean observation: 3.176 [-1.393, 10.184], loss: 1.479713, mae: 5.041350, mean_q: 5.222448
 52634/100000: episode: 5374, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.940, mean reward: 0.394 [0.325, 0.483], mean action: 49.600 [4.000, 79.000], mean observation: 3.151 [-1.662, 10.292], loss: 1.460397, mae: 5.041042, mean_q: 5.224024
 52644/100000: episode: 5375, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.516, mean reward: 0.452 [0.393, 0.522], mean action: 47.700 [10.000, 79.000], mean observation: 3.150 [-1.333, 10.361], loss: 1.078503, mae: 5.039187, mean_q: 5.226057
 52654/100000: episode: 5376, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.061, mean reward: 0.406 [0.371, 0.494], mean action: 64.500 [15.000, 98.000], mean observation: 3.146 [-1.524, 10.322], loss: 1.164505, mae: 5.039574, mean_q: 5.228387
 52663/100000: episode: 5377, duration: 0.130s, episode steps: 9, steps per second: 69, episode reward: 13.554, mean reward: 1.506 [0.444, 10.000], mean action: 57.000 [5.000, 92.000], mean observation: 3.137 [-1.357, 10.279], loss: 1.463854, mae: 5.040655, mean_q: 5.229660
 52673/100000: episode: 5378, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.071, mean reward: 0.407 [0.382, 0.492], mean action: 66.600 [14.000, 86.000], mean observation: 3.160 [-1.226, 10.430], loss: 1.140568, mae: 5.039658, mean_q: 5.227592
 52683/100000: episode: 5379, duration: 0.150s, episode steps: 10, steps per second: 66, episode reward: 4.791, mean reward: 0.479 [0.393, 0.501], mean action: 60.000 [14.000, 79.000], mean observation: 3.160 [-0.946, 10.363], loss: 1.194731, mae: 5.039922, mean_q: 5.227737
 52693/100000: episode: 5380, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.220, mean reward: 0.422 [0.372, 0.469], mean action: 58.800 [16.000, 79.000], mean observation: 3.146 [-1.692, 10.304], loss: 1.110796, mae: 5.039416, mean_q: 5.228855
 52703/100000: episode: 5381, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 3.922, mean reward: 0.392 [0.385, 0.412], mean action: 65.100 [21.000, 92.000], mean observation: 3.167 [-0.887, 10.267], loss: 1.457918, mae: 5.041049, mean_q: 5.230699
 52713/100000: episode: 5382, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 3.664, mean reward: 0.366 [0.284, 0.471], mean action: 73.000 [35.000, 81.000], mean observation: 3.163 [-1.136, 10.393], loss: 1.510195, mae: 5.041032, mean_q: 5.232374
 52723/100000: episode: 5383, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.997, mean reward: 0.400 [0.352, 0.454], mean action: 59.600 [12.000, 86.000], mean observation: 3.154 [-0.865, 10.254], loss: 1.621963, mae: 5.041189, mean_q: 5.233866
 52733/100000: episode: 5384, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.837, mean reward: 0.384 [0.285, 0.483], mean action: 70.900 [43.000, 84.000], mean observation: 3.157 [-0.927, 10.302], loss: 1.432048, mae: 5.039993, mean_q: 5.236178
 52743/100000: episode: 5385, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 5.001, mean reward: 0.500 [0.402, 0.525], mean action: 73.800 [16.000, 88.000], mean observation: 3.162 [-1.536, 10.287], loss: 1.185390, mae: 5.038502, mean_q: 5.233486
 52753/100000: episode: 5386, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 5.305, mean reward: 0.531 [0.417, 0.559], mean action: 59.900 [6.000, 80.000], mean observation: 3.150 [-1.301, 10.570], loss: 1.204933, mae: 5.038483, mean_q: 5.228771
 52763/100000: episode: 5387, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.681, mean reward: 0.468 [0.405, 0.569], mean action: 61.100 [6.000, 97.000], mean observation: 3.157 [-1.286, 10.168], loss: 1.338741, mae: 5.038824, mean_q: 5.229484
 52773/100000: episode: 5388, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.012, mean reward: 0.401 [0.351, 0.559], mean action: 62.100 [0.000, 100.000], mean observation: 3.154 [-1.456, 10.358], loss: 1.176538, mae: 5.038090, mean_q: 5.232131
 52783/100000: episode: 5389, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.728, mean reward: 0.373 [0.321, 0.452], mean action: 58.000 [23.000, 79.000], mean observation: 3.157 [-2.327, 10.352], loss: 1.560121, mae: 5.039627, mean_q: 5.235897
 52793/100000: episode: 5390, duration: 0.095s, episode steps: 10, steps per second: 105, episode reward: 4.396, mean reward: 0.440 [0.440, 0.440], mean action: 81.400 [68.000, 100.000], mean observation: 3.138 [-1.275, 10.353], loss: 1.319508, mae: 5.038380, mean_q: 5.238273
 52803/100000: episode: 5391, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.002, mean reward: 0.400 [0.331, 0.457], mean action: 64.700 [8.000, 99.000], mean observation: 3.145 [-1.469, 10.216], loss: 1.417043, mae: 5.038877, mean_q: 5.236664
 52813/100000: episode: 5392, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.299, mean reward: 0.430 [0.392, 0.497], mean action: 72.800 [17.000, 79.000], mean observation: 3.146 [-1.458, 10.231], loss: 1.535533, mae: 5.039208, mean_q: 5.236800
 52823/100000: episode: 5393, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.251, mean reward: 0.425 [0.395, 0.517], mean action: 60.600 [7.000, 86.000], mean observation: 3.148 [-1.713, 10.471], loss: 1.339128, mae: 5.038164, mean_q: 5.238160
 52833/100000: episode: 5394, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.333, mean reward: 0.433 [0.403, 0.497], mean action: 62.500 [14.000, 79.000], mean observation: 3.163 [-0.884, 10.589], loss: 1.230054, mae: 5.037627, mean_q: 5.239750
 52843/100000: episode: 5395, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 5.394, mean reward: 0.539 [0.539, 0.539], mean action: 79.200 [79.000, 81.000], mean observation: 3.165 [-0.936, 10.343], loss: 1.278127, mae: 5.037865, mean_q: 5.237911
 52853/100000: episode: 5396, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.440, mean reward: 0.444 [0.377, 0.524], mean action: 64.200 [6.000, 79.000], mean observation: 3.149 [-1.264, 10.322], loss: 1.246760, mae: 5.037759, mean_q: 5.238729
 52863/100000: episode: 5397, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.902, mean reward: 0.390 [0.374, 0.427], mean action: 73.400 [19.000, 92.000], mean observation: 3.171 [-1.331, 10.326], loss: 0.931369, mae: 5.036731, mean_q: 5.236375
 52873/100000: episode: 5398, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.665, mean reward: 0.367 [0.324, 0.436], mean action: 61.000 [16.000, 79.000], mean observation: 3.156 [-0.841, 10.332], loss: 1.220288, mae: 5.038179, mean_q: 5.235210
 52883/100000: episode: 5399, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 4.118, mean reward: 0.412 [0.388, 0.441], mean action: 63.100 [2.000, 86.000], mean observation: 3.151 [-1.414, 10.351], loss: 1.049640, mae: 5.037924, mean_q: 5.233948
 52893/100000: episode: 5400, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.069, mean reward: 0.407 [0.386, 0.440], mean action: 56.500 [15.000, 79.000], mean observation: 3.170 [-1.311, 10.407], loss: 1.791510, mae: 5.041051, mean_q: 5.234630
 52903/100000: episode: 5401, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.506, mean reward: 0.451 [0.381, 0.489], mean action: 72.300 [22.000, 100.000], mean observation: 3.154 [-0.957, 10.403], loss: 1.363809, mae: 5.039139, mean_q: 5.231532
 52913/100000: episode: 5402, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.692, mean reward: 0.369 [0.331, 0.403], mean action: 58.200 [13.000, 79.000], mean observation: 3.159 [-1.161, 10.213], loss: 1.217413, mae: 5.038125, mean_q: 5.226640
 52923/100000: episode: 5403, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.531, mean reward: 0.353 [0.331, 0.456], mean action: 61.800 [32.000, 90.000], mean observation: 3.157 [-1.543, 10.319], loss: 1.209367, mae: 5.037740, mean_q: 5.215867
 52933/100000: episode: 5404, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.283, mean reward: 0.428 [0.336, 0.490], mean action: 39.800 [2.000, 81.000], mean observation: 3.162 [-1.385, 10.286], loss: 1.234760, mae: 5.037816, mean_q: 5.214482
 52943/100000: episode: 5405, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.047, mean reward: 0.405 [0.350, 0.504], mean action: 37.200 [3.000, 76.000], mean observation: 3.166 [-1.796, 10.338], loss: 0.959704, mae: 5.036546, mean_q: 5.212291
 52953/100000: episode: 5406, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.887, mean reward: 0.389 [0.366, 0.508], mean action: 43.000 [6.000, 65.000], mean observation: 3.154 [-1.172, 10.301], loss: 1.291482, mae: 5.037852, mean_q: 5.212220
 52963/100000: episode: 5407, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.346, mean reward: 0.435 [0.380, 0.549], mean action: 19.800 [0.000, 100.000], mean observation: 3.159 [-1.370, 10.488], loss: 1.156936, mae: 5.037469, mean_q: 5.213000
 52973/100000: episode: 5408, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 3.711, mean reward: 0.371 [0.307, 0.438], mean action: 20.500 [6.000, 66.000], mean observation: 3.159 [-1.133, 10.313], loss: 1.440533, mae: 5.038317, mean_q: 5.213645
 52983/100000: episode: 5409, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.558, mean reward: 0.456 [0.369, 0.514], mean action: 47.400 [6.000, 96.000], mean observation: 3.155 [-1.667, 10.306], loss: 1.232899, mae: 5.037222, mean_q: 5.214458
 52993/100000: episode: 5410, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.390, mean reward: 0.439 [0.356, 0.503], mean action: 26.400 [6.000, 63.000], mean observation: 3.167 [-1.203, 10.207], loss: 0.957262, mae: 5.036076, mean_q: 5.215924
 53003/100000: episode: 5411, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.871, mean reward: 0.387 [0.314, 0.454], mean action: 27.900 [6.000, 89.000], mean observation: 3.152 [-1.923, 10.388], loss: 1.471016, mae: 5.038313, mean_q: 5.217182
 53013/100000: episode: 5412, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 3.915, mean reward: 0.392 [0.319, 0.434], mean action: 23.700 [6.000, 96.000], mean observation: 3.159 [-1.721, 10.448], loss: 1.339346, mae: 5.037545, mean_q: 5.215233
 53023/100000: episode: 5413, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.687, mean reward: 0.369 [0.320, 0.444], mean action: 31.000 [6.000, 94.000], mean observation: 3.159 [-1.212, 10.415], loss: 1.261492, mae: 5.037190, mean_q: 5.215134
 53033/100000: episode: 5414, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.385, mean reward: 0.438 [0.358, 0.519], mean action: 15.000 [6.000, 92.000], mean observation: 3.152 [-1.383, 10.436], loss: 1.329283, mae: 5.037280, mean_q: 5.216905
 53043/100000: episode: 5415, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.726, mean reward: 0.373 [0.252, 0.473], mean action: 30.800 [2.000, 90.000], mean observation: 3.157 [-1.552, 10.305], loss: 1.309536, mae: 5.036984, mean_q: 5.217887
 53053/100000: episode: 5416, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 3.800, mean reward: 0.380 [0.304, 0.454], mean action: 29.100 [6.000, 100.000], mean observation: 3.154 [-1.093, 10.251], loss: 1.206353, mae: 5.036357, mean_q: 5.218328
 53063/100000: episode: 5417, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.117, mean reward: 0.412 [0.318, 0.492], mean action: 29.600 [3.000, 83.000], mean observation: 3.154 [-1.739, 10.385], loss: 1.172512, mae: 5.036115, mean_q: 5.216579
 53073/100000: episode: 5418, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.121, mean reward: 0.412 [0.384, 0.478], mean action: 64.000 [11.000, 95.000], mean observation: 3.140 [-1.644, 10.351], loss: 1.059891, mae: 5.035500, mean_q: 5.214203
 53083/100000: episode: 5419, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.476, mean reward: 0.448 [0.388, 0.492], mean action: 71.200 [20.000, 90.000], mean observation: 3.157 [-1.002, 10.407], loss: 1.136167, mae: 5.035738, mean_q: 5.215207
 53093/100000: episode: 5420, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.305, mean reward: 0.431 [0.364, 0.517], mean action: 67.300 [20.000, 83.000], mean observation: 3.159 [-1.693, 10.337], loss: 1.437076, mae: 5.036782, mean_q: 5.215786
 53103/100000: episode: 5421, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.742, mean reward: 0.374 [0.290, 0.422], mean action: 71.300 [31.000, 89.000], mean observation: 3.148 [-1.095, 10.274], loss: 1.533659, mae: 5.036665, mean_q: 5.216386
 53113/100000: episode: 5422, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.289, mean reward: 0.429 [0.414, 0.522], mean action: 58.500 [3.000, 83.000], mean observation: 3.163 [-1.276, 10.346], loss: 1.060557, mae: 5.034472, mean_q: 5.215198
 53123/100000: episode: 5423, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.119, mean reward: 0.412 [0.404, 0.414], mean action: 66.000 [13.000, 83.000], mean observation: 3.153 [-0.855, 10.352], loss: 0.865421, mae: 5.033816, mean_q: 5.210705
 53133/100000: episode: 5424, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.879, mean reward: 0.388 [0.329, 0.469], mean action: 69.300 [17.000, 83.000], mean observation: 3.152 [-1.101, 10.244], loss: 1.337118, mae: 5.035868, mean_q: 5.210093
 53143/100000: episode: 5425, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 4.636, mean reward: 0.464 [0.363, 0.570], mean action: 67.000 [22.000, 83.000], mean observation: 3.165 [-0.836, 10.310], loss: 1.163798, mae: 5.035270, mean_q: 5.210842
 53153/100000: episode: 5426, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 5.043, mean reward: 0.504 [0.504, 0.504], mean action: 75.700 [2.000, 93.000], mean observation: 3.153 [-0.825, 10.385], loss: 1.140325, mae: 5.035265, mean_q: 5.211398
 53163/100000: episode: 5427, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.070, mean reward: 0.407 [0.367, 0.498], mean action: 69.800 [31.000, 93.000], mean observation: 3.150 [-1.842, 10.363], loss: 0.936770, mae: 5.034634, mean_q: 5.212054
 53173/100000: episode: 5428, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 4.072, mean reward: 0.407 [0.400, 0.442], mean action: 69.900 [16.000, 83.000], mean observation: 3.161 [-1.146, 10.496], loss: 1.028426, mae: 5.035232, mean_q: 5.212712
 53183/100000: episode: 5429, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.178, mean reward: 0.418 [0.339, 0.527], mean action: 32.800 [0.000, 81.000], mean observation: 3.158 [-1.658, 10.357], loss: 0.935156, mae: 5.034838, mean_q: 5.212667
 53193/100000: episode: 5430, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.066, mean reward: 0.407 [0.326, 0.529], mean action: 19.500 [0.000, 75.000], mean observation: 3.157 [-1.508, 10.279], loss: 1.210315, mae: 5.036080, mean_q: 5.214028
 53203/100000: episode: 5431, duration: 0.223s, episode steps: 10, steps per second: 45, episode reward: 3.984, mean reward: 0.398 [0.315, 0.469], mean action: 16.300 [0.000, 55.000], mean observation: 3.153 [-1.456, 10.441], loss: 1.305783, mae: 5.036458, mean_q: 5.215410
 53213/100000: episode: 5432, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.054, mean reward: 0.405 [0.362, 0.435], mean action: 40.800 [0.000, 95.000], mean observation: 3.158 [-1.474, 10.406], loss: 1.134229, mae: 5.035625, mean_q: 5.216702
 53223/100000: episode: 5433, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.059, mean reward: 0.406 [0.336, 0.505], mean action: 11.500 [0.000, 68.000], mean observation: 3.160 [-1.485, 10.248], loss: 1.538919, mae: 5.036973, mean_q: 5.214447
 53233/100000: episode: 5434, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.944, mean reward: 0.394 [0.366, 0.471], mean action: 39.800 [0.000, 83.000], mean observation: 3.155 [-1.779, 10.329], loss: 1.177542, mae: 5.035000, mean_q: 5.211304
 53243/100000: episode: 5435, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.261, mean reward: 0.426 [0.328, 0.481], mean action: 55.100 [5.000, 93.000], mean observation: 3.159 [-1.642, 10.266], loss: 1.103318, mae: 5.034751, mean_q: 5.212596
 53253/100000: episode: 5436, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.829, mean reward: 0.383 [0.266, 0.505], mean action: 64.900 [9.000, 83.000], mean observation: 3.172 [-1.684, 10.252], loss: 0.868608, mae: 5.034050, mean_q: 5.214398
 53263/100000: episode: 5437, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.194, mean reward: 0.419 [0.394, 0.468], mean action: 67.300 [13.000, 83.000], mean observation: 3.151 [-1.261, 10.267], loss: 1.224975, mae: 5.035779, mean_q: 5.212619
 53273/100000: episode: 5438, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.057, mean reward: 0.406 [0.367, 0.553], mean action: 53.800 [0.000, 88.000], mean observation: 3.163 [-1.503, 10.314], loss: 1.163646, mae: 5.035512, mean_q: 5.205749
 53276/100000: episode: 5439, duration: 0.057s, episode steps: 3, steps per second: 53, episode reward: 10.946, mean reward: 3.649 [0.453, 10.000], mean action: 60.667 [36.000, 88.000], mean observation: 3.162 [-1.139, 10.199], loss: 1.344541, mae: 5.036256, mean_q: 5.205915
 53286/100000: episode: 5440, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 4.700, mean reward: 0.470 [0.370, 0.547], mean action: 2.000 [0.000, 20.000], mean observation: 3.162 [-1.185, 10.399], loss: 1.514470, mae: 5.037067, mean_q: 5.207160
 53296/100000: episode: 5441, duration: 0.239s, episode steps: 10, steps per second: 42, episode reward: 4.217, mean reward: 0.422 [0.350, 0.468], mean action: 0.200 [0.000, 2.000], mean observation: 3.160 [-1.527, 10.409], loss: 1.212948, mae: 5.035785, mean_q: 5.208001
 53306/100000: episode: 5442, duration: 0.222s, episode steps: 10, steps per second: 45, episode reward: 4.082, mean reward: 0.408 [0.317, 0.511], mean action: 14.600 [0.000, 83.000], mean observation: 3.156 [-1.008, 10.218], loss: 1.360517, mae: 5.036107, mean_q: 5.206191
 53316/100000: episode: 5443, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.490, mean reward: 0.349 [0.299, 0.357], mean action: 69.400 [16.000, 83.000], mean observation: 3.142 [-1.724, 10.304], loss: 1.297976, mae: 5.035792, mean_q: 5.206315
 53326/100000: episode: 5444, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.273, mean reward: 0.427 [0.321, 0.511], mean action: 73.100 [11.000, 99.000], mean observation: 3.162 [-1.254, 10.305], loss: 1.131731, mae: 5.035077, mean_q: 5.207857
 53336/100000: episode: 5445, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.929, mean reward: 0.393 [0.391, 0.411], mean action: 75.000 [5.000, 94.000], mean observation: 3.140 [-0.882, 10.222], loss: 1.540881, mae: 5.036603, mean_q: 5.208900
 53346/100000: episode: 5446, duration: 0.131s, episode steps: 10, steps per second: 77, episode reward: 3.803, mean reward: 0.380 [0.331, 0.450], mean action: 67.900 [34.000, 83.000], mean observation: 3.154 [-0.984, 10.223], loss: 1.193967, mae: 5.035226, mean_q: 5.209970
 53353/100000: episode: 5447, duration: 0.115s, episode steps: 7, steps per second: 61, episode reward: 12.131, mean reward: 1.733 [0.321, 10.000], mean action: 56.714 [22.000, 92.000], mean observation: 3.134 [-2.733, 10.264], loss: 0.964629, mae: 5.034218, mean_q: 5.210821
 53363/100000: episode: 5448, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.470, mean reward: 0.447 [0.432, 0.515], mean action: 69.700 [11.000, 97.000], mean observation: 3.168 [-0.802, 10.407], loss: 1.382260, mae: 5.036054, mean_q: 5.211576
 53373/100000: episode: 5449, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.836, mean reward: 0.484 [0.428, 0.494], mean action: 73.600 [17.000, 93.000], mean observation: 3.151 [-1.302, 10.380], loss: 1.464450, mae: 5.036259, mean_q: 5.212806
 53383/100000: episode: 5450, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 3.985, mean reward: 0.399 [0.388, 0.430], mean action: 60.500 [18.000, 83.000], mean observation: 3.157 [-1.814, 10.317], loss: 1.237408, mae: 5.035202, mean_q: 5.214946
 53393/100000: episode: 5451, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.070, mean reward: 0.407 [0.365, 0.445], mean action: 59.100 [8.000, 83.000], mean observation: 3.161 [-1.597, 10.421], loss: 1.101895, mae: 5.034715, mean_q: 5.217558
 53403/100000: episode: 5452, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.125, mean reward: 0.413 [0.355, 0.537], mean action: 72.600 [32.000, 98.000], mean observation: 3.153 [-1.356, 10.294], loss: 1.203869, mae: 5.035478, mean_q: 5.219096
 53413/100000: episode: 5453, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 3.616, mean reward: 0.362 [0.301, 0.459], mean action: 77.700 [11.000, 98.000], mean observation: 3.156 [-1.509, 10.222], loss: 1.188992, mae: 5.035372, mean_q: 5.219881
 53423/100000: episode: 5454, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.165, mean reward: 0.416 [0.319, 0.532], mean action: 53.700 [0.000, 100.000], mean observation: 3.162 [-1.599, 10.310], loss: 1.169172, mae: 5.035314, mean_q: 5.221046
 53433/100000: episode: 5455, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.380, mean reward: 0.438 [0.412, 0.489], mean action: 57.800 [3.000, 83.000], mean observation: 3.155 [-1.311, 10.380], loss: 1.100108, mae: 5.035130, mean_q: 5.223866
 53443/100000: episode: 5456, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.728, mean reward: 0.373 [0.310, 0.438], mean action: 56.100 [9.000, 83.000], mean observation: 3.155 [-1.110, 10.351], loss: 1.112375, mae: 5.035301, mean_q: 5.226771
 53453/100000: episode: 5457, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 4.199, mean reward: 0.420 [0.380, 0.490], mean action: 69.300 [22.000, 83.000], mean observation: 3.144 [-1.012, 10.274], loss: 1.214064, mae: 5.035910, mean_q: 5.229572
 53463/100000: episode: 5458, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.872, mean reward: 0.387 [0.323, 0.475], mean action: 62.200 [29.000, 83.000], mean observation: 3.164 [-1.128, 10.372], loss: 1.331559, mae: 5.036607, mean_q: 5.232497
 53473/100000: episode: 5459, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.076, mean reward: 0.408 [0.358, 0.477], mean action: 50.800 [1.000, 93.000], mean observation: 3.152 [-2.467, 10.253], loss: 1.232489, mae: 5.036088, mean_q: 5.234040
 53474/100000: episode: 5460, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 37.000 [37.000, 37.000], mean observation: 3.148 [-0.529, 10.295], loss: 0.819313, mae: 5.034741, mean_q: 5.231392
 53484/100000: episode: 5461, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.052, mean reward: 0.405 [0.377, 0.466], mean action: 70.700 [17.000, 92.000], mean observation: 3.158 [-1.785, 10.341], loss: 1.178865, mae: 5.036130, mean_q: 5.230630
 53494/100000: episode: 5462, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.231, mean reward: 0.423 [0.359, 0.545], mean action: 71.200 [26.000, 83.000], mean observation: 3.136 [-1.652, 10.354], loss: 1.515199, mae: 5.037055, mean_q: 5.231755
 53504/100000: episode: 5463, duration: 0.114s, episode steps: 10, steps per second: 87, episode reward: 3.900, mean reward: 0.390 [0.370, 0.489], mean action: 81.500 [63.000, 88.000], mean observation: 3.153 [-1.163, 10.301], loss: 1.362709, mae: 5.036211, mean_q: 5.230905
 53514/100000: episode: 5464, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.786, mean reward: 0.379 [0.356, 0.445], mean action: 54.900 [9.000, 94.000], mean observation: 3.161 [-1.498, 10.361], loss: 1.357771, mae: 5.036157, mean_q: 5.226714
 53524/100000: episode: 5465, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.942, mean reward: 0.394 [0.368, 0.438], mean action: 43.500 [5.000, 86.000], mean observation: 3.149 [-1.133, 10.362], loss: 1.469422, mae: 5.036637, mean_q: 5.226177
 53534/100000: episode: 5466, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.023, mean reward: 0.402 [0.356, 0.424], mean action: 40.600 [11.000, 51.000], mean observation: 3.155 [-1.780, 10.280], loss: 1.511399, mae: 5.036398, mean_q: 5.225916
 53544/100000: episode: 5467, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.151, mean reward: 0.415 [0.323, 0.503], mean action: 49.800 [7.000, 83.000], mean observation: 3.157 [-1.543, 10.300], loss: 1.238071, mae: 5.034798, mean_q: 5.223122
 53554/100000: episode: 5468, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.021, mean reward: 0.402 [0.349, 0.441], mean action: 57.100 [0.000, 99.000], mean observation: 3.146 [-1.443, 10.258], loss: 1.051832, mae: 5.033846, mean_q: 5.225980
 53564/100000: episode: 5469, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.889, mean reward: 0.489 [0.489, 0.489], mean action: 83.100 [69.000, 98.000], mean observation: 3.143 [-1.159, 10.354], loss: 1.004367, mae: 5.033794, mean_q: 5.228959
 53574/100000: episode: 5470, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.044, mean reward: 0.404 [0.348, 0.491], mean action: 63.300 [19.000, 83.000], mean observation: 3.146 [-1.543, 10.449], loss: 1.586091, mae: 5.036138, mean_q: 5.231158
 53584/100000: episode: 5471, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.020, mean reward: 0.402 [0.337, 0.498], mean action: 71.200 [10.000, 100.000], mean observation: 3.166 [-1.292, 10.240], loss: 1.296565, mae: 5.034932, mean_q: 5.233377
 53594/100000: episode: 5472, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.868, mean reward: 0.387 [0.354, 0.544], mean action: 71.400 [17.000, 83.000], mean observation: 3.152 [-1.089, 10.379], loss: 1.208975, mae: 5.034435, mean_q: 5.232017
 53604/100000: episode: 5473, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.046, mean reward: 0.405 [0.345, 0.599], mean action: 51.000 [1.000, 85.000], mean observation: 3.150 [-1.561, 10.305], loss: 1.165994, mae: 5.033973, mean_q: 5.230916
 53614/100000: episode: 5474, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.173, mean reward: 0.417 [0.405, 0.499], mean action: 74.800 [40.000, 89.000], mean observation: 3.140 [-1.192, 10.333], loss: 0.908847, mae: 5.033138, mean_q: 5.228983
 53624/100000: episode: 5475, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.769, mean reward: 0.377 [0.328, 0.465], mean action: 67.400 [33.000, 83.000], mean observation: 3.154 [-1.426, 10.374], loss: 1.136338, mae: 5.034074, mean_q: 5.227301
 53634/100000: episode: 5476, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.250, mean reward: 0.425 [0.363, 0.526], mean action: 61.900 [0.000, 94.000], mean observation: 3.152 [-1.370, 10.306], loss: 1.078487, mae: 5.034005, mean_q: 5.226811
 53644/100000: episode: 5477, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.377, mean reward: 0.438 [0.390, 0.503], mean action: 62.700 [1.000, 83.000], mean observation: 3.150 [-1.449, 10.252], loss: 1.231387, mae: 5.034663, mean_q: 5.224998
 53654/100000: episode: 5478, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.159, mean reward: 0.416 [0.369, 0.552], mean action: 48.700 [25.000, 83.000], mean observation: 3.171 [-1.915, 10.614], loss: 1.374250, mae: 5.035100, mean_q: 5.226443
 53664/100000: episode: 5479, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.369, mean reward: 0.437 [0.395, 0.551], mean action: 43.300 [13.000, 70.000], mean observation: 3.152 [-1.459, 10.384], loss: 1.205834, mae: 5.034330, mean_q: 5.228856
 53674/100000: episode: 5480, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.075, mean reward: 0.407 [0.329, 0.483], mean action: 44.600 [10.000, 89.000], mean observation: 3.157 [-1.086, 10.309], loss: 1.167384, mae: 5.034123, mean_q: 5.227169
 53679/100000: episode: 5481, duration: 0.083s, episode steps: 5, steps per second: 60, episode reward: 11.192, mean reward: 2.238 [0.280, 10.000], mean action: 45.200 [0.000, 89.000], mean observation: 3.167 [-0.920, 10.135], loss: 1.289769, mae: 5.034448, mean_q: 5.228084
 53689/100000: episode: 5482, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.315, mean reward: 0.432 [0.309, 0.486], mean action: 37.700 [0.000, 81.000], mean observation: 3.159 [-1.277, 10.353], loss: 1.208570, mae: 5.034116, mean_q: 5.228209
 53699/100000: episode: 5483, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.357, mean reward: 0.436 [0.360, 0.510], mean action: 32.900 [0.000, 100.000], mean observation: 3.154 [-1.862, 10.305], loss: 1.141750, mae: 5.033743, mean_q: 5.230518
 53709/100000: episode: 5484, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.368, mean reward: 0.437 [0.376, 0.516], mean action: 39.900 [0.000, 99.000], mean observation: 3.157 [-1.546, 10.230], loss: 1.000984, mae: 5.033203, mean_q: 5.230376
 53719/100000: episode: 5485, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.940, mean reward: 0.394 [0.348, 0.448], mean action: 26.800 [0.000, 74.000], mean observation: 3.156 [-2.019, 10.305], loss: 1.087359, mae: 5.033607, mean_q: 5.228385
 53729/100000: episode: 5486, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.596, mean reward: 0.460 [0.382, 0.600], mean action: 35.500 [0.000, 92.000], mean observation: 3.160 [-1.473, 10.283], loss: 1.492765, mae: 5.035047, mean_q: 5.227918
 53739/100000: episode: 5487, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.912, mean reward: 0.391 [0.369, 0.422], mean action: 31.800 [0.000, 97.000], mean observation: 3.141 [-1.367, 10.241], loss: 1.137057, mae: 5.033410, mean_q: 5.225355
 53749/100000: episode: 5488, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.227, mean reward: 0.423 [0.346, 0.496], mean action: 38.000 [11.000, 77.000], mean observation: 3.149 [-1.727, 10.314], loss: 1.312244, mae: 5.034165, mean_q: 5.225013
 53759/100000: episode: 5489, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.423, mean reward: 0.342 [0.255, 0.436], mean action: 50.100 [33.000, 91.000], mean observation: 3.150 [-1.208, 10.363], loss: 1.291977, mae: 5.034039, mean_q: 5.221059
 53766/100000: episode: 5490, duration: 0.127s, episode steps: 7, steps per second: 55, episode reward: 12.453, mean reward: 1.779 [0.407, 10.000], mean action: 47.286 [36.000, 70.000], mean observation: 3.156 [-1.157, 10.218], loss: 1.091864, mae: 5.033208, mean_q: 5.219236
 53776/100000: episode: 5491, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.212, mean reward: 0.421 [0.412, 0.454], mean action: 56.500 [48.000, 86.000], mean observation: 3.164 [-1.234, 10.247], loss: 1.464736, mae: 5.034473, mean_q: 5.220533
 53786/100000: episode: 5492, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.783, mean reward: 0.378 [0.348, 0.466], mean action: 46.700 [6.000, 81.000], mean observation: 3.152 [-1.374, 10.289], loss: 1.486143, mae: 5.034180, mean_q: 5.220685
 53793/100000: episode: 5493, duration: 0.150s, episode steps: 7, steps per second: 47, episode reward: 13.261, mean reward: 1.894 [0.544, 10.000], mean action: 38.000 [0.000, 57.000], mean observation: 3.163 [-1.195, 10.336], loss: 0.649333, mae: 5.030681, mean_q: 5.217539
 53803/100000: episode: 5494, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.078, mean reward: 0.408 [0.312, 0.522], mean action: 5.100 [0.000, 32.000], mean observation: 3.158 [-1.839, 10.363], loss: 0.895534, mae: 5.032046, mean_q: 5.217925
 53813/100000: episode: 5495, duration: 0.221s, episode steps: 10, steps per second: 45, episode reward: 3.984, mean reward: 0.398 [0.329, 0.520], mean action: 11.700 [0.000, 50.000], mean observation: 3.152 [-1.633, 10.302], loss: 1.097131, mae: 5.033010, mean_q: 5.220028
 53823/100000: episode: 5496, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.978, mean reward: 0.398 [0.321, 0.461], mean action: 34.500 [0.000, 79.000], mean observation: 3.152 [-1.550, 10.368], loss: 1.174469, mae: 5.033343, mean_q: 5.221057
 53833/100000: episode: 5497, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.080, mean reward: 0.408 [0.343, 0.596], mean action: 25.700 [0.000, 78.000], mean observation: 3.158 [-2.892, 10.446], loss: 1.242606, mae: 5.033623, mean_q: 5.223068
 53843/100000: episode: 5498, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.873, mean reward: 0.387 [0.349, 0.432], mean action: 26.900 [0.000, 57.000], mean observation: 3.148 [-1.285, 10.373], loss: 1.143268, mae: 5.033270, mean_q: 5.227255
 53853/100000: episode: 5499, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.840, mean reward: 0.384 [0.324, 0.520], mean action: 31.700 [0.000, 95.000], mean observation: 3.152 [-1.207, 10.277], loss: 1.086847, mae: 5.032851, mean_q: 5.229811
 53863/100000: episode: 5500, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.335, mean reward: 0.434 [0.331, 0.480], mean action: 26.600 [0.000, 67.000], mean observation: 3.148 [-1.591, 10.253], loss: 1.237097, mae: 5.033580, mean_q: 5.230520
 53873/100000: episode: 5501, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.083, mean reward: 0.408 [0.352, 0.536], mean action: 26.000 [0.000, 78.000], mean observation: 3.161 [-1.686, 10.328], loss: 1.227525, mae: 5.033457, mean_q: 5.230363
 53883/100000: episode: 5502, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.193, mean reward: 0.419 [0.360, 0.466], mean action: 23.600 [0.000, 94.000], mean observation: 3.159 [-1.670, 10.314], loss: 1.338067, mae: 5.033730, mean_q: 5.232673
 53893/100000: episode: 5503, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.040, mean reward: 0.404 [0.329, 0.529], mean action: 31.800 [0.000, 91.000], mean observation: 3.156 [-1.168, 10.315], loss: 1.234033, mae: 5.033039, mean_q: 5.235269
 53903/100000: episode: 5504, duration: 0.241s, episode steps: 10, steps per second: 41, episode reward: 4.758, mean reward: 0.476 [0.337, 0.589], mean action: 13.100 [0.000, 40.000], mean observation: 3.155 [-1.328, 10.305], loss: 1.237546, mae: 5.033166, mean_q: 5.235709
 53913/100000: episode: 5505, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.118, mean reward: 0.412 [0.370, 0.438], mean action: 36.700 [0.000, 92.000], mean observation: 3.148 [-1.243, 10.325], loss: 1.280446, mae: 5.033513, mean_q: 5.238883
 53923/100000: episode: 5506, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 3.951, mean reward: 0.395 [0.307, 0.531], mean action: 16.300 [0.000, 54.000], mean observation: 3.161 [-1.142, 10.394], loss: 1.247188, mae: 5.033387, mean_q: 5.242016
 53933/100000: episode: 5507, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.183, mean reward: 0.418 [0.343, 0.502], mean action: 20.700 [0.000, 71.000], mean observation: 3.158 [-1.739, 10.460], loss: 1.240638, mae: 5.033399, mean_q: 5.243766
 53943/100000: episode: 5508, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.342, mean reward: 0.434 [0.330, 0.532], mean action: 15.500 [0.000, 90.000], mean observation: 3.154 [-1.375, 10.385], loss: 1.172390, mae: 5.033123, mean_q: 5.240853
 53953/100000: episode: 5509, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.688, mean reward: 0.369 [0.320, 0.440], mean action: 30.600 [0.000, 93.000], mean observation: 3.161 [-1.696, 10.361], loss: 1.032561, mae: 5.032565, mean_q: 5.239795
 53963/100000: episode: 5510, duration: 0.235s, episode steps: 10, steps per second: 43, episode reward: 4.341, mean reward: 0.434 [0.350, 0.583], mean action: 27.200 [0.000, 93.000], mean observation: 3.148 [-2.252, 10.279], loss: 1.448591, mae: 5.034286, mean_q: 5.238273
 53973/100000: episode: 5511, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.813, mean reward: 0.381 [0.276, 0.563], mean action: 31.400 [0.000, 101.000], mean observation: 3.156 [-1.658, 10.375], loss: 1.043884, mae: 5.032443, mean_q: 5.237822
 53983/100000: episode: 5512, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.423, mean reward: 0.442 [0.348, 0.596], mean action: 19.200 [0.000, 97.000], mean observation: 3.159 [-1.549, 10.337], loss: 1.007358, mae: 5.032596, mean_q: 5.239593
 53993/100000: episode: 5513, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.163, mean reward: 0.416 [0.319, 0.545], mean action: 25.100 [0.000, 81.000], mean observation: 3.151 [-2.239, 10.387], loss: 1.462988, mae: 5.034375, mean_q: 5.240545
 54003/100000: episode: 5514, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.823, mean reward: 0.382 [0.327, 0.460], mean action: 24.600 [0.000, 67.000], mean observation: 3.160 [-1.497, 10.355], loss: 1.332269, mae: 5.033978, mean_q: 5.238767
 54013/100000: episode: 5515, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.099, mean reward: 0.410 [0.296, 0.514], mean action: 17.700 [0.000, 58.000], mean observation: 3.148 [-1.982, 10.364], loss: 1.464933, mae: 5.034285, mean_q: 5.236957
 54023/100000: episode: 5516, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.273, mean reward: 0.427 [0.359, 0.527], mean action: 37.000 [0.000, 99.000], mean observation: 3.154 [-1.966, 10.263], loss: 1.112490, mae: 5.033000, mean_q: 5.233553
 54033/100000: episode: 5517, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.970, mean reward: 0.397 [0.318, 0.554], mean action: 24.700 [0.000, 75.000], mean observation: 3.152 [-1.793, 10.405], loss: 1.312482, mae: 5.033929, mean_q: 5.231926
 54043/100000: episode: 5518, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.166, mean reward: 0.417 [0.348, 0.483], mean action: 18.900 [0.000, 99.000], mean observation: 3.158 [-1.437, 10.281], loss: 1.429974, mae: 5.034194, mean_q: 5.231090
 54053/100000: episode: 5519, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.320, mean reward: 0.432 [0.350, 0.504], mean action: 16.500 [0.000, 48.000], mean observation: 3.152 [-1.487, 10.284], loss: 1.365087, mae: 5.033980, mean_q: 5.228572
 54063/100000: episode: 5520, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.274, mean reward: 0.427 [0.302, 0.486], mean action: 26.800 [0.000, 101.000], mean observation: 3.149 [-1.249, 10.302], loss: 1.323851, mae: 5.033490, mean_q: 5.227746
 54073/100000: episode: 5521, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.221, mean reward: 0.422 [0.404, 0.500], mean action: 49.500 [0.000, 97.000], mean observation: 3.159 [-1.929, 10.309], loss: 1.096157, mae: 5.032476, mean_q: 5.228808
 54083/100000: episode: 5522, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.921, mean reward: 0.392 [0.310, 0.516], mean action: 53.900 [44.000, 92.000], mean observation: 3.160 [-0.800, 10.509], loss: 1.357810, mae: 5.033586, mean_q: 5.234920
 54093/100000: episode: 5523, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.604, mean reward: 0.360 [0.298, 0.413], mean action: 48.500 [14.000, 87.000], mean observation: 3.157 [-1.393, 10.289], loss: 1.028692, mae: 5.032576, mean_q: 5.238831
 54103/100000: episode: 5524, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.780, mean reward: 0.378 [0.351, 0.477], mean action: 57.500 [33.000, 97.000], mean observation: 3.167 [-1.247, 10.402], loss: 1.172220, mae: 5.033032, mean_q: 5.240636
 54113/100000: episode: 5525, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.131, mean reward: 0.413 [0.400, 0.497], mean action: 55.900 [13.000, 90.000], mean observation: 3.159 [-0.995, 10.277], loss: 1.168065, mae: 5.033083, mean_q: 5.242024
 54123/100000: episode: 5526, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.068, mean reward: 0.407 [0.335, 0.540], mean action: 46.100 [11.000, 61.000], mean observation: 3.162 [-1.271, 10.345], loss: 1.246712, mae: 5.033388, mean_q: 5.243543
 54133/100000: episode: 5527, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.102, mean reward: 0.410 [0.328, 0.475], mean action: 39.300 [14.000, 70.000], mean observation: 3.145 [-1.819, 10.449], loss: 1.246814, mae: 5.033295, mean_q: 5.244464
 54143/100000: episode: 5528, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.685, mean reward: 0.368 [0.332, 0.412], mean action: 48.500 [22.000, 79.000], mean observation: 3.166 [-1.205, 10.308], loss: 1.505858, mae: 5.034310, mean_q: 5.244797
 54153/100000: episode: 5529, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.865, mean reward: 0.387 [0.336, 0.466], mean action: 45.800 [28.000, 62.000], mean observation: 3.162 [-1.530, 10.395], loss: 1.402279, mae: 5.033944, mean_q: 5.239820
 54163/100000: episode: 5530, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.542, mean reward: 0.354 [0.310, 0.397], mean action: 49.800 [21.000, 87.000], mean observation: 3.177 [-1.667, 10.300], loss: 1.346717, mae: 5.033408, mean_q: 5.236522
 54173/100000: episode: 5531, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.942, mean reward: 0.394 [0.387, 0.442], mean action: 58.900 [48.000, 94.000], mean observation: 3.167 [-1.595, 10.434], loss: 0.937949, mae: 5.031704, mean_q: 5.232615
 54183/100000: episode: 5532, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.058, mean reward: 0.406 [0.386, 0.466], mean action: 48.600 [18.000, 84.000], mean observation: 3.151 [-1.413, 10.267], loss: 1.209645, mae: 5.032776, mean_q: 5.224555
 54193/100000: episode: 5533, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.158, mean reward: 0.416 [0.318, 0.478], mean action: 49.500 [14.000, 99.000], mean observation: 3.161 [-1.557, 10.299], loss: 1.163840, mae: 5.032790, mean_q: 5.219292
 54203/100000: episode: 5534, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.382, mean reward: 0.438 [0.393, 0.475], mean action: 45.000 [5.000, 95.000], mean observation: 3.154 [-1.467, 10.272], loss: 1.725043, mae: 5.034600, mean_q: 5.218319
 54213/100000: episode: 5535, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 3.946, mean reward: 0.395 [0.290, 0.556], mean action: 47.000 [6.000, 97.000], mean observation: 3.163 [-1.409, 10.442], loss: 1.356323, mae: 5.032823, mean_q: 5.218715
 54223/100000: episode: 5536, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.916, mean reward: 0.392 [0.361, 0.471], mean action: 48.100 [5.000, 89.000], mean observation: 3.142 [-1.493, 10.420], loss: 1.107965, mae: 5.031686, mean_q: 5.219815
 54233/100000: episode: 5537, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.570, mean reward: 0.457 [0.395, 0.523], mean action: 47.900 [3.000, 100.000], mean observation: 3.175 [-1.682, 10.295], loss: 0.980284, mae: 5.031425, mean_q: 5.220387
 54243/100000: episode: 5538, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.400, mean reward: 0.440 [0.400, 0.469], mean action: 49.800 [20.000, 90.000], mean observation: 3.155 [-1.598, 10.199], loss: 0.918920, mae: 5.031453, mean_q: 5.218156
 54253/100000: episode: 5539, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.524, mean reward: 0.452 [0.379, 0.540], mean action: 48.600 [27.000, 80.000], mean observation: 3.174 [-2.085, 10.424], loss: 1.115814, mae: 5.032393, mean_q: 5.218150
 54263/100000: episode: 5540, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 3.527, mean reward: 0.353 [0.317, 0.445], mean action: 64.800 [38.000, 98.000], mean observation: 3.156 [-1.292, 10.327], loss: 1.183832, mae: 5.032743, mean_q: 5.218533
 54273/100000: episode: 5541, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.017, mean reward: 0.402 [0.346, 0.530], mean action: 63.700 [11.000, 101.000], mean observation: 3.155 [-1.160, 10.255], loss: 1.410291, mae: 5.033720, mean_q: 5.218820
 54283/100000: episode: 5542, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.142, mean reward: 0.414 [0.405, 0.452], mean action: 55.400 [30.000, 95.000], mean observation: 3.148 [-1.486, 10.199], loss: 1.448133, mae: 5.033905, mean_q: 5.219812
 54293/100000: episode: 5543, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.969, mean reward: 0.397 [0.380, 0.552], mean action: 57.000 [26.000, 100.000], mean observation: 3.168 [-0.914, 10.510], loss: 1.146286, mae: 5.032737, mean_q: 5.221031
 54302/100000: episode: 5544, duration: 0.152s, episode steps: 9, steps per second: 59, episode reward: 13.657, mean reward: 1.517 [0.378, 10.000], mean action: 45.111 [4.000, 83.000], mean observation: 3.150 [-1.601, 10.448], loss: 1.430666, mae: 5.033601, mean_q: 5.222994
 54312/100000: episode: 5545, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.439, mean reward: 0.344 [0.304, 0.462], mean action: 43.400 [24.000, 48.000], mean observation: 3.144 [-1.536, 10.233], loss: 1.108446, mae: 5.032279, mean_q: 5.225566
 54322/100000: episode: 5546, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.137, mean reward: 0.414 [0.377, 0.529], mean action: 53.400 [7.000, 97.000], mean observation: 3.167 [-0.744, 10.380], loss: 1.752359, mae: 5.034657, mean_q: 5.227931
 54332/100000: episode: 5547, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.475, mean reward: 0.447 [0.386, 0.533], mean action: 50.200 [5.000, 100.000], mean observation: 3.173 [-1.351, 10.604], loss: 1.173761, mae: 5.032369, mean_q: 5.230247
 54342/100000: episode: 5548, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.620, mean reward: 0.362 [0.293, 0.402], mean action: 47.700 [3.000, 97.000], mean observation: 3.161 [-1.958, 10.323], loss: 1.061848, mae: 5.031866, mean_q: 5.232234
 54352/100000: episode: 5549, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.951, mean reward: 0.495 [0.477, 0.500], mean action: 45.700 [2.000, 90.000], mean observation: 3.151 [-1.751, 10.244], loss: 1.234378, mae: 5.032489, mean_q: 5.234370
 54362/100000: episode: 5550, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.121, mean reward: 0.412 [0.404, 0.462], mean action: 57.800 [29.000, 100.000], mean observation: 3.152 [-1.157, 10.311], loss: 1.481144, mae: 5.033319, mean_q: 5.237071
 54372/100000: episode: 5551, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.086, mean reward: 0.409 [0.395, 0.499], mean action: 48.700 [36.000, 64.000], mean observation: 3.164 [-1.153, 10.289], loss: 1.310987, mae: 5.032273, mean_q: 5.235635
 54382/100000: episode: 5552, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.053, mean reward: 0.405 [0.361, 0.440], mean action: 50.900 [32.000, 96.000], mean observation: 3.152 [-1.077, 10.331], loss: 0.932615, mae: 5.030919, mean_q: 5.235002
 54392/100000: episode: 5553, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.471, mean reward: 0.447 [0.336, 0.462], mean action: 58.000 [5.000, 87.000], mean observation: 3.178 [-1.393, 10.289], loss: 1.558841, mae: 5.033475, mean_q: 5.234196
 54402/100000: episode: 5554, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.153, mean reward: 0.415 [0.400, 0.438], mean action: 53.900 [3.000, 99.000], mean observation: 3.161 [-0.806, 10.455], loss: 1.119091, mae: 5.031651, mean_q: 5.230480
 54412/100000: episode: 5555, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.814, mean reward: 0.381 [0.335, 0.441], mean action: 54.800 [5.000, 100.000], mean observation: 3.151 [-1.304, 10.265], loss: 1.325089, mae: 5.032614, mean_q: 5.229593
 54422/100000: episode: 5556, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.953, mean reward: 0.395 [0.283, 0.464], mean action: 55.000 [15.000, 101.000], mean observation: 3.146 [-1.388, 10.323], loss: 1.290832, mae: 5.032567, mean_q: 5.230182
 54432/100000: episode: 5557, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.108, mean reward: 0.411 [0.389, 0.519], mean action: 58.100 [30.000, 97.000], mean observation: 3.153 [-1.214, 10.247], loss: 0.964525, mae: 5.031395, mean_q: 5.227076
 54442/100000: episode: 5558, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.114, mean reward: 0.411 [0.363, 0.516], mean action: 48.100 [6.000, 100.000], mean observation: 3.151 [-1.181, 10.356], loss: 1.211938, mae: 5.032858, mean_q: 5.226003
 54452/100000: episode: 5559, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.996, mean reward: 0.400 [0.394, 0.423], mean action: 64.700 [48.000, 97.000], mean observation: 3.153 [-1.025, 10.407], loss: 1.365050, mae: 5.033494, mean_q: 5.226956
 54453/100000: episode: 5560, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 48.000 [48.000, 48.000], mean observation: 3.144 [-1.327, 10.100], loss: 1.113569, mae: 5.032921, mean_q: 5.227607
 54463/100000: episode: 5561, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.707, mean reward: 0.471 [0.382, 0.494], mean action: 60.400 [22.000, 100.000], mean observation: 3.165 [-1.187, 10.171], loss: 0.773329, mae: 5.031321, mean_q: 5.228599
 54473/100000: episode: 5562, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.037, mean reward: 0.404 [0.389, 0.507], mean action: 48.200 [7.000, 95.000], mean observation: 3.151 [-0.999, 10.271], loss: 1.171227, mae: 5.033088, mean_q: 5.230545
 54483/100000: episode: 5563, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.784, mean reward: 0.378 [0.338, 0.511], mean action: 42.100 [9.000, 49.000], mean observation: 3.142 [-1.449, 10.328], loss: 1.052060, mae: 5.032623, mean_q: 5.232425
 54493/100000: episode: 5564, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.544, mean reward: 0.454 [0.384, 0.483], mean action: 45.100 [0.000, 84.000], mean observation: 3.156 [-2.425, 10.240], loss: 1.007361, mae: 5.032725, mean_q: 5.233946
 54503/100000: episode: 5565, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.983, mean reward: 0.398 [0.375, 0.461], mean action: 55.700 [37.000, 93.000], mean observation: 3.158 [-1.490, 10.372], loss: 1.037983, mae: 5.033241, mean_q: 5.235652
 54513/100000: episode: 5566, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.353, mean reward: 0.435 [0.378, 0.494], mean action: 41.100 [4.000, 92.000], mean observation: 3.149 [-1.534, 10.305], loss: 1.032896, mae: 5.033444, mean_q: 5.237314
 54523/100000: episode: 5567, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.028, mean reward: 0.403 [0.374, 0.474], mean action: 41.700 [0.000, 72.000], mean observation: 3.173 [-1.483, 10.451], loss: 0.879361, mae: 5.033086, mean_q: 5.239619
 54533/100000: episode: 5568, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.931, mean reward: 0.393 [0.345, 0.501], mean action: 47.900 [0.000, 79.000], mean observation: 3.151 [-1.168, 10.394], loss: 1.285113, mae: 5.034957, mean_q: 5.241870
 54543/100000: episode: 5569, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.002, mean reward: 0.400 [0.335, 0.452], mean action: 48.000 [10.000, 76.000], mean observation: 3.160 [-1.984, 10.258], loss: 1.050578, mae: 5.034242, mean_q: 5.241252
 54553/100000: episode: 5570, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.236, mean reward: 0.424 [0.371, 0.512], mean action: 52.600 [18.000, 98.000], mean observation: 3.155 [-1.300, 10.396], loss: 1.379326, mae: 5.035843, mean_q: 5.239151
 54563/100000: episode: 5571, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.048, mean reward: 0.405 [0.376, 0.520], mean action: 62.300 [10.000, 99.000], mean observation: 3.153 [-0.997, 10.304], loss: 0.991371, mae: 5.034255, mean_q: 5.236351
 54573/100000: episode: 5572, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.779, mean reward: 0.378 [0.292, 0.440], mean action: 52.400 [18.000, 99.000], mean observation: 3.156 [-1.209, 10.406], loss: 1.057103, mae: 5.034614, mean_q: 5.234267
 54583/100000: episode: 5573, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.205, mean reward: 0.421 [0.385, 0.466], mean action: 54.000 [8.000, 101.000], mean observation: 3.139 [-1.523, 10.306], loss: 1.547774, mae: 5.036458, mean_q: 5.234750
 54593/100000: episode: 5574, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.244, mean reward: 0.424 [0.369, 0.444], mean action: 44.800 [4.000, 74.000], mean observation: 3.167 [-1.611, 10.346], loss: 0.911959, mae: 5.033976, mean_q: 5.230273
 54603/100000: episode: 5575, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.768, mean reward: 0.377 [0.352, 0.528], mean action: 44.900 [14.000, 80.000], mean observation: 3.155 [-1.863, 10.217], loss: 1.274645, mae: 5.035404, mean_q: 5.224951
 54613/100000: episode: 5576, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.154, mean reward: 0.415 [0.356, 0.547], mean action: 56.300 [48.000, 85.000], mean observation: 3.144 [-1.379, 10.279], loss: 1.077004, mae: 5.034696, mean_q: 5.219701
 54623/100000: episode: 5577, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.369, mean reward: 0.437 [0.379, 0.489], mean action: 50.900 [12.000, 76.000], mean observation: 3.159 [-1.339, 10.292], loss: 1.141954, mae: 5.035239, mean_q: 5.220366
 54633/100000: episode: 5578, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.265, mean reward: 0.427 [0.368, 0.497], mean action: 61.500 [5.000, 101.000], mean observation: 3.164 [-1.567, 10.362], loss: 1.335620, mae: 5.036060, mean_q: 5.221797
 54643/100000: episode: 5579, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.840, mean reward: 0.384 [0.340, 0.475], mean action: 44.400 [7.000, 67.000], mean observation: 3.149 [-1.177, 10.299], loss: 1.338693, mae: 5.035771, mean_q: 5.222773
 54653/100000: episode: 5580, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.419, mean reward: 0.442 [0.365, 0.509], mean action: 53.800 [33.000, 89.000], mean observation: 3.153 [-1.011, 10.591], loss: 1.240245, mae: 5.035101, mean_q: 5.224074
 54663/100000: episode: 5581, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.982, mean reward: 0.398 [0.357, 0.514], mean action: 35.500 [0.000, 89.000], mean observation: 3.152 [-1.848, 10.399], loss: 1.440640, mae: 5.035957, mean_q: 5.226234
 54673/100000: episode: 5582, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.077, mean reward: 0.408 [0.358, 0.454], mean action: 37.800 [2.000, 74.000], mean observation: 3.141 [-0.979, 10.322], loss: 1.174728, mae: 5.034688, mean_q: 5.227012
 54683/100000: episode: 5583, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.435, mean reward: 0.344 [0.268, 0.493], mean action: 47.900 [0.000, 83.000], mean observation: 3.138 [-0.805, 10.213], loss: 1.475208, mae: 5.035762, mean_q: 5.220030
 54693/100000: episode: 5584, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.294, mean reward: 0.429 [0.337, 0.489], mean action: 53.300 [26.000, 91.000], mean observation: 3.172 [-1.396, 10.291], loss: 1.172682, mae: 5.034477, mean_q: 5.216242
 54703/100000: episode: 5585, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.169, mean reward: 0.417 [0.391, 0.454], mean action: 47.900 [7.000, 82.000], mean observation: 3.155 [-0.984, 10.277], loss: 1.138293, mae: 5.034451, mean_q: 5.214417
 54713/100000: episode: 5586, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.020, mean reward: 0.402 [0.367, 0.493], mean action: 49.000 [12.000, 78.000], mean observation: 3.156 [-2.197, 10.475], loss: 1.183241, mae: 5.034496, mean_q: 5.214572
 54723/100000: episode: 5587, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.924, mean reward: 0.392 [0.341, 0.475], mean action: 55.100 [48.000, 93.000], mean observation: 3.154 [-1.745, 10.193], loss: 1.103608, mae: 5.034400, mean_q: 5.215726
 54733/100000: episode: 5588, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.865, mean reward: 0.387 [0.371, 0.418], mean action: 54.500 [2.000, 98.000], mean observation: 3.147 [-1.181, 10.226], loss: 1.282836, mae: 5.035066, mean_q: 5.217053
 54743/100000: episode: 5589, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.226, mean reward: 0.423 [0.378, 0.447], mean action: 33.200 [4.000, 50.000], mean observation: 3.149 [-1.467, 10.405], loss: 1.398921, mae: 5.035562, mean_q: 5.218507
 54753/100000: episode: 5590, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.640, mean reward: 0.464 [0.464, 0.464], mean action: 53.200 [22.000, 86.000], mean observation: 3.171 [-1.100, 10.261], loss: 1.196139, mae: 5.034500, mean_q: 5.215798
 54763/100000: episode: 5591, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.625, mean reward: 0.462 [0.455, 0.530], mean action: 40.100 [6.000, 91.000], mean observation: 3.164 [-1.188, 10.247], loss: 1.046354, mae: 5.034054, mean_q: 5.208401
 54773/100000: episode: 5592, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.256, mean reward: 0.426 [0.399, 0.491], mean action: 57.400 [24.000, 101.000], mean observation: 3.144 [-1.599, 10.395], loss: 1.319190, mae: 5.034926, mean_q: 5.204022
 54783/100000: episode: 5593, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.400, mean reward: 0.440 [0.356, 0.545], mean action: 58.300 [31.000, 99.000], mean observation: 3.163 [-1.420, 10.295], loss: 1.184460, mae: 5.034314, mean_q: 5.201921
 54793/100000: episode: 5594, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.287, mean reward: 0.429 [0.314, 0.457], mean action: 54.100 [31.000, 100.000], mean observation: 3.157 [-0.998, 10.311], loss: 1.192621, mae: 5.034143, mean_q: 5.203093
 54803/100000: episode: 5595, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.317, mean reward: 0.432 [0.424, 0.502], mean action: 57.200 [48.000, 86.000], mean observation: 3.134 [-1.181, 10.277], loss: 1.691840, mae: 5.035876, mean_q: 5.206145
 54813/100000: episode: 5596, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.291, mean reward: 0.429 [0.388, 0.551], mean action: 54.100 [48.000, 78.000], mean observation: 3.151 [-1.576, 10.311], loss: 1.078109, mae: 5.033277, mean_q: 5.210440
 54822/100000: episode: 5597, duration: 0.124s, episode steps: 9, steps per second: 72, episode reward: 13.384, mean reward: 1.487 [0.393, 10.000], mean action: 56.556 [48.000, 75.000], mean observation: 3.153 [-1.295, 10.297], loss: 1.587490, mae: 5.035295, mean_q: 5.210017
 54832/100000: episode: 5598, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.040, mean reward: 0.404 [0.390, 0.438], mean action: 55.400 [19.000, 94.000], mean observation: 3.156 [-2.144, 10.317], loss: 1.125032, mae: 5.033357, mean_q: 5.206659
 54842/100000: episode: 5599, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.921, mean reward: 0.392 [0.317, 0.430], mean action: 74.200 [19.000, 94.000], mean observation: 3.164 [-0.997, 10.354], loss: 1.055075, mae: 5.033077, mean_q: 5.208517
 54852/100000: episode: 5600, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 3.663, mean reward: 0.366 [0.319, 0.411], mean action: 76.600 [3.000, 94.000], mean observation: 3.149 [-0.875, 10.347], loss: 1.363814, mae: 5.034150, mean_q: 5.210065
 54862/100000: episode: 5601, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.753, mean reward: 0.375 [0.345, 0.422], mean action: 68.300 [2.000, 94.000], mean observation: 3.143 [-1.533, 10.369], loss: 1.378761, mae: 5.033942, mean_q: 5.211513
 54872/100000: episode: 5602, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.116, mean reward: 0.412 [0.385, 0.457], mean action: 62.600 [6.000, 94.000], mean observation: 3.157 [-1.275, 10.328], loss: 1.058345, mae: 5.032765, mean_q: 5.212050
 54882/100000: episode: 5603, duration: 0.087s, episode steps: 10, steps per second: 114, episode reward: 3.442, mean reward: 0.344 [0.336, 0.393], mean action: 87.900 [26.000, 101.000], mean observation: 3.175 [-1.584, 10.210], loss: 1.306501, mae: 5.033885, mean_q: 5.213006
 54892/100000: episode: 5604, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 4.531, mean reward: 0.453 [0.347, 0.492], mean action: 87.000 [33.000, 94.000], mean observation: 3.158 [-1.030, 10.372], loss: 1.296813, mae: 5.033739, mean_q: 5.214439
 54902/100000: episode: 5605, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.439, mean reward: 0.444 [0.381, 0.517], mean action: 67.500 [7.000, 94.000], mean observation: 3.164 [-1.355, 10.263], loss: 1.288749, mae: 5.033597, mean_q: 5.215771
 54912/100000: episode: 5606, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.985, mean reward: 0.398 [0.342, 0.437], mean action: 62.000 [2.000, 94.000], mean observation: 3.146 [-1.269, 10.364], loss: 1.074791, mae: 5.032685, mean_q: 5.216838
 54922/100000: episode: 5607, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.071, mean reward: 0.407 [0.320, 0.539], mean action: 29.200 [0.000, 94.000], mean observation: 3.149 [-1.478, 10.273], loss: 0.986283, mae: 5.032533, mean_q: 5.213130
 54932/100000: episode: 5608, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.703, mean reward: 0.370 [0.329, 0.420], mean action: 54.900 [0.000, 94.000], mean observation: 3.150 [-1.893, 10.414], loss: 0.989181, mae: 5.032632, mean_q: 5.212491
 54942/100000: episode: 5609, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.861, mean reward: 0.386 [0.345, 0.476], mean action: 71.400 [33.000, 94.000], mean observation: 3.173 [-1.400, 10.279], loss: 1.342109, mae: 5.033799, mean_q: 5.212115
 54952/100000: episode: 5610, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.960, mean reward: 0.396 [0.356, 0.466], mean action: 46.900 [0.000, 94.000], mean observation: 3.157 [-1.323, 10.388], loss: 1.420352, mae: 5.034139, mean_q: 5.208250
 54955/100000: episode: 5611, duration: 0.072s, episode steps: 3, steps per second: 42, episode reward: 10.735, mean reward: 3.578 [0.358, 10.000], mean action: 0.000 [0.000, 0.000], mean observation: 3.160 [-1.325, 10.454], loss: 1.478697, mae: 5.033979, mean_q: 5.208442
 54965/100000: episode: 5612, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.917, mean reward: 0.392 [0.339, 0.446], mean action: 12.400 [0.000, 52.000], mean observation: 3.152 [-1.566, 10.334], loss: 1.062772, mae: 5.032527, mean_q: 5.208581
 54975/100000: episode: 5613, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.031, mean reward: 0.403 [0.338, 0.464], mean action: 52.700 [0.000, 94.000], mean observation: 3.162 [-0.810, 10.540], loss: 1.342007, mae: 5.033885, mean_q: 5.206941
 54985/100000: episode: 5614, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.058, mean reward: 0.406 [0.355, 0.526], mean action: 71.800 [24.000, 94.000], mean observation: 3.156 [-2.272, 10.329], loss: 1.439329, mae: 5.034363, mean_q: 5.208109
 54995/100000: episode: 5615, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.937, mean reward: 0.394 [0.333, 0.518], mean action: 59.200 [0.000, 94.000], mean observation: 3.153 [-1.389, 10.385], loss: 1.275810, mae: 5.033471, mean_q: 5.207415
 55005/100000: episode: 5616, duration: 0.220s, episode steps: 10, steps per second: 45, episode reward: 3.979, mean reward: 0.398 [0.311, 0.508], mean action: 15.100 [0.000, 82.000], mean observation: 3.157 [-1.905, 10.491], loss: 0.958313, mae: 5.032218, mean_q: 5.206141
 55015/100000: episode: 5617, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.390, mean reward: 0.439 [0.343, 0.498], mean action: 31.400 [0.000, 97.000], mean observation: 3.162 [-1.349, 10.393], loss: 1.027259, mae: 5.032670, mean_q: 5.206188
 55025/100000: episode: 5618, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.029, mean reward: 0.403 [0.336, 0.520], mean action: 22.000 [0.000, 58.000], mean observation: 3.156 [-0.995, 10.305], loss: 1.003641, mae: 5.032788, mean_q: 5.207554
 55035/100000: episode: 5619, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 3.839, mean reward: 0.384 [0.376, 0.425], mean action: 44.200 [8.000, 76.000], mean observation: 3.155 [-1.179, 10.335], loss: 1.157919, mae: 5.033614, mean_q: 5.208230
 55045/100000: episode: 5620, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.046, mean reward: 0.405 [0.319, 0.552], mean action: 48.800 [12.000, 78.000], mean observation: 3.156 [-0.974, 10.269], loss: 1.284116, mae: 5.034242, mean_q: 5.210417
 55055/100000: episode: 5621, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.888, mean reward: 0.389 [0.301, 0.526], mean action: 54.800 [42.000, 94.000], mean observation: 3.158 [-1.988, 10.270], loss: 1.130177, mae: 5.033590, mean_q: 5.210050
 55065/100000: episode: 5622, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.104, mean reward: 0.410 [0.340, 0.561], mean action: 31.300 [3.000, 88.000], mean observation: 3.158 [-2.197, 10.259], loss: 1.259753, mae: 5.033948, mean_q: 5.204086
 55075/100000: episode: 5623, duration: 0.220s, episode steps: 10, steps per second: 46, episode reward: 4.351, mean reward: 0.435 [0.355, 0.526], mean action: 9.900 [0.000, 55.000], mean observation: 3.158 [-1.213, 10.503], loss: 1.141309, mae: 5.033293, mean_q: 5.202037
 55085/100000: episode: 5624, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 3.753, mean reward: 0.375 [0.349, 0.427], mean action: 66.200 [37.000, 94.000], mean observation: 3.152 [-1.265, 10.333], loss: 1.226958, mae: 5.033878, mean_q: 5.202498
 55095/100000: episode: 5625, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.780, mean reward: 0.378 [0.345, 0.417], mean action: 59.300 [5.000, 94.000], mean observation: 3.165 [-1.288, 10.331], loss: 1.369242, mae: 5.034099, mean_q: 5.204009
 55105/100000: episode: 5626, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 3.767, mean reward: 0.377 [0.376, 0.379], mean action: 91.300 [60.000, 101.000], mean observation: 3.152 [-1.365, 10.285], loss: 1.209985, mae: 5.033390, mean_q: 5.205059
 55115/100000: episode: 5627, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.718, mean reward: 0.372 [0.324, 0.414], mean action: 69.200 [7.000, 94.000], mean observation: 3.160 [-0.984, 10.260], loss: 0.989753, mae: 5.032367, mean_q: 5.206149
 55125/100000: episode: 5628, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 4.204, mean reward: 0.420 [0.380, 0.512], mean action: 82.900 [12.000, 100.000], mean observation: 3.158 [-1.322, 10.340], loss: 1.292338, mae: 5.033667, mean_q: 5.207608
 55135/100000: episode: 5629, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.228, mean reward: 0.423 [0.289, 0.482], mean action: 59.400 [7.000, 94.000], mean observation: 3.174 [-1.544, 10.309], loss: 1.158759, mae: 5.033144, mean_q: 5.209306
 55145/100000: episode: 5630, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 4.275, mean reward: 0.427 [0.345, 0.546], mean action: 76.000 [11.000, 99.000], mean observation: 3.153 [-1.323, 10.219], loss: 1.175636, mae: 5.032938, mean_q: 5.207996
 55155/100000: episode: 5631, duration: 0.120s, episode steps: 10, steps per second: 84, episode reward: 3.647, mean reward: 0.365 [0.325, 0.549], mean action: 81.700 [42.000, 94.000], mean observation: 3.146 [-0.938, 10.260], loss: 1.327847, mae: 5.033779, mean_q: 5.205338
 55165/100000: episode: 5632, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.186, mean reward: 0.419 [0.311, 0.500], mean action: 63.000 [3.000, 87.000], mean observation: 3.150 [-0.981, 10.285], loss: 1.074197, mae: 5.032561, mean_q: 5.205940
 55175/100000: episode: 5633, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.265, mean reward: 0.426 [0.298, 0.522], mean action: 68.100 [11.000, 98.000], mean observation: 3.154 [-1.048, 10.363], loss: 1.198440, mae: 5.033101, mean_q: 5.205998
 55185/100000: episode: 5634, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.093, mean reward: 0.409 [0.372, 0.458], mean action: 67.200 [21.000, 94.000], mean observation: 3.150 [-1.075, 10.213], loss: 1.256518, mae: 5.033419, mean_q: 5.206552
 55195/100000: episode: 5635, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.118, mean reward: 0.412 [0.369, 0.443], mean action: 66.000 [26.000, 94.000], mean observation: 3.148 [-1.817, 10.379], loss: 1.284292, mae: 5.033305, mean_q: 5.208421
 55205/100000: episode: 5636, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.884, mean reward: 0.388 [0.378, 0.480], mean action: 84.200 [34.000, 98.000], mean observation: 3.162 [-0.949, 10.350], loss: 1.114338, mae: 5.032259, mean_q: 5.210840
 55215/100000: episode: 5637, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 4.200, mean reward: 0.420 [0.416, 0.423], mean action: 78.700 [21.000, 94.000], mean observation: 3.149 [-1.062, 10.387], loss: 1.315036, mae: 5.032800, mean_q: 5.211274
 55225/100000: episode: 5638, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.161, mean reward: 0.416 [0.414, 0.425], mean action: 52.000 [4.000, 100.000], mean observation: 3.156 [-1.419, 10.339], loss: 1.421660, mae: 5.032753, mean_q: 5.210458
 55235/100000: episode: 5639, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.278, mean reward: 0.428 [0.349, 0.491], mean action: 46.300 [13.000, 91.000], mean observation: 3.161 [-1.532, 10.316], loss: 1.386600, mae: 5.032334, mean_q: 5.208683
 55245/100000: episode: 5640, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.004, mean reward: 0.400 [0.337, 0.472], mean action: 49.200 [17.000, 94.000], mean observation: 3.155 [-1.012, 10.386], loss: 1.141536, mae: 5.031076, mean_q: 5.207501
 55255/100000: episode: 5641, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.629, mean reward: 0.463 [0.398, 0.546], mean action: 70.700 [2.000, 94.000], mean observation: 3.164 [-2.191, 10.334], loss: 0.927277, mae: 5.030331, mean_q: 5.208817
 55265/100000: episode: 5642, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.855, mean reward: 0.386 [0.353, 0.496], mean action: 78.500 [8.000, 98.000], mean observation: 3.164 [-1.246, 10.350], loss: 1.445080, mae: 5.032360, mean_q: 5.210578
 55275/100000: episode: 5643, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.434, mean reward: 0.443 [0.359, 0.497], mean action: 73.800 [24.000, 94.000], mean observation: 3.143 [-1.562, 10.294], loss: 1.064692, mae: 5.030591, mean_q: 5.213155
 55285/100000: episode: 5644, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.355, mean reward: 0.436 [0.324, 0.530], mean action: 65.500 [5.000, 94.000], mean observation: 3.169 [-0.787, 10.365], loss: 1.156146, mae: 5.030707, mean_q: 5.214904
 55295/100000: episode: 5645, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 3.855, mean reward: 0.385 [0.344, 0.465], mean action: 67.800 [7.000, 95.000], mean observation: 3.175 [-1.337, 10.439], loss: 1.339954, mae: 5.031550, mean_q: 5.216216
 55305/100000: episode: 5646, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 3.978, mean reward: 0.398 [0.365, 0.509], mean action: 75.400 [22.000, 94.000], mean observation: 3.145 [-1.173, 10.451], loss: 1.292621, mae: 5.031110, mean_q: 5.218474
 55315/100000: episode: 5647, duration: 0.113s, episode steps: 10, steps per second: 88, episode reward: 3.654, mean reward: 0.365 [0.315, 0.386], mean action: 80.000 [20.000, 100.000], mean observation: 3.164 [-1.220, 10.415], loss: 1.321444, mae: 5.031004, mean_q: 5.222861
 55325/100000: episode: 5648, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.188, mean reward: 0.419 [0.373, 0.493], mean action: 62.900 [6.000, 95.000], mean observation: 3.162 [-1.019, 10.245], loss: 1.492817, mae: 5.031356, mean_q: 5.224767
 55335/100000: episode: 5649, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 3.807, mean reward: 0.381 [0.273, 0.445], mean action: 73.600 [18.000, 94.000], mean observation: 3.147 [-1.480, 10.276], loss: 1.135527, mae: 5.029840, mean_q: 5.222997
 55345/100000: episode: 5650, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.155, mean reward: 0.415 [0.399, 0.497], mean action: 69.000 [27.000, 94.000], mean observation: 3.164 [-1.371, 10.281], loss: 1.529685, mae: 5.031275, mean_q: 5.223301
 55355/100000: episode: 5651, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 3.974, mean reward: 0.397 [0.393, 0.413], mean action: 80.300 [39.000, 94.000], mean observation: 3.147 [-0.988, 10.373], loss: 1.544502, mae: 5.030931, mean_q: 5.224198
 55365/100000: episode: 5652, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.093, mean reward: 0.409 [0.350, 0.460], mean action: 61.500 [14.000, 101.000], mean observation: 3.152 [-0.933, 10.271], loss: 1.154220, mae: 5.028736, mean_q: 5.223938
 55375/100000: episode: 5653, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.852, mean reward: 0.485 [0.485, 0.485], mean action: 69.300 [29.000, 94.000], mean observation: 3.149 [-0.761, 10.290], loss: 1.472902, mae: 5.029888, mean_q: 5.220645
 55385/100000: episode: 5654, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 3.322, mean reward: 0.332 [0.316, 0.402], mean action: 93.800 [84.000, 101.000], mean observation: 3.160 [-0.607, 10.434], loss: 1.171570, mae: 5.028359, mean_q: 5.220129
 55395/100000: episode: 5655, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.924, mean reward: 0.492 [0.490, 0.518], mean action: 71.000 [30.000, 94.000], mean observation: 3.148 [-0.847, 10.284], loss: 1.062524, mae: 5.028052, mean_q: 5.220902
 55405/100000: episode: 5656, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.211, mean reward: 0.421 [0.421, 0.421], mean action: 78.400 [28.000, 100.000], mean observation: 3.153 [-1.413, 10.287], loss: 1.380548, mae: 5.029464, mean_q: 5.222359
 55415/100000: episode: 5657, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.981, mean reward: 0.398 [0.300, 0.479], mean action: 76.600 [2.000, 94.000], mean observation: 3.144 [-1.519, 10.248], loss: 1.573022, mae: 5.030077, mean_q: 5.220135
 55425/100000: episode: 5658, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.296, mean reward: 0.430 [0.390, 0.538], mean action: 59.400 [10.000, 94.000], mean observation: 3.147 [-1.202, 10.312], loss: 1.036494, mae: 5.027701, mean_q: 5.219593
 55435/100000: episode: 5659, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.469, mean reward: 0.447 [0.343, 0.508], mean action: 67.300 [15.000, 98.000], mean observation: 3.166 [-1.173, 10.420], loss: 1.225885, mae: 5.028452, mean_q: 5.220282
 55445/100000: episode: 5660, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 3.684, mean reward: 0.368 [0.340, 0.405], mean action: 75.800 [20.000, 94.000], mean observation: 3.159 [-1.247, 10.262], loss: 1.074074, mae: 5.027741, mean_q: 5.222372
 55455/100000: episode: 5661, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 5.399, mean reward: 0.540 [0.540, 0.540], mean action: 71.800 [6.000, 94.000], mean observation: 3.149 [-0.775, 10.565], loss: 1.115543, mae: 5.027676, mean_q: 5.224301
 55465/100000: episode: 5662, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.013, mean reward: 0.401 [0.390, 0.428], mean action: 84.900 [3.000, 94.000], mean observation: 3.145 [-1.155, 10.333], loss: 1.161486, mae: 5.027825, mean_q: 5.225332
 55475/100000: episode: 5663, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.610, mean reward: 0.461 [0.413, 0.563], mean action: 67.000 [2.000, 94.000], mean observation: 3.143 [-1.618, 10.299], loss: 1.105820, mae: 5.027796, mean_q: 5.220986
 55485/100000: episode: 5664, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 3.524, mean reward: 0.352 [0.351, 0.360], mean action: 82.300 [4.000, 94.000], mean observation: 3.127 [-1.605, 10.253], loss: 1.369192, mae: 5.028778, mean_q: 5.219604
 55495/100000: episode: 5665, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.087, mean reward: 0.409 [0.405, 0.430], mean action: 75.000 [6.000, 94.000], mean observation: 3.165 [-0.631, 10.354], loss: 0.979954, mae: 5.027297, mean_q: 5.221220
 55505/100000: episode: 5666, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.384, mean reward: 0.438 [0.339, 0.488], mean action: 70.800 [14.000, 94.000], mean observation: 3.148 [-1.471, 10.249], loss: 1.022862, mae: 5.027600, mean_q: 5.223652
 55515/100000: episode: 5667, duration: 0.118s, episode steps: 10, steps per second: 84, episode reward: 4.112, mean reward: 0.411 [0.372, 0.530], mean action: 73.200 [16.000, 94.000], mean observation: 3.153 [-1.363, 10.263], loss: 1.567751, mae: 5.029868, mean_q: 5.223352
 55525/100000: episode: 5668, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 4.238, mean reward: 0.424 [0.356, 0.511], mean action: 84.200 [35.000, 94.000], mean observation: 3.165 [-0.918, 10.389], loss: 1.235787, mae: 5.028345, mean_q: 5.216141
 55535/100000: episode: 5669, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 3.943, mean reward: 0.394 [0.347, 0.514], mean action: 76.100 [16.000, 94.000], mean observation: 3.155 [-1.423, 10.356], loss: 1.437764, mae: 5.028911, mean_q: 5.211848
 55545/100000: episode: 5670, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.358, mean reward: 0.436 [0.350, 0.479], mean action: 75.200 [15.000, 97.000], mean observation: 3.145 [-1.018, 10.355], loss: 1.410969, mae: 5.028871, mean_q: 5.208064
 55555/100000: episode: 5671, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.372, mean reward: 0.437 [0.432, 0.450], mean action: 75.200 [10.000, 100.000], mean observation: 3.153 [-1.428, 10.446], loss: 0.863817, mae: 5.026641, mean_q: 5.207453
 55565/100000: episode: 5672, duration: 0.097s, episode steps: 10, steps per second: 104, episode reward: 3.574, mean reward: 0.357 [0.319, 0.540], mean action: 83.900 [32.000, 94.000], mean observation: 3.147 [-1.605, 10.225], loss: 1.170508, mae: 5.027892, mean_q: 5.209779
 55575/100000: episode: 5673, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.959, mean reward: 0.396 [0.364, 0.439], mean action: 58.900 [0.000, 99.000], mean observation: 3.151 [-1.492, 10.234], loss: 0.976758, mae: 5.027404, mean_q: 5.212085
 55585/100000: episode: 5674, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.066, mean reward: 0.407 [0.399, 0.449], mean action: 71.000 [32.000, 95.000], mean observation: 3.173 [-1.516, 10.253], loss: 1.577223, mae: 5.029990, mean_q: 5.213631
 55595/100000: episode: 5675, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.806, mean reward: 0.381 [0.314, 0.452], mean action: 27.900 [4.000, 100.000], mean observation: 3.161 [-1.767, 10.318], loss: 1.151147, mae: 5.028193, mean_q: 5.215297
 55605/100000: episode: 5676, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.550, mean reward: 0.455 [0.362, 0.521], mean action: 34.500 [4.000, 95.000], mean observation: 3.155 [-1.529, 10.388], loss: 1.373692, mae: 5.028749, mean_q: 5.217897
 55615/100000: episode: 5677, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.215, mean reward: 0.422 [0.352, 0.585], mean action: 29.300 [4.000, 73.000], mean observation: 3.160 [-1.326, 10.361], loss: 1.365319, mae: 5.028613, mean_q: 5.216904
 55625/100000: episode: 5678, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.053, mean reward: 0.405 [0.349, 0.471], mean action: 19.900 [4.000, 87.000], mean observation: 3.149 [-2.130, 10.446], loss: 1.408158, mae: 5.028648, mean_q: 5.215766
 55635/100000: episode: 5679, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.224, mean reward: 0.422 [0.370, 0.525], mean action: 31.200 [4.000, 95.000], mean observation: 3.161 [-0.921, 10.364], loss: 1.281175, mae: 5.027757, mean_q: 5.217296
 55645/100000: episode: 5680, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.001, mean reward: 0.400 [0.335, 0.490], mean action: 13.500 [4.000, 67.000], mean observation: 3.154 [-1.897, 10.308], loss: 1.636505, mae: 5.029059, mean_q: 5.216763
 55655/100000: episode: 5681, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 4.405, mean reward: 0.440 [0.380, 0.522], mean action: 21.000 [4.000, 78.000], mean observation: 3.154 [-1.787, 10.239], loss: 1.276379, mae: 5.027017, mean_q: 5.212651
 55665/100000: episode: 5682, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.963, mean reward: 0.396 [0.357, 0.481], mean action: 45.000 [4.000, 91.000], mean observation: 3.161 [-1.714, 10.283], loss: 1.135275, mae: 5.026090, mean_q: 5.209570
 55675/100000: episode: 5683, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.929, mean reward: 0.393 [0.305, 0.495], mean action: 39.100 [19.000, 86.000], mean observation: 3.155 [-0.903, 10.324], loss: 1.383451, mae: 5.027054, mean_q: 5.208372
 55685/100000: episode: 5684, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.985, mean reward: 0.399 [0.295, 0.494], mean action: 48.500 [20.000, 93.000], mean observation: 3.158 [-1.158, 10.317], loss: 1.131165, mae: 5.025821, mean_q: 5.209465
 55695/100000: episode: 5685, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.275, mean reward: 0.428 [0.360, 0.511], mean action: 40.300 [17.000, 66.000], mean observation: 3.151 [-1.507, 10.295], loss: 1.193320, mae: 5.025860, mean_q: 5.210175
 55705/100000: episode: 5686, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 5.878, mean reward: 0.588 [0.588, 0.588], mean action: 50.200 [38.000, 99.000], mean observation: 3.162 [-1.553, 10.366], loss: 1.089152, mae: 5.025420, mean_q: 5.212188
 55715/100000: episode: 5687, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.748, mean reward: 0.375 [0.352, 0.479], mean action: 55.600 [8.000, 91.000], mean observation: 3.152 [-0.971, 10.401], loss: 1.326099, mae: 5.026351, mean_q: 5.212387
 55725/100000: episode: 5688, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.972, mean reward: 0.397 [0.295, 0.520], mean action: 45.500 [27.000, 73.000], mean observation: 3.153 [-1.215, 10.232], loss: 1.063390, mae: 5.025193, mean_q: 5.210683
 55735/100000: episode: 5689, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.466, mean reward: 0.447 [0.348, 0.561], mean action: 44.400 [11.000, 100.000], mean observation: 3.146 [-1.264, 10.422], loss: 1.290756, mae: 5.026167, mean_q: 5.209812
 55745/100000: episode: 5690, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.327, mean reward: 0.433 [0.392, 0.583], mean action: 45.800 [20.000, 100.000], mean observation: 3.166 [-1.550, 10.237], loss: 1.700199, mae: 5.027319, mean_q: 5.206959
 55746/100000: episode: 5691, duration: 0.026s, episode steps: 1, steps per second: 39, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 38.000 [38.000, 38.000], mean observation: 3.165 [-0.419, 10.100], loss: 1.762238, mae: 5.027854, mean_q: 5.206401
 55756/100000: episode: 5692, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.219, mean reward: 0.422 [0.393, 0.489], mean action: 65.800 [21.000, 91.000], mean observation: 3.142 [-1.517, 10.392], loss: 1.133166, mae: 5.024911, mean_q: 5.204238
 55766/100000: episode: 5693, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.749, mean reward: 0.375 [0.323, 0.457], mean action: 63.900 [13.000, 84.000], mean observation: 3.169 [-1.577, 10.363], loss: 1.310258, mae: 5.025392, mean_q: 5.204460
 55776/100000: episode: 5694, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.130, mean reward: 0.413 [0.390, 0.449], mean action: 56.800 [2.000, 84.000], mean observation: 3.157 [-1.243, 10.423], loss: 1.373730, mae: 5.025460, mean_q: 5.207011
 55786/100000: episode: 5695, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 3.943, mean reward: 0.394 [0.371, 0.473], mean action: 74.700 [33.000, 91.000], mean observation: 3.155 [-2.191, 10.254], loss: 1.533548, mae: 5.026053, mean_q: 5.208450
 55796/100000: episode: 5696, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.856, mean reward: 0.386 [0.340, 0.518], mean action: 70.600 [30.000, 95.000], mean observation: 3.159 [-1.378, 10.305], loss: 1.345887, mae: 5.025061, mean_q: 5.209779
 55806/100000: episode: 5697, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 5.370, mean reward: 0.537 [0.537, 0.537], mean action: 56.000 [17.000, 90.000], mean observation: 3.164 [-1.942, 10.284], loss: 1.099877, mae: 5.024313, mean_q: 5.208596
 55816/100000: episode: 5698, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.525, mean reward: 0.452 [0.420, 0.483], mean action: 51.400 [8.000, 95.000], mean observation: 3.174 [-1.504, 10.364], loss: 1.380519, mae: 5.025623, mean_q: 5.210425
 55826/100000: episode: 5699, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.325, mean reward: 0.433 [0.351, 0.524], mean action: 35.000 [0.000, 54.000], mean observation: 3.158 [-1.240, 10.374], loss: 0.953841, mae: 5.024049, mean_q: 5.209750
 55836/100000: episode: 5700, duration: 0.150s, episode steps: 10, steps per second: 66, episode reward: 3.949, mean reward: 0.395 [0.314, 0.486], mean action: 50.800 [5.000, 83.000], mean observation: 3.167 [-1.555, 10.308], loss: 1.031458, mae: 5.024089, mean_q: 5.210067
 55846/100000: episode: 5701, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.050, mean reward: 0.405 [0.386, 0.497], mean action: 57.800 [41.000, 98.000], mean observation: 3.155 [-1.162, 10.303], loss: 1.505900, mae: 5.026274, mean_q: 5.212705
 55856/100000: episode: 5702, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.135, mean reward: 0.414 [0.367, 0.474], mean action: 45.500 [4.000, 83.000], mean observation: 3.155 [-1.579, 10.342], loss: 1.125954, mae: 5.024795, mean_q: 5.215851
 55866/100000: episode: 5703, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.784, mean reward: 0.378 [0.335, 0.425], mean action: 49.800 [0.000, 100.000], mean observation: 3.161 [-0.937, 10.285], loss: 1.107177, mae: 5.024765, mean_q: 5.218087
 55876/100000: episode: 5704, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.898, mean reward: 0.390 [0.321, 0.553], mean action: 51.200 [17.000, 95.000], mean observation: 3.142 [-1.563, 10.381], loss: 1.051481, mae: 5.024386, mean_q: 5.219982
 55886/100000: episode: 5705, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 5.239, mean reward: 0.524 [0.524, 0.524], mean action: 61.300 [48.000, 97.000], mean observation: 3.151 [-1.810, 10.315], loss: 1.106158, mae: 5.024787, mean_q: 5.221675
 55896/100000: episode: 5706, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.191, mean reward: 0.419 [0.385, 0.452], mean action: 48.300 [4.000, 100.000], mean observation: 3.153 [-1.577, 10.270], loss: 1.208052, mae: 5.025379, mean_q: 5.223615
 55906/100000: episode: 5707, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.119, mean reward: 0.412 [0.368, 0.521], mean action: 49.400 [2.000, 98.000], mean observation: 3.152 [-1.180, 10.350], loss: 1.170710, mae: 5.025151, mean_q: 5.225705
 55916/100000: episode: 5708, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.122, mean reward: 0.412 [0.410, 0.432], mean action: 55.800 [43.000, 84.000], mean observation: 3.158 [-1.174, 10.205], loss: 1.197233, mae: 5.025498, mean_q: 5.226981
 55926/100000: episode: 5709, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.331, mean reward: 0.433 [0.341, 0.467], mean action: 49.100 [29.000, 73.000], mean observation: 3.144 [-1.484, 10.251], loss: 0.904025, mae: 5.024418, mean_q: 5.225266
 55936/100000: episode: 5710, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.831, mean reward: 0.383 [0.375, 0.401], mean action: 46.900 [6.000, 79.000], mean observation: 3.154 [-1.650, 10.288], loss: 1.205777, mae: 5.025515, mean_q: 5.225866
 55946/100000: episode: 5711, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.538, mean reward: 0.454 [0.344, 0.472], mean action: 61.500 [14.000, 101.000], mean observation: 3.157 [-1.691, 10.288], loss: 1.169064, mae: 5.025606, mean_q: 5.227945
 55956/100000: episode: 5712, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.410, mean reward: 0.441 [0.344, 0.531], mean action: 43.500 [14.000, 76.000], mean observation: 3.154 [-1.059, 10.364], loss: 1.244974, mae: 5.025929, mean_q: 5.230104
 55966/100000: episode: 5713, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.644, mean reward: 0.364 [0.299, 0.446], mean action: 50.900 [21.000, 84.000], mean observation: 3.151 [-0.967, 10.291], loss: 1.296079, mae: 5.026189, mean_q: 5.232806
 55976/100000: episode: 5714, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.205, mean reward: 0.421 [0.391, 0.545], mean action: 46.100 [13.000, 98.000], mean observation: 3.150 [-1.217, 10.357], loss: 1.102757, mae: 5.025306, mean_q: 5.234992
 55986/100000: episode: 5715, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.001, mean reward: 0.400 [0.365, 0.460], mean action: 54.700 [27.000, 80.000], mean observation: 3.153 [-2.170, 10.234], loss: 1.035077, mae: 5.025102, mean_q: 5.236333
 55996/100000: episode: 5716, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.805, mean reward: 0.380 [0.375, 0.390], mean action: 54.800 [48.000, 98.000], mean observation: 3.157 [-1.174, 10.348], loss: 1.645716, mae: 5.027552, mean_q: 5.237520
 56006/100000: episode: 5717, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.983, mean reward: 0.398 [0.384, 0.439], mean action: 42.300 [14.000, 75.000], mean observation: 3.149 [-1.342, 10.269], loss: 0.862979, mae: 5.024174, mean_q: 5.238097
 56016/100000: episode: 5718, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.244, mean reward: 0.424 [0.368, 0.480], mean action: 58.700 [44.000, 96.000], mean observation: 3.163 [-1.143, 10.411], loss: 1.059648, mae: 5.025159, mean_q: 5.233527
 56026/100000: episode: 5719, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.868, mean reward: 0.387 [0.305, 0.460], mean action: 51.400 [24.000, 99.000], mean observation: 3.162 [-1.566, 10.365], loss: 1.325958, mae: 5.026253, mean_q: 5.226546
 56036/100000: episode: 5720, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.339, mean reward: 0.434 [0.370, 0.542], mean action: 38.300 [0.000, 65.000], mean observation: 3.151 [-1.992, 10.242], loss: 1.315987, mae: 5.026031, mean_q: 5.224629
 56046/100000: episode: 5721, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.201, mean reward: 0.420 [0.377, 0.552], mean action: 53.500 [8.000, 92.000], mean observation: 3.151 [-1.604, 10.285], loss: 1.410547, mae: 5.026516, mean_q: 5.225791
 56049/100000: episode: 5722, duration: 0.052s, episode steps: 3, steps per second: 57, episode reward: 10.826, mean reward: 3.609 [0.389, 10.000], mean action: 60.667 [48.000, 86.000], mean observation: 3.161 [-0.788, 10.243], loss: 1.139403, mae: 5.025340, mean_q: 5.227259
 56059/100000: episode: 5723, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.003, mean reward: 0.400 [0.299, 0.522], mean action: 44.000 [2.000, 75.000], mean observation: 3.170 [-1.342, 10.249], loss: 1.281921, mae: 5.025937, mean_q: 5.228736
 56069/100000: episode: 5724, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.201, mean reward: 0.420 [0.365, 0.441], mean action: 53.100 [5.000, 101.000], mean observation: 3.156 [-1.969, 10.367], loss: 1.121639, mae: 5.025225, mean_q: 5.228801
 56079/100000: episode: 5725, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.109, mean reward: 0.411 [0.313, 0.503], mean action: 44.000 [1.000, 79.000], mean observation: 3.167 [-1.583, 10.278], loss: 1.248744, mae: 5.025882, mean_q: 5.224460
 56089/100000: episode: 5726, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.623, mean reward: 0.362 [0.331, 0.422], mean action: 64.300 [44.000, 94.000], mean observation: 3.152 [-1.031, 10.389], loss: 0.898631, mae: 5.024703, mean_q: 5.218406
 56099/100000: episode: 5727, duration: 0.153s, episode steps: 10, steps per second: 66, episode reward: 3.788, mean reward: 0.379 [0.315, 0.468], mean action: 51.900 [13.000, 98.000], mean observation: 3.155 [-1.013, 10.301], loss: 1.353508, mae: 5.026475, mean_q: 5.216361
 56109/100000: episode: 5728, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.086, mean reward: 0.409 [0.395, 0.496], mean action: 56.000 [22.000, 99.000], mean observation: 3.170 [-1.395, 10.437], loss: 1.197039, mae: 5.026068, mean_q: 5.217853
 56119/100000: episode: 5729, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.871, mean reward: 0.387 [0.345, 0.472], mean action: 52.200 [48.000, 72.000], mean observation: 3.159 [-1.453, 10.245], loss: 1.233183, mae: 5.026013, mean_q: 5.219495
 56129/100000: episode: 5730, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.625, mean reward: 0.363 [0.317, 0.404], mean action: 44.700 [2.000, 61.000], mean observation: 3.153 [-1.058, 10.276], loss: 1.282626, mae: 5.026281, mean_q: 5.221600
 56139/100000: episode: 5731, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.124, mean reward: 0.412 [0.335, 0.522], mean action: 60.100 [33.000, 100.000], mean observation: 3.147 [-1.693, 10.328], loss: 1.238065, mae: 5.026161, mean_q: 5.224031
 56149/100000: episode: 5732, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.596, mean reward: 0.460 [0.458, 0.470], mean action: 52.700 [30.000, 83.000], mean observation: 3.152 [-1.754, 10.313], loss: 1.199606, mae: 5.025986, mean_q: 5.226358
 56159/100000: episode: 5733, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.952, mean reward: 0.395 [0.286, 0.474], mean action: 32.400 [11.000, 57.000], mean observation: 3.153 [-1.286, 10.283], loss: 1.245675, mae: 5.026372, mean_q: 5.228543
 56169/100000: episode: 5734, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.922, mean reward: 0.392 [0.341, 0.506], mean action: 48.900 [0.000, 91.000], mean observation: 3.158 [-1.795, 10.318], loss: 0.991409, mae: 5.025486, mean_q: 5.230348
 56179/100000: episode: 5735, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.239, mean reward: 0.424 [0.269, 0.533], mean action: 45.900 [25.000, 65.000], mean observation: 3.160 [-1.448, 10.290], loss: 1.420096, mae: 5.027335, mean_q: 5.231009
 56189/100000: episode: 5736, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.149, mean reward: 0.415 [0.392, 0.487], mean action: 57.700 [29.000, 95.000], mean observation: 3.160 [-0.955, 10.288], loss: 1.127597, mae: 5.026211, mean_q: 5.230589
 56199/100000: episode: 5737, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.184, mean reward: 0.418 [0.345, 0.510], mean action: 31.900 [12.000, 48.000], mean observation: 3.161 [-1.195, 10.269], loss: 1.152408, mae: 5.026540, mean_q: 5.228356
 56209/100000: episode: 5738, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.053, mean reward: 0.405 [0.379, 0.437], mean action: 47.100 [1.000, 87.000], mean observation: 3.152 [-1.151, 10.280], loss: 1.446012, mae: 5.027566, mean_q: 5.226094
 56219/100000: episode: 5739, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.877, mean reward: 0.388 [0.285, 0.502], mean action: 54.200 [0.000, 100.000], mean observation: 3.152 [-1.263, 10.273], loss: 1.485832, mae: 5.027418, mean_q: 5.226331
 56229/100000: episode: 5740, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.013, mean reward: 0.401 [0.331, 0.519], mean action: 39.700 [5.000, 70.000], mean observation: 3.157 [-1.136, 10.336], loss: 1.318459, mae: 5.026531, mean_q: 5.224886
 56239/100000: episode: 5741, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.974, mean reward: 0.397 [0.344, 0.473], mean action: 47.900 [12.000, 96.000], mean observation: 3.166 [-1.253, 10.372], loss: 1.683841, mae: 5.027598, mean_q: 5.223197
 56249/100000: episode: 5742, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.156, mean reward: 0.416 [0.413, 0.421], mean action: 54.700 [13.000, 80.000], mean observation: 3.158 [-0.976, 10.274], loss: 1.105355, mae: 5.024895, mean_q: 5.224092
 56259/100000: episode: 5743, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.204, mean reward: 0.420 [0.343, 0.470], mean action: 54.500 [37.000, 98.000], mean observation: 3.156 [-1.397, 10.343], loss: 0.947325, mae: 5.024348, mean_q: 5.226616
 56269/100000: episode: 5744, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.573, mean reward: 0.457 [0.457, 0.457], mean action: 51.000 [48.000, 78.000], mean observation: 3.166 [-1.717, 10.223], loss: 1.273568, mae: 5.025777, mean_q: 5.228621
 56279/100000: episode: 5745, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.729, mean reward: 0.373 [0.293, 0.435], mean action: 43.200 [12.000, 80.000], mean observation: 3.157 [-1.621, 10.394], loss: 1.290134, mae: 5.025903, mean_q: 5.229985
 56289/100000: episode: 5746, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.423, mean reward: 0.442 [0.386, 0.555], mean action: 39.900 [3.000, 48.000], mean observation: 3.154 [-1.871, 10.439], loss: 1.061477, mae: 5.024921, mean_q: 5.228752
 56299/100000: episode: 5747, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.322, mean reward: 0.432 [0.363, 0.481], mean action: 58.200 [13.000, 97.000], mean observation: 3.158 [-1.337, 10.333], loss: 1.301617, mae: 5.026087, mean_q: 5.230009
 56309/100000: episode: 5748, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.906, mean reward: 0.391 [0.352, 0.501], mean action: 40.400 [5.000, 89.000], mean observation: 3.154 [-1.411, 10.161], loss: 1.279927, mae: 5.025907, mean_q: 5.231812
 56319/100000: episode: 5749, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.190, mean reward: 0.419 [0.374, 0.551], mean action: 38.500 [3.000, 54.000], mean observation: 3.159 [-1.125, 10.395], loss: 1.290415, mae: 5.025970, mean_q: 5.230314
 56329/100000: episode: 5750, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.802, mean reward: 0.380 [0.333, 0.463], mean action: 43.400 [16.000, 77.000], mean observation: 3.154 [-1.718, 10.262], loss: 1.154751, mae: 5.025335, mean_q: 5.230512
 56339/100000: episode: 5751, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.098, mean reward: 0.410 [0.404, 0.441], mean action: 46.200 [13.000, 74.000], mean observation: 3.151 [-1.550, 10.305], loss: 1.537337, mae: 5.026566, mean_q: 5.228460
 56349/100000: episode: 5752, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.201, mean reward: 0.420 [0.339, 0.520], mean action: 37.300 [0.000, 61.000], mean observation: 3.161 [-1.413, 10.283], loss: 1.478406, mae: 5.026006, mean_q: 5.226388
 56359/100000: episode: 5753, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.462, mean reward: 0.446 [0.386, 0.564], mean action: 43.600 [3.000, 96.000], mean observation: 3.148 [-1.233, 10.278], loss: 0.908697, mae: 5.023442, mean_q: 5.227311
 56369/100000: episode: 5754, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.970, mean reward: 0.397 [0.338, 0.456], mean action: 47.600 [9.000, 95.000], mean observation: 3.159 [-1.165, 10.218], loss: 1.077116, mae: 5.024203, mean_q: 5.229231
 56379/100000: episode: 5755, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.552, mean reward: 0.355 [0.316, 0.409], mean action: 45.300 [34.000, 48.000], mean observation: 3.160 [-0.984, 10.271], loss: 1.282007, mae: 5.024951, mean_q: 5.231072
 56389/100000: episode: 5756, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.741, mean reward: 0.374 [0.357, 0.433], mean action: 50.300 [4.000, 90.000], mean observation: 3.165 [-1.783, 10.355], loss: 1.449803, mae: 5.025546, mean_q: 5.233658
 56399/100000: episode: 5757, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.285, mean reward: 0.429 [0.319, 0.562], mean action: 48.700 [33.000, 81.000], mean observation: 3.159 [-1.281, 10.336], loss: 0.962468, mae: 5.023686, mean_q: 5.240045
 56409/100000: episode: 5758, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.961, mean reward: 0.396 [0.387, 0.439], mean action: 53.100 [36.000, 83.000], mean observation: 3.164 [-1.168, 10.298], loss: 1.397923, mae: 5.025247, mean_q: 5.246115
 56419/100000: episode: 5759, duration: 0.150s, episode steps: 10, steps per second: 66, episode reward: 3.788, mean reward: 0.379 [0.303, 0.528], mean action: 56.800 [20.000, 88.000], mean observation: 3.155 [-1.250, 10.321], loss: 1.353282, mae: 5.024848, mean_q: 5.249238
 56429/100000: episode: 5760, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.877, mean reward: 0.388 [0.311, 0.497], mean action: 46.200 [3.000, 66.000], mean observation: 3.155 [-1.341, 10.364], loss: 1.135829, mae: 5.023828, mean_q: 5.248675
 56439/100000: episode: 5761, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.682, mean reward: 0.468 [0.359, 0.512], mean action: 43.700 [2.000, 95.000], mean observation: 3.161 [-1.439, 10.290], loss: 1.202053, mae: 5.024138, mean_q: 5.249211
 56449/100000: episode: 5762, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.107, mean reward: 0.411 [0.408, 0.440], mean action: 49.500 [34.000, 72.000], mean observation: 3.164 [-1.022, 10.325], loss: 1.419862, mae: 5.024926, mean_q: 5.246458
 56459/100000: episode: 5763, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.950, mean reward: 0.395 [0.319, 0.517], mean action: 54.600 [26.000, 95.000], mean observation: 3.155 [-1.080, 10.381], loss: 1.255051, mae: 5.024392, mean_q: 5.243786
 56469/100000: episode: 5764, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 5.455, mean reward: 0.546 [0.546, 0.546], mean action: 51.900 [37.000, 92.000], mean observation: 3.170 [-1.559, 10.296], loss: 1.043647, mae: 5.023617, mean_q: 5.240697
 56479/100000: episode: 5765, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.476, mean reward: 0.448 [0.387, 0.469], mean action: 41.500 [3.000, 89.000], mean observation: 3.163 [-1.307, 10.265], loss: 1.302734, mae: 5.024870, mean_q: 5.240212
 56489/100000: episode: 5766, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.114, mean reward: 0.411 [0.350, 0.433], mean action: 51.100 [17.000, 82.000], mean observation: 3.143 [-1.074, 10.305], loss: 1.473564, mae: 5.025203, mean_q: 5.241226
 56499/100000: episode: 5767, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.215, mean reward: 0.421 [0.370, 0.508], mean action: 58.000 [28.000, 99.000], mean observation: 3.148 [-1.452, 10.346], loss: 1.149864, mae: 5.023681, mean_q: 5.243425
 56509/100000: episode: 5768, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.900, mean reward: 0.390 [0.303, 0.468], mean action: 44.000 [7.000, 78.000], mean observation: 3.163 [-1.751, 10.271], loss: 1.436726, mae: 5.024678, mean_q: 5.246003
 56519/100000: episode: 5769, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.146, mean reward: 0.415 [0.348, 0.465], mean action: 41.900 [11.000, 71.000], mean observation: 3.157 [-1.304, 10.276], loss: 1.051199, mae: 5.023006, mean_q: 5.249987
 56529/100000: episode: 5770, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.805, mean reward: 0.381 [0.319, 0.461], mean action: 29.400 [0.000, 48.000], mean observation: 3.160 [-1.094, 10.416], loss: 1.220487, mae: 5.024214, mean_q: 5.253469
 56539/100000: episode: 5771, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.989, mean reward: 0.399 [0.322, 0.446], mean action: 44.800 [6.000, 99.000], mean observation: 3.148 [-1.436, 10.246], loss: 1.223416, mae: 5.024292, mean_q: 5.249773
 56549/100000: episode: 5772, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.792, mean reward: 0.379 [0.304, 0.512], mean action: 50.500 [12.000, 86.000], mean observation: 3.161 [-1.288, 10.282], loss: 1.307437, mae: 5.024678, mean_q: 5.246703
 56559/100000: episode: 5773, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.901, mean reward: 0.390 [0.372, 0.447], mean action: 54.300 [27.000, 84.000], mean observation: 3.158 [-1.438, 10.225], loss: 1.353161, mae: 5.024975, mean_q: 5.246078
 56569/100000: episode: 5774, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.214, mean reward: 0.421 [0.364, 0.516], mean action: 46.400 [6.000, 81.000], mean observation: 3.162 [-1.294, 10.306], loss: 1.192465, mae: 5.024265, mean_q: 5.242097
 56579/100000: episode: 5775, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.090, mean reward: 0.409 [0.365, 0.550], mean action: 50.600 [14.000, 95.000], mean observation: 3.164 [-2.089, 10.264], loss: 0.966787, mae: 5.023426, mean_q: 5.236649
 56589/100000: episode: 5776, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.053, mean reward: 0.405 [0.276, 0.555], mean action: 45.500 [3.000, 95.000], mean observation: 3.157 [-2.317, 10.286], loss: 1.176600, mae: 5.024543, mean_q: 5.236114
 56599/100000: episode: 5777, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.924, mean reward: 0.392 [0.363, 0.459], mean action: 48.900 [13.000, 92.000], mean observation: 3.159 [-0.896, 10.548], loss: 1.243245, mae: 5.024756, mean_q: 5.237407
 56609/100000: episode: 5778, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.946, mean reward: 0.395 [0.341, 0.500], mean action: 56.600 [17.000, 96.000], mean observation: 3.162 [-1.565, 10.234], loss: 1.392607, mae: 5.025390, mean_q: 5.235232
 56619/100000: episode: 5779, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.339, mean reward: 0.434 [0.336, 0.508], mean action: 34.700 [1.000, 74.000], mean observation: 3.151 [-1.980, 10.352], loss: 1.375810, mae: 5.025224, mean_q: 5.234689
 56629/100000: episode: 5780, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.373, mean reward: 0.437 [0.293, 0.496], mean action: 46.000 [3.000, 97.000], mean observation: 3.150 [-1.578, 10.367], loss: 1.120519, mae: 5.024269, mean_q: 5.234864
 56639/100000: episode: 5781, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.675, mean reward: 0.367 [0.307, 0.421], mean action: 54.200 [13.000, 99.000], mean observation: 3.160 [-1.330, 10.278], loss: 1.356790, mae: 5.025640, mean_q: 5.235089
 56649/100000: episode: 5782, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.673, mean reward: 0.367 [0.349, 0.416], mean action: 60.200 [2.000, 101.000], mean observation: 3.156 [-2.098, 10.409], loss: 1.319435, mae: 5.025624, mean_q: 5.233251
 56659/100000: episode: 5783, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.923, mean reward: 0.392 [0.338, 0.553], mean action: 56.900 [44.000, 99.000], mean observation: 3.145 [-1.810, 10.215], loss: 1.264992, mae: 5.025329, mean_q: 5.233510
 56669/100000: episode: 5784, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.347, mean reward: 0.435 [0.407, 0.513], mean action: 46.000 [22.000, 74.000], mean observation: 3.162 [-1.283, 10.350], loss: 1.513905, mae: 5.026451, mean_q: 5.237195
 56679/100000: episode: 5785, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.950, mean reward: 0.395 [0.312, 0.519], mean action: 54.400 [44.000, 89.000], mean observation: 3.152 [-1.997, 10.372], loss: 1.771998, mae: 5.027253, mean_q: 5.242577
 56689/100000: episode: 5786, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.304, mean reward: 0.430 [0.362, 0.513], mean action: 56.600 [42.000, 92.000], mean observation: 3.161 [-1.741, 10.292], loss: 1.084735, mae: 5.024504, mean_q: 5.246140
 56699/100000: episode: 5787, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.832, mean reward: 0.383 [0.374, 0.468], mean action: 56.100 [48.000, 99.000], mean observation: 3.152 [-0.924, 10.316], loss: 1.514914, mae: 5.025978, mean_q: 5.249884
 56709/100000: episode: 5788, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.421, mean reward: 0.442 [0.433, 0.503], mean action: 59.800 [22.000, 93.000], mean observation: 3.166 [-0.849, 10.342], loss: 0.936470, mae: 5.023726, mean_q: 5.245130
 56719/100000: episode: 5789, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.338, mean reward: 0.434 [0.410, 0.530], mean action: 56.800 [48.000, 96.000], mean observation: 3.165 [-1.394, 10.290], loss: 1.079322, mae: 5.024586, mean_q: 5.242179
 56729/100000: episode: 5790, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 3.811, mean reward: 0.381 [0.332, 0.478], mean action: 44.500 [13.000, 72.000], mean observation: 3.148 [-1.505, 10.280], loss: 1.508169, mae: 5.026303, mean_q: 5.241947
 56739/100000: episode: 5791, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.461, mean reward: 0.446 [0.414, 0.535], mean action: 52.500 [22.000, 88.000], mean observation: 3.172 [-1.655, 10.426], loss: 1.455716, mae: 5.025796, mean_q: 5.241950
 56749/100000: episode: 5792, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.254, mean reward: 0.425 [0.320, 0.588], mean action: 47.100 [19.000, 80.000], mean observation: 3.157 [-1.061, 10.331], loss: 1.808609, mae: 5.026858, mean_q: 5.238066
 56759/100000: episode: 5793, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.297, mean reward: 0.430 [0.366, 0.476], mean action: 47.300 [3.000, 99.000], mean observation: 3.147 [-1.796, 10.388], loss: 1.416402, mae: 5.024735, mean_q: 5.234251
 56767/100000: episode: 5794, duration: 0.134s, episode steps: 8, steps per second: 60, episode reward: 13.252, mean reward: 1.656 [0.463, 10.000], mean action: 38.500 [10.000, 86.000], mean observation: 3.155 [-1.558, 10.293], loss: 1.346978, mae: 5.024034, mean_q: 5.232038
 56777/100000: episode: 5795, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.747, mean reward: 0.375 [0.356, 0.467], mean action: 56.600 [48.000, 94.000], mean observation: 3.161 [-1.389, 10.302], loss: 1.320751, mae: 5.023655, mean_q: 5.230048
 56787/100000: episode: 5796, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.619, mean reward: 0.362 [0.265, 0.420], mean action: 52.000 [44.000, 77.000], mean observation: 3.166 [-1.793, 10.249], loss: 1.111405, mae: 5.022834, mean_q: 5.230287
 56797/100000: episode: 5797, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.050, mean reward: 0.405 [0.334, 0.455], mean action: 48.800 [27.000, 89.000], mean observation: 3.172 [-1.050, 10.241], loss: 1.507022, mae: 5.024606, mean_q: 5.230163
 56807/100000: episode: 5798, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.449, mean reward: 0.445 [0.387, 0.476], mean action: 40.500 [4.000, 80.000], mean observation: 3.154 [-1.082, 10.250], loss: 1.361000, mae: 5.023618, mean_q: 5.226770
 56817/100000: episode: 5799, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.828, mean reward: 0.383 [0.336, 0.441], mean action: 45.300 [21.000, 48.000], mean observation: 3.161 [-1.741, 10.321], loss: 1.143290, mae: 5.022324, mean_q: 5.224137
 56827/100000: episode: 5800, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.930, mean reward: 0.393 [0.358, 0.469], mean action: 56.100 [48.000, 83.000], mean observation: 3.160 [-1.274, 10.270], loss: 1.338901, mae: 5.023150, mean_q: 5.224484
 56837/100000: episode: 5801, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.650, mean reward: 0.365 [0.306, 0.448], mean action: 52.600 [17.000, 93.000], mean observation: 3.151 [-1.325, 10.418], loss: 1.150284, mae: 5.022450, mean_q: 5.224961
 56847/100000: episode: 5802, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.402, mean reward: 0.440 [0.383, 0.509], mean action: 48.800 [4.000, 93.000], mean observation: 3.159 [-1.431, 10.303], loss: 0.778084, mae: 5.021636, mean_q: 5.227799
 56857/100000: episode: 5803, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 5.216, mean reward: 0.522 [0.522, 0.522], mean action: 58.500 [28.000, 101.000], mean observation: 3.152 [-1.001, 10.252], loss: 1.002751, mae: 5.022930, mean_q: 5.231189
 56867/100000: episode: 5804, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.223, mean reward: 0.422 [0.368, 0.493], mean action: 44.800 [5.000, 100.000], mean observation: 3.153 [-1.050, 10.431], loss: 1.117386, mae: 5.023810, mean_q: 5.231708
 56877/100000: episode: 5805, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.002, mean reward: 0.400 [0.380, 0.480], mean action: 54.200 [30.000, 76.000], mean observation: 3.163 [-1.057, 10.303], loss: 1.297534, mae: 5.024540, mean_q: 5.231824
 56887/100000: episode: 5806, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.590, mean reward: 0.459 [0.378, 0.554], mean action: 39.900 [3.000, 82.000], mean observation: 3.153 [-1.921, 10.232], loss: 0.842462, mae: 5.023231, mean_q: 5.231312
 56897/100000: episode: 5807, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.941, mean reward: 0.394 [0.385, 0.433], mean action: 49.400 [9.000, 84.000], mean observation: 3.167 [-1.507, 10.229], loss: 1.429562, mae: 5.025496, mean_q: 5.230003
 56907/100000: episode: 5808, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.047, mean reward: 0.405 [0.339, 0.479], mean action: 54.800 [11.000, 93.000], mean observation: 3.160 [-1.598, 10.341], loss: 1.496376, mae: 5.025706, mean_q: 5.227749
 56917/100000: episode: 5809, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.963, mean reward: 0.396 [0.274, 0.543], mean action: 43.700 [13.000, 81.000], mean observation: 3.152 [-1.654, 10.200], loss: 1.216784, mae: 5.024431, mean_q: 5.228694
 56927/100000: episode: 5810, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.845, mean reward: 0.385 [0.343, 0.408], mean action: 62.100 [27.000, 100.000], mean observation: 3.163 [-2.159, 10.241], loss: 0.994596, mae: 5.023374, mean_q: 5.229856
 56937/100000: episode: 5811, duration: 0.131s, episode steps: 10, steps per second: 77, episode reward: 3.927, mean reward: 0.393 [0.294, 0.462], mean action: 62.600 [4.000, 99.000], mean observation: 3.156 [-1.457, 10.273], loss: 1.150291, mae: 5.024357, mean_q: 5.230275
 56947/100000: episode: 5812, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.156, mean reward: 0.416 [0.379, 0.519], mean action: 41.600 [11.000, 97.000], mean observation: 3.155 [-1.570, 10.394], loss: 1.076389, mae: 5.024562, mean_q: 5.229522
 56957/100000: episode: 5813, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.925, mean reward: 0.393 [0.295, 0.517], mean action: 39.000 [28.000, 72.000], mean observation: 3.158 [-1.586, 10.411], loss: 1.133456, mae: 5.024819, mean_q: 5.230948
 56967/100000: episode: 5814, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.660, mean reward: 0.366 [0.307, 0.436], mean action: 27.500 [1.000, 37.000], mean observation: 3.156 [-1.443, 10.285], loss: 1.110170, mae: 5.024907, mean_q: 5.232859
 56977/100000: episode: 5815, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.731, mean reward: 0.373 [0.309, 0.403], mean action: 45.400 [15.000, 90.000], mean observation: 3.159 [-1.471, 10.214], loss: 1.213579, mae: 5.025287, mean_q: 5.233855
 56978/100000: episode: 5816, duration: 0.028s, episode steps: 1, steps per second: 35, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 37.000 [37.000, 37.000], mean observation: 3.150 [-1.664, 10.136], loss: 1.425096, mae: 5.025717, mean_q: 5.234238
 56988/100000: episode: 5817, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.852, mean reward: 0.385 [0.351, 0.418], mean action: 35.600 [7.000, 81.000], mean observation: 3.154 [-1.785, 10.372], loss: 0.942549, mae: 5.024317, mean_q: 5.233474
 56998/100000: episode: 5818, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.518, mean reward: 0.452 [0.337, 0.590], mean action: 40.000 [3.000, 97.000], mean observation: 3.167 [-0.987, 10.507], loss: 1.188215, mae: 5.025368, mean_q: 5.229496
 57008/100000: episode: 5819, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.121, mean reward: 0.412 [0.373, 0.485], mean action: 49.900 [47.000, 65.000], mean observation: 3.143 [-0.879, 10.272], loss: 1.461016, mae: 5.026317, mean_q: 5.224805
 57018/100000: episode: 5820, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.478, mean reward: 0.448 [0.321, 0.500], mean action: 42.300 [4.000, 86.000], mean observation: 3.161 [-1.423, 10.322], loss: 1.218672, mae: 5.025277, mean_q: 5.227688
 57028/100000: episode: 5821, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.951, mean reward: 0.395 [0.342, 0.508], mean action: 41.300 [1.000, 84.000], mean observation: 3.160 [-1.403, 10.367], loss: 1.093677, mae: 5.024948, mean_q: 5.228614
 57038/100000: episode: 5822, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.899, mean reward: 0.390 [0.364, 0.453], mean action: 48.500 [2.000, 88.000], mean observation: 3.152 [-1.456, 10.319], loss: 1.006448, mae: 5.024533, mean_q: 5.229288
 57048/100000: episode: 5823, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.931, mean reward: 0.393 [0.323, 0.434], mean action: 60.900 [13.000, 100.000], mean observation: 3.157 [-1.081, 10.294], loss: 1.250964, mae: 5.025851, mean_q: 5.231430
 57058/100000: episode: 5824, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.517, mean reward: 0.352 [0.275, 0.397], mean action: 48.100 [33.000, 65.000], mean observation: 3.159 [-1.009, 10.189], loss: 1.163305, mae: 5.025637, mean_q: 5.232202
 57068/100000: episode: 5825, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.879, mean reward: 0.388 [0.277, 0.536], mean action: 51.100 [13.000, 85.000], mean observation: 3.155 [-1.863, 10.383], loss: 1.190791, mae: 5.025948, mean_q: 5.229194
 57078/100000: episode: 5826, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.699, mean reward: 0.470 [0.346, 0.585], mean action: 47.500 [4.000, 99.000], mean observation: 3.165 [-1.228, 10.479], loss: 1.063668, mae: 5.025501, mean_q: 5.229027
 57088/100000: episode: 5827, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 3.858, mean reward: 0.386 [0.297, 0.435], mean action: 42.800 [3.000, 84.000], mean observation: 3.153 [-1.337, 10.305], loss: 0.943797, mae: 5.025327, mean_q: 5.230214
 57098/100000: episode: 5828, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.184, mean reward: 0.418 [0.402, 0.471], mean action: 58.800 [43.000, 90.000], mean observation: 3.161 [-1.422, 10.361], loss: 1.149335, mae: 5.026356, mean_q: 5.228933
 57108/100000: episode: 5829, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.094, mean reward: 0.409 [0.315, 0.495], mean action: 50.300 [12.000, 90.000], mean observation: 3.154 [-1.622, 10.283], loss: 1.237587, mae: 5.027007, mean_q: 5.230911
 57118/100000: episode: 5830, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.321, mean reward: 0.432 [0.403, 0.530], mean action: 50.700 [31.000, 76.000], mean observation: 3.162 [-1.139, 10.237], loss: 1.141756, mae: 5.026986, mean_q: 5.234455
 57128/100000: episode: 5831, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.749, mean reward: 0.375 [0.305, 0.465], mean action: 48.500 [5.000, 81.000], mean observation: 3.157 [-1.380, 10.458], loss: 1.377457, mae: 5.027666, mean_q: 5.237595
 57138/100000: episode: 5832, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.118, mean reward: 0.412 [0.347, 0.440], mean action: 59.700 [24.000, 95.000], mean observation: 3.164 [-1.578, 10.248], loss: 1.413715, mae: 5.027702, mean_q: 5.240203
 57148/100000: episode: 5833, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.830, mean reward: 0.383 [0.361, 0.492], mean action: 49.600 [10.000, 96.000], mean observation: 3.155 [-0.891, 10.374], loss: 0.981967, mae: 5.025834, mean_q: 5.240317
 57158/100000: episode: 5834, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.701, mean reward: 0.370 [0.359, 0.406], mean action: 46.100 [10.000, 92.000], mean observation: 3.149 [-1.152, 10.302], loss: 1.037400, mae: 5.026422, mean_q: 5.241783
 57168/100000: episode: 5835, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.769, mean reward: 0.377 [0.327, 0.446], mean action: 49.800 [27.000, 76.000], mean observation: 3.147 [-1.265, 10.191], loss: 1.214619, mae: 5.027413, mean_q: 5.243036
 57178/100000: episode: 5836, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.383, mean reward: 0.338 [0.309, 0.397], mean action: 39.000 [4.000, 96.000], mean observation: 3.153 [-1.501, 10.251], loss: 0.992400, mae: 5.026672, mean_q: 5.238711
 57188/100000: episode: 5837, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.989, mean reward: 0.399 [0.350, 0.487], mean action: 46.200 [0.000, 99.000], mean observation: 3.150 [-0.955, 10.382], loss: 1.420239, mae: 5.028499, mean_q: 5.237199
 57198/100000: episode: 5838, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.198, mean reward: 0.420 [0.354, 0.469], mean action: 50.600 [2.000, 96.000], mean observation: 3.164 [-1.488, 10.272], loss: 1.162125, mae: 5.027752, mean_q: 5.238494
 57208/100000: episode: 5839, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.627, mean reward: 0.463 [0.328, 0.497], mean action: 48.100 [7.000, 84.000], mean observation: 3.164 [-1.351, 10.290], loss: 1.415036, mae: 5.028859, mean_q: 5.238361
 57218/100000: episode: 5840, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.472, mean reward: 0.447 [0.404, 0.491], mean action: 55.600 [19.000, 96.000], mean observation: 3.167 [-1.213, 10.279], loss: 1.270261, mae: 5.028328, mean_q: 5.237836
 57228/100000: episode: 5841, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 5.391, mean reward: 0.539 [0.539, 0.539], mean action: 55.200 [32.000, 96.000], mean observation: 3.142 [-1.762, 10.126], loss: 1.220777, mae: 5.028042, mean_q: 5.238395
 57238/100000: episode: 5842, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.877, mean reward: 0.388 [0.313, 0.515], mean action: 51.200 [2.000, 92.000], mean observation: 3.160 [-1.583, 10.307], loss: 1.317363, mae: 5.028512, mean_q: 5.238030
 57248/100000: episode: 5843, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.791, mean reward: 0.479 [0.435, 0.571], mean action: 47.000 [17.000, 69.000], mean observation: 3.154 [-1.444, 10.278], loss: 1.361079, mae: 5.028741, mean_q: 5.237009
 57258/100000: episode: 5844, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.853, mean reward: 0.385 [0.351, 0.505], mean action: 46.800 [22.000, 70.000], mean observation: 3.156 [-1.203, 10.479], loss: 1.320454, mae: 5.028689, mean_q: 5.232877
 57268/100000: episode: 5845, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.106, mean reward: 0.411 [0.338, 0.459], mean action: 57.800 [18.000, 88.000], mean observation: 3.165 [-1.241, 10.297], loss: 1.068757, mae: 5.027245, mean_q: 5.230092
 57278/100000: episode: 5846, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.310, mean reward: 0.431 [0.416, 0.510], mean action: 52.000 [24.000, 101.000], mean observation: 3.157 [-1.202, 10.387], loss: 1.155339, mae: 5.027632, mean_q: 5.231595
 57288/100000: episode: 5847, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.780, mean reward: 0.378 [0.320, 0.478], mean action: 46.700 [0.000, 91.000], mean observation: 3.159 [-1.318, 10.341], loss: 1.370598, mae: 5.028574, mean_q: 5.232915
 57298/100000: episode: 5848, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.174, mean reward: 0.417 [0.327, 0.486], mean action: 62.000 [19.000, 97.000], mean observation: 3.163 [-1.737, 10.263], loss: 1.170477, mae: 5.027728, mean_q: 5.234164
 57308/100000: episode: 5849, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.056, mean reward: 0.406 [0.341, 0.495], mean action: 55.800 [17.000, 98.000], mean observation: 3.167 [-1.246, 10.536], loss: 1.101907, mae: 5.027450, mean_q: 5.235966
 57318/100000: episode: 5850, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.923, mean reward: 0.492 [0.469, 0.495], mean action: 55.800 [8.000, 92.000], mean observation: 3.157 [-1.667, 10.320], loss: 1.272380, mae: 5.028136, mean_q: 5.236747
 57328/100000: episode: 5851, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.116, mean reward: 0.412 [0.371, 0.440], mean action: 46.700 [10.000, 60.000], mean observation: 3.171 [-1.219, 10.295], loss: 1.275331, mae: 5.027890, mean_q: 5.233620
 57338/100000: episode: 5852, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.167, mean reward: 0.417 [0.326, 0.488], mean action: 40.600 [1.000, 59.000], mean observation: 3.153 [-2.122, 10.216], loss: 1.077932, mae: 5.027139, mean_q: 5.234111
 57348/100000: episode: 5853, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.170, mean reward: 0.417 [0.347, 0.465], mean action: 26.600 [4.000, 94.000], mean observation: 3.150 [-1.846, 10.409], loss: 1.132696, mae: 5.027573, mean_q: 5.235563
 57358/100000: episode: 5854, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.913, mean reward: 0.391 [0.369, 0.434], mean action: 49.100 [12.000, 101.000], mean observation: 3.150 [-1.003, 10.237], loss: 1.176194, mae: 5.027956, mean_q: 5.236253
 57364/100000: episode: 5855, duration: 0.114s, episode steps: 6, steps per second: 53, episode reward: 12.033, mean reward: 2.005 [0.331, 10.000], mean action: 39.667 [1.000, 90.000], mean observation: 3.153 [-1.302, 10.225], loss: 1.030026, mae: 5.027640, mean_q: 5.237330
 57374/100000: episode: 5856, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.957, mean reward: 0.396 [0.339, 0.559], mean action: 49.200 [22.000, 66.000], mean observation: 3.148 [-1.858, 10.388], loss: 1.246211, mae: 5.028579, mean_q: 5.238439
 57384/100000: episode: 5857, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.340, mean reward: 0.434 [0.409, 0.525], mean action: 57.900 [41.000, 79.000], mean observation: 3.147 [-1.610, 10.322], loss: 1.032151, mae: 5.027981, mean_q: 5.240389
 57394/100000: episode: 5858, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.313, mean reward: 0.431 [0.357, 0.485], mean action: 65.000 [16.000, 101.000], mean observation: 3.167 [-1.256, 10.371], loss: 1.244300, mae: 5.028949, mean_q: 5.242510
 57404/100000: episode: 5859, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.371, mean reward: 0.437 [0.416, 0.489], mean action: 60.600 [5.000, 99.000], mean observation: 3.169 [-1.368, 10.297], loss: 0.898130, mae: 5.027579, mean_q: 5.244567
 57414/100000: episode: 5860, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.321, mean reward: 0.432 [0.403, 0.439], mean action: 61.600 [31.000, 81.000], mean observation: 3.159 [-1.686, 10.204], loss: 1.146800, mae: 5.028975, mean_q: 5.245494
 57424/100000: episode: 5861, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.816, mean reward: 0.482 [0.476, 0.533], mean action: 47.500 [10.000, 59.000], mean observation: 3.146 [-1.884, 10.205], loss: 1.424608, mae: 5.030053, mean_q: 5.246445
 57426/100000: episode: 5862, duration: 0.054s, episode steps: 2, steps per second: 37, episode reward: 10.406, mean reward: 5.203 [0.406, 10.000], mean action: 74.500 [50.000, 99.000], mean observation: 3.140 [-0.957, 10.100], loss: 1.655536, mae: 5.031035, mean_q: 5.244889
 57436/100000: episode: 5863, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.152, mean reward: 0.415 [0.308, 0.514], mean action: 32.200 [8.000, 59.000], mean observation: 3.147 [-1.496, 10.394], loss: 1.145178, mae: 5.029069, mean_q: 5.242150
 57446/100000: episode: 5864, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.881, mean reward: 0.388 [0.339, 0.490], mean action: 63.000 [56.000, 101.000], mean observation: 3.155 [-1.390, 10.323], loss: 1.333179, mae: 5.029975, mean_q: 5.241277
 57456/100000: episode: 5865, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.827, mean reward: 0.483 [0.483, 0.483], mean action: 49.900 [6.000, 66.000], mean observation: 3.166 [-1.005, 10.286], loss: 1.213573, mae: 5.029289, mean_q: 5.242409
 57466/100000: episode: 5866, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.747, mean reward: 0.375 [0.309, 0.451], mean action: 47.000 [8.000, 85.000], mean observation: 3.160 [-1.265, 10.274], loss: 1.342060, mae: 5.029531, mean_q: 5.243906
 57476/100000: episode: 5867, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.098, mean reward: 0.410 [0.361, 0.473], mean action: 58.900 [29.000, 85.000], mean observation: 3.151 [-1.202, 10.245], loss: 1.104335, mae: 5.028427, mean_q: 5.240638
 57486/100000: episode: 5868, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.284, mean reward: 0.428 [0.387, 0.491], mean action: 56.400 [11.000, 77.000], mean observation: 3.154 [-1.195, 10.317], loss: 1.699032, mae: 5.030829, mean_q: 5.241704
 57496/100000: episode: 5869, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.240, mean reward: 0.424 [0.361, 0.554], mean action: 40.800 [1.000, 91.000], mean observation: 3.161 [-2.037, 10.298], loss: 1.010338, mae: 5.027991, mean_q: 5.244236
 57506/100000: episode: 5870, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.957, mean reward: 0.396 [0.373, 0.507], mean action: 54.900 [0.000, 92.000], mean observation: 3.150 [-1.288, 10.283], loss: 1.419387, mae: 5.029247, mean_q: 5.244398
 57516/100000: episode: 5871, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.989, mean reward: 0.399 [0.371, 0.466], mean action: 61.300 [42.000, 94.000], mean observation: 3.152 [-1.372, 10.200], loss: 1.134262, mae: 5.028070, mean_q: 5.241536
 57526/100000: episode: 5872, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.411, mean reward: 0.441 [0.411, 0.558], mean action: 50.300 [6.000, 59.000], mean observation: 3.169 [-1.610, 10.307], loss: 1.149147, mae: 5.028069, mean_q: 5.241942
 57536/100000: episode: 5873, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.990, mean reward: 0.499 [0.326, 0.528], mean action: 53.500 [3.000, 97.000], mean observation: 3.142 [-1.633, 10.214], loss: 1.175310, mae: 5.028346, mean_q: 5.244671
 57546/100000: episode: 5874, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.090, mean reward: 0.409 [0.409, 0.409], mean action: 59.700 [39.000, 82.000], mean observation: 3.151 [-1.093, 10.246], loss: 1.217654, mae: 5.028459, mean_q: 5.247220
 57556/100000: episode: 5875, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.143, mean reward: 0.414 [0.389, 0.487], mean action: 61.800 [9.000, 101.000], mean observation: 3.138 [-1.154, 10.358], loss: 1.230396, mae: 5.028441, mean_q: 5.249197
 57566/100000: episode: 5876, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.674, mean reward: 0.467 [0.336, 0.565], mean action: 54.400 [15.000, 98.000], mean observation: 3.170 [-1.527, 10.321], loss: 1.277638, mae: 5.028832, mean_q: 5.252017
 57576/100000: episode: 5877, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.306, mean reward: 0.431 [0.355, 0.523], mean action: 72.200 [33.000, 98.000], mean observation: 3.157 [-1.453, 10.282], loss: 0.880095, mae: 5.027474, mean_q: 5.254585
 57586/100000: episode: 5878, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.305, mean reward: 0.431 [0.430, 0.436], mean action: 52.500 [13.000, 59.000], mean observation: 3.149 [-1.517, 10.327], loss: 1.308679, mae: 5.029389, mean_q: 5.255836
 57596/100000: episode: 5879, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 5.384, mean reward: 0.538 [0.538, 0.538], mean action: 48.500 [10.000, 59.000], mean observation: 3.154 [-1.654, 10.379], loss: 1.119186, mae: 5.028892, mean_q: 5.252351
 57606/100000: episode: 5880, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.269, mean reward: 0.427 [0.391, 0.500], mean action: 53.000 [13.000, 83.000], mean observation: 3.155 [-1.701, 10.287], loss: 1.126388, mae: 5.029081, mean_q: 5.250964
 57616/100000: episode: 5881, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.322, mean reward: 0.432 [0.329, 0.551], mean action: 61.900 [41.000, 98.000], mean observation: 3.144 [-1.189, 10.315], loss: 1.281364, mae: 5.030145, mean_q: 5.251628
 57626/100000: episode: 5882, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.885, mean reward: 0.388 [0.304, 0.442], mean action: 49.400 [26.000, 59.000], mean observation: 3.161 [-1.348, 10.219], loss: 1.406403, mae: 5.030699, mean_q: 5.253416
 57636/100000: episode: 5883, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.859, mean reward: 0.486 [0.486, 0.486], mean action: 48.000 [22.000, 59.000], mean observation: 3.175 [-1.731, 10.383], loss: 1.180929, mae: 5.029468, mean_q: 5.255553
 57646/100000: episode: 5884, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.006, mean reward: 0.401 [0.398, 0.426], mean action: 62.200 [16.000, 101.000], mean observation: 3.155 [-2.260, 10.362], loss: 1.209556, mae: 5.029666, mean_q: 5.257668
 57656/100000: episode: 5885, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.433, mean reward: 0.443 [0.381, 0.580], mean action: 55.200 [0.000, 98.000], mean observation: 3.161 [-1.643, 10.380], loss: 1.452591, mae: 5.030608, mean_q: 5.260358
 57666/100000: episode: 5886, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.806, mean reward: 0.381 [0.346, 0.426], mean action: 59.100 [26.000, 87.000], mean observation: 3.154 [-1.245, 10.296], loss: 0.974121, mae: 5.028848, mean_q: 5.263236
 57676/100000: episode: 5887, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.605, mean reward: 0.460 [0.449, 0.567], mean action: 54.800 [33.000, 59.000], mean observation: 3.163 [-1.313, 10.420], loss: 1.452117, mae: 5.031058, mean_q: 5.265873
 57686/100000: episode: 5888, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 13.963, mean reward: 1.396 [0.384, 10.000], mean action: 58.500 [21.000, 88.000], mean observation: 3.170 [-2.392, 10.274], loss: 1.406093, mae: 5.030776, mean_q: 5.268185
 57696/100000: episode: 5889, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.055, mean reward: 0.406 [0.339, 0.488], mean action: 57.500 [11.000, 98.000], mean observation: 3.139 [-1.603, 10.297], loss: 1.022474, mae: 5.029430, mean_q: 5.270333
 57706/100000: episode: 5890, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.717, mean reward: 0.372 [0.337, 0.443], mean action: 39.300 [7.000, 77.000], mean observation: 3.155 [-1.518, 10.332], loss: 0.980337, mae: 5.029542, mean_q: 5.271391
 57716/100000: episode: 5891, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.953, mean reward: 0.395 [0.306, 0.474], mean action: 56.200 [26.000, 100.000], mean observation: 3.140 [-1.775, 10.203], loss: 1.421844, mae: 5.031585, mean_q: 5.272258
 57726/100000: episode: 5892, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.289, mean reward: 0.429 [0.305, 0.553], mean action: 35.100 [3.000, 79.000], mean observation: 3.145 [-1.236, 10.378], loss: 1.221330, mae: 5.030984, mean_q: 5.273061
 57736/100000: episode: 5893, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 3.536, mean reward: 0.354 [0.322, 0.429], mean action: 68.800 [59.000, 100.000], mean observation: 3.148 [-1.020, 10.230], loss: 1.092885, mae: 5.030343, mean_q: 5.268727
 57746/100000: episode: 5894, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.132, mean reward: 0.413 [0.321, 0.471], mean action: 51.500 [5.000, 92.000], mean observation: 3.152 [-1.345, 10.334], loss: 1.676086, mae: 5.032666, mean_q: 5.267709
 57756/100000: episode: 5895, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.038, mean reward: 0.404 [0.382, 0.470], mean action: 61.200 [44.000, 100.000], mean observation: 3.144 [-1.820, 10.252], loss: 1.188767, mae: 5.030890, mean_q: 5.268631
 57766/100000: episode: 5896, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.345, mean reward: 0.434 [0.370, 0.485], mean action: 57.200 [22.000, 86.000], mean observation: 3.154 [-1.385, 10.357], loss: 1.312481, mae: 5.031764, mean_q: 5.266057
 57776/100000: episode: 5897, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 3.937, mean reward: 0.394 [0.332, 0.456], mean action: 66.700 [38.000, 86.000], mean observation: 3.172 [-1.427, 10.482], loss: 1.111406, mae: 5.030951, mean_q: 5.264247
 57786/100000: episode: 5898, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.170, mean reward: 0.417 [0.404, 0.458], mean action: 62.300 [27.000, 99.000], mean observation: 3.151 [-1.495, 10.225], loss: 1.457781, mae: 5.032300, mean_q: 5.265211
 57796/100000: episode: 5899, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.993, mean reward: 0.399 [0.385, 0.531], mean action: 63.800 [27.000, 99.000], mean observation: 3.160 [-1.531, 10.331], loss: 1.332947, mae: 5.031640, mean_q: 5.262180
 57806/100000: episode: 5900, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.232, mean reward: 0.323 [0.287, 0.395], mean action: 47.200 [2.000, 87.000], mean observation: 3.153 [-1.330, 10.364], loss: 0.864123, mae: 5.029737, mean_q: 5.260596
 57816/100000: episode: 5901, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.025, mean reward: 0.403 [0.374, 0.423], mean action: 48.200 [12.000, 63.000], mean observation: 3.166 [-1.286, 10.251], loss: 1.187739, mae: 5.031077, mean_q: 5.262499
 57826/100000: episode: 5902, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.083, mean reward: 0.408 [0.351, 0.526], mean action: 51.200 [14.000, 66.000], mean observation: 3.143 [-2.355, 10.333], loss: 1.219295, mae: 5.031537, mean_q: 5.263103
 57836/100000: episode: 5903, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.582, mean reward: 0.358 [0.284, 0.435], mean action: 56.100 [33.000, 101.000], mean observation: 3.154 [-1.507, 10.313], loss: 1.297677, mae: 5.031934, mean_q: 5.266459
 57846/100000: episode: 5904, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.012, mean reward: 0.401 [0.356, 0.561], mean action: 52.200 [24.000, 63.000], mean observation: 3.154 [-1.531, 10.403], loss: 1.393229, mae: 5.032202, mean_q: 5.268120
 57856/100000: episode: 5905, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.936, mean reward: 0.394 [0.383, 0.459], mean action: 58.100 [29.000, 99.000], mean observation: 3.161 [-0.765, 10.433], loss: 1.292052, mae: 5.031981, mean_q: 5.265413
 57866/100000: episode: 5906, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.949, mean reward: 0.395 [0.349, 0.494], mean action: 52.800 [1.000, 96.000], mean observation: 3.164 [-1.327, 10.342], loss: 1.506573, mae: 5.032382, mean_q: 5.263053
 57876/100000: episode: 5907, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.634, mean reward: 0.463 [0.328, 0.561], mean action: 48.400 [15.000, 73.000], mean observation: 3.161 [-1.427, 10.332], loss: 1.339406, mae: 5.031524, mean_q: 5.261357
 57886/100000: episode: 5908, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 3.907, mean reward: 0.391 [0.314, 0.544], mean action: 46.300 [7.000, 74.000], mean observation: 3.159 [-1.446, 10.435], loss: 1.329651, mae: 5.031549, mean_q: 5.262572
 57896/100000: episode: 5909, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.605, mean reward: 0.361 [0.286, 0.416], mean action: 54.100 [12.000, 94.000], mean observation: 3.170 [-1.128, 10.247], loss: 1.092922, mae: 5.030704, mean_q: 5.264562
 57906/100000: episode: 5910, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.035, mean reward: 0.404 [0.322, 0.508], mean action: 38.400 [2.000, 72.000], mean observation: 3.157 [-1.402, 10.318], loss: 1.018158, mae: 5.030537, mean_q: 5.263488
 57916/100000: episode: 5911, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.290, mean reward: 0.429 [0.393, 0.456], mean action: 48.000 [18.000, 67.000], mean observation: 3.149 [-1.539, 10.364], loss: 1.127112, mae: 5.031294, mean_q: 5.262329
 57926/100000: episode: 5912, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 5.203, mean reward: 0.520 [0.519, 0.532], mean action: 45.000 [12.000, 75.000], mean observation: 3.170 [-1.849, 10.289], loss: 1.144513, mae: 5.031461, mean_q: 5.263206
 57936/100000: episode: 5913, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.748, mean reward: 0.375 [0.318, 0.494], mean action: 55.200 [50.000, 85.000], mean observation: 3.149 [-1.265, 10.359], loss: 1.564915, mae: 5.033179, mean_q: 5.264504
 57946/100000: episode: 5914, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.027, mean reward: 0.403 [0.368, 0.470], mean action: 58.200 [21.000, 92.000], mean observation: 3.151 [-1.605, 10.228], loss: 1.366163, mae: 5.032283, mean_q: 5.264411
 57956/100000: episode: 5915, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.302, mean reward: 0.430 [0.342, 0.472], mean action: 50.400 [33.000, 74.000], mean observation: 3.163 [-1.319, 10.475], loss: 1.529162, mae: 5.032855, mean_q: 5.263793
 57966/100000: episode: 5916, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.066, mean reward: 0.407 [0.349, 0.514], mean action: 61.400 [10.000, 99.000], mean observation: 3.156 [-1.100, 10.234], loss: 1.511658, mae: 5.032698, mean_q: 5.263806
 57976/100000: episode: 5917, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 3.945, mean reward: 0.395 [0.344, 0.465], mean action: 33.500 [0.000, 66.000], mean observation: 3.160 [-1.173, 10.312], loss: 1.109696, mae: 5.031059, mean_q: 5.258593
 57986/100000: episode: 5918, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.050, mean reward: 0.405 [0.356, 0.502], mean action: 25.500 [0.000, 99.000], mean observation: 3.159 [-2.192, 10.403], loss: 1.407964, mae: 5.032165, mean_q: 5.257821
 57996/100000: episode: 5919, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.991, mean reward: 0.399 [0.341, 0.506], mean action: 22.700 [0.000, 78.000], mean observation: 3.153 [-1.060, 10.304], loss: 1.289080, mae: 5.031753, mean_q: 5.257002
 58006/100000: episode: 5920, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.220, mean reward: 0.422 [0.319, 0.486], mean action: 27.300 [0.000, 84.000], mean observation: 3.161 [-1.193, 10.387], loss: 1.433179, mae: 5.032290, mean_q: 5.257577
 58016/100000: episode: 5921, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.143, mean reward: 0.414 [0.325, 0.531], mean action: 33.100 [0.000, 95.000], mean observation: 3.164 [-1.688, 10.363], loss: 1.161012, mae: 5.031157, mean_q: 5.259474
 58026/100000: episode: 5922, duration: 0.217s, episode steps: 10, steps per second: 46, episode reward: 3.619, mean reward: 0.362 [0.295, 0.418], mean action: 25.700 [0.000, 82.000], mean observation: 3.154 [-1.227, 10.273], loss: 1.075205, mae: 5.031070, mean_q: 5.262022
 58034/100000: episode: 5923, duration: 0.161s, episode steps: 8, steps per second: 50, episode reward: 12.845, mean reward: 1.606 [0.290, 10.000], mean action: 32.500 [0.000, 88.000], mean observation: 3.153 [-1.061, 10.285], loss: 1.433204, mae: 5.032597, mean_q: 5.264428
 58044/100000: episode: 5924, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.357, mean reward: 0.436 [0.329, 0.543], mean action: 29.500 [0.000, 101.000], mean observation: 3.149 [-1.078, 10.547], loss: 1.258222, mae: 5.031730, mean_q: 5.266792
 58054/100000: episode: 5925, duration: 0.229s, episode steps: 10, steps per second: 44, episode reward: 4.401, mean reward: 0.440 [0.322, 0.541], mean action: 11.400 [0.000, 62.000], mean observation: 3.156 [-1.731, 10.234], loss: 1.120106, mae: 5.031070, mean_q: 5.266720
 58064/100000: episode: 5926, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.801, mean reward: 0.380 [0.332, 0.471], mean action: 34.200 [0.000, 93.000], mean observation: 3.150 [-1.168, 10.300], loss: 1.658243, mae: 5.033276, mean_q: 5.265532
 58071/100000: episode: 5927, duration: 0.143s, episode steps: 7, steps per second: 49, episode reward: 12.637, mean reward: 1.805 [0.351, 10.000], mean action: 36.429 [0.000, 99.000], mean observation: 3.141 [-1.478, 10.289], loss: 1.439741, mae: 5.032291, mean_q: 5.264299
 58081/100000: episode: 5928, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.098, mean reward: 0.410 [0.349, 0.481], mean action: 39.100 [0.000, 97.000], mean observation: 3.159 [-1.148, 10.286], loss: 1.205578, mae: 5.031280, mean_q: 5.261664
 58091/100000: episode: 5929, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.107, mean reward: 0.411 [0.366, 0.493], mean action: 26.500 [0.000, 92.000], mean observation: 3.163 [-1.436, 10.410], loss: 1.240304, mae: 5.031452, mean_q: 5.259479
 58101/100000: episode: 5930, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.461, mean reward: 0.446 [0.318, 0.540], mean action: 33.900 [0.000, 92.000], mean observation: 3.151 [-1.238, 10.294], loss: 1.358646, mae: 5.032187, mean_q: 5.260875
 58111/100000: episode: 5931, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.529, mean reward: 0.453 [0.357, 0.528], mean action: 37.600 [0.000, 101.000], mean observation: 3.151 [-1.656, 10.311], loss: 1.106566, mae: 5.031513, mean_q: 5.257556
 58121/100000: episode: 5932, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.049, mean reward: 0.405 [0.342, 0.448], mean action: 27.800 [0.000, 92.000], mean observation: 3.156 [-1.432, 10.260], loss: 1.296645, mae: 5.032231, mean_q: 5.254519
 58131/100000: episode: 5933, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.182, mean reward: 0.418 [0.324, 0.527], mean action: 35.500 [0.000, 96.000], mean observation: 3.154 [-1.323, 10.283], loss: 1.586522, mae: 5.033891, mean_q: 5.254531
 58141/100000: episode: 5934, duration: 0.224s, episode steps: 10, steps per second: 45, episode reward: 4.145, mean reward: 0.415 [0.367, 0.501], mean action: 8.200 [0.000, 40.000], mean observation: 3.152 [-1.431, 10.382], loss: 1.357523, mae: 5.032762, mean_q: 5.253445
 58151/100000: episode: 5935, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.214, mean reward: 0.421 [0.322, 0.550], mean action: 17.700 [0.000, 90.000], mean observation: 3.156 [-0.965, 10.429], loss: 1.486847, mae: 5.033153, mean_q: 5.251773
 58161/100000: episode: 5936, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.858, mean reward: 0.386 [0.369, 0.409], mean action: 49.400 [37.000, 83.000], mean observation: 3.159 [-2.064, 10.307], loss: 1.319733, mae: 5.032307, mean_q: 5.249173
 58171/100000: episode: 5937, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.917, mean reward: 0.392 [0.338, 0.469], mean action: 37.200 [9.000, 80.000], mean observation: 3.156 [-2.081, 10.350], loss: 1.324225, mae: 5.032053, mean_q: 5.250173
 58181/100000: episode: 5938, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.188, mean reward: 0.419 [0.313, 0.579], mean action: 39.900 [6.000, 89.000], mean observation: 3.144 [-1.652, 10.245], loss: 1.377937, mae: 5.032445, mean_q: 5.247893
 58191/100000: episode: 5939, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.607, mean reward: 0.361 [0.288, 0.499], mean action: 37.100 [11.000, 79.000], mean observation: 3.156 [-1.298, 10.302], loss: 1.567354, mae: 5.032928, mean_q: 5.246626
 58201/100000: episode: 5940, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.121, mean reward: 0.412 [0.328, 0.465], mean action: 37.200 [12.000, 87.000], mean observation: 3.160 [-1.896, 10.238], loss: 1.189800, mae: 5.031544, mean_q: 5.247538
 58211/100000: episode: 5941, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 3.749, mean reward: 0.375 [0.271, 0.495], mean action: 41.600 [28.000, 70.000], mean observation: 3.161 [-1.706, 10.245], loss: 1.123931, mae: 5.031435, mean_q: 5.248670
 58221/100000: episode: 5942, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.996, mean reward: 0.400 [0.333, 0.434], mean action: 46.900 [14.000, 100.000], mean observation: 3.170 [-1.669, 10.448], loss: 1.263100, mae: 5.032188, mean_q: 5.250816
 58231/100000: episode: 5943, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.482, mean reward: 0.448 [0.418, 0.517], mean action: 46.400 [12.000, 95.000], mean observation: 3.168 [-1.783, 10.317], loss: 1.240151, mae: 5.032295, mean_q: 5.252649
 58241/100000: episode: 5944, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.478, mean reward: 0.448 [0.445, 0.477], mean action: 43.500 [30.000, 101.000], mean observation: 3.162 [-2.149, 10.194], loss: 1.077488, mae: 5.031733, mean_q: 5.253716
 58251/100000: episode: 5945, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.094, mean reward: 0.409 [0.302, 0.493], mean action: 43.900 [31.000, 98.000], mean observation: 3.166 [-1.493, 10.253], loss: 1.080074, mae: 5.032007, mean_q: 5.254281
 58261/100000: episode: 5946, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 3.883, mean reward: 0.388 [0.326, 0.445], mean action: 49.400 [36.000, 85.000], mean observation: 3.172 [-1.616, 10.497], loss: 1.484341, mae: 5.033932, mean_q: 5.253462
 58271/100000: episode: 5947, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.047, mean reward: 0.405 [0.331, 0.507], mean action: 38.600 [5.000, 98.000], mean observation: 3.156 [-0.888, 10.344], loss: 1.328166, mae: 5.033191, mean_q: 5.253150
 58281/100000: episode: 5948, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.206, mean reward: 0.421 [0.407, 0.458], mean action: 39.200 [37.000, 51.000], mean observation: 3.162 [-1.949, 10.399], loss: 1.382701, mae: 5.033206, mean_q: 5.253886
 58291/100000: episode: 5949, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.300, mean reward: 0.430 [0.351, 0.499], mean action: 43.000 [7.000, 83.000], mean observation: 3.155 [-1.596, 10.292], loss: 1.246155, mae: 5.032510, mean_q: 5.255115
 58301/100000: episode: 5950, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.077, mean reward: 0.408 [0.362, 0.438], mean action: 56.200 [23.000, 93.000], mean observation: 3.156 [-1.234, 10.269], loss: 1.254760, mae: 5.032725, mean_q: 5.255989
 58311/100000: episode: 5951, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.302, mean reward: 0.430 [0.350, 0.511], mean action: 40.200 [1.000, 92.000], mean observation: 3.149 [-1.400, 10.349], loss: 1.291852, mae: 5.032830, mean_q: 5.256650
 58321/100000: episode: 5952, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.923, mean reward: 0.392 [0.321, 0.529], mean action: 42.800 [25.000, 100.000], mean observation: 3.152 [-1.018, 10.337], loss: 1.038783, mae: 5.031551, mean_q: 5.255753
 58331/100000: episode: 5953, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.961, mean reward: 0.396 [0.332, 0.437], mean action: 31.200 [4.000, 50.000], mean observation: 3.154 [-1.062, 10.338], loss: 1.307203, mae: 5.032592, mean_q: 5.253812
 58341/100000: episode: 5954, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.858, mean reward: 0.386 [0.339, 0.458], mean action: 42.500 [21.000, 94.000], mean observation: 3.150 [-1.653, 10.295], loss: 0.973902, mae: 5.031828, mean_q: 5.253818
 58351/100000: episode: 5955, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.947, mean reward: 0.395 [0.328, 0.477], mean action: 48.100 [37.000, 59.000], mean observation: 3.164 [-1.328, 10.395], loss: 1.217141, mae: 5.032706, mean_q: 5.254198
 58361/100000: episode: 5956, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.805, mean reward: 0.380 [0.336, 0.421], mean action: 45.200 [9.000, 59.000], mean observation: 3.168 [-1.354, 10.305], loss: 0.876722, mae: 5.031968, mean_q: 5.255485
 58371/100000: episode: 5957, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.811, mean reward: 0.381 [0.312, 0.455], mean action: 55.900 [10.000, 92.000], mean observation: 3.149 [-2.186, 10.301], loss: 1.281673, mae: 5.033732, mean_q: 5.254179
 58381/100000: episode: 5958, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.182, mean reward: 0.418 [0.347, 0.426], mean action: 42.700 [7.000, 66.000], mean observation: 3.161 [-1.490, 10.253], loss: 1.327579, mae: 5.033746, mean_q: 5.252894
 58391/100000: episode: 5959, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.981, mean reward: 0.398 [0.341, 0.489], mean action: 42.500 [2.000, 83.000], mean observation: 3.167 [-1.677, 10.442], loss: 1.495407, mae: 5.034372, mean_q: 5.250364
 58401/100000: episode: 5960, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.999, mean reward: 0.400 [0.375, 0.457], mean action: 49.100 [18.000, 101.000], mean observation: 3.137 [-1.400, 10.287], loss: 1.261940, mae: 5.033813, mean_q: 5.246685
 58411/100000: episode: 5961, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.432, mean reward: 0.443 [0.355, 0.467], mean action: 43.500 [3.000, 101.000], mean observation: 3.165 [-1.271, 10.468], loss: 1.314486, mae: 5.033899, mean_q: 5.248666
 58417/100000: episode: 5962, duration: 0.105s, episode steps: 6, steps per second: 57, episode reward: 12.200, mean reward: 2.033 [0.418, 10.000], mean action: 34.333 [1.000, 50.000], mean observation: 3.157 [-1.858, 10.481], loss: 1.659189, mae: 5.034994, mean_q: 5.250251
 58427/100000: episode: 5963, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.688, mean reward: 0.369 [0.315, 0.439], mean action: 49.200 [30.000, 81.000], mean observation: 3.158 [-1.379, 10.255], loss: 1.051178, mae: 5.032535, mean_q: 5.250395
 58437/100000: episode: 5964, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.564, mean reward: 0.456 [0.432, 0.555], mean action: 57.700 [50.000, 95.000], mean observation: 3.158 [-2.135, 10.295], loss: 0.998683, mae: 5.032256, mean_q: 5.251543
 58447/100000: episode: 5965, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.461, mean reward: 0.446 [0.334, 0.516], mean action: 52.100 [13.000, 100.000], mean observation: 3.154 [-1.161, 10.238], loss: 1.144154, mae: 5.033194, mean_q: 5.253338
 58457/100000: episode: 5966, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.212, mean reward: 0.421 [0.343, 0.535], mean action: 47.800 [4.000, 101.000], mean observation: 3.165 [-1.062, 10.281], loss: 1.211174, mae: 5.033512, mean_q: 5.253330
 58467/100000: episode: 5967, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.965, mean reward: 0.396 [0.348, 0.435], mean action: 53.600 [19.000, 101.000], mean observation: 3.149 [-1.935, 10.218], loss: 1.113484, mae: 5.033187, mean_q: 5.251152
 58477/100000: episode: 5968, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.403, mean reward: 0.440 [0.430, 0.487], mean action: 44.700 [0.000, 84.000], mean observation: 3.172 [-2.369, 10.377], loss: 1.256955, mae: 5.033849, mean_q: 5.249032
 58487/100000: episode: 5969, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.680, mean reward: 0.368 [0.316, 0.506], mean action: 59.600 [50.000, 92.000], mean observation: 3.152 [-0.978, 10.348], loss: 0.963362, mae: 5.032996, mean_q: 5.246766
 58497/100000: episode: 5970, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 5.074, mean reward: 0.507 [0.375, 0.540], mean action: 49.400 [11.000, 101.000], mean observation: 3.169 [-0.952, 10.307], loss: 1.187489, mae: 5.034133, mean_q: 5.246960
 58507/100000: episode: 5971, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.997, mean reward: 0.400 [0.311, 0.516], mean action: 54.500 [11.000, 93.000], mean observation: 3.141 [-1.522, 10.261], loss: 1.458105, mae: 5.035308, mean_q: 5.244934
 58517/100000: episode: 5972, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.725, mean reward: 0.373 [0.258, 0.476], mean action: 36.600 [0.000, 79.000], mean observation: 3.154 [-1.230, 10.292], loss: 1.111111, mae: 5.033938, mean_q: 5.245127
 58527/100000: episode: 5973, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.790, mean reward: 0.379 [0.374, 0.396], mean action: 64.600 [49.000, 94.000], mean observation: 3.159 [-1.103, 10.305], loss: 1.009272, mae: 5.033628, mean_q: 5.245965
 58537/100000: episode: 5974, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.784, mean reward: 0.378 [0.292, 0.416], mean action: 53.600 [40.000, 59.000], mean observation: 3.169 [-1.185, 10.272], loss: 1.164379, mae: 5.034548, mean_q: 5.245289
 58547/100000: episode: 5975, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.077, mean reward: 0.408 [0.307, 0.520], mean action: 47.600 [0.000, 71.000], mean observation: 3.155 [-1.242, 10.363], loss: 1.259603, mae: 5.035028, mean_q: 5.243209
 58557/100000: episode: 5976, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.326, mean reward: 0.433 [0.357, 0.493], mean action: 43.000 [1.000, 88.000], mean observation: 3.156 [-1.095, 10.334], loss: 1.120824, mae: 5.034896, mean_q: 5.242847
 58567/100000: episode: 5977, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 13.793, mean reward: 1.379 [0.319, 10.000], mean action: 46.300 [12.000, 59.000], mean observation: 3.157 [-1.911, 10.392], loss: 1.060941, mae: 5.034667, mean_q: 5.239738
 58577/100000: episode: 5978, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 3.879, mean reward: 0.388 [0.371, 0.408], mean action: 58.600 [47.000, 87.000], mean observation: 3.161 [-1.442, 10.296], loss: 1.104995, mae: 5.035032, mean_q: 5.237461
 58587/100000: episode: 5979, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.911, mean reward: 0.391 [0.332, 0.502], mean action: 50.500 [2.000, 88.000], mean observation: 3.156 [-1.021, 10.286], loss: 1.127389, mae: 5.035486, mean_q: 5.235902
 58597/100000: episode: 5980, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.989, mean reward: 0.399 [0.385, 0.452], mean action: 44.800 [3.000, 59.000], mean observation: 3.164 [-0.855, 10.535], loss: 1.084230, mae: 5.035512, mean_q: 5.232817
 58607/100000: episode: 5981, duration: 0.153s, episode steps: 10, steps per second: 66, episode reward: 3.617, mean reward: 0.362 [0.294, 0.508], mean action: 58.700 [11.000, 88.000], mean observation: 3.154 [-1.067, 10.276], loss: 1.217142, mae: 5.036005, mean_q: 5.231119
 58617/100000: episode: 5982, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.077, mean reward: 0.408 [0.342, 0.498], mean action: 18.500 [1.000, 92.000], mean observation: 3.153 [-2.330, 10.439], loss: 1.277802, mae: 5.036414, mean_q: 5.230579
 58627/100000: episode: 5983, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.411, mean reward: 0.441 [0.377, 0.582], mean action: 27.100 [2.000, 91.000], mean observation: 3.149 [-2.405, 10.392], loss: 1.245524, mae: 5.036584, mean_q: 5.231343
 58637/100000: episode: 5984, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.101, mean reward: 0.410 [0.308, 0.512], mean action: 34.600 [1.000, 96.000], mean observation: 3.154 [-1.256, 10.278], loss: 1.072822, mae: 5.035837, mean_q: 5.231956
 58647/100000: episode: 5985, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.079, mean reward: 0.408 [0.327, 0.517], mean action: 19.100 [2.000, 61.000], mean observation: 3.149 [-1.844, 10.289], loss: 1.557887, mae: 5.037948, mean_q: 5.234041
 58657/100000: episode: 5986, duration: 0.238s, episode steps: 10, steps per second: 42, episode reward: 4.280, mean reward: 0.428 [0.369, 0.555], mean action: 16.400 [2.000, 65.000], mean observation: 3.150 [-1.932, 10.211], loss: 1.464087, mae: 5.037349, mean_q: 5.235837
 58667/100000: episode: 5987, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 3.956, mean reward: 0.396 [0.365, 0.480], mean action: 26.200 [2.000, 95.000], mean observation: 3.159 [-1.415, 10.511], loss: 1.552499, mae: 5.037426, mean_q: 5.237260
 58677/100000: episode: 5988, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.153, mean reward: 0.415 [0.305, 0.599], mean action: 31.100 [2.000, 96.000], mean observation: 3.150 [-1.531, 10.209], loss: 1.421697, mae: 5.036347, mean_q: 5.238434
 58687/100000: episode: 5989, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.049, mean reward: 0.405 [0.363, 0.458], mean action: 54.600 [2.000, 96.000], mean observation: 3.173 [-1.355, 10.364], loss: 1.526126, mae: 5.036399, mean_q: 5.239728
 58697/100000: episode: 5990, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.202, mean reward: 0.420 [0.326, 0.479], mean action: 30.500 [2.000, 91.000], mean observation: 3.148 [-1.926, 10.361], loss: 1.253531, mae: 5.034928, mean_q: 5.240823
 58707/100000: episode: 5991, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.844, mean reward: 0.384 [0.313, 0.513], mean action: 43.000 [2.000, 96.000], mean observation: 3.152 [-1.459, 10.158], loss: 1.013148, mae: 5.033899, mean_q: 5.241900
 58717/100000: episode: 5992, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.319, mean reward: 0.432 [0.341, 0.520], mean action: 14.000 [2.000, 89.000], mean observation: 3.157 [-1.218, 10.404], loss: 1.313652, mae: 5.035461, mean_q: 5.244987
 58727/100000: episode: 5993, duration: 0.222s, episode steps: 10, steps per second: 45, episode reward: 4.195, mean reward: 0.420 [0.329, 0.516], mean action: 14.400 [2.000, 48.000], mean observation: 3.163 [-1.413, 10.201], loss: 1.018855, mae: 5.034385, mean_q: 5.242262
 58737/100000: episode: 5994, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.269, mean reward: 0.427 [0.342, 0.479], mean action: 22.200 [2.000, 83.000], mean observation: 3.159 [-1.668, 10.388], loss: 1.611327, mae: 5.037035, mean_q: 5.239283
 58747/100000: episode: 5995, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.078, mean reward: 0.408 [0.324, 0.540], mean action: 26.400 [0.000, 91.000], mean observation: 3.159 [-1.728, 10.268], loss: 1.274405, mae: 5.035417, mean_q: 5.239212
 58757/100000: episode: 5996, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.213, mean reward: 0.421 [0.338, 0.530], mean action: 25.500 [2.000, 95.000], mean observation: 3.151 [-1.525, 10.368], loss: 0.978324, mae: 5.034030, mean_q: 5.241281
 58767/100000: episode: 5997, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 3.861, mean reward: 0.386 [0.348, 0.502], mean action: 19.300 [2.000, 68.000], mean observation: 3.156 [-1.748, 10.357], loss: 1.326928, mae: 5.035341, mean_q: 5.242228
 58777/100000: episode: 5998, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.283, mean reward: 0.428 [0.363, 0.561], mean action: 30.000 [2.000, 90.000], mean observation: 3.160 [-1.474, 10.354], loss: 1.284281, mae: 5.035150, mean_q: 5.238148
 58787/100000: episode: 5999, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.050, mean reward: 0.405 [0.355, 0.530], mean action: 17.400 [2.000, 78.000], mean observation: 3.160 [-1.444, 10.495], loss: 1.233001, mae: 5.034791, mean_q: 5.237512
 58797/100000: episode: 6000, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.908, mean reward: 0.391 [0.358, 0.438], mean action: 20.400 [2.000, 92.000], mean observation: 3.156 [-1.894, 10.366], loss: 1.305950, mae: 5.035038, mean_q: 5.239120
 58807/100000: episode: 6001, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.305, mean reward: 0.430 [0.337, 0.532], mean action: 17.300 [2.000, 78.000], mean observation: 3.164 [-1.845, 10.486], loss: 1.121886, mae: 5.034530, mean_q: 5.241952
 58817/100000: episode: 6002, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.728, mean reward: 0.473 [0.361, 0.559], mean action: 32.800 [2.000, 83.000], mean observation: 3.161 [-1.158, 10.284], loss: 1.276323, mae: 5.035395, mean_q: 5.243529
 58827/100000: episode: 6003, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.327, mean reward: 0.433 [0.348, 0.487], mean action: 21.700 [2.000, 64.000], mean observation: 3.161 [-1.309, 10.340], loss: 1.093748, mae: 5.034783, mean_q: 5.242984
 58837/100000: episode: 6004, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.458, mean reward: 0.446 [0.404, 0.536], mean action: 48.700 [2.000, 98.000], mean observation: 3.163 [-1.585, 10.298], loss: 0.952693, mae: 5.034596, mean_q: 5.238789
 58847/100000: episode: 6005, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.061, mean reward: 0.406 [0.366, 0.499], mean action: 35.100 [2.000, 101.000], mean observation: 3.149 [-1.350, 10.321], loss: 1.049297, mae: 5.035097, mean_q: 5.238558
 58857/100000: episode: 6006, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.226, mean reward: 0.423 [0.381, 0.517], mean action: 17.200 [2.000, 75.000], mean observation: 3.161 [-1.737, 10.363], loss: 1.272318, mae: 5.036359, mean_q: 5.241510
 58867/100000: episode: 6007, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.833, mean reward: 0.483 [0.372, 0.626], mean action: 20.300 [2.000, 90.000], mean observation: 3.157 [-1.170, 10.408], loss: 1.263755, mae: 5.036509, mean_q: 5.244611
 58877/100000: episode: 6008, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.191, mean reward: 0.419 [0.364, 0.515], mean action: 36.900 [2.000, 96.000], mean observation: 3.161 [-1.106, 10.300], loss: 1.135067, mae: 5.035846, mean_q: 5.246799
 58887/100000: episode: 6009, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.763, mean reward: 0.376 [0.317, 0.428], mean action: 25.800 [2.000, 92.000], mean observation: 3.162 [-2.143, 10.268], loss: 1.156600, mae: 5.035892, mean_q: 5.242726
 58897/100000: episode: 6010, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.407, mean reward: 0.441 [0.348, 0.596], mean action: 44.600 [2.000, 100.000], mean observation: 3.151 [-0.820, 10.619], loss: 1.277907, mae: 5.036057, mean_q: 5.240700
 58907/100000: episode: 6011, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 4.094, mean reward: 0.409 [0.343, 0.493], mean action: 20.400 [2.000, 66.000], mean observation: 3.156 [-2.281, 10.268], loss: 1.442481, mae: 5.036784, mean_q: 5.237590
 58917/100000: episode: 6012, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.190, mean reward: 0.419 [0.362, 0.509], mean action: 34.800 [2.000, 84.000], mean observation: 3.169 [-1.378, 10.424], loss: 1.307056, mae: 5.036288, mean_q: 5.236084
 58927/100000: episode: 6013, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.119, mean reward: 0.412 [0.361, 0.497], mean action: 26.200 [2.000, 70.000], mean observation: 3.159 [-1.170, 10.310], loss: 1.517694, mae: 5.036959, mean_q: 5.236299
 58937/100000: episode: 6014, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.776, mean reward: 0.378 [0.326, 0.453], mean action: 14.800 [2.000, 62.000], mean observation: 3.159 [-1.638, 10.323], loss: 1.120325, mae: 5.035251, mean_q: 5.237872
 58947/100000: episode: 6015, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.992, mean reward: 0.399 [0.312, 0.533], mean action: 9.200 [2.000, 27.000], mean observation: 3.152 [-1.730, 10.487], loss: 1.198233, mae: 5.035906, mean_q: 5.239160
 58957/100000: episode: 6016, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.201, mean reward: 0.420 [0.296, 0.542], mean action: 41.900 [2.000, 101.000], mean observation: 3.140 [-1.510, 10.369], loss: 1.176437, mae: 5.036082, mean_q: 5.240627
 58967/100000: episode: 6017, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.307, mean reward: 0.431 [0.361, 0.520], mean action: 32.300 [2.000, 101.000], mean observation: 3.164 [-1.739, 10.607], loss: 1.077986, mae: 5.035644, mean_q: 5.243071
 58977/100000: episode: 6018, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.097, mean reward: 0.410 [0.326, 0.457], mean action: 20.800 [2.000, 95.000], mean observation: 3.159 [-1.800, 10.304], loss: 1.251824, mae: 5.036366, mean_q: 5.241817
 58987/100000: episode: 6019, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.195, mean reward: 0.419 [0.366, 0.564], mean action: 25.000 [1.000, 100.000], mean observation: 3.151 [-1.586, 10.308], loss: 1.298395, mae: 5.036760, mean_q: 5.241498
 58997/100000: episode: 6020, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.060, mean reward: 0.406 [0.343, 0.559], mean action: 11.000 [2.000, 59.000], mean observation: 3.159 [-1.397, 10.354], loss: 1.283252, mae: 5.036549, mean_q: 5.238182
 59007/100000: episode: 6021, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.006, mean reward: 0.401 [0.331, 0.474], mean action: 26.200 [2.000, 98.000], mean observation: 3.159 [-0.975, 10.252], loss: 1.419707, mae: 5.037038, mean_q: 5.236118
 59017/100000: episode: 6022, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.821, mean reward: 0.482 [0.469, 0.583], mean action: 26.300 [2.000, 82.000], mean observation: 3.166 [-1.631, 10.302], loss: 1.151203, mae: 5.036181, mean_q: 5.236969
 59027/100000: episode: 6023, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.277, mean reward: 0.428 [0.404, 0.559], mean action: 27.200 [2.000, 87.000], mean observation: 3.160 [-1.411, 10.322], loss: 1.305605, mae: 5.036803, mean_q: 5.235032
 59037/100000: episode: 6024, duration: 0.230s, episode steps: 10, steps per second: 43, episode reward: 3.980, mean reward: 0.398 [0.343, 0.457], mean action: 15.500 [2.000, 84.000], mean observation: 3.151 [-1.471, 10.287], loss: 1.208240, mae: 5.036610, mean_q: 5.233921
 59047/100000: episode: 6025, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.054, mean reward: 0.405 [0.359, 0.489], mean action: 44.600 [1.000, 99.000], mean observation: 3.149 [-1.314, 10.304], loss: 1.539229, mae: 5.037589, mean_q: 5.233000
 59057/100000: episode: 6026, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 3.980, mean reward: 0.398 [0.344, 0.578], mean action: 22.100 [2.000, 100.000], mean observation: 3.150 [-2.238, 10.307], loss: 1.552992, mae: 5.037471, mean_q: 5.230418
 59067/100000: episode: 6027, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.083, mean reward: 0.408 [0.341, 0.527], mean action: 12.200 [2.000, 73.000], mean observation: 3.160 [-1.557, 10.274], loss: 1.530292, mae: 5.036943, mean_q: 5.230062
 59077/100000: episode: 6028, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.040, mean reward: 0.404 [0.326, 0.473], mean action: 33.600 [2.000, 75.000], mean observation: 3.153 [-1.141, 10.212], loss: 1.251622, mae: 5.035933, mean_q: 5.227549
 59087/100000: episode: 6029, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.090, mean reward: 0.409 [0.352, 0.462], mean action: 19.900 [2.000, 85.000], mean observation: 3.158 [-1.495, 10.336], loss: 1.213185, mae: 5.035613, mean_q: 5.224738
 59097/100000: episode: 6030, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 3.940, mean reward: 0.394 [0.343, 0.459], mean action: 17.000 [2.000, 94.000], mean observation: 3.160 [-1.213, 10.417], loss: 1.381220, mae: 5.036114, mean_q: 5.222377
 59107/100000: episode: 6031, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.828, mean reward: 0.483 [0.358, 0.573], mean action: 30.300 [2.000, 76.000], mean observation: 3.154 [-1.209, 10.324], loss: 1.624992, mae: 5.036942, mean_q: 5.223055
 59117/100000: episode: 6032, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.991, mean reward: 0.399 [0.363, 0.481], mean action: 37.100 [2.000, 99.000], mean observation: 3.156 [-1.679, 10.303], loss: 1.180434, mae: 5.034947, mean_q: 5.225188
 59127/100000: episode: 6033, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.574, mean reward: 0.457 [0.307, 0.558], mean action: 29.200 [2.000, 90.000], mean observation: 3.149 [-1.686, 10.383], loss: 0.964746, mae: 5.033943, mean_q: 5.219516
 59137/100000: episode: 6034, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.894, mean reward: 0.389 [0.334, 0.521], mean action: 33.200 [2.000, 87.000], mean observation: 3.149 [-1.173, 10.448], loss: 1.582846, mae: 5.036044, mean_q: 5.217197
 59147/100000: episode: 6035, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.751, mean reward: 0.375 [0.338, 0.444], mean action: 46.800 [4.000, 94.000], mean observation: 3.155 [-0.976, 10.286], loss: 1.574442, mae: 5.035562, mean_q: 5.221315
 59157/100000: episode: 6036, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.810, mean reward: 0.381 [0.348, 0.405], mean action: 43.200 [16.000, 96.000], mean observation: 3.148 [-0.780, 10.257], loss: 1.005214, mae: 5.033158, mean_q: 5.224036
 59167/100000: episode: 6037, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 5.127, mean reward: 0.513 [0.503, 0.597], mean action: 52.200 [37.000, 91.000], mean observation: 3.139 [-1.157, 10.472], loss: 1.508675, mae: 5.034784, mean_q: 5.224974
 59177/100000: episode: 6038, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.925, mean reward: 0.392 [0.379, 0.452], mean action: 39.100 [19.000, 76.000], mean observation: 3.158 [-1.708, 10.502], loss: 1.486691, mae: 5.034324, mean_q: 5.225821
 59187/100000: episode: 6039, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.682, mean reward: 0.368 [0.292, 0.422], mean action: 37.800 [18.000, 78.000], mean observation: 3.173 [-1.362, 10.385], loss: 1.282676, mae: 5.033397, mean_q: 5.224851
 59197/100000: episode: 6040, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.342, mean reward: 0.434 [0.325, 0.489], mean action: 51.600 [22.000, 89.000], mean observation: 3.165 [-1.079, 10.421], loss: 0.970987, mae: 5.032103, mean_q: 5.221728
 59207/100000: episode: 6041, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.181, mean reward: 0.418 [0.392, 0.489], mean action: 51.700 [4.000, 100.000], mean observation: 3.156 [-0.978, 10.265], loss: 1.607558, mae: 5.034678, mean_q: 5.220281
 59217/100000: episode: 6042, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.025, mean reward: 0.403 [0.336, 0.439], mean action: 44.800 [14.000, 78.000], mean observation: 3.164 [-1.940, 10.220], loss: 1.002623, mae: 5.032227, mean_q: 5.220307
 59227/100000: episode: 6043, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.215, mean reward: 0.422 [0.384, 0.510], mean action: 45.300 [6.000, 90.000], mean observation: 3.162 [-1.221, 10.200], loss: 1.246446, mae: 5.033266, mean_q: 5.221377
 59237/100000: episode: 6044, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.786, mean reward: 0.379 [0.348, 0.431], mean action: 47.500 [7.000, 95.000], mean observation: 3.159 [-1.584, 10.395], loss: 1.134536, mae: 5.032645, mean_q: 5.221930
 59247/100000: episode: 6045, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.051, mean reward: 0.405 [0.364, 0.517], mean action: 71.500 [26.000, 95.000], mean observation: 3.156 [-1.630, 10.274], loss: 1.439202, mae: 5.033632, mean_q: 5.222834
 59257/100000: episode: 6046, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.752, mean reward: 0.375 [0.364, 0.476], mean action: 79.700 [28.000, 95.000], mean observation: 3.147 [-0.885, 10.314], loss: 1.311876, mae: 5.033044, mean_q: 5.223355
 59267/100000: episode: 6047, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 3.927, mean reward: 0.393 [0.331, 0.493], mean action: 64.800 [3.000, 95.000], mean observation: 3.151 [-1.391, 10.328], loss: 1.246012, mae: 5.032777, mean_q: 5.222438
 59277/100000: episode: 6048, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.965, mean reward: 0.497 [0.487, 0.581], mean action: 76.400 [23.000, 95.000], mean observation: 3.175 [-1.966, 10.437], loss: 1.269649, mae: 5.032873, mean_q: 5.222182
 59287/100000: episode: 6049, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.091, mean reward: 0.409 [0.329, 0.518], mean action: 50.500 [10.000, 95.000], mean observation: 3.167 [-1.431, 10.422], loss: 1.301701, mae: 5.032912, mean_q: 5.220777
 59297/100000: episode: 6050, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.038, mean reward: 0.404 [0.345, 0.475], mean action: 39.100 [6.000, 74.000], mean observation: 3.154 [-1.230, 10.357], loss: 1.171595, mae: 5.032074, mean_q: 5.221519
 59307/100000: episode: 6051, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.318, mean reward: 0.432 [0.431, 0.443], mean action: 43.900 [37.000, 63.000], mean observation: 3.149 [-1.891, 10.327], loss: 0.890304, mae: 5.030995, mean_q: 5.222116
 59317/100000: episode: 6052, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.170, mean reward: 0.417 [0.312, 0.523], mean action: 40.400 [12.000, 97.000], mean observation: 3.158 [-2.137, 10.278], loss: 1.303523, mae: 5.032773, mean_q: 5.223801
 59327/100000: episode: 6053, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 3.942, mean reward: 0.394 [0.329, 0.525], mean action: 45.000 [25.000, 95.000], mean observation: 3.165 [-1.581, 10.378], loss: 1.156684, mae: 5.032116, mean_q: 5.225817
 59337/100000: episode: 6054, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.012, mean reward: 0.401 [0.320, 0.507], mean action: 36.900 [3.000, 84.000], mean observation: 3.154 [-1.476, 10.247], loss: 1.234948, mae: 5.032402, mean_q: 5.227563
 59347/100000: episode: 6055, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.058, mean reward: 0.406 [0.349, 0.500], mean action: 42.500 [18.000, 84.000], mean observation: 3.164 [-1.256, 10.300], loss: 1.275121, mae: 5.032464, mean_q: 5.228448
 59357/100000: episode: 6056, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.071, mean reward: 0.407 [0.391, 0.446], mean action: 50.500 [3.000, 87.000], mean observation: 3.142 [-0.911, 10.288], loss: 0.935960, mae: 5.031392, mean_q: 5.229759
 59367/100000: episode: 6057, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.514, mean reward: 0.451 [0.389, 0.520], mean action: 65.900 [37.000, 99.000], mean observation: 3.145 [-1.246, 10.399], loss: 1.162646, mae: 5.032404, mean_q: 5.230749
 59377/100000: episode: 6058, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.913, mean reward: 0.491 [0.485, 0.545], mean action: 45.200 [37.000, 73.000], mean observation: 3.152 [-2.037, 10.141], loss: 0.978379, mae: 5.031944, mean_q: 5.231505
 59387/100000: episode: 6059, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.972, mean reward: 0.397 [0.354, 0.478], mean action: 38.700 [0.000, 92.000], mean observation: 3.153 [-1.971, 10.316], loss: 1.484960, mae: 5.034029, mean_q: 5.231934
 59392/100000: episode: 6060, duration: 0.114s, episode steps: 5, steps per second: 44, episode reward: 11.834, mean reward: 2.367 [0.405, 10.000], mean action: 24.400 [7.000, 37.000], mean observation: 3.164 [-1.305, 10.416], loss: 1.342650, mae: 5.033488, mean_q: 5.229984
 59402/100000: episode: 6061, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.317, mean reward: 0.432 [0.409, 0.523], mean action: 35.100 [2.000, 72.000], mean observation: 3.156 [-1.320, 10.321], loss: 1.637125, mae: 5.034204, mean_q: 5.229356
 59412/100000: episode: 6062, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.976, mean reward: 0.398 [0.287, 0.509], mean action: 34.800 [0.000, 62.000], mean observation: 3.155 [-1.602, 10.318], loss: 1.271395, mae: 5.032300, mean_q: 5.229920
 59422/100000: episode: 6063, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.835, mean reward: 0.483 [0.461, 0.492], mean action: 49.300 [11.000, 88.000], mean observation: 3.144 [-1.925, 10.265], loss: 1.318397, mae: 5.032233, mean_q: 5.229060
 59432/100000: episode: 6064, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.345, mean reward: 0.435 [0.341, 0.598], mean action: 33.500 [3.000, 49.000], mean observation: 3.158 [-1.229, 10.336], loss: 1.635935, mae: 5.033357, mean_q: 5.228477
 59442/100000: episode: 6065, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.257, mean reward: 0.426 [0.401, 0.515], mean action: 47.800 [0.000, 92.000], mean observation: 3.166 [-0.955, 10.603], loss: 0.919727, mae: 5.030404, mean_q: 5.229107
 59452/100000: episode: 6066, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.237, mean reward: 0.424 [0.362, 0.574], mean action: 34.700 [3.000, 79.000], mean observation: 3.161 [-1.537, 10.543], loss: 1.340937, mae: 5.032256, mean_q: 5.230175
 59453/100000: episode: 6067, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 37.000 [37.000, 37.000], mean observation: 3.131 [-0.848, 10.100], loss: 0.783227, mae: 5.029577, mean_q: 5.228836
 59463/100000: episode: 6068, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.193, mean reward: 0.419 [0.365, 0.570], mean action: 49.500 [37.000, 98.000], mean observation: 3.140 [-1.885, 10.331], loss: 1.515250, mae: 5.032269, mean_q: 5.227142
 59473/100000: episode: 6069, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.188, mean reward: 0.419 [0.413, 0.429], mean action: 45.600 [2.000, 101.000], mean observation: 3.151 [-1.838, 10.195], loss: 1.272755, mae: 5.031212, mean_q: 5.221344
 59483/100000: episode: 6070, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.691, mean reward: 0.369 [0.321, 0.415], mean action: 44.700 [20.000, 88.000], mean observation: 3.149 [-1.605, 10.276], loss: 1.693490, mae: 5.032474, mean_q: 5.221019
 59493/100000: episode: 6071, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.454, mean reward: 0.445 [0.370, 0.506], mean action: 33.300 [12.000, 79.000], mean observation: 3.163 [-1.193, 10.360], loss: 1.002788, mae: 5.029497, mean_q: 5.221443
 59503/100000: episode: 6072, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.536, mean reward: 0.454 [0.362, 0.511], mean action: 61.400 [20.000, 99.000], mean observation: 3.153 [-1.011, 10.286], loss: 1.271892, mae: 5.030237, mean_q: 5.221969
 59513/100000: episode: 6073, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 4.198, mean reward: 0.420 [0.392, 0.536], mean action: 74.300 [24.000, 95.000], mean observation: 3.162 [-1.066, 10.444], loss: 1.063089, mae: 5.029386, mean_q: 5.221984
 59523/100000: episode: 6074, duration: 0.117s, episode steps: 10, steps per second: 86, episode reward: 5.274, mean reward: 0.527 [0.497, 0.558], mean action: 77.700 [21.000, 95.000], mean observation: 3.170 [-0.893, 10.313], loss: 1.242742, mae: 5.029875, mean_q: 5.223331
 59533/100000: episode: 6075, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.465, mean reward: 0.447 [0.325, 0.485], mean action: 75.100 [1.000, 95.000], mean observation: 3.172 [-1.694, 10.551], loss: 1.311909, mae: 5.030190, mean_q: 5.224471
 59534/100000: episode: 6076, duration: 0.025s, episode steps: 1, steps per second: 40, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 95.000 [95.000, 95.000], mean observation: 3.128 [-1.025, 10.100], loss: 1.169467, mae: 5.029418, mean_q: 5.225359
 59544/100000: episode: 6077, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.041, mean reward: 0.404 [0.330, 0.452], mean action: 70.400 [32.000, 95.000], mean observation: 3.153 [-1.113, 10.404], loss: 1.357619, mae: 5.030200, mean_q: 5.225942
 59554/100000: episode: 6078, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.967, mean reward: 0.397 [0.370, 0.520], mean action: 53.200 [0.000, 95.000], mean observation: 3.158 [-1.020, 10.378], loss: 1.099063, mae: 5.029014, mean_q: 5.227070
 59564/100000: episode: 6079, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.088, mean reward: 0.409 [0.407, 0.428], mean action: 81.800 [35.000, 95.000], mean observation: 3.158 [-0.445, 10.429], loss: 1.405551, mae: 5.030242, mean_q: 5.227976
 59574/100000: episode: 6080, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.311, mean reward: 0.431 [0.431, 0.431], mean action: 81.400 [40.000, 95.000], mean observation: 3.174 [-0.594, 10.388], loss: 1.069278, mae: 5.028908, mean_q: 5.227183
 59584/100000: episode: 6081, duration: 0.097s, episode steps: 10, steps per second: 104, episode reward: 4.287, mean reward: 0.429 [0.427, 0.442], mean action: 91.600 [67.000, 96.000], mean observation: 3.164 [-0.831, 10.276], loss: 1.143463, mae: 5.029088, mean_q: 5.228544
 59594/100000: episode: 6082, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.924, mean reward: 0.392 [0.323, 0.425], mean action: 62.700 [10.000, 95.000], mean observation: 3.153 [-1.169, 10.361], loss: 1.307468, mae: 5.029646, mean_q: 5.230151
 59604/100000: episode: 6083, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.893, mean reward: 0.389 [0.314, 0.486], mean action: 59.200 [9.000, 95.000], mean observation: 3.160 [-1.219, 10.268], loss: 1.257206, mae: 5.029551, mean_q: 5.232688
 59607/100000: episode: 6084, duration: 0.051s, episode steps: 3, steps per second: 58, episode reward: 10.974, mean reward: 3.658 [0.487, 10.000], mean action: 90.000 [75.000, 100.000], mean observation: 3.161 [-0.379, 10.287], loss: 1.265592, mae: 5.029541, mean_q: 5.233838
 59617/100000: episode: 6085, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.190, mean reward: 0.419 [0.376, 0.443], mean action: 57.400 [3.000, 96.000], mean observation: 3.169 [-0.926, 10.343], loss: 1.290150, mae: 5.029403, mean_q: 5.232715
 59622/100000: episode: 6086, duration: 0.057s, episode steps: 5, steps per second: 88, episode reward: 12.263, mean reward: 2.453 [0.566, 10.000], mean action: 83.600 [58.000, 95.000], mean observation: 3.149 [-0.555, 10.202], loss: 1.504598, mae: 5.030107, mean_q: 5.232652
 59632/100000: episode: 6087, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.219, mean reward: 0.422 [0.375, 0.547], mean action: 57.500 [9.000, 95.000], mean observation: 3.163 [-0.852, 10.298], loss: 1.272050, mae: 5.029132, mean_q: 5.230602
 59642/100000: episode: 6088, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.217, mean reward: 0.422 [0.367, 0.519], mean action: 48.100 [1.000, 95.000], mean observation: 3.157 [-2.326, 10.612], loss: 1.244156, mae: 5.028901, mean_q: 5.228067
 59652/100000: episode: 6089, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 3.735, mean reward: 0.374 [0.345, 0.455], mean action: 80.400 [35.000, 95.000], mean observation: 3.142 [-1.812, 10.363], loss: 1.269899, mae: 5.028855, mean_q: 5.225941
 59662/100000: episode: 6090, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.074, mean reward: 0.407 [0.403, 0.435], mean action: 72.900 [22.000, 95.000], mean observation: 3.161 [-2.060, 10.459], loss: 1.149562, mae: 5.028548, mean_q: 5.226462
 59672/100000: episode: 6091, duration: 0.090s, episode steps: 10, steps per second: 111, episode reward: 4.059, mean reward: 0.406 [0.362, 0.425], mean action: 91.800 [61.000, 97.000], mean observation: 3.180 [-0.572, 10.353], loss: 0.992975, mae: 5.028068, mean_q: 5.225070
 59682/100000: episode: 6092, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.783, mean reward: 0.378 [0.313, 0.456], mean action: 61.400 [24.000, 101.000], mean observation: 3.162 [-1.272, 10.479], loss: 1.178834, mae: 5.029096, mean_q: 5.226404
 59692/100000: episode: 6093, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 3.879, mean reward: 0.388 [0.337, 0.496], mean action: 56.800 [37.000, 97.000], mean observation: 3.158 [-1.158, 10.391], loss: 1.338364, mae: 5.029907, mean_q: 5.230367
 59702/100000: episode: 6094, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.144, mean reward: 0.414 [0.359, 0.460], mean action: 38.900 [11.000, 80.000], mean observation: 3.154 [-1.731, 10.243], loss: 0.940861, mae: 5.028647, mean_q: 5.232671
 59712/100000: episode: 6095, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.449, mean reward: 0.445 [0.375, 0.549], mean action: 35.400 [3.000, 92.000], mean observation: 3.153 [-1.199, 10.305], loss: 1.292404, mae: 5.030173, mean_q: 5.231810
 59722/100000: episode: 6096, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.473, mean reward: 0.447 [0.373, 0.543], mean action: 40.600 [14.000, 90.000], mean observation: 3.159 [-1.257, 10.308], loss: 1.233793, mae: 5.029888, mean_q: 5.230887
 59732/100000: episode: 6097, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.280, mean reward: 0.428 [0.384, 0.505], mean action: 33.000 [1.000, 86.000], mean observation: 3.161 [-1.614, 10.376], loss: 1.399122, mae: 5.030214, mean_q: 5.230014
 59742/100000: episode: 6098, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.362, mean reward: 0.436 [0.416, 0.506], mean action: 44.300 [26.000, 87.000], mean observation: 3.138 [-0.730, 10.204], loss: 1.213193, mae: 5.029305, mean_q: 5.228734
 59752/100000: episode: 6099, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 3.942, mean reward: 0.394 [0.331, 0.443], mean action: 30.900 [3.000, 44.000], mean observation: 3.156 [-1.857, 10.401], loss: 0.873736, mae: 5.027812, mean_q: 5.229756
 59762/100000: episode: 6100, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 5.518, mean reward: 0.552 [0.437, 0.565], mean action: 54.100 [1.000, 95.000], mean observation: 3.152 [-1.476, 10.328], loss: 1.273466, mae: 5.029694, mean_q: 5.231249
 59772/100000: episode: 6101, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.424, mean reward: 0.442 [0.425, 0.512], mean action: 38.800 [29.000, 61.000], mean observation: 3.166 [-1.552, 10.328], loss: 1.275514, mae: 5.029795, mean_q: 5.232547
 59782/100000: episode: 6102, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.801, mean reward: 0.480 [0.480, 0.480], mean action: 45.500 [37.000, 99.000], mean observation: 3.146 [-1.516, 10.326], loss: 1.484893, mae: 5.030538, mean_q: 5.234230
 59792/100000: episode: 6103, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.062, mean reward: 0.406 [0.365, 0.471], mean action: 42.000 [9.000, 84.000], mean observation: 3.154 [-1.165, 10.319], loss: 1.372267, mae: 5.030027, mean_q: 5.235695
 59802/100000: episode: 6104, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.917, mean reward: 0.392 [0.361, 0.435], mean action: 36.400 [31.000, 37.000], mean observation: 3.152 [-1.304, 10.243], loss: 1.072042, mae: 5.028378, mean_q: 5.236400
 59812/100000: episode: 6105, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.970, mean reward: 0.397 [0.335, 0.478], mean action: 36.800 [34.000, 38.000], mean observation: 3.152 [-1.458, 10.310], loss: 1.119892, mae: 5.028854, mean_q: 5.237792
 59822/100000: episode: 6106, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.020, mean reward: 0.402 [0.300, 0.534], mean action: 42.400 [37.000, 74.000], mean observation: 3.158 [-1.736, 10.397], loss: 1.154175, mae: 5.029066, mean_q: 5.236444
 59832/100000: episode: 6107, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 3.420, mean reward: 0.342 [0.296, 0.419], mean action: 36.300 [2.000, 85.000], mean observation: 3.154 [-1.134, 10.345], loss: 1.107930, mae: 5.029159, mean_q: 5.236347
 59842/100000: episode: 6108, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.141, mean reward: 0.414 [0.370, 0.469], mean action: 50.300 [15.000, 100.000], mean observation: 3.160 [-1.392, 10.275], loss: 1.312114, mae: 5.030078, mean_q: 5.238376
 59852/100000: episode: 6109, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.149, mean reward: 0.415 [0.396, 0.459], mean action: 39.100 [29.000, 66.000], mean observation: 3.145 [-1.055, 10.264], loss: 1.056792, mae: 5.029415, mean_q: 5.241428
 59862/100000: episode: 6110, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.231, mean reward: 0.423 [0.379, 0.484], mean action: 37.600 [0.000, 75.000], mean observation: 3.150 [-1.178, 10.363], loss: 0.936028, mae: 5.029414, mean_q: 5.243854
 59872/100000: episode: 6111, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.003, mean reward: 0.400 [0.368, 0.502], mean action: 64.500 [37.000, 101.000], mean observation: 3.157 [-0.697, 10.364], loss: 1.309198, mae: 5.031079, mean_q: 5.244757
 59882/100000: episode: 6112, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.878, mean reward: 0.388 [0.312, 0.482], mean action: 43.200 [3.000, 79.000], mean observation: 3.171 [-1.871, 10.371], loss: 1.755842, mae: 5.032973, mean_q: 5.244607
 59892/100000: episode: 6113, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.083, mean reward: 0.408 [0.339, 0.545], mean action: 30.000 [8.000, 37.000], mean observation: 3.155 [-1.036, 10.513], loss: 1.200611, mae: 5.030424, mean_q: 5.245508
 59902/100000: episode: 6114, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.036, mean reward: 0.404 [0.335, 0.443], mean action: 40.800 [7.000, 101.000], mean observation: 3.166 [-1.780, 10.419], loss: 1.361030, mae: 5.030865, mean_q: 5.244313
 59912/100000: episode: 6115, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.478, mean reward: 0.448 [0.432, 0.483], mean action: 51.300 [30.000, 101.000], mean observation: 3.147 [-1.517, 10.514], loss: 0.867336, mae: 5.028722, mean_q: 5.242142
 59922/100000: episode: 6116, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.329, mean reward: 0.433 [0.339, 0.500], mean action: 46.200 [0.000, 98.000], mean observation: 3.152 [-2.173, 10.277], loss: 1.308414, mae: 5.030371, mean_q: 5.241903
 59932/100000: episode: 6117, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.930, mean reward: 0.393 [0.355, 0.459], mean action: 49.700 [16.000, 91.000], mean observation: 3.158 [-1.482, 10.322], loss: 1.381860, mae: 5.030699, mean_q: 5.240536
 59942/100000: episode: 6118, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.076, mean reward: 0.408 [0.396, 0.448], mean action: 48.500 [37.000, 92.000], mean observation: 3.150 [-1.180, 10.296], loss: 1.106252, mae: 5.029456, mean_q: 5.244731
 59952/100000: episode: 6119, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.205, mean reward: 0.421 [0.387, 0.538], mean action: 44.600 [28.000, 91.000], mean observation: 3.153 [-1.764, 10.565], loss: 1.350566, mae: 5.030302, mean_q: 5.247705
 59962/100000: episode: 6120, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.836, mean reward: 0.384 [0.329, 0.426], mean action: 49.500 [18.000, 100.000], mean observation: 3.164 [-1.239, 10.363], loss: 1.320037, mae: 5.030124, mean_q: 5.249860
 59972/100000: episode: 6121, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.290, mean reward: 0.429 [0.409, 0.508], mean action: 38.400 [24.000, 63.000], mean observation: 3.155 [-2.036, 10.274], loss: 1.361630, mae: 5.030069, mean_q: 5.252117
 59982/100000: episode: 6122, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.144, mean reward: 0.414 [0.362, 0.465], mean action: 36.900 [1.000, 100.000], mean observation: 3.149 [-2.122, 10.189], loss: 1.336918, mae: 5.029502, mean_q: 5.257459
 59992/100000: episode: 6123, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.184, mean reward: 0.418 [0.316, 0.532], mean action: 29.900 [0.000, 70.000], mean observation: 3.166 [-2.072, 10.328], loss: 1.302404, mae: 5.029173, mean_q: 5.265031
 60002/100000: episode: 6124, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.774, mean reward: 0.377 [0.332, 0.473], mean action: 33.700 [6.000, 87.000], mean observation: 3.153 [-2.260, 10.277], loss: 1.220890, mae: 5.028932, mean_q: 5.267214
 60012/100000: episode: 6125, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.091, mean reward: 0.409 [0.408, 0.421], mean action: 53.700 [28.000, 95.000], mean observation: 3.145 [-0.824, 10.427], loss: 1.501938, mae: 5.030187, mean_q: 5.265956
 60022/100000: episode: 6126, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.143, mean reward: 0.414 [0.331, 0.509], mean action: 48.300 [2.000, 98.000], mean observation: 3.160 [-2.345, 10.329], loss: 1.597387, mae: 5.030478, mean_q: 5.264328
 60032/100000: episode: 6127, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.871, mean reward: 0.387 [0.299, 0.433], mean action: 55.500 [37.000, 101.000], mean observation: 3.142 [-2.007, 10.177], loss: 1.317793, mae: 5.029381, mean_q: 5.258505
 60042/100000: episode: 6128, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.308, mean reward: 0.431 [0.368, 0.496], mean action: 38.000 [0.000, 95.000], mean observation: 3.155 [-1.926, 10.380], loss: 1.600203, mae: 5.030176, mean_q: 5.254010
 60052/100000: episode: 6129, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.937, mean reward: 0.394 [0.315, 0.464], mean action: 32.800 [5.000, 49.000], mean observation: 3.160 [-1.581, 10.289], loss: 1.159280, mae: 5.028268, mean_q: 5.253365
 60062/100000: episode: 6130, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.991, mean reward: 0.399 [0.361, 0.451], mean action: 40.300 [6.000, 84.000], mean observation: 3.146 [-1.190, 10.239], loss: 1.314751, mae: 5.028899, mean_q: 5.251125
 60072/100000: episode: 6131, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.897, mean reward: 0.390 [0.379, 0.421], mean action: 56.200 [29.000, 95.000], mean observation: 3.156 [-0.993, 10.230], loss: 0.852838, mae: 5.027312, mean_q: 5.250081
 60082/100000: episode: 6132, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.873, mean reward: 0.387 [0.338, 0.452], mean action: 44.100 [30.000, 65.000], mean observation: 3.147 [-1.340, 10.355], loss: 1.163140, mae: 5.028754, mean_q: 5.245768
 60092/100000: episode: 6133, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.341, mean reward: 0.434 [0.408, 0.508], mean action: 46.900 [37.000, 95.000], mean observation: 3.158 [-0.960, 10.330], loss: 1.353934, mae: 5.029908, mean_q: 5.243985
 60102/100000: episode: 6134, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.900, mean reward: 0.390 [0.360, 0.490], mean action: 43.600 [7.000, 83.000], mean observation: 3.164 [-1.349, 10.248], loss: 1.419025, mae: 5.030243, mean_q: 5.245417
 60112/100000: episode: 6135, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.077, mean reward: 0.408 [0.367, 0.492], mean action: 47.800 [37.000, 86.000], mean observation: 3.163 [-1.158, 10.235], loss: 0.850237, mae: 5.027871, mean_q: 5.241288
 60122/100000: episode: 6136, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.054, mean reward: 0.405 [0.366, 0.518], mean action: 39.400 [8.000, 78.000], mean observation: 3.149 [-1.270, 10.385], loss: 1.353297, mae: 5.030141, mean_q: 5.234983
 60132/100000: episode: 6137, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.146, mean reward: 0.415 [0.388, 0.450], mean action: 51.300 [15.000, 99.000], mean observation: 3.172 [-1.587, 10.283], loss: 1.146107, mae: 5.029537, mean_q: 5.233333
 60142/100000: episode: 6138, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.837, mean reward: 0.384 [0.311, 0.475], mean action: 45.200 [22.000, 89.000], mean observation: 3.152 [-1.225, 10.378], loss: 1.369331, mae: 5.030393, mean_q: 5.233838
 60152/100000: episode: 6139, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.907, mean reward: 0.391 [0.303, 0.461], mean action: 36.700 [4.000, 91.000], mean observation: 3.155 [-1.966, 10.313], loss: 1.232837, mae: 5.030070, mean_q: 5.233456
 60162/100000: episode: 6140, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.749, mean reward: 0.375 [0.310, 0.535], mean action: 43.300 [8.000, 92.000], mean observation: 3.149 [-1.286, 10.424], loss: 1.037431, mae: 5.029412, mean_q: 5.231244
 60172/100000: episode: 6141, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.086, mean reward: 0.409 [0.394, 0.517], mean action: 40.900 [19.000, 60.000], mean observation: 3.155 [-1.419, 10.228], loss: 1.353073, mae: 5.030928, mean_q: 5.227544
 60182/100000: episode: 6142, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.786, mean reward: 0.379 [0.297, 0.500], mean action: 46.200 [7.000, 81.000], mean observation: 3.153 [-0.942, 10.306], loss: 1.336094, mae: 5.030979, mean_q: 5.226632
 60192/100000: episode: 6143, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.566, mean reward: 0.457 [0.304, 0.536], mean action: 34.400 [15.000, 64.000], mean observation: 3.145 [-1.131, 10.284], loss: 1.069854, mae: 5.029716, mean_q: 5.225246
 60202/100000: episode: 6144, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.787, mean reward: 0.379 [0.314, 0.474], mean action: 47.300 [15.000, 82.000], mean observation: 3.162 [-1.571, 10.380], loss: 1.118792, mae: 5.029946, mean_q: 5.226104
 60212/100000: episode: 6145, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.788, mean reward: 0.479 [0.388, 0.516], mean action: 46.500 [5.000, 95.000], mean observation: 3.154 [-1.150, 10.349], loss: 1.079334, mae: 5.030162, mean_q: 5.228003
 60222/100000: episode: 6146, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.886, mean reward: 0.389 [0.333, 0.437], mean action: 39.200 [29.000, 63.000], mean observation: 3.150 [-1.266, 10.183], loss: 1.341695, mae: 5.031306, mean_q: 5.227398
 60232/100000: episode: 6147, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.149, mean reward: 0.415 [0.351, 0.477], mean action: 53.500 [0.000, 97.000], mean observation: 3.157 [-1.479, 10.303], loss: 1.346967, mae: 5.031382, mean_q: 5.227793
 60242/100000: episode: 6148, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.161, mean reward: 0.416 [0.343, 0.565], mean action: 44.400 [18.000, 93.000], mean observation: 3.148 [-1.478, 10.403], loss: 1.137809, mae: 5.030498, mean_q: 5.229097
 60252/100000: episode: 6149, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.162, mean reward: 0.416 [0.353, 0.527], mean action: 55.300 [29.000, 93.000], mean observation: 3.176 [-1.238, 10.502], loss: 1.397771, mae: 5.031160, mean_q: 5.228787
 60262/100000: episode: 6150, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.398, mean reward: 0.440 [0.392, 0.493], mean action: 40.300 [20.000, 77.000], mean observation: 3.163 [-1.629, 10.382], loss: 1.527310, mae: 5.031624, mean_q: 5.233330
 60263/100000: episode: 6151, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 37.000 [37.000, 37.000], mean observation: 3.152 [-1.059, 10.118], loss: 1.503887, mae: 5.031931, mean_q: 5.236248
 60273/100000: episode: 6152, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.342, mean reward: 0.434 [0.348, 0.494], mean action: 41.100 [4.000, 90.000], mean observation: 3.153 [-1.481, 10.518], loss: 1.283343, mae: 5.030807, mean_q: 5.237854
 60283/100000: episode: 6153, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.031, mean reward: 0.403 [0.370, 0.539], mean action: 40.500 [17.000, 78.000], mean observation: 3.155 [-1.722, 10.329], loss: 1.331701, mae: 5.030955, mean_q: 5.239545
 60293/100000: episode: 6154, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.325, mean reward: 0.432 [0.410, 0.539], mean action: 46.100 [27.000, 100.000], mean observation: 3.171 [-0.746, 10.370], loss: 1.317363, mae: 5.030971, mean_q: 5.240943
 60303/100000: episode: 6155, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.293, mean reward: 0.429 [0.339, 0.501], mean action: 34.000 [10.000, 87.000], mean observation: 3.147 [-1.346, 10.416], loss: 1.365621, mae: 5.031099, mean_q: 5.243281
 60313/100000: episode: 6156, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.031, mean reward: 0.403 [0.333, 0.527], mean action: 51.900 [0.000, 101.000], mean observation: 3.149 [-1.493, 10.326], loss: 1.278240, mae: 5.031021, mean_q: 5.245039
 60323/100000: episode: 6157, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.246, mean reward: 0.425 [0.401, 0.497], mean action: 52.300 [25.000, 101.000], mean observation: 3.172 [-1.650, 10.376], loss: 1.180088, mae: 5.030428, mean_q: 5.244403
 60333/100000: episode: 6158, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.947, mean reward: 0.395 [0.354, 0.440], mean action: 35.000 [13.000, 58.000], mean observation: 3.150 [-2.307, 10.296], loss: 1.279146, mae: 5.030592, mean_q: 5.245685
 60343/100000: episode: 6159, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.971, mean reward: 0.397 [0.326, 0.572], mean action: 33.900 [6.000, 37.000], mean observation: 3.138 [-1.412, 10.116], loss: 1.584744, mae: 5.031631, mean_q: 5.247639
 60353/100000: episode: 6160, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.286, mean reward: 0.429 [0.358, 0.485], mean action: 37.600 [11.000, 67.000], mean observation: 3.157 [-0.854, 10.450], loss: 1.241035, mae: 5.030379, mean_q: 5.246522
 60363/100000: episode: 6161, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.003, mean reward: 0.400 [0.362, 0.439], mean action: 52.100 [28.000, 96.000], mean observation: 3.162 [-1.207, 10.494], loss: 1.387326, mae: 5.030628, mean_q: 5.244391
 60370/100000: episode: 6162, duration: 0.133s, episode steps: 7, steps per second: 53, episode reward: 12.630, mean reward: 1.804 [0.397, 10.000], mean action: 37.000 [37.000, 37.000], mean observation: 3.159 [-1.859, 10.273], loss: 1.204369, mae: 5.029717, mean_q: 5.242638
 60380/100000: episode: 6163, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.033, mean reward: 0.403 [0.339, 0.476], mean action: 40.500 [2.000, 81.000], mean observation: 3.165 [-1.988, 10.375], loss: 1.240423, mae: 5.029727, mean_q: 5.241700
 60390/100000: episode: 6164, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.848, mean reward: 0.385 [0.383, 0.396], mean action: 46.800 [21.000, 98.000], mean observation: 3.149 [-1.737, 10.296], loss: 1.217294, mae: 5.029763, mean_q: 5.240133
 60400/100000: episode: 6165, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 3.982, mean reward: 0.398 [0.315, 0.467], mean action: 54.100 [11.000, 94.000], mean observation: 3.137 [-1.767, 10.324], loss: 1.105243, mae: 5.029511, mean_q: 5.239354
 60410/100000: episode: 6166, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.983, mean reward: 0.398 [0.340, 0.449], mean action: 45.900 [10.000, 95.000], mean observation: 3.160 [-0.947, 10.325], loss: 1.104513, mae: 5.029523, mean_q: 5.237901
 60420/100000: episode: 6167, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.060, mean reward: 0.406 [0.306, 0.525], mean action: 28.100 [0.000, 37.000], mean observation: 3.159 [-1.133, 10.526], loss: 1.138940, mae: 5.029714, mean_q: 5.236293
 60430/100000: episode: 6168, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.194, mean reward: 0.419 [0.338, 0.505], mean action: 53.300 [37.000, 94.000], mean observation: 3.154 [-2.400, 10.236], loss: 1.012417, mae: 5.029149, mean_q: 5.232544
 60440/100000: episode: 6169, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.815, mean reward: 0.381 [0.299, 0.522], mean action: 44.900 [37.000, 72.000], mean observation: 3.148 [-1.364, 10.468], loss: 1.334258, mae: 5.030689, mean_q: 5.232173
 60450/100000: episode: 6170, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.250, mean reward: 0.425 [0.360, 0.510], mean action: 41.000 [37.000, 77.000], mean observation: 3.162 [-1.129, 10.460], loss: 1.311546, mae: 5.030925, mean_q: 5.233354
 60460/100000: episode: 6171, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.002, mean reward: 0.400 [0.396, 0.437], mean action: 49.800 [17.000, 101.000], mean observation: 3.152 [-1.376, 10.372], loss: 1.138728, mae: 5.030310, mean_q: 5.235334
 60470/100000: episode: 6172, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.321, mean reward: 0.432 [0.397, 0.502], mean action: 35.200 [8.000, 64.000], mean observation: 3.158 [-1.303, 10.364], loss: 1.013586, mae: 5.030062, mean_q: 5.235551
 60480/100000: episode: 6173, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.611, mean reward: 0.361 [0.298, 0.468], mean action: 50.500 [21.000, 100.000], mean observation: 3.151 [-1.374, 10.346], loss: 1.096077, mae: 5.030714, mean_q: 5.235770
 60490/100000: episode: 6174, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.255, mean reward: 0.426 [0.341, 0.550], mean action: 46.100 [29.000, 96.000], mean observation: 3.152 [-1.629, 10.265], loss: 1.327192, mae: 5.031821, mean_q: 5.237554
 60500/100000: episode: 6175, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.052, mean reward: 0.405 [0.336, 0.520], mean action: 42.700 [13.000, 88.000], mean observation: 3.159 [-1.392, 10.211], loss: 0.958702, mae: 5.030570, mean_q: 5.239593
 60510/100000: episode: 6176, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.542, mean reward: 0.454 [0.446, 0.524], mean action: 41.500 [14.000, 65.000], mean observation: 3.151 [-1.194, 10.220], loss: 1.460075, mae: 5.032906, mean_q: 5.240626
 60520/100000: episode: 6177, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.202, mean reward: 0.420 [0.392, 0.430], mean action: 40.800 [5.000, 97.000], mean observation: 3.162 [-1.611, 10.352], loss: 1.178547, mae: 5.031771, mean_q: 5.239642
 60530/100000: episode: 6178, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.847, mean reward: 0.385 [0.329, 0.467], mean action: 37.500 [15.000, 91.000], mean observation: 3.168 [-1.836, 10.237], loss: 1.188191, mae: 5.032047, mean_q: 5.241204
 60540/100000: episode: 6179, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.723, mean reward: 0.472 [0.470, 0.494], mean action: 40.500 [37.000, 72.000], mean observation: 3.145 [-1.800, 10.380], loss: 1.178060, mae: 5.032254, mean_q: 5.243696
 60550/100000: episode: 6180, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.981, mean reward: 0.398 [0.332, 0.467], mean action: 53.200 [17.000, 99.000], mean observation: 3.151 [-1.438, 10.395], loss: 1.292575, mae: 5.032746, mean_q: 5.240714
 60560/100000: episode: 6181, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.628, mean reward: 0.363 [0.274, 0.486], mean action: 46.000 [30.000, 77.000], mean observation: 3.167 [-1.581, 10.284], loss: 1.355164, mae: 5.033014, mean_q: 5.226945
 60570/100000: episode: 6182, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.020, mean reward: 0.402 [0.325, 0.488], mean action: 45.100 [14.000, 78.000], mean observation: 3.159 [-1.273, 10.255], loss: 1.451097, mae: 5.033141, mean_q: 5.216479
 60580/100000: episode: 6183, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.160, mean reward: 0.416 [0.325, 0.462], mean action: 38.200 [18.000, 50.000], mean observation: 3.150 [-2.476, 10.244], loss: 1.178448, mae: 5.031919, mean_q: 5.213225
 60590/100000: episode: 6184, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.801, mean reward: 0.480 [0.470, 0.559], mean action: 48.300 [37.000, 85.000], mean observation: 3.154 [-1.625, 10.231], loss: 1.217439, mae: 5.032134, mean_q: 5.211648
 60600/100000: episode: 6185, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.017, mean reward: 0.402 [0.341, 0.511], mean action: 43.800 [22.000, 101.000], mean observation: 3.156 [-1.088, 10.343], loss: 1.554027, mae: 5.033280, mean_q: 5.209880
 60610/100000: episode: 6186, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.166, mean reward: 0.417 [0.338, 0.493], mean action: 35.800 [13.000, 90.000], mean observation: 3.166 [-1.434, 10.340], loss: 1.265209, mae: 5.032055, mean_q: 5.209126
 60620/100000: episode: 6187, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.069, mean reward: 0.407 [0.369, 0.505], mean action: 50.800 [24.000, 99.000], mean observation: 3.156 [-1.347, 10.299], loss: 1.383342, mae: 5.032321, mean_q: 5.208977
 60630/100000: episode: 6188, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.565, mean reward: 0.456 [0.417, 0.466], mean action: 54.800 [11.000, 101.000], mean observation: 3.153 [-1.286, 10.340], loss: 0.990954, mae: 5.030800, mean_q: 5.208421
 60640/100000: episode: 6189, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.112, mean reward: 0.411 [0.354, 0.521], mean action: 38.200 [4.000, 85.000], mean observation: 3.164 [-1.287, 10.313], loss: 1.193850, mae: 5.031429, mean_q: 5.208116
 60650/100000: episode: 6190, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 3.917, mean reward: 0.392 [0.341, 0.454], mean action: 54.300 [17.000, 96.000], mean observation: 3.157 [-1.122, 10.459], loss: 1.513703, mae: 5.032672, mean_q: 5.207843
 60660/100000: episode: 6191, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.064, mean reward: 0.406 [0.339, 0.527], mean action: 37.400 [2.000, 73.000], mean observation: 3.153 [-1.662, 10.230], loss: 1.362451, mae: 5.032054, mean_q: 5.211757
 60670/100000: episode: 6192, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.483, mean reward: 0.448 [0.324, 0.544], mean action: 37.100 [5.000, 81.000], mean observation: 3.157 [-1.738, 10.467], loss: 1.410125, mae: 5.031869, mean_q: 5.215445
 60680/100000: episode: 6193, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.907, mean reward: 0.391 [0.301, 0.471], mean action: 42.600 [11.000, 84.000], mean observation: 3.158 [-1.286, 10.277], loss: 1.215909, mae: 5.030777, mean_q: 5.212056
 60690/100000: episode: 6194, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.020, mean reward: 0.402 [0.384, 0.455], mean action: 43.800 [37.000, 73.000], mean observation: 3.167 [-0.941, 10.279], loss: 1.028828, mae: 5.030132, mean_q: 5.209096
 60700/100000: episode: 6195, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.165, mean reward: 0.417 [0.356, 0.481], mean action: 38.900 [1.000, 84.000], mean observation: 3.151 [-1.403, 10.466], loss: 1.245297, mae: 5.031006, mean_q: 5.210281
 60710/100000: episode: 6196, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.471, mean reward: 0.347 [0.302, 0.415], mean action: 37.900 [17.000, 70.000], mean observation: 3.160 [-0.985, 10.403], loss: 1.684092, mae: 5.032680, mean_q: 5.210402
 60720/100000: episode: 6197, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.248, mean reward: 0.425 [0.345, 0.599], mean action: 39.300 [10.000, 77.000], mean observation: 3.161 [-1.194, 10.413], loss: 1.342882, mae: 5.031065, mean_q: 5.211565
 60730/100000: episode: 6198, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.138, mean reward: 0.414 [0.341, 0.508], mean action: 49.300 [17.000, 99.000], mean observation: 3.146 [-2.132, 10.444], loss: 1.450613, mae: 5.031192, mean_q: 5.212226
 60740/100000: episode: 6199, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.127, mean reward: 0.413 [0.355, 0.531], mean action: 49.100 [11.000, 86.000], mean observation: 3.168 [-1.158, 10.485], loss: 1.573539, mae: 5.031312, mean_q: 5.209977
 60750/100000: episode: 6200, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.027, mean reward: 0.403 [0.368, 0.460], mean action: 54.600 [37.000, 101.000], mean observation: 3.154 [-1.546, 10.344], loss: 0.914498, mae: 5.028591, mean_q: 5.209875
 60760/100000: episode: 6201, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.950, mean reward: 0.395 [0.317, 0.528], mean action: 29.600 [7.000, 45.000], mean observation: 3.151 [-1.050, 10.139], loss: 1.140141, mae: 5.029529, mean_q: 5.211890
 60770/100000: episode: 6202, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.113, mean reward: 0.411 [0.357, 0.485], mean action: 36.700 [23.000, 67.000], mean observation: 3.156 [-1.137, 10.318], loss: 1.295317, mae: 5.030354, mean_q: 5.214373
 60780/100000: episode: 6203, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.600, mean reward: 0.360 [0.289, 0.409], mean action: 61.000 [32.000, 101.000], mean observation: 3.155 [-1.636, 10.285], loss: 0.953758, mae: 5.029014, mean_q: 5.216511
 60790/100000: episode: 6204, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.411, mean reward: 0.441 [0.349, 0.548], mean action: 49.100 [31.000, 89.000], mean observation: 3.150 [-1.054, 10.272], loss: 0.997550, mae: 5.029330, mean_q: 5.218850
 60800/100000: episode: 6205, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.788, mean reward: 0.479 [0.470, 0.523], mean action: 34.000 [14.000, 37.000], mean observation: 3.169 [-1.163, 10.312], loss: 0.837680, mae: 5.029031, mean_q: 5.221569
 60810/100000: episode: 6206, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.930, mean reward: 0.393 [0.357, 0.454], mean action: 48.300 [36.000, 89.000], mean observation: 3.154 [-0.967, 10.265], loss: 1.264841, mae: 5.030995, mean_q: 5.223150
 60820/100000: episode: 6207, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.535, mean reward: 0.453 [0.453, 0.453], mean action: 47.100 [8.000, 92.000], mean observation: 3.158 [-2.099, 10.294], loss: 1.307216, mae: 5.031242, mean_q: 5.223267
 60830/100000: episode: 6208, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.372, mean reward: 0.437 [0.343, 0.582], mean action: 42.300 [6.000, 85.000], mean observation: 3.154 [-1.069, 10.326], loss: 1.040570, mae: 5.030208, mean_q: 5.225184
 60840/100000: episode: 6209, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.770, mean reward: 0.477 [0.475, 0.498], mean action: 48.800 [13.000, 101.000], mean observation: 3.144 [-1.530, 10.341], loss: 1.261303, mae: 5.031368, mean_q: 5.227268
 60850/100000: episode: 6210, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.745, mean reward: 0.375 [0.312, 0.441], mean action: 50.100 [8.000, 90.000], mean observation: 3.161 [-1.417, 10.382], loss: 1.509936, mae: 5.032319, mean_q: 5.228818
 60860/100000: episode: 6211, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.375, mean reward: 0.438 [0.406, 0.493], mean action: 44.600 [3.000, 93.000], mean observation: 3.151 [-1.656, 10.267], loss: 1.088073, mae: 5.030370, mean_q: 5.227025
 60870/100000: episode: 6212, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.294, mean reward: 0.429 [0.327, 0.492], mean action: 38.300 [14.000, 79.000], mean observation: 3.137 [-1.037, 10.235], loss: 1.502342, mae: 5.031909, mean_q: 5.225184
 60880/100000: episode: 6213, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.531, mean reward: 0.453 [0.442, 0.544], mean action: 39.600 [25.000, 72.000], mean observation: 3.159 [-1.586, 10.185], loss: 1.452980, mae: 5.031446, mean_q: 5.223493
 60890/100000: episode: 6214, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.182, mean reward: 0.418 [0.374, 0.584], mean action: 34.100 [14.000, 50.000], mean observation: 3.146 [-1.387, 10.352], loss: 0.999968, mae: 5.029357, mean_q: 5.224746
 60900/100000: episode: 6215, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.502, mean reward: 0.450 [0.420, 0.556], mean action: 33.100 [12.000, 45.000], mean observation: 3.146 [-1.397, 10.300], loss: 0.880716, mae: 5.028781, mean_q: 5.226990
 60910/100000: episode: 6216, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.635, mean reward: 0.464 [0.454, 0.506], mean action: 27.000 [9.000, 50.000], mean observation: 3.161 [-1.512, 10.350], loss: 1.365210, mae: 5.031044, mean_q: 5.226976
 60920/100000: episode: 6217, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.006, mean reward: 0.401 [0.321, 0.493], mean action: 39.700 [26.000, 72.000], mean observation: 3.157 [-1.283, 10.456], loss: 1.838095, mae: 5.032671, mean_q: 5.227417
 60930/100000: episode: 6218, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.144, mean reward: 0.414 [0.372, 0.594], mean action: 33.100 [4.000, 96.000], mean observation: 3.155 [-1.696, 10.288], loss: 1.138512, mae: 5.029565, mean_q: 5.225850
 60940/100000: episode: 6219, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.114, mean reward: 0.411 [0.369, 0.456], mean action: 31.900 [1.000, 37.000], mean observation: 3.168 [-1.343, 10.416], loss: 1.067601, mae: 5.029261, mean_q: 5.225740
 60950/100000: episode: 6220, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.619, mean reward: 0.362 [0.325, 0.461], mean action: 55.200 [37.000, 99.000], mean observation: 3.152 [-1.157, 10.376], loss: 1.408723, mae: 5.030548, mean_q: 5.222103
 60960/100000: episode: 6221, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.839, mean reward: 0.384 [0.284, 0.513], mean action: 37.700 [8.000, 96.000], mean observation: 3.158 [-1.231, 10.524], loss: 1.179182, mae: 5.029420, mean_q: 5.220871
 60970/100000: episode: 6222, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.047, mean reward: 0.405 [0.336, 0.557], mean action: 41.900 [37.000, 79.000], mean observation: 3.155 [-1.630, 10.494], loss: 1.135694, mae: 5.029477, mean_q: 5.221757
 60980/100000: episode: 6223, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.834, mean reward: 0.383 [0.345, 0.458], mean action: 40.900 [18.000, 98.000], mean observation: 3.159 [-1.693, 10.481], loss: 0.840196, mae: 5.028358, mean_q: 5.223962
 60982/100000: episode: 6224, duration: 0.057s, episode steps: 2, steps per second: 35, episode reward: 10.396, mean reward: 5.198 [0.396, 10.000], mean action: 21.000 [5.000, 37.000], mean observation: 3.162 [-1.167, 10.365], loss: 1.649384, mae: 5.031400, mean_q: 5.225598
 60992/100000: episode: 6225, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.379, mean reward: 0.438 [0.394, 0.578], mean action: 36.400 [21.000, 51.000], mean observation: 3.145 [-1.079, 10.328], loss: 0.969736, mae: 5.029132, mean_q: 5.226858
 61002/100000: episode: 6226, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.053, mean reward: 0.405 [0.386, 0.470], mean action: 44.800 [11.000, 82.000], mean observation: 3.143 [-1.271, 10.314], loss: 1.241001, mae: 5.030549, mean_q: 5.227238
 61012/100000: episode: 6227, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.339, mean reward: 0.434 [0.401, 0.580], mean action: 40.400 [5.000, 66.000], mean observation: 3.164 [-1.228, 10.288], loss: 1.312359, mae: 5.030862, mean_q: 5.227989
 61022/100000: episode: 6228, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.995, mean reward: 0.399 [0.357, 0.451], mean action: 42.100 [5.000, 97.000], mean observation: 3.159 [-1.320, 10.337], loss: 1.224964, mae: 5.030515, mean_q: 5.226369
 61032/100000: episode: 6229, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.137, mean reward: 0.414 [0.339, 0.448], mean action: 41.900 [25.000, 81.000], mean observation: 3.155 [-2.195, 10.342], loss: 1.533131, mae: 5.031371, mean_q: 5.223186
 61042/100000: episode: 6230, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.113, mean reward: 0.411 [0.365, 0.475], mean action: 48.700 [23.000, 88.000], mean observation: 3.169 [-1.404, 10.367], loss: 1.177062, mae: 5.029648, mean_q: 5.221428
 61052/100000: episode: 6231, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 3.838, mean reward: 0.384 [0.309, 0.466], mean action: 77.400 [16.000, 97.000], mean observation: 3.152 [-0.647, 10.473], loss: 1.219325, mae: 5.029565, mean_q: 5.223937
 61062/100000: episode: 6232, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.075, mean reward: 0.408 [0.296, 0.514], mean action: 71.000 [12.000, 93.000], mean observation: 3.153 [-1.512, 10.332], loss: 1.306337, mae: 5.029627, mean_q: 5.226477
 61072/100000: episode: 6233, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 4.021, mean reward: 0.402 [0.382, 0.424], mean action: 71.100 [10.000, 98.000], mean observation: 3.142 [-1.229, 10.304], loss: 1.294101, mae: 5.029507, mean_q: 5.228698
 61082/100000: episode: 6234, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.594, mean reward: 0.459 [0.407, 0.493], mean action: 71.700 [25.000, 92.000], mean observation: 3.161 [-1.259, 10.266], loss: 1.389030, mae: 5.029866, mean_q: 5.230472
 61092/100000: episode: 6235, duration: 0.106s, episode steps: 10, steps per second: 95, episode reward: 4.297, mean reward: 0.430 [0.411, 0.445], mean action: 79.200 [30.000, 94.000], mean observation: 3.150 [-0.894, 10.270], loss: 1.489636, mae: 5.029927, mean_q: 5.231862
 61102/100000: episode: 6236, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.345, mean reward: 0.434 [0.402, 0.466], mean action: 68.800 [5.000, 97.000], mean observation: 3.161 [-0.865, 10.462], loss: 1.025367, mae: 5.027966, mean_q: 5.232719
 61112/100000: episode: 6237, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.583, mean reward: 0.358 [0.317, 0.435], mean action: 55.600 [10.000, 88.000], mean observation: 3.143 [-1.928, 10.284], loss: 0.965051, mae: 5.028049, mean_q: 5.233638
 61122/100000: episode: 6238, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.284, mean reward: 0.428 [0.367, 0.479], mean action: 74.100 [12.000, 98.000], mean observation: 3.165 [-1.972, 10.297], loss: 1.419669, mae: 5.029675, mean_q: 5.234311
 61132/100000: episode: 6239, duration: 0.120s, episode steps: 10, steps per second: 84, episode reward: 4.827, mean reward: 0.483 [0.353, 0.499], mean action: 73.500 [6.000, 97.000], mean observation: 3.142 [-1.180, 10.385], loss: 1.301738, mae: 5.029139, mean_q: 5.232181
 61142/100000: episode: 6240, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.739, mean reward: 0.374 [0.347, 0.444], mean action: 78.400 [42.000, 100.000], mean observation: 3.153 [-1.403, 10.365], loss: 1.132472, mae: 5.028203, mean_q: 5.225636
 61152/100000: episode: 6241, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.096, mean reward: 0.410 [0.374, 0.469], mean action: 43.000 [10.000, 98.000], mean observation: 3.147 [-1.379, 10.367], loss: 1.209395, mae: 5.028373, mean_q: 5.225561
 61162/100000: episode: 6242, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.527, mean reward: 0.453 [0.357, 0.489], mean action: 35.400 [3.000, 99.000], mean observation: 3.154 [-1.366, 10.462], loss: 1.203639, mae: 5.028403, mean_q: 5.226533
 61172/100000: episode: 6243, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.273, mean reward: 0.427 [0.406, 0.475], mean action: 34.000 [6.000, 51.000], mean observation: 3.157 [-2.312, 10.478], loss: 1.144511, mae: 5.028409, mean_q: 5.228357
 61182/100000: episode: 6244, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.241, mean reward: 0.424 [0.402, 0.560], mean action: 43.900 [35.000, 88.000], mean observation: 3.161 [-1.237, 10.485], loss: 1.236025, mae: 5.028882, mean_q: 5.229165
 61192/100000: episode: 6245, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.157, mean reward: 0.416 [0.307, 0.530], mean action: 50.900 [9.000, 99.000], mean observation: 3.164 [-1.144, 10.470], loss: 1.005286, mae: 5.027969, mean_q: 5.230228
 61202/100000: episode: 6246, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.341, mean reward: 0.434 [0.356, 0.481], mean action: 42.100 [16.000, 70.000], mean observation: 3.164 [-1.604, 10.317], loss: 1.207612, mae: 5.029157, mean_q: 5.230528
 61212/100000: episode: 6247, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.334, mean reward: 0.433 [0.364, 0.481], mean action: 43.200 [6.000, 96.000], mean observation: 3.153 [-1.298, 10.356], loss: 1.338863, mae: 5.029998, mean_q: 5.230010
 61222/100000: episode: 6248, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.745, mean reward: 0.474 [0.474, 0.474], mean action: 46.600 [37.000, 68.000], mean observation: 3.167 [-1.416, 10.385], loss: 1.337870, mae: 5.029889, mean_q: 5.228497
 61232/100000: episode: 6249, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.964, mean reward: 0.396 [0.325, 0.508], mean action: 43.000 [10.000, 86.000], mean observation: 3.151 [-1.493, 10.377], loss: 1.413358, mae: 5.029643, mean_q: 5.227413
 61242/100000: episode: 6250, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.867, mean reward: 0.387 [0.324, 0.503], mean action: 42.700 [21.000, 84.000], mean observation: 3.152 [-1.484, 10.406], loss: 1.574814, mae: 5.029802, mean_q: 5.226133
 61252/100000: episode: 6251, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.852, mean reward: 0.385 [0.335, 0.543], mean action: 45.800 [3.000, 101.000], mean observation: 3.159 [-1.440, 10.251], loss: 1.302691, mae: 5.028459, mean_q: 5.225774
 61262/100000: episode: 6252, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.861, mean reward: 0.386 [0.341, 0.453], mean action: 42.400 [20.000, 83.000], mean observation: 3.157 [-1.683, 10.300], loss: 1.318550, mae: 5.028450, mean_q: 5.223268
 61272/100000: episode: 6253, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.985, mean reward: 0.498 [0.357, 0.536], mean action: 44.600 [26.000, 91.000], mean observation: 3.166 [-1.050, 10.345], loss: 1.271895, mae: 5.028231, mean_q: 5.219264
 61282/100000: episode: 6254, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.675, mean reward: 0.367 [0.339, 0.395], mean action: 41.900 [21.000, 83.000], mean observation: 3.155 [-1.707, 10.309], loss: 1.277491, mae: 5.028281, mean_q: 5.219519
 61292/100000: episode: 6255, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.118, mean reward: 0.412 [0.345, 0.550], mean action: 41.600 [26.000, 99.000], mean observation: 3.147 [-1.218, 10.381], loss: 1.530702, mae: 5.029235, mean_q: 5.216866
 61302/100000: episode: 6256, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.253, mean reward: 0.425 [0.361, 0.510], mean action: 44.700 [10.000, 96.000], mean observation: 3.167 [-1.592, 10.317], loss: 0.864069, mae: 5.026468, mean_q: 5.211057
 61312/100000: episode: 6257, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.254, mean reward: 0.425 [0.422, 0.460], mean action: 51.800 [17.000, 100.000], mean observation: 3.163 [-2.066, 10.377], loss: 1.305790, mae: 5.028235, mean_q: 5.209601
 61322/100000: episode: 6258, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.010, mean reward: 0.401 [0.301, 0.461], mean action: 39.900 [14.000, 71.000], mean observation: 3.159 [-2.077, 10.239], loss: 1.096083, mae: 5.027259, mean_q: 5.207442
 61332/100000: episode: 6259, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.044, mean reward: 0.404 [0.396, 0.449], mean action: 73.000 [29.000, 101.000], mean observation: 3.148 [-1.580, 10.233], loss: 1.320601, mae: 5.028037, mean_q: 5.206332
 61342/100000: episode: 6260, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 4.122, mean reward: 0.412 [0.392, 0.498], mean action: 76.500 [10.000, 93.000], mean observation: 3.145 [-1.715, 10.464], loss: 1.509966, mae: 5.028701, mean_q: 5.207747
 61352/100000: episode: 6261, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.455, mean reward: 0.445 [0.367, 0.464], mean action: 65.800 [15.000, 99.000], mean observation: 3.154 [-1.340, 10.471], loss: 1.479271, mae: 5.028187, mean_q: 5.208487
 61362/100000: episode: 6262, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.533, mean reward: 0.453 [0.339, 0.500], mean action: 63.500 [5.000, 88.000], mean observation: 3.154 [-1.274, 10.440], loss: 1.424911, mae: 5.027602, mean_q: 5.208998
 61372/100000: episode: 6263, duration: 0.116s, episode steps: 10, steps per second: 87, episode reward: 4.021, mean reward: 0.402 [0.344, 0.422], mean action: 69.800 [25.000, 88.000], mean observation: 3.151 [-1.143, 10.454], loss: 1.759826, mae: 5.028322, mean_q: 5.209794
 61382/100000: episode: 6264, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 4.055, mean reward: 0.405 [0.357, 0.424], mean action: 80.200 [40.000, 101.000], mean observation: 3.160 [-1.047, 10.260], loss: 1.384822, mae: 5.026280, mean_q: 5.210477
 61392/100000: episode: 6265, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.743, mean reward: 0.374 [0.334, 0.508], mean action: 68.200 [2.000, 88.000], mean observation: 3.159 [-1.171, 10.418], loss: 1.518025, mae: 5.026573, mean_q: 5.212064
 61402/100000: episode: 6266, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.868, mean reward: 0.387 [0.336, 0.569], mean action: 49.400 [3.000, 90.000], mean observation: 3.161 [-1.747, 10.488], loss: 1.580606, mae: 5.026300, mean_q: 5.211423
 61412/100000: episode: 6267, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.026, mean reward: 0.403 [0.355, 0.514], mean action: 37.700 [33.000, 50.000], mean observation: 3.159 [-1.392, 10.366], loss: 1.102908, mae: 5.023917, mean_q: 5.213982
 61418/100000: episode: 6268, duration: 0.112s, episode steps: 6, steps per second: 54, episode reward: 12.231, mean reward: 2.038 [0.419, 10.000], mean action: 44.333 [37.000, 81.000], mean observation: 3.139 [-1.224, 10.530], loss: 1.406092, mae: 5.024928, mean_q: 5.216938
 61428/100000: episode: 6269, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.986, mean reward: 0.399 [0.308, 0.476], mean action: 42.000 [10.000, 84.000], mean observation: 3.168 [-1.415, 10.366], loss: 1.408606, mae: 5.024760, mean_q: 5.221578
 61438/100000: episode: 6270, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.386, mean reward: 0.439 [0.413, 0.523], mean action: 32.400 [6.000, 37.000], mean observation: 3.150 [-1.317, 10.253], loss: 1.268339, mae: 5.023960, mean_q: 5.222733
 61448/100000: episode: 6271, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.565, mean reward: 0.457 [0.454, 0.483], mean action: 59.500 [37.000, 92.000], mean observation: 3.149 [-1.969, 10.269], loss: 1.344567, mae: 5.024277, mean_q: 5.224080
 61458/100000: episode: 6272, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.974, mean reward: 0.397 [0.334, 0.485], mean action: 50.300 [22.000, 97.000], mean observation: 3.147 [-1.459, 10.366], loss: 1.306689, mae: 5.023945, mean_q: 5.225492
 61468/100000: episode: 6273, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.932, mean reward: 0.393 [0.366, 0.568], mean action: 44.700 [17.000, 96.000], mean observation: 3.151 [-1.067, 10.220], loss: 1.532071, mae: 5.024653, mean_q: 5.224598
 61478/100000: episode: 6274, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.107, mean reward: 0.411 [0.392, 0.459], mean action: 38.000 [0.000, 95.000], mean observation: 3.149 [-1.033, 10.231], loss: 1.516940, mae: 5.024132, mean_q: 5.223225
 61488/100000: episode: 6275, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.698, mean reward: 0.370 [0.337, 0.460], mean action: 39.200 [4.000, 100.000], mean observation: 3.167 [-2.014, 10.230], loss: 1.312893, mae: 5.023198, mean_q: 5.221417
 61498/100000: episode: 6276, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.051, mean reward: 0.405 [0.323, 0.443], mean action: 36.400 [4.000, 79.000], mean observation: 3.154 [-1.784, 10.284], loss: 1.404559, mae: 5.023524, mean_q: 5.220042
 61508/100000: episode: 6277, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.871, mean reward: 0.387 [0.288, 0.511], mean action: 45.700 [17.000, 83.000], mean observation: 3.154 [-1.819, 10.447], loss: 1.308134, mae: 5.022784, mean_q: 5.220569
 61512/100000: episode: 6278, duration: 0.075s, episode steps: 4, steps per second: 53, episode reward: 11.369, mean reward: 2.842 [0.449, 10.000], mean action: 51.000 [30.000, 100.000], mean observation: 3.132 [-1.519, 10.284], loss: 0.979592, mae: 5.021303, mean_q: 5.221515
 61522/100000: episode: 6279, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.104, mean reward: 0.410 [0.361, 0.477], mean action: 48.300 [2.000, 88.000], mean observation: 3.156 [-1.431, 10.331], loss: 1.321832, mae: 5.022976, mean_q: 5.223020
 61528/100000: episode: 6280, duration: 0.093s, episode steps: 6, steps per second: 65, episode reward: 12.118, mean reward: 2.020 [0.354, 10.000], mean action: 50.167 [37.000, 76.000], mean observation: 3.166 [-1.420, 10.344], loss: 1.349438, mae: 5.023048, mean_q: 5.225120
 61538/100000: episode: 6281, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.265, mean reward: 0.427 [0.427, 0.427], mean action: 45.000 [37.000, 100.000], mean observation: 3.163 [-1.116, 10.265], loss: 0.944432, mae: 5.021429, mean_q: 5.225515
 61548/100000: episode: 6282, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.746, mean reward: 0.375 [0.304, 0.435], mean action: 42.700 [7.000, 95.000], mean observation: 3.158 [-1.251, 10.354], loss: 1.332524, mae: 5.023380, mean_q: 5.225183
 61558/100000: episode: 6283, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.057, mean reward: 0.406 [0.367, 0.450], mean action: 48.900 [20.000, 94.000], mean observation: 3.163 [-1.119, 10.232], loss: 1.185388, mae: 5.022841, mean_q: 5.225204
 61559/100000: episode: 6284, duration: 0.036s, episode steps: 1, steps per second: 28, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 37.000 [37.000, 37.000], mean observation: 3.129 [-0.777, 10.100], loss: 1.788231, mae: 5.025629, mean_q: 5.224352
 61569/100000: episode: 6285, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.703, mean reward: 0.370 [0.319, 0.421], mean action: 43.000 [27.000, 76.000], mean observation: 3.160 [-1.424, 10.332], loss: 1.163690, mae: 5.023149, mean_q: 5.225051
 61579/100000: episode: 6286, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.106, mean reward: 0.411 [0.304, 0.499], mean action: 43.300 [37.000, 84.000], mean observation: 3.154 [-1.460, 10.334], loss: 1.448047, mae: 5.024289, mean_q: 5.226780
 61589/100000: episode: 6287, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.603, mean reward: 0.360 [0.316, 0.423], mean action: 48.300 [37.000, 95.000], mean observation: 3.143 [-1.921, 10.310], loss: 1.036714, mae: 5.022554, mean_q: 5.223110
 61599/100000: episode: 6288, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.985, mean reward: 0.398 [0.321, 0.497], mean action: 36.200 [1.000, 88.000], mean observation: 3.155 [-1.453, 10.308], loss: 1.372941, mae: 5.023849, mean_q: 5.220447
 61609/100000: episode: 6289, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.598, mean reward: 0.360 [0.323, 0.457], mean action: 34.700 [9.000, 68.000], mean observation: 3.163 [-1.267, 10.377], loss: 1.035294, mae: 5.022466, mean_q: 5.218029
 61619/100000: episode: 6290, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.866, mean reward: 0.387 [0.344, 0.460], mean action: 42.700 [37.000, 74.000], mean observation: 3.155 [-1.474, 10.505], loss: 1.622523, mae: 5.024897, mean_q: 5.216575
 61629/100000: episode: 6291, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 3.719, mean reward: 0.372 [0.322, 0.513], mean action: 36.800 [35.000, 37.000], mean observation: 3.170 [-1.812, 10.339], loss: 0.997699, mae: 5.021958, mean_q: 5.217319
 61639/100000: episode: 6292, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.304, mean reward: 0.430 [0.370, 0.483], mean action: 52.000 [14.000, 99.000], mean observation: 3.147 [-2.226, 10.280], loss: 1.654307, mae: 5.024203, mean_q: 5.220441
 61649/100000: episode: 6293, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.131, mean reward: 0.413 [0.332, 0.456], mean action: 35.200 [0.000, 61.000], mean observation: 3.155 [-1.538, 10.330], loss: 1.279065, mae: 5.022587, mean_q: 5.223220
 61659/100000: episode: 6294, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.345, mean reward: 0.435 [0.317, 0.584], mean action: 47.800 [13.000, 91.000], mean observation: 3.150 [-1.590, 10.326], loss: 1.198632, mae: 5.022023, mean_q: 5.225835
 61669/100000: episode: 6295, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.953, mean reward: 0.395 [0.309, 0.477], mean action: 43.500 [15.000, 94.000], mean observation: 3.153 [-1.328, 10.344], loss: 0.996914, mae: 5.021144, mean_q: 5.227572
 61679/100000: episode: 6296, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.395, mean reward: 0.440 [0.311, 0.529], mean action: 55.300 [16.000, 88.000], mean observation: 3.169 [-1.026, 10.313], loss: 1.535170, mae: 5.023349, mean_q: 5.230629
 61689/100000: episode: 6297, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.471, mean reward: 0.447 [0.428, 0.506], mean action: 51.100 [37.000, 94.000], mean observation: 3.150 [-1.459, 10.251], loss: 1.573876, mae: 5.023141, mean_q: 5.230959
 61699/100000: episode: 6298, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.228, mean reward: 0.423 [0.359, 0.539], mean action: 40.200 [10.000, 75.000], mean observation: 3.158 [-1.456, 10.193], loss: 0.902623, mae: 5.020478, mean_q: 5.231433
 61709/100000: episode: 6299, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.621, mean reward: 0.362 [0.334, 0.441], mean action: 40.500 [13.000, 97.000], mean observation: 3.155 [-1.802, 10.422], loss: 1.197873, mae: 5.021755, mean_q: 5.233279
 61719/100000: episode: 6300, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.609, mean reward: 0.461 [0.297, 0.558], mean action: 43.300 [5.000, 89.000], mean observation: 3.158 [-1.444, 10.415], loss: 0.806946, mae: 5.020276, mean_q: 5.232738
 61729/100000: episode: 6301, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.141, mean reward: 0.414 [0.354, 0.511], mean action: 44.100 [9.000, 93.000], mean observation: 3.153 [-1.148, 10.186], loss: 1.574879, mae: 5.023370, mean_q: 5.225254
 61739/100000: episode: 6302, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.024, mean reward: 0.402 [0.356, 0.472], mean action: 36.300 [4.000, 71.000], mean observation: 3.154 [-1.328, 10.416], loss: 1.272075, mae: 5.021998, mean_q: 5.220356
 61749/100000: episode: 6303, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.828, mean reward: 0.383 [0.305, 0.517], mean action: 29.200 [2.000, 37.000], mean observation: 3.151 [-1.462, 10.301], loss: 1.203568, mae: 5.021810, mean_q: 5.221639
 61759/100000: episode: 6304, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.935, mean reward: 0.393 [0.379, 0.449], mean action: 37.600 [6.000, 94.000], mean observation: 3.169 [-1.580, 10.444], loss: 1.240655, mae: 5.021832, mean_q: 5.223058
 61769/100000: episode: 6305, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.422, mean reward: 0.442 [0.395, 0.530], mean action: 26.100 [4.000, 38.000], mean observation: 3.149 [-1.234, 10.307], loss: 1.094992, mae: 5.021538, mean_q: 5.221222
 61779/100000: episode: 6306, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.368, mean reward: 0.437 [0.432, 0.460], mean action: 63.500 [37.000, 96.000], mean observation: 3.156 [-1.354, 10.387], loss: 1.100661, mae: 5.021714, mean_q: 5.221732
 61789/100000: episode: 6307, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.152, mean reward: 0.415 [0.403, 0.456], mean action: 48.400 [29.000, 96.000], mean observation: 3.166 [-1.057, 10.402], loss: 1.232111, mae: 5.022430, mean_q: 5.222369
 61799/100000: episode: 6308, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 3.712, mean reward: 0.371 [0.293, 0.452], mean action: 31.700 [2.000, 74.000], mean observation: 3.152 [-1.593, 10.274], loss: 1.394286, mae: 5.022976, mean_q: 5.220840
 61809/100000: episode: 6309, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.938, mean reward: 0.394 [0.359, 0.458], mean action: 59.300 [25.000, 98.000], mean observation: 3.178 [-1.182, 10.324], loss: 1.266001, mae: 5.022267, mean_q: 5.220858
 61819/100000: episode: 6310, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.075, mean reward: 0.407 [0.363, 0.490], mean action: 62.700 [25.000, 88.000], mean observation: 3.156 [-0.967, 10.335], loss: 1.035331, mae: 5.021420, mean_q: 5.222384
 61829/100000: episode: 6311, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.677, mean reward: 0.368 [0.323, 0.461], mean action: 64.000 [11.000, 88.000], mean observation: 3.161 [-1.373, 10.518], loss: 1.333476, mae: 5.022613, mean_q: 5.224329
 61839/100000: episode: 6312, duration: 0.098s, episode steps: 10, steps per second: 102, episode reward: 3.758, mean reward: 0.376 [0.354, 0.507], mean action: 84.100 [58.000, 93.000], mean observation: 3.154 [-0.949, 10.262], loss: 0.999383, mae: 5.021295, mean_q: 5.225748
 61849/100000: episode: 6313, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.402, mean reward: 0.440 [0.440, 0.440], mean action: 74.100 [23.000, 95.000], mean observation: 3.152 [-1.591, 10.320], loss: 0.785979, mae: 5.020717, mean_q: 5.226460
 61859/100000: episode: 6314, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.576, mean reward: 0.358 [0.296, 0.442], mean action: 74.400 [19.000, 101.000], mean observation: 3.166 [-1.128, 10.329], loss: 1.366182, mae: 5.023158, mean_q: 5.228279
 61869/100000: episode: 6315, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.077, mean reward: 0.408 [0.296, 0.533], mean action: 51.500 [3.000, 88.000], mean observation: 3.158 [-2.072, 10.339], loss: 1.165172, mae: 5.022293, mean_q: 5.230007
 61879/100000: episode: 6316, duration: 0.114s, episode steps: 10, steps per second: 87, episode reward: 4.170, mean reward: 0.417 [0.372, 0.448], mean action: 70.100 [12.000, 93.000], mean observation: 3.150 [-2.153, 10.433], loss: 1.269844, mae: 5.022619, mean_q: 5.231866
 61889/100000: episode: 6317, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.006, mean reward: 0.401 [0.350, 0.516], mean action: 68.400 [2.000, 88.000], mean observation: 3.155 [-1.779, 10.353], loss: 1.154603, mae: 5.022266, mean_q: 5.234071
 61899/100000: episode: 6318, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 4.536, mean reward: 0.454 [0.406, 0.486], mean action: 75.200 [22.000, 101.000], mean observation: 3.151 [-1.683, 10.425], loss: 1.292955, mae: 5.022726, mean_q: 5.236283
 61909/100000: episode: 6319, duration: 0.098s, episode steps: 10, steps per second: 102, episode reward: 3.990, mean reward: 0.399 [0.379, 0.576], mean action: 82.500 [22.000, 101.000], mean observation: 3.157 [-1.040, 10.449], loss: 1.139070, mae: 5.022025, mean_q: 5.235761
 61919/100000: episode: 6320, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.170, mean reward: 0.417 [0.369, 0.474], mean action: 60.900 [6.000, 88.000], mean observation: 3.160 [-1.825, 10.461], loss: 1.067284, mae: 5.021710, mean_q: 5.232542
 61929/100000: episode: 6321, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.893, mean reward: 0.389 [0.341, 0.437], mean action: 78.800 [8.000, 91.000], mean observation: 3.150 [-1.072, 10.273], loss: 1.517774, mae: 5.023377, mean_q: 5.228764
 61939/100000: episode: 6322, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.038, mean reward: 0.404 [0.359, 0.484], mean action: 61.400 [4.000, 88.000], mean observation: 3.167 [-1.449, 10.393], loss: 1.362826, mae: 5.022801, mean_q: 5.225345
 61949/100000: episode: 6323, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.129, mean reward: 0.413 [0.383, 0.441], mean action: 75.200 [37.000, 98.000], mean observation: 3.176 [-1.206, 10.355], loss: 1.147788, mae: 5.021877, mean_q: 5.221688
 61950/100000: episode: 6324, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 37.000 [37.000, 37.000], mean observation: 3.149 [-0.979, 10.100], loss: 1.422991, mae: 5.022747, mean_q: 5.220448
 61960/100000: episode: 6325, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.993, mean reward: 0.399 [0.320, 0.492], mean action: 42.500 [11.000, 96.000], mean observation: 3.156 [-1.589, 10.426], loss: 0.995493, mae: 5.021228, mean_q: 5.220461
 61970/100000: episode: 6326, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.416, mean reward: 0.442 [0.366, 0.528], mean action: 19.200 [4.000, 100.000], mean observation: 3.162 [-1.607, 10.499], loss: 1.113040, mae: 5.021934, mean_q: 5.222285
 61980/100000: episode: 6327, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.246, mean reward: 0.425 [0.347, 0.527], mean action: 29.300 [4.000, 83.000], mean observation: 3.153 [-1.353, 10.274], loss: 1.556389, mae: 5.023721, mean_q: 5.221541
 61990/100000: episode: 6328, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.999, mean reward: 0.500 [0.500, 0.500], mean action: 38.600 [2.000, 72.000], mean observation: 3.170 [-0.761, 10.320], loss: 1.195629, mae: 5.021923, mean_q: 5.216427
 62000/100000: episode: 6329, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.922, mean reward: 0.392 [0.322, 0.482], mean action: 32.800 [4.000, 73.000], mean observation: 3.151 [-1.534, 10.278], loss: 1.476740, mae: 5.022584, mean_q: 5.213008
 62010/100000: episode: 6330, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.174, mean reward: 0.417 [0.361, 0.525], mean action: 22.000 [2.000, 74.000], mean observation: 3.152 [-2.109, 10.472], loss: 1.243494, mae: 5.021293, mean_q: 5.209894
 62018/100000: episode: 6331, duration: 0.167s, episode steps: 8, steps per second: 48, episode reward: 13.058, mean reward: 1.632 [0.365, 10.000], mean action: 21.375 [0.000, 67.000], mean observation: 3.163 [-1.316, 10.369], loss: 1.405554, mae: 5.022091, mean_q: 5.209784
 62028/100000: episode: 6332, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.355, mean reward: 0.435 [0.321, 0.504], mean action: 22.800 [2.000, 66.000], mean observation: 3.159 [-1.495, 10.373], loss: 1.411758, mae: 5.022202, mean_q: 5.207988
 62038/100000: episode: 6333, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.961, mean reward: 0.396 [0.337, 0.516], mean action: 46.100 [11.000, 99.000], mean observation: 3.147 [-2.176, 10.367], loss: 1.231226, mae: 5.021581, mean_q: 5.205888
 62048/100000: episode: 6334, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.293, mean reward: 0.429 [0.363, 0.539], mean action: 20.800 [11.000, 64.000], mean observation: 3.156 [-1.536, 10.334], loss: 1.243731, mae: 5.021667, mean_q: 5.205870
 62058/100000: episode: 6335, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.393, mean reward: 0.439 [0.438, 0.452], mean action: 76.100 [31.000, 95.000], mean observation: 3.164 [-1.347, 10.414], loss: 0.978955, mae: 5.020554, mean_q: 5.206059
 62068/100000: episode: 6336, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.829, mean reward: 0.383 [0.361, 0.419], mean action: 68.400 [4.000, 88.000], mean observation: 3.159 [-1.453, 10.261], loss: 1.211894, mae: 5.021610, mean_q: 5.207803
 62078/100000: episode: 6337, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.993, mean reward: 0.399 [0.341, 0.500], mean action: 77.000 [36.000, 99.000], mean observation: 3.176 [-0.959, 10.343], loss: 1.230902, mae: 5.021888, mean_q: 5.210507
 62088/100000: episode: 6338, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.356, mean reward: 0.436 [0.397, 0.451], mean action: 67.000 [12.000, 95.000], mean observation: 3.162 [-1.376, 10.357], loss: 1.390867, mae: 5.022504, mean_q: 5.213362
 62098/100000: episode: 6339, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.520, mean reward: 0.352 [0.310, 0.435], mean action: 75.200 [34.000, 92.000], mean observation: 3.153 [-1.388, 10.333], loss: 0.967035, mae: 5.020780, mean_q: 5.215553
 62108/100000: episode: 6340, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.226, mean reward: 0.423 [0.414, 0.436], mean action: 56.100 [27.000, 90.000], mean observation: 3.151 [-1.297, 10.404], loss: 1.471359, mae: 5.022753, mean_q: 5.218014
 62118/100000: episode: 6341, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.964, mean reward: 0.396 [0.365, 0.444], mean action: 74.300 [11.000, 97.000], mean observation: 3.147 [-1.074, 10.343], loss: 1.023698, mae: 5.020955, mean_q: 5.219800
 62128/100000: episode: 6342, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 5.223, mean reward: 0.522 [0.522, 0.522], mean action: 80.200 [43.000, 88.000], mean observation: 3.162 [-1.037, 10.409], loss: 1.369060, mae: 5.022004, mean_q: 5.221007
 62138/100000: episode: 6343, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 5.006, mean reward: 0.501 [0.501, 0.501], mean action: 67.600 [22.000, 88.000], mean observation: 3.164 [-0.564, 10.450], loss: 1.233329, mae: 5.021237, mean_q: 5.218070
 62148/100000: episode: 6344, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.563, mean reward: 0.456 [0.424, 0.531], mean action: 60.300 [11.000, 95.000], mean observation: 3.170 [-1.124, 10.264], loss: 1.278261, mae: 5.021257, mean_q: 5.217596
 62158/100000: episode: 6345, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.819, mean reward: 0.482 [0.381, 0.574], mean action: 69.600 [26.000, 88.000], mean observation: 3.158 [-1.662, 10.331], loss: 1.441483, mae: 5.021735, mean_q: 5.218738
 62168/100000: episode: 6346, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.106, mean reward: 0.411 [0.361, 0.540], mean action: 53.900 [0.000, 88.000], mean observation: 3.171 [-1.045, 10.376], loss: 0.965032, mae: 5.019712, mean_q: 5.221284
 62178/100000: episode: 6347, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.043, mean reward: 0.404 [0.388, 0.415], mean action: 79.600 [53.000, 95.000], mean observation: 3.159 [-1.422, 10.282], loss: 1.213183, mae: 5.020553, mean_q: 5.224235
 62188/100000: episode: 6348, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.469, mean reward: 0.347 [0.268, 0.478], mean action: 59.500 [7.000, 88.000], mean observation: 3.158 [-1.572, 10.256], loss: 1.201305, mae: 5.020382, mean_q: 5.226491
 62198/100000: episode: 6349, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.888, mean reward: 0.489 [0.380, 0.549], mean action: 66.900 [8.000, 88.000], mean observation: 3.161 [-1.262, 10.423], loss: 0.927988, mae: 5.019475, mean_q: 5.228230
 62208/100000: episode: 6350, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.526, mean reward: 0.453 [0.414, 0.501], mean action: 60.200 [1.000, 88.000], mean observation: 3.148 [-1.507, 10.297], loss: 1.226191, mae: 5.020728, mean_q: 5.226962
 62218/100000: episode: 6351, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.344, mean reward: 0.434 [0.433, 0.444], mean action: 63.800 [24.000, 88.000], mean observation: 3.143 [-1.823, 10.280], loss: 1.523135, mae: 5.022151, mean_q: 5.226072
 62228/100000: episode: 6352, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.397, mean reward: 0.440 [0.344, 0.512], mean action: 72.600 [25.000, 97.000], mean observation: 3.161 [-1.880, 10.229], loss: 0.802661, mae: 5.019106, mean_q: 5.227489
 62238/100000: episode: 6353, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.949, mean reward: 0.395 [0.359, 0.525], mean action: 63.800 [10.000, 91.000], mean observation: 3.155 [-1.755, 10.301], loss: 1.301257, mae: 5.021121, mean_q: 5.229772
 62248/100000: episode: 6354, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.329, mean reward: 0.433 [0.418, 0.494], mean action: 75.800 [21.000, 88.000], mean observation: 3.134 [-0.829, 10.215], loss: 1.470650, mae: 5.022069, mean_q: 5.231249
 62258/100000: episode: 6355, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 3.852, mean reward: 0.385 [0.371, 0.399], mean action: 74.700 [20.000, 100.000], mean observation: 3.143 [-1.173, 10.275], loss: 1.020572, mae: 5.020113, mean_q: 5.232533
 62268/100000: episode: 6356, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.407, mean reward: 0.441 [0.395, 0.592], mean action: 70.700 [17.000, 89.000], mean observation: 3.149 [-1.733, 10.354], loss: 1.161818, mae: 5.020515, mean_q: 5.233883
 62278/100000: episode: 6357, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.131, mean reward: 0.413 [0.394, 0.501], mean action: 74.700 [34.000, 88.000], mean observation: 3.150 [-0.916, 10.489], loss: 1.421669, mae: 5.021706, mean_q: 5.235179
 62288/100000: episode: 6358, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.990, mean reward: 0.399 [0.323, 0.451], mean action: 67.800 [37.000, 88.000], mean observation: 3.162 [-1.069, 10.232], loss: 1.606828, mae: 5.021905, mean_q: 5.236991
 62298/100000: episode: 6359, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.031, mean reward: 0.403 [0.356, 0.552], mean action: 69.400 [21.000, 88.000], mean observation: 3.155 [-0.941, 10.419], loss: 1.474561, mae: 5.020761, mean_q: 5.235537
 62308/100000: episode: 6360, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.017, mean reward: 0.402 [0.342, 0.448], mean action: 77.600 [36.000, 88.000], mean observation: 3.177 [-0.641, 10.370], loss: 1.268650, mae: 5.019506, mean_q: 5.234668
 62318/100000: episode: 6361, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.834, mean reward: 0.383 [0.349, 0.439], mean action: 74.500 [13.000, 88.000], mean observation: 3.163 [-1.869, 10.252], loss: 1.005359, mae: 5.018283, mean_q: 5.234576
 62328/100000: episode: 6362, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.427, mean reward: 0.443 [0.395, 0.487], mean action: 70.900 [16.000, 88.000], mean observation: 3.162 [-1.596, 10.425], loss: 1.004071, mae: 5.018557, mean_q: 5.232643
 62338/100000: episode: 6363, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.822, mean reward: 0.382 [0.331, 0.422], mean action: 79.800 [4.000, 97.000], mean observation: 3.158 [-1.605, 10.388], loss: 1.090390, mae: 5.019169, mean_q: 5.233366
 62348/100000: episode: 6364, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 3.946, mean reward: 0.395 [0.381, 0.426], mean action: 85.500 [75.000, 93.000], mean observation: 3.145 [-1.574, 10.413], loss: 1.373515, mae: 5.020621, mean_q: 5.230657
 62358/100000: episode: 6365, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 4.068, mean reward: 0.407 [0.397, 0.444], mean action: 73.200 [31.000, 100.000], mean observation: 3.152 [-1.040, 10.289], loss: 1.554308, mae: 5.021318, mean_q: 5.230003
 62368/100000: episode: 6366, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.830, mean reward: 0.383 [0.312, 0.486], mean action: 68.400 [19.000, 88.000], mean observation: 3.142 [-1.268, 10.338], loss: 1.006570, mae: 5.018946, mean_q: 5.231518
 62378/100000: episode: 6367, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.954, mean reward: 0.395 [0.375, 0.458], mean action: 63.700 [1.000, 92.000], mean observation: 3.142 [-1.625, 10.291], loss: 1.696667, mae: 5.021483, mean_q: 5.233812
 62388/100000: episode: 6368, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.760, mean reward: 0.376 [0.353, 0.453], mean action: 74.300 [41.000, 88.000], mean observation: 3.155 [-2.135, 10.317], loss: 1.490430, mae: 5.020709, mean_q: 5.235778
 62398/100000: episode: 6369, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.393, mean reward: 0.439 [0.362, 0.554], mean action: 70.000 [11.000, 88.000], mean observation: 3.151 [-1.419, 10.303], loss: 1.493894, mae: 5.020546, mean_q: 5.234475
 62408/100000: episode: 6370, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.044, mean reward: 0.404 [0.348, 0.486], mean action: 79.700 [41.000, 89.000], mean observation: 3.149 [-1.382, 10.259], loss: 1.518121, mae: 5.020329, mean_q: 5.234280
 62418/100000: episode: 6371, duration: 0.109s, episode steps: 10, steps per second: 91, episode reward: 3.879, mean reward: 0.388 [0.354, 0.455], mean action: 75.800 [35.000, 101.000], mean observation: 3.160 [-0.786, 10.421], loss: 1.430777, mae: 5.019460, mean_q: 5.234960
 62428/100000: episode: 6372, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.259, mean reward: 0.426 [0.385, 0.579], mean action: 69.600 [3.000, 88.000], mean observation: 3.149 [-1.069, 10.383], loss: 1.615227, mae: 5.020038, mean_q: 5.234500
 62438/100000: episode: 6373, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.828, mean reward: 0.383 [0.369, 0.446], mean action: 74.700 [33.000, 88.000], mean observation: 3.168 [-1.599, 10.360], loss: 1.320961, mae: 5.018829, mean_q: 5.233121
 62448/100000: episode: 6374, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.044, mean reward: 0.404 [0.294, 0.466], mean action: 66.900 [18.000, 88.000], mean observation: 3.154 [-1.004, 10.269], loss: 1.113111, mae: 5.017750, mean_q: 5.233533
 62458/100000: episode: 6375, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.279, mean reward: 0.428 [0.410, 0.530], mean action: 81.500 [61.000, 88.000], mean observation: 3.145 [-1.336, 10.299], loss: 1.448731, mae: 5.019233, mean_q: 5.229526
 62468/100000: episode: 6376, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.146, mean reward: 0.415 [0.393, 0.435], mean action: 64.100 [26.000, 88.000], mean observation: 3.157 [-1.160, 10.333], loss: 1.004013, mae: 5.017426, mean_q: 5.223571
 62478/100000: episode: 6377, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.979, mean reward: 0.398 [0.317, 0.437], mean action: 65.800 [0.000, 96.000], mean observation: 3.143 [-1.527, 10.319], loss: 1.252590, mae: 5.018690, mean_q: 5.222265
 62488/100000: episode: 6378, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.444, mean reward: 0.444 [0.347, 0.527], mean action: 68.700 [21.000, 100.000], mean observation: 3.147 [-2.227, 10.369], loss: 1.104654, mae: 5.018098, mean_q: 5.220790
 62498/100000: episode: 6379, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.372, mean reward: 0.337 [0.318, 0.403], mean action: 76.100 [20.000, 92.000], mean observation: 3.156 [-1.503, 10.267], loss: 1.196293, mae: 5.018486, mean_q: 5.221188
 62508/100000: episode: 6380, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 13.955, mean reward: 1.395 [0.390, 10.000], mean action: 73.700 [7.000, 90.000], mean observation: 3.153 [-1.767, 10.311], loss: 1.587550, mae: 5.020129, mean_q: 5.221805
 62518/100000: episode: 6381, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.270, mean reward: 0.427 [0.414, 0.442], mean action: 76.000 [2.000, 98.000], mean observation: 3.153 [-1.021, 10.462], loss: 1.401583, mae: 5.019226, mean_q: 5.219743
 62528/100000: episode: 6382, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.077, mean reward: 0.408 [0.333, 0.494], mean action: 53.400 [2.000, 100.000], mean observation: 3.153 [-1.647, 10.271], loss: 0.942694, mae: 5.017194, mean_q: 5.217583
 62538/100000: episode: 6383, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.424, mean reward: 0.442 [0.356, 0.575], mean action: 48.600 [37.000, 90.000], mean observation: 3.149 [-1.491, 10.279], loss: 1.233093, mae: 5.018124, mean_q: 5.218925
 62548/100000: episode: 6384, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.992, mean reward: 0.399 [0.381, 0.438], mean action: 47.400 [8.000, 90.000], mean observation: 3.148 [-1.250, 10.420], loss: 1.450501, mae: 5.019090, mean_q: 5.219419
 62558/100000: episode: 6385, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.807, mean reward: 0.381 [0.287, 0.494], mean action: 45.900 [9.000, 88.000], mean observation: 3.150 [-1.432, 10.474], loss: 1.043646, mae: 5.017518, mean_q: 5.217575
 62568/100000: episode: 6386, duration: 0.107s, episode steps: 10, steps per second: 94, episode reward: 4.032, mean reward: 0.403 [0.307, 0.484], mean action: 76.800 [8.000, 88.000], mean observation: 3.163 [-1.009, 10.321], loss: 1.343995, mae: 5.019026, mean_q: 5.218910
 62578/100000: episode: 6387, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.901, mean reward: 0.390 [0.360, 0.454], mean action: 56.000 [24.000, 90.000], mean observation: 3.154 [-1.452, 10.339], loss: 1.517276, mae: 5.019525, mean_q: 5.217478
 62588/100000: episode: 6388, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.000, mean reward: 0.400 [0.359, 0.452], mean action: 49.400 [21.000, 101.000], mean observation: 3.151 [-1.878, 10.227], loss: 0.997698, mae: 5.017313, mean_q: 5.218474
 62598/100000: episode: 6389, duration: 0.150s, episode steps: 10, steps per second: 66, episode reward: 3.992, mean reward: 0.399 [0.326, 0.506], mean action: 52.400 [37.000, 83.000], mean observation: 3.161 [-1.431, 10.388], loss: 1.339108, mae: 5.018748, mean_q: 5.218120
 62608/100000: episode: 6390, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.918, mean reward: 0.392 [0.338, 0.439], mean action: 42.600 [2.000, 80.000], mean observation: 3.165 [-1.260, 10.299], loss: 1.211646, mae: 5.018117, mean_q: 5.216596
 62618/100000: episode: 6391, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.994, mean reward: 0.399 [0.369, 0.478], mean action: 49.600 [37.000, 95.000], mean observation: 3.148 [-1.607, 10.495], loss: 1.344981, mae: 5.018548, mean_q: 5.215287
 62628/100000: episode: 6392, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.371, mean reward: 0.437 [0.391, 0.518], mean action: 27.700 [4.000, 83.000], mean observation: 3.155 [-1.704, 10.539], loss: 1.105987, mae: 5.017731, mean_q: 5.214517
 62638/100000: episode: 6393, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.101, mean reward: 0.410 [0.330, 0.464], mean action: 40.000 [4.000, 97.000], mean observation: 3.149 [-1.894, 10.306], loss: 1.061908, mae: 5.017719, mean_q: 5.213525
 62648/100000: episode: 6394, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.996, mean reward: 0.400 [0.279, 0.510], mean action: 20.600 [4.000, 92.000], mean observation: 3.155 [-2.204, 10.357], loss: 1.356333, mae: 5.018888, mean_q: 5.212574
 62658/100000: episode: 6395, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.099, mean reward: 0.410 [0.299, 0.524], mean action: 17.400 [1.000, 79.000], mean observation: 3.150 [-1.753, 10.450], loss: 1.214645, mae: 5.018352, mean_q: 5.213865
 62668/100000: episode: 6396, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.380, mean reward: 0.438 [0.377, 0.502], mean action: 37.300 [4.000, 99.000], mean observation: 3.160 [-2.339, 10.297], loss: 1.480507, mae: 5.019521, mean_q: 5.214335
 62678/100000: episode: 6397, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.422, mean reward: 0.442 [0.334, 0.596], mean action: 35.000 [7.000, 91.000], mean observation: 3.164 [-1.722, 10.323], loss: 1.032930, mae: 5.017762, mean_q: 5.216241
 62688/100000: episode: 6398, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.940, mean reward: 0.394 [0.311, 0.574], mean action: 46.100 [11.000, 98.000], mean observation: 3.167 [-1.456, 10.416], loss: 1.014233, mae: 5.018006, mean_q: 5.218187
 62698/100000: episode: 6399, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.911, mean reward: 0.391 [0.350, 0.524], mean action: 38.400 [29.000, 59.000], mean observation: 3.172 [-1.240, 10.499], loss: 1.063934, mae: 5.018207, mean_q: 5.218752
 62708/100000: episode: 6400, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.256, mean reward: 0.426 [0.373, 0.475], mean action: 49.600 [17.000, 84.000], mean observation: 3.149 [-1.301, 10.434], loss: 1.475413, mae: 5.019986, mean_q: 5.218326
 62718/100000: episode: 6401, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.143, mean reward: 0.414 [0.350, 0.502], mean action: 43.000 [37.000, 67.000], mean observation: 3.153 [-1.341, 10.320], loss: 1.407395, mae: 5.019337, mean_q: 5.217129
 62728/100000: episode: 6402, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.894, mean reward: 0.389 [0.354, 0.435], mean action: 40.800 [30.000, 74.000], mean observation: 3.145 [-0.913, 10.391], loss: 1.503183, mae: 5.019307, mean_q: 5.216832
 62738/100000: episode: 6403, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.937, mean reward: 0.394 [0.317, 0.480], mean action: 38.200 [2.000, 79.000], mean observation: 3.162 [-1.086, 10.362], loss: 1.131527, mae: 5.017570, mean_q: 5.216324
 62748/100000: episode: 6404, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.434, mean reward: 0.443 [0.388, 0.477], mean action: 38.500 [0.000, 100.000], mean observation: 3.169 [-1.776, 10.327], loss: 0.996941, mae: 5.017139, mean_q: 5.215690
 62758/100000: episode: 6405, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.890, mean reward: 0.389 [0.348, 0.473], mean action: 45.500 [19.000, 90.000], mean observation: 3.158 [-1.659, 10.224], loss: 0.822367, mae: 5.016776, mean_q: 5.216447
 62768/100000: episode: 6406, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.190, mean reward: 0.419 [0.416, 0.435], mean action: 48.200 [37.000, 97.000], mean observation: 3.140 [-1.915, 10.307], loss: 1.071107, mae: 5.018212, mean_q: 5.217873
 62778/100000: episode: 6407, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.523, mean reward: 0.452 [0.442, 0.466], mean action: 42.400 [4.000, 97.000], mean observation: 3.151 [-1.700, 10.239], loss: 1.058274, mae: 5.018527, mean_q: 5.219426
 62787/100000: episode: 6408, duration: 0.143s, episode steps: 9, steps per second: 63, episode reward: 13.092, mean reward: 1.455 [0.334, 10.000], mean action: 46.556 [32.000, 74.000], mean observation: 3.149 [-1.738, 10.235], loss: 1.351964, mae: 5.020163, mean_q: 5.221021
 62797/100000: episode: 6409, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.093, mean reward: 0.409 [0.329, 0.468], mean action: 37.000 [7.000, 92.000], mean observation: 3.142 [-1.848, 10.359], loss: 1.345934, mae: 5.019981, mean_q: 5.220897
 62807/100000: episode: 6410, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.349, mean reward: 0.435 [0.423, 0.507], mean action: 38.500 [3.000, 73.000], mean observation: 3.152 [-1.573, 10.315], loss: 1.206187, mae: 5.019599, mean_q: 5.220654
 62817/100000: episode: 6411, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.876, mean reward: 0.388 [0.334, 0.437], mean action: 40.400 [3.000, 74.000], mean observation: 3.161 [-1.599, 10.388], loss: 0.874898, mae: 5.018342, mean_q: 5.220922
 62827/100000: episode: 6412, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.794, mean reward: 0.379 [0.367, 0.421], mean action: 51.800 [30.000, 94.000], mean observation: 3.152 [-1.532, 10.334], loss: 1.282327, mae: 5.020322, mean_q: 5.222148
 62837/100000: episode: 6413, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.257, mean reward: 0.426 [0.350, 0.538], mean action: 29.200 [5.000, 49.000], mean observation: 3.155 [-1.285, 10.390], loss: 1.043085, mae: 5.019651, mean_q: 5.222258
 62847/100000: episode: 6414, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.944, mean reward: 0.394 [0.306, 0.480], mean action: 31.000 [0.000, 47.000], mean observation: 3.151 [-1.296, 10.376], loss: 1.247998, mae: 5.020467, mean_q: 5.223281
 62857/100000: episode: 6415, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.039, mean reward: 0.404 [0.365, 0.514], mean action: 39.200 [12.000, 86.000], mean observation: 3.148 [-1.584, 10.259], loss: 1.269913, mae: 5.020978, mean_q: 5.224263
 62867/100000: episode: 6416, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.172, mean reward: 0.417 [0.338, 0.491], mean action: 48.600 [0.000, 96.000], mean observation: 3.160 [-0.929, 10.325], loss: 1.350026, mae: 5.021678, mean_q: 5.224362
 62877/100000: episode: 6417, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.030, mean reward: 0.403 [0.354, 0.455], mean action: 37.700 [11.000, 67.000], mean observation: 3.152 [-1.020, 10.263], loss: 1.291307, mae: 5.021369, mean_q: 5.226133
 62887/100000: episode: 6418, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.150, mean reward: 0.415 [0.357, 0.428], mean action: 35.800 [12.000, 78.000], mean observation: 3.169 [-1.504, 10.387], loss: 1.142401, mae: 5.020770, mean_q: 5.229775
 62897/100000: episode: 6419, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.917, mean reward: 0.392 [0.309, 0.486], mean action: 56.300 [37.000, 98.000], mean observation: 3.156 [-1.382, 10.352], loss: 1.323590, mae: 5.021501, mean_q: 5.232270
 62907/100000: episode: 6420, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 3.891, mean reward: 0.389 [0.299, 0.465], mean action: 47.700 [15.000, 101.000], mean observation: 3.153 [-0.830, 10.410], loss: 1.241444, mae: 5.021317, mean_q: 5.234323
 62917/100000: episode: 6421, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.177, mean reward: 0.418 [0.352, 0.532], mean action: 47.000 [21.000, 96.000], mean observation: 3.141 [-1.343, 10.257], loss: 1.129831, mae: 5.021081, mean_q: 5.236571
 62927/100000: episode: 6422, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.163, mean reward: 0.416 [0.333, 0.501], mean action: 49.600 [6.000, 100.000], mean observation: 3.160 [-0.969, 10.557], loss: 1.271156, mae: 5.021710, mean_q: 5.237046
 62937/100000: episode: 6423, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.110, mean reward: 0.411 [0.311, 0.535], mean action: 41.600 [37.000, 73.000], mean observation: 3.164 [-0.836, 10.443], loss: 1.251911, mae: 5.021490, mean_q: 5.237348
 62947/100000: episode: 6424, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.759, mean reward: 0.376 [0.309, 0.481], mean action: 49.000 [1.000, 93.000], mean observation: 3.158 [-1.285, 10.422], loss: 1.210742, mae: 5.021260, mean_q: 5.238931
 62957/100000: episode: 6425, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.901, mean reward: 0.390 [0.326, 0.525], mean action: 34.400 [5.000, 79.000], mean observation: 3.156 [-1.620, 10.154], loss: 1.621751, mae: 5.022596, mean_q: 5.236196
 62967/100000: episode: 6426, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.777, mean reward: 0.378 [0.359, 0.418], mean action: 66.700 [37.000, 101.000], mean observation: 3.162 [-1.214, 10.432], loss: 1.177350, mae: 5.020601, mean_q: 5.226632
 62977/100000: episode: 6427, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.057, mean reward: 0.406 [0.332, 0.507], mean action: 37.200 [6.000, 98.000], mean observation: 3.151 [-1.651, 10.336], loss: 1.075307, mae: 5.020144, mean_q: 5.224886
 62985/100000: episode: 6428, duration: 0.146s, episode steps: 8, steps per second: 55, episode reward: 12.903, mean reward: 1.613 [0.409, 10.000], mean action: 53.375 [17.000, 94.000], mean observation: 3.164 [-1.135, 10.290], loss: 1.281912, mae: 5.021145, mean_q: 5.224645
 62995/100000: episode: 6429, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.889, mean reward: 0.389 [0.330, 0.444], mean action: 36.600 [21.000, 84.000], mean observation: 3.157 [-1.480, 10.391], loss: 1.387283, mae: 5.021604, mean_q: 5.222664
 63005/100000: episode: 6430, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.096, mean reward: 0.410 [0.361, 0.535], mean action: 49.300 [36.000, 89.000], mean observation: 3.177 [-1.433, 10.364], loss: 1.296119, mae: 5.021151, mean_q: 5.223203
 63015/100000: episode: 6431, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.478, mean reward: 0.448 [0.412, 0.554], mean action: 48.500 [10.000, 87.000], mean observation: 3.165 [-1.297, 10.422], loss: 0.971080, mae: 5.019658, mean_q: 5.224555
 63025/100000: episode: 6432, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.738, mean reward: 0.474 [0.310, 0.542], mean action: 38.100 [3.000, 64.000], mean observation: 3.159 [-1.214, 10.394], loss: 1.195384, mae: 5.020586, mean_q: 5.224849
 63035/100000: episode: 6433, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.774, mean reward: 0.377 [0.319, 0.421], mean action: 50.500 [36.000, 101.000], mean observation: 3.166 [-1.252, 10.242], loss: 1.052699, mae: 5.020154, mean_q: 5.223602
 63045/100000: episode: 6434, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.199, mean reward: 0.420 [0.355, 0.455], mean action: 36.200 [5.000, 96.000], mean observation: 3.155 [-1.450, 10.225], loss: 1.197884, mae: 5.020691, mean_q: 5.224167
 63052/100000: episode: 6435, duration: 0.128s, episode steps: 7, steps per second: 55, episode reward: 12.494, mean reward: 1.785 [0.337, 10.000], mean action: 36.571 [23.000, 72.000], mean observation: 3.154 [-0.960, 10.217], loss: 1.187691, mae: 5.020881, mean_q: 5.225832
 63062/100000: episode: 6436, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.310, mean reward: 0.431 [0.378, 0.527], mean action: 53.600 [25.000, 97.000], mean observation: 3.147 [-1.728, 10.283], loss: 1.272212, mae: 5.021187, mean_q: 5.227652
 63072/100000: episode: 6437, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.163, mean reward: 0.416 [0.337, 0.517], mean action: 35.000 [12.000, 68.000], mean observation: 3.154 [-1.431, 10.448], loss: 1.257425, mae: 5.021305, mean_q: 5.229273
 63081/100000: episode: 6438, duration: 0.197s, episode steps: 9, steps per second: 46, episode reward: 13.213, mean reward: 1.468 [0.302, 10.000], mean action: 24.444 [20.000, 25.000], mean observation: 3.155 [-1.278, 10.354], loss: 1.279461, mae: 5.021410, mean_q: 5.230395
 63091/100000: episode: 6439, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.229, mean reward: 0.423 [0.331, 0.587], mean action: 34.100 [6.000, 79.000], mean observation: 3.151 [-1.609, 10.257], loss: 1.556605, mae: 5.022227, mean_q: 5.230402
 63101/100000: episode: 6440, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.113, mean reward: 0.411 [0.378, 0.459], mean action: 44.800 [4.000, 100.000], mean observation: 3.148 [-1.633, 10.426], loss: 1.264066, mae: 5.020883, mean_q: 5.226640
 63111/100000: episode: 6441, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.908, mean reward: 0.391 [0.331, 0.572], mean action: 30.000 [11.000, 81.000], mean observation: 3.165 [-1.620, 10.390], loss: 1.131546, mae: 5.019942, mean_q: 5.221874
 63117/100000: episode: 6442, duration: 0.133s, episode steps: 6, steps per second: 45, episode reward: 11.837, mean reward: 1.973 [0.332, 10.000], mean action: 28.500 [12.000, 47.000], mean observation: 3.160 [-1.548, 10.422], loss: 1.497225, mae: 5.021307, mean_q: 5.218040
 63127/100000: episode: 6443, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.965, mean reward: 0.396 [0.330, 0.490], mean action: 34.600 [25.000, 67.000], mean observation: 3.161 [-1.936, 10.368], loss: 1.396054, mae: 5.021066, mean_q: 5.215106
 63137/100000: episode: 6444, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.981, mean reward: 0.498 [0.496, 0.503], mean action: 43.500 [25.000, 94.000], mean observation: 3.140 [-1.308, 10.254], loss: 1.236259, mae: 5.020136, mean_q: 5.215897
 63147/100000: episode: 6445, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.206, mean reward: 0.421 [0.339, 0.550], mean action: 43.100 [16.000, 88.000], mean observation: 3.149 [-1.305, 10.230], loss: 1.228698, mae: 5.019933, mean_q: 5.217392
 63157/100000: episode: 6446, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.109, mean reward: 0.411 [0.342, 0.494], mean action: 43.800 [3.000, 98.000], mean observation: 3.150 [-1.272, 10.263], loss: 1.169861, mae: 5.019519, mean_q: 5.217293
 63167/100000: episode: 6447, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.976, mean reward: 0.398 [0.342, 0.456], mean action: 42.500 [11.000, 97.000], mean observation: 3.148 [-2.169, 10.408], loss: 1.197032, mae: 5.019713, mean_q: 5.215040
 63177/100000: episode: 6448, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.138, mean reward: 0.414 [0.377, 0.467], mean action: 46.000 [18.000, 79.000], mean observation: 3.168 [-1.579, 10.319], loss: 1.043924, mae: 5.019075, mean_q: 5.214036
 63187/100000: episode: 6449, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.901, mean reward: 0.390 [0.347, 0.487], mean action: 37.000 [25.000, 87.000], mean observation: 3.156 [-1.630, 10.388], loss: 1.007112, mae: 5.019155, mean_q: 5.212546
 63197/100000: episode: 6450, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.225, mean reward: 0.422 [0.361, 0.527], mean action: 37.300 [22.000, 88.000], mean observation: 3.165 [-1.492, 10.378], loss: 1.306655, mae: 5.020434, mean_q: 5.212832
 63207/100000: episode: 6451, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.356, mean reward: 0.436 [0.402, 0.527], mean action: 39.000 [19.000, 91.000], mean observation: 3.162 [-1.805, 10.478], loss: 1.212102, mae: 5.020026, mean_q: 5.214450
 63217/100000: episode: 6452, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.656, mean reward: 0.366 [0.315, 0.472], mean action: 43.000 [16.000, 93.000], mean observation: 3.159 [-1.276, 10.353], loss: 1.301319, mae: 5.020374, mean_q: 5.216797
 63227/100000: episode: 6453, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.924, mean reward: 0.392 [0.337, 0.551], mean action: 51.000 [15.000, 97.000], mean observation: 3.164 [-1.739, 10.380], loss: 0.957751, mae: 5.019228, mean_q: 5.216291
 63237/100000: episode: 6454, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.953, mean reward: 0.395 [0.317, 0.465], mean action: 35.300 [25.000, 91.000], mean observation: 3.159 [-1.416, 10.332], loss: 1.014379, mae: 5.019801, mean_q: 5.216715
 63247/100000: episode: 6455, duration: 0.217s, episode steps: 10, steps per second: 46, episode reward: 3.880, mean reward: 0.388 [0.337, 0.508], mean action: 26.800 [7.000, 53.000], mean observation: 3.159 [-1.666, 10.249], loss: 1.394004, mae: 5.021403, mean_q: 5.215395
 63257/100000: episode: 6456, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.457, mean reward: 0.446 [0.367, 0.533], mean action: 37.800 [14.000, 75.000], mean observation: 3.165 [-1.506, 10.246], loss: 1.275649, mae: 5.020839, mean_q: 5.212890
 63267/100000: episode: 6457, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.389, mean reward: 0.439 [0.420, 0.493], mean action: 36.700 [19.000, 72.000], mean observation: 3.150 [-1.440, 10.309], loss: 1.272822, mae: 5.020724, mean_q: 5.213099
 63277/100000: episode: 6458, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.453, mean reward: 0.445 [0.438, 0.513], mean action: 38.500 [13.000, 91.000], mean observation: 3.158 [-1.193, 10.444], loss: 1.028129, mae: 5.019900, mean_q: 5.215142
 63287/100000: episode: 6459, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 4.004, mean reward: 0.400 [0.370, 0.457], mean action: 40.100 [25.000, 87.000], mean observation: 3.153 [-1.233, 10.345], loss: 0.860867, mae: 5.019434, mean_q: 5.217179
 63297/100000: episode: 6460, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.929, mean reward: 0.393 [0.352, 0.441], mean action: 36.900 [25.000, 86.000], mean observation: 3.160 [-1.684, 10.257], loss: 0.947760, mae: 5.020159, mean_q: 5.217239
 63307/100000: episode: 6461, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.886, mean reward: 0.389 [0.329, 0.488], mean action: 50.100 [10.000, 101.000], mean observation: 3.145 [-1.240, 10.204], loss: 1.058734, mae: 5.020986, mean_q: 5.214538
 63317/100000: episode: 6462, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.718, mean reward: 0.372 [0.348, 0.449], mean action: 56.200 [25.000, 98.000], mean observation: 3.153 [-0.906, 10.484], loss: 1.491133, mae: 5.022307, mean_q: 5.210460
 63327/100000: episode: 6463, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.403, mean reward: 0.440 [0.327, 0.576], mean action: 34.700 [3.000, 90.000], mean observation: 3.151 [-1.702, 10.225], loss: 1.443397, mae: 5.022166, mean_q: 5.210988
 63337/100000: episode: 6464, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.995, mean reward: 0.399 [0.353, 0.458], mean action: 68.800 [7.000, 99.000], mean observation: 3.168 [-1.241, 10.337], loss: 1.468453, mae: 5.021658, mean_q: 5.211505
 63347/100000: episode: 6465, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.088, mean reward: 0.409 [0.310, 0.484], mean action: 44.500 [11.000, 88.000], mean observation: 3.161 [-1.097, 10.349], loss: 1.136546, mae: 5.020179, mean_q: 5.213004
 63357/100000: episode: 6466, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.929, mean reward: 0.393 [0.344, 0.522], mean action: 61.500 [35.000, 94.000], mean observation: 3.161 [-1.122, 10.262], loss: 1.512817, mae: 5.021436, mean_q: 5.213756
 63367/100000: episode: 6467, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.928, mean reward: 0.493 [0.493, 0.493], mean action: 41.600 [2.000, 70.000], mean observation: 3.155 [-1.869, 10.300], loss: 1.369473, mae: 5.020868, mean_q: 5.214636
 63377/100000: episode: 6468, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.884, mean reward: 0.388 [0.331, 0.451], mean action: 59.600 [0.000, 92.000], mean observation: 3.152 [-0.968, 10.286], loss: 1.254803, mae: 5.020121, mean_q: 5.215382
 63387/100000: episode: 6469, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.106, mean reward: 0.411 [0.371, 0.481], mean action: 58.600 [27.000, 79.000], mean observation: 3.169 [-1.594, 10.325], loss: 1.402480, mae: 5.020646, mean_q: 5.215869
 63397/100000: episode: 6470, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.874, mean reward: 0.387 [0.322, 0.493], mean action: 68.100 [12.000, 99.000], mean observation: 3.158 [-1.629, 10.354], loss: 1.284580, mae: 5.019856, mean_q: 5.211461
 63407/100000: episode: 6471, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.061, mean reward: 0.406 [0.393, 0.457], mean action: 63.900 [19.000, 96.000], mean observation: 3.145 [-1.561, 10.305], loss: 0.937782, mae: 5.018491, mean_q: 5.210931
 63417/100000: episode: 6472, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.600, mean reward: 0.460 [0.417, 0.506], mean action: 62.200 [13.000, 92.000], mean observation: 3.159 [-1.449, 10.369], loss: 1.220049, mae: 5.019742, mean_q: 5.211827
 63427/100000: episode: 6473, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.052, mean reward: 0.405 [0.326, 0.443], mean action: 61.300 [19.000, 82.000], mean observation: 3.153 [-1.186, 10.430], loss: 1.343401, mae: 5.020274, mean_q: 5.212719
 63437/100000: episode: 6474, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.094, mean reward: 0.409 [0.340, 0.434], mean action: 61.500 [20.000, 98.000], mean observation: 3.165 [-1.523, 10.390], loss: 0.901950, mae: 5.018746, mean_q: 5.213729
 63447/100000: episode: 6475, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.145, mean reward: 0.415 [0.385, 0.454], mean action: 64.900 [39.000, 89.000], mean observation: 3.157 [-1.110, 10.255], loss: 1.451281, mae: 5.020991, mean_q: 5.215803
 63457/100000: episode: 6476, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.806, mean reward: 0.481 [0.416, 0.497], mean action: 57.400 [2.000, 73.000], mean observation: 3.145 [-1.151, 10.345], loss: 0.962264, mae: 5.019271, mean_q: 5.217658
 63467/100000: episode: 6477, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.336, mean reward: 0.434 [0.422, 0.471], mean action: 52.200 [12.000, 62.000], mean observation: 3.164 [-1.253, 10.308], loss: 0.827298, mae: 5.018976, mean_q: 5.218844
 63477/100000: episode: 6478, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.404, mean reward: 0.440 [0.440, 0.440], mean action: 55.500 [13.000, 65.000], mean observation: 3.152 [-1.969, 10.339], loss: 1.202025, mae: 5.020658, mean_q: 5.220962
 63479/100000: episode: 6479, duration: 0.060s, episode steps: 2, steps per second: 34, episode reward: 10.393, mean reward: 5.197 [0.393, 10.000], mean action: 3.000 [3.000, 3.000], mean observation: 3.162 [-0.780, 10.295], loss: 1.306481, mae: 5.021612, mean_q: 5.221789
 63489/100000: episode: 6480, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.150, mean reward: 0.415 [0.353, 0.550], mean action: 38.700 [0.000, 90.000], mean observation: 3.147 [-1.346, 10.428], loss: 1.182479, mae: 5.020847, mean_q: 5.222885
 63499/100000: episode: 6481, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.180, mean reward: 0.418 [0.314, 0.491], mean action: 42.000 [25.000, 99.000], mean observation: 3.166 [-1.182, 10.430], loss: 1.065583, mae: 5.020370, mean_q: 5.224086
 63509/100000: episode: 6482, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 3.523, mean reward: 0.352 [0.292, 0.417], mean action: 32.300 [12.000, 69.000], mean observation: 3.164 [-1.294, 10.297], loss: 1.089888, mae: 5.020791, mean_q: 5.226156
 63519/100000: episode: 6483, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.436, mean reward: 0.444 [0.328, 0.533], mean action: 40.800 [25.000, 62.000], mean observation: 3.156 [-2.115, 10.272], loss: 1.438020, mae: 5.022172, mean_q: 5.227323
 63529/100000: episode: 6484, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.760, mean reward: 0.376 [0.335, 0.525], mean action: 54.900 [2.000, 87.000], mean observation: 3.165 [-1.562, 10.251], loss: 1.399077, mae: 5.021802, mean_q: 5.228250
 63535/100000: episode: 6485, duration: 0.102s, episode steps: 6, steps per second: 59, episode reward: 11.907, mean reward: 1.985 [0.367, 10.000], mean action: 42.667 [5.000, 62.000], mean observation: 3.153 [-1.183, 10.254], loss: 1.716260, mae: 5.022924, mean_q: 5.229434
 63545/100000: episode: 6486, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.046, mean reward: 0.405 [0.392, 0.447], mean action: 44.800 [12.000, 62.000], mean observation: 3.150 [-1.635, 10.328], loss: 1.250268, mae: 5.021094, mean_q: 5.231505
 63555/100000: episode: 6487, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.398, mean reward: 0.440 [0.385, 0.498], mean action: 60.100 [22.000, 86.000], mean observation: 3.165 [-1.002, 10.503], loss: 1.045305, mae: 5.020226, mean_q: 5.234032
 63565/100000: episode: 6488, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.275, mean reward: 0.427 [0.359, 0.510], mean action: 45.000 [2.000, 87.000], mean observation: 3.164 [-1.665, 10.319], loss: 1.184203, mae: 5.020703, mean_q: 5.236217
 63575/100000: episode: 6489, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.617, mean reward: 0.362 [0.297, 0.418], mean action: 53.600 [4.000, 80.000], mean observation: 3.154 [-0.843, 10.359], loss: 1.348973, mae: 5.021724, mean_q: 5.238048
 63585/100000: episode: 6490, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.716, mean reward: 0.372 [0.334, 0.454], mean action: 61.800 [19.000, 97.000], mean observation: 3.147 [-1.089, 10.423], loss: 1.281666, mae: 5.021285, mean_q: 5.232478
 63595/100000: episode: 6491, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.095, mean reward: 0.409 [0.372, 0.448], mean action: 52.900 [2.000, 79.000], mean observation: 3.147 [-1.246, 10.424], loss: 1.236412, mae: 5.021114, mean_q: 5.223340
 63605/100000: episode: 6492, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 3.976, mean reward: 0.398 [0.323, 0.497], mean action: 26.500 [1.000, 65.000], mean observation: 3.152 [-1.165, 10.259], loss: 1.206573, mae: 5.020862, mean_q: 5.219890
 63615/100000: episode: 6493, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.215, mean reward: 0.422 [0.349, 0.496], mean action: 29.300 [12.000, 70.000], mean observation: 3.155 [-1.052, 10.192], loss: 1.073513, mae: 5.020437, mean_q: 5.218814
 63625/100000: episode: 6494, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.121, mean reward: 0.412 [0.321, 0.553], mean action: 38.300 [25.000, 78.000], mean observation: 3.165 [-1.653, 10.400], loss: 1.498057, mae: 5.022156, mean_q: 5.218539
 63635/100000: episode: 6495, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.526, mean reward: 0.453 [0.386, 0.476], mean action: 34.300 [4.000, 89.000], mean observation: 3.160 [-1.375, 10.369], loss: 1.407948, mae: 5.021633, mean_q: 5.216903
 63645/100000: episode: 6496, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.098, mean reward: 0.410 [0.325, 0.564], mean action: 38.700 [8.000, 101.000], mean observation: 3.158 [-1.165, 10.165], loss: 1.305462, mae: 5.020820, mean_q: 5.217741
 63655/100000: episode: 6497, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.394, mean reward: 0.439 [0.375, 0.524], mean action: 43.800 [7.000, 91.000], mean observation: 3.160 [-2.156, 10.379], loss: 1.204610, mae: 5.020433, mean_q: 5.219613
 63665/100000: episode: 6498, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.046, mean reward: 0.405 [0.321, 0.506], mean action: 35.500 [0.000, 101.000], mean observation: 3.156 [-1.226, 10.294], loss: 1.202482, mae: 5.020470, mean_q: 5.219552
 63675/100000: episode: 6499, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.855, mean reward: 0.386 [0.362, 0.443], mean action: 45.500 [25.000, 97.000], mean observation: 3.152 [-1.382, 10.350], loss: 1.453634, mae: 5.021201, mean_q: 5.220887
 63685/100000: episode: 6500, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.887, mean reward: 0.489 [0.489, 0.489], mean action: 42.300 [25.000, 99.000], mean observation: 3.157 [-1.842, 10.378], loss: 1.232853, mae: 5.020179, mean_q: 5.221002
 63695/100000: episode: 6501, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.926, mean reward: 0.393 [0.303, 0.504], mean action: 44.900 [24.000, 86.000], mean observation: 3.156 [-1.229, 10.378], loss: 1.180739, mae: 5.020169, mean_q: 5.222153
 63705/100000: episode: 6502, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.848, mean reward: 0.385 [0.283, 0.566], mean action: 28.400 [3.000, 76.000], mean observation: 3.156 [-1.506, 10.648], loss: 0.930667, mae: 5.019521, mean_q: 5.224417
 63715/100000: episode: 6503, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.076, mean reward: 0.408 [0.355, 0.453], mean action: 31.800 [6.000, 79.000], mean observation: 3.153 [-1.607, 10.362], loss: 1.263860, mae: 5.020932, mean_q: 5.227667
 63725/100000: episode: 6504, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.589, mean reward: 0.459 [0.363, 0.492], mean action: 44.000 [0.000, 99.000], mean observation: 3.147 [-1.933, 10.218], loss: 1.139095, mae: 5.020617, mean_q: 5.231483
 63735/100000: episode: 6505, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.090, mean reward: 0.409 [0.373, 0.514], mean action: 28.300 [25.000, 47.000], mean observation: 3.166 [-1.500, 10.556], loss: 1.410172, mae: 5.021932, mean_q: 5.231880
 63745/100000: episode: 6506, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.800, mean reward: 0.380 [0.352, 0.440], mean action: 54.200 [20.000, 100.000], mean observation: 3.164 [-1.090, 10.241], loss: 1.239607, mae: 5.021325, mean_q: 5.229976
 63755/100000: episode: 6507, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 3.934, mean reward: 0.393 [0.330, 0.456], mean action: 24.600 [21.000, 25.000], mean observation: 3.156 [-1.126, 10.418], loss: 1.493437, mae: 5.021945, mean_q: 5.227203
 63765/100000: episode: 6508, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.815, mean reward: 0.381 [0.327, 0.469], mean action: 43.200 [1.000, 94.000], mean observation: 3.159 [-1.457, 10.503], loss: 1.141083, mae: 5.020769, mean_q: 5.226337
 63775/100000: episode: 6509, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.065, mean reward: 0.407 [0.395, 0.478], mean action: 61.100 [12.000, 85.000], mean observation: 3.160 [-0.807, 10.362], loss: 1.371090, mae: 5.021533, mean_q: 5.227289
 63785/100000: episode: 6510, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.509, mean reward: 0.351 [0.326, 0.452], mean action: 70.300 [64.000, 91.000], mean observation: 3.159 [-1.564, 10.304], loss: 1.044187, mae: 5.020522, mean_q: 5.228356
 63795/100000: episode: 6511, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.183, mean reward: 0.418 [0.333, 0.509], mean action: 53.900 [34.000, 71.000], mean observation: 3.154 [-1.533, 10.365], loss: 1.086319, mae: 5.020941, mean_q: 5.229511
 63796/100000: episode: 6512, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 64.000 [64.000, 64.000], mean observation: 3.182 [-1.141, 10.260], loss: 0.809694, mae: 5.019877, mean_q: 5.227781
 63806/100000: episode: 6513, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.366, mean reward: 0.437 [0.415, 0.481], mean action: 53.400 [10.000, 64.000], mean observation: 3.141 [-1.238, 10.324], loss: 1.584413, mae: 5.023321, mean_q: 5.225066
 63816/100000: episode: 6514, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.317, mean reward: 0.432 [0.430, 0.443], mean action: 64.500 [41.000, 91.000], mean observation: 3.153 [-1.191, 10.358], loss: 1.112638, mae: 5.021413, mean_q: 5.223927
 63826/100000: episode: 6515, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.304, mean reward: 0.430 [0.364, 0.484], mean action: 52.800 [1.000, 64.000], mean observation: 3.161 [-1.353, 10.192], loss: 0.936575, mae: 5.020795, mean_q: 5.224603
 63836/100000: episode: 6516, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.210, mean reward: 0.421 [0.419, 0.429], mean action: 42.200 [6.000, 84.000], mean observation: 3.156 [-0.907, 10.341], loss: 1.249394, mae: 5.021980, mean_q: 5.225830
 63846/100000: episode: 6517, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.677, mean reward: 0.368 [0.330, 0.566], mean action: 58.600 [30.000, 71.000], mean observation: 3.160 [-0.895, 10.234], loss: 1.346772, mae: 5.022641, mean_q: 5.228366
 63856/100000: episode: 6518, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.224, mean reward: 0.422 [0.342, 0.496], mean action: 56.400 [6.000, 64.000], mean observation: 3.176 [-0.947, 10.349], loss: 1.244672, mae: 5.022170, mean_q: 5.229995
 63866/100000: episode: 6519, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.513, mean reward: 0.451 [0.444, 0.469], mean action: 60.100 [6.000, 83.000], mean observation: 3.164 [-1.977, 10.266], loss: 1.381244, mae: 5.022896, mean_q: 5.230527
 63876/100000: episode: 6520, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.255, mean reward: 0.426 [0.342, 0.499], mean action: 62.500 [12.000, 96.000], mean observation: 3.161 [-1.074, 10.408], loss: 1.250039, mae: 5.022441, mean_q: 5.230859
 63886/100000: episode: 6521, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.533, mean reward: 0.453 [0.347, 0.515], mean action: 48.000 [2.000, 88.000], mean observation: 3.158 [-1.826, 10.301], loss: 1.030331, mae: 5.021687, mean_q: 5.232139
 63896/100000: episode: 6522, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.787, mean reward: 0.379 [0.326, 0.435], mean action: 51.300 [0.000, 76.000], mean observation: 3.161 [-1.240, 10.421], loss: 1.300777, mae: 5.022793, mean_q: 5.229686
 63906/100000: episode: 6523, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.028, mean reward: 0.403 [0.376, 0.510], mean action: 32.600 [15.000, 61.000], mean observation: 3.163 [-1.326, 10.331], loss: 1.726744, mae: 5.024055, mean_q: 5.225705
 63916/100000: episode: 6524, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.974, mean reward: 0.497 [0.497, 0.501], mean action: 39.800 [14.000, 96.000], mean observation: 3.148 [-1.996, 10.377], loss: 1.252136, mae: 5.021676, mean_q: 5.226384
 63926/100000: episode: 6525, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.257, mean reward: 0.426 [0.331, 0.516], mean action: 39.500 [25.000, 96.000], mean observation: 3.163 [-2.767, 10.224], loss: 1.380158, mae: 5.021969, mean_q: 5.224464
 63936/100000: episode: 6526, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.445, mean reward: 0.445 [0.372, 0.534], mean action: 31.900 [8.000, 70.000], mean observation: 3.165 [-1.234, 10.409], loss: 1.143836, mae: 5.020994, mean_q: 5.222886
 63946/100000: episode: 6527, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.987, mean reward: 0.399 [0.296, 0.511], mean action: 33.800 [0.000, 83.000], mean observation: 3.156 [-1.041, 10.372], loss: 1.239204, mae: 5.021348, mean_q: 5.220321
 63956/100000: episode: 6528, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.321, mean reward: 0.432 [0.326, 0.481], mean action: 44.300 [14.000, 101.000], mean observation: 3.161 [-1.012, 10.344], loss: 0.962902, mae: 5.020021, mean_q: 5.218915
 63964/100000: episode: 6529, duration: 0.157s, episode steps: 8, steps per second: 51, episode reward: 13.075, mean reward: 1.634 [0.361, 10.000], mean action: 24.375 [7.000, 34.000], mean observation: 3.145 [-1.463, 10.468], loss: 1.352598, mae: 5.021589, mean_q: 5.217178
 63974/100000: episode: 6530, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.435, mean reward: 0.443 [0.402, 0.545], mean action: 37.200 [1.000, 101.000], mean observation: 3.155 [-1.362, 10.271], loss: 1.252403, mae: 5.021340, mean_q: 5.216852
 63984/100000: episode: 6531, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.119, mean reward: 0.412 [0.345, 0.499], mean action: 41.600 [7.000, 98.000], mean observation: 3.146 [-2.420, 10.344], loss: 1.204721, mae: 5.020692, mean_q: 5.219061
 63994/100000: episode: 6532, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.431, mean reward: 0.443 [0.398, 0.541], mean action: 41.300 [6.000, 92.000], mean observation: 3.155 [-1.824, 10.227], loss: 1.207165, mae: 5.020857, mean_q: 5.222059
 64004/100000: episode: 6533, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.054, mean reward: 0.405 [0.267, 0.464], mean action: 32.700 [7.000, 92.000], mean observation: 3.155 [-1.893, 10.402], loss: 1.045267, mae: 5.020095, mean_q: 5.222695
 64014/100000: episode: 6534, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.102, mean reward: 0.410 [0.307, 0.485], mean action: 37.600 [2.000, 83.000], mean observation: 3.156 [-1.573, 10.410], loss: 1.320915, mae: 5.021381, mean_q: 5.223270
 64024/100000: episode: 6535, duration: 0.230s, episode steps: 10, steps per second: 43, episode reward: 4.245, mean reward: 0.425 [0.298, 0.512], mean action: 6.800 [0.000, 12.000], mean observation: 3.155 [-2.350, 10.362], loss: 1.036229, mae: 5.020401, mean_q: 5.224132
 64034/100000: episode: 6536, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.042, mean reward: 0.404 [0.343, 0.478], mean action: 40.400 [7.000, 99.000], mean observation: 3.159 [-1.734, 10.390], loss: 1.249468, mae: 5.020907, mean_q: 5.222824
 64044/100000: episode: 6537, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.041, mean reward: 0.404 [0.309, 0.432], mean action: 37.200 [3.000, 95.000], mean observation: 3.152 [-1.439, 10.341], loss: 1.182161, mae: 5.020720, mean_q: 5.219992
 64054/100000: episode: 6538, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.501, mean reward: 0.450 [0.412, 0.538], mean action: 39.600 [7.000, 73.000], mean observation: 3.148 [-1.670, 10.243], loss: 1.088333, mae: 5.020339, mean_q: 5.219579
 64064/100000: episode: 6539, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.332, mean reward: 0.433 [0.387, 0.498], mean action: 19.000 [7.000, 64.000], mean observation: 3.158 [-2.033, 10.561], loss: 1.213289, mae: 5.020900, mean_q: 5.215649
 64074/100000: episode: 6540, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.327, mean reward: 0.433 [0.432, 0.436], mean action: 64.500 [25.000, 94.000], mean observation: 3.174 [-2.032, 10.291], loss: 1.103911, mae: 5.020596, mean_q: 5.214816
 64084/100000: episode: 6541, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.530, mean reward: 0.453 [0.337, 0.496], mean action: 29.900 [6.000, 101.000], mean observation: 3.156 [-1.559, 10.270], loss: 1.464417, mae: 5.022107, mean_q: 5.214856
 64094/100000: episode: 6542, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.114, mean reward: 0.411 [0.384, 0.500], mean action: 23.300 [4.000, 29.000], mean observation: 3.163 [-2.034, 10.324], loss: 1.033695, mae: 5.020343, mean_q: 5.216481
 64104/100000: episode: 6543, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.115, mean reward: 0.411 [0.363, 0.456], mean action: 49.100 [24.000, 87.000], mean observation: 3.152 [-1.573, 10.269], loss: 1.093348, mae: 5.020820, mean_q: 5.218737
 64114/100000: episode: 6544, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.929, mean reward: 0.393 [0.321, 0.487], mean action: 50.900 [2.000, 99.000], mean observation: 3.144 [-1.454, 10.312], loss: 1.514231, mae: 5.022272, mean_q: 5.219194
 64124/100000: episode: 6545, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.919, mean reward: 0.392 [0.382, 0.411], mean action: 35.700 [12.000, 96.000], mean observation: 3.156 [-1.620, 10.380], loss: 1.267584, mae: 5.021155, mean_q: 5.220293
 64134/100000: episode: 6546, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.479, mean reward: 0.448 [0.343, 0.582], mean action: 21.900 [7.000, 50.000], mean observation: 3.158 [-1.124, 10.367], loss: 1.109604, mae: 5.020674, mean_q: 5.218699
 64144/100000: episode: 6547, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.390, mean reward: 0.439 [0.366, 0.583], mean action: 46.000 [7.000, 84.000], mean observation: 3.152 [-1.437, 10.292], loss: 1.312880, mae: 5.021578, mean_q: 5.219424
 64154/100000: episode: 6548, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.222, mean reward: 0.422 [0.329, 0.646], mean action: 33.700 [7.000, 89.000], mean observation: 3.156 [-1.382, 10.372], loss: 1.365546, mae: 5.021703, mean_q: 5.221557
 64164/100000: episode: 6549, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.966, mean reward: 0.497 [0.487, 0.554], mean action: 19.400 [7.000, 84.000], mean observation: 3.148 [-2.140, 10.414], loss: 1.029102, mae: 5.020360, mean_q: 5.224356
 64174/100000: episode: 6550, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 3.665, mean reward: 0.366 [0.300, 0.427], mean action: 38.200 [4.000, 99.000], mean observation: 3.154 [-1.664, 10.257], loss: 1.440981, mae: 5.021736, mean_q: 5.225806
 64184/100000: episode: 6551, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.096, mean reward: 0.410 [0.350, 0.465], mean action: 36.900 [7.000, 94.000], mean observation: 3.159 [-1.321, 10.242], loss: 1.419262, mae: 5.021505, mean_q: 5.226813
 64194/100000: episode: 6552, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.998, mean reward: 0.400 [0.348, 0.473], mean action: 28.700 [7.000, 93.000], mean observation: 3.154 [-1.709, 10.307], loss: 1.374306, mae: 5.021136, mean_q: 5.232752
 64204/100000: episode: 6553, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 4.161, mean reward: 0.416 [0.315, 0.540], mean action: 9.200 [0.000, 26.000], mean observation: 3.160 [-1.805, 10.516], loss: 1.107826, mae: 5.020228, mean_q: 5.237639
 64214/100000: episode: 6554, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.100, mean reward: 0.410 [0.340, 0.530], mean action: 27.700 [7.000, 93.000], mean observation: 3.150 [-1.590, 10.344], loss: 1.174386, mae: 5.020481, mean_q: 5.240029
 64224/100000: episode: 6555, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.522, mean reward: 0.452 [0.401, 0.527], mean action: 40.800 [7.000, 90.000], mean observation: 3.167 [-1.335, 10.359], loss: 1.323920, mae: 5.021022, mean_q: 5.241706
 64234/100000: episode: 6556, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 4.138, mean reward: 0.414 [0.370, 0.474], mean action: 33.500 [7.000, 85.000], mean observation: 3.158 [-2.164, 10.244], loss: 1.030598, mae: 5.020003, mean_q: 5.243426
 64244/100000: episode: 6557, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.020, mean reward: 0.402 [0.350, 0.470], mean action: 26.400 [0.000, 87.000], mean observation: 3.168 [-1.858, 10.559], loss: 1.404840, mae: 5.021544, mean_q: 5.244749
 64254/100000: episode: 6558, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.568, mean reward: 0.457 [0.358, 0.564], mean action: 21.800 [7.000, 100.000], mean observation: 3.156 [-1.867, 10.369], loss: 1.017343, mae: 5.020122, mean_q: 5.246429
 64264/100000: episode: 6559, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.954, mean reward: 0.395 [0.313, 0.515], mean action: 33.500 [7.000, 94.000], mean observation: 3.150 [-1.190, 10.332], loss: 1.320773, mae: 5.021453, mean_q: 5.248204
 64274/100000: episode: 6560, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 4.196, mean reward: 0.420 [0.341, 0.523], mean action: 23.300 [5.000, 69.000], mean observation: 3.162 [-1.275, 10.328], loss: 1.096453, mae: 5.020770, mean_q: 5.249593
 64284/100000: episode: 6561, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.163, mean reward: 0.416 [0.334, 0.544], mean action: 13.300 [6.000, 71.000], mean observation: 3.158 [-1.269, 10.409], loss: 1.078778, mae: 5.021007, mean_q: 5.250104
 64294/100000: episode: 6562, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.188, mean reward: 0.419 [0.359, 0.557], mean action: 23.300 [7.000, 92.000], mean observation: 3.161 [-1.559, 10.379], loss: 1.012357, mae: 5.021000, mean_q: 5.247948
 64304/100000: episode: 6563, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.236, mean reward: 0.424 [0.329, 0.507], mean action: 42.500 [7.000, 100.000], mean observation: 3.165 [-1.576, 10.311], loss: 0.940698, mae: 5.020994, mean_q: 5.248104
 64314/100000: episode: 6564, duration: 0.223s, episode steps: 10, steps per second: 45, episode reward: 4.276, mean reward: 0.428 [0.355, 0.489], mean action: 7.600 [7.000, 13.000], mean observation: 3.156 [-1.988, 10.384], loss: 1.375397, mae: 5.022976, mean_q: 5.248793
 64324/100000: episode: 6565, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.242, mean reward: 0.424 [0.372, 0.495], mean action: 31.600 [7.000, 99.000], mean observation: 3.162 [-1.297, 10.320], loss: 1.381359, mae: 5.023105, mean_q: 5.245077
 64334/100000: episode: 6566, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.925, mean reward: 0.392 [0.320, 0.517], mean action: 24.000 [7.000, 98.000], mean observation: 3.149 [-0.832, 10.585], loss: 1.456523, mae: 5.023623, mean_q: 5.243845
 64344/100000: episode: 6567, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.065, mean reward: 0.407 [0.346, 0.500], mean action: 19.000 [7.000, 46.000], mean observation: 3.159 [-1.961, 10.381], loss: 1.015561, mae: 5.022069, mean_q: 5.243561
 64354/100000: episode: 6568, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.035, mean reward: 0.404 [0.309, 0.507], mean action: 45.500 [7.000, 101.000], mean observation: 3.146 [-1.085, 10.366], loss: 0.810592, mae: 5.021677, mean_q: 5.241069
 64364/100000: episode: 6569, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.875, mean reward: 0.387 [0.308, 0.546], mean action: 40.800 [0.000, 88.000], mean observation: 3.167 [-1.766, 10.303], loss: 1.239652, mae: 5.023887, mean_q: 5.241291
 64374/100000: episode: 6570, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.032, mean reward: 0.403 [0.323, 0.506], mean action: 34.700 [7.000, 88.000], mean observation: 3.163 [-1.131, 10.392], loss: 0.815856, mae: 5.022569, mean_q: 5.240559
 64384/100000: episode: 6571, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 4.050, mean reward: 0.405 [0.356, 0.463], mean action: 40.700 [7.000, 98.000], mean observation: 3.159 [-1.605, 10.358], loss: 1.346293, mae: 5.025052, mean_q: 5.233562
 64394/100000: episode: 6572, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.133, mean reward: 0.413 [0.358, 0.506], mean action: 27.600 [7.000, 97.000], mean observation: 3.162 [-1.327, 10.388], loss: 1.179421, mae: 5.024574, mean_q: 5.230128
 64404/100000: episode: 6573, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.503, mean reward: 0.450 [0.334, 0.558], mean action: 46.000 [7.000, 97.000], mean observation: 3.163 [-1.742, 10.340], loss: 1.148364, mae: 5.024221, mean_q: 5.230099
 64414/100000: episode: 6574, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.105, mean reward: 0.411 [0.358, 0.493], mean action: 25.900 [7.000, 77.000], mean observation: 3.152 [-1.492, 10.578], loss: 1.507135, mae: 5.025828, mean_q: 5.231019
 64424/100000: episode: 6575, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.302, mean reward: 0.430 [0.341, 0.524], mean action: 26.500 [7.000, 92.000], mean observation: 3.149 [-1.765, 10.235], loss: 1.107982, mae: 5.024055, mean_q: 5.232308
 64434/100000: episode: 6576, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.786, mean reward: 0.479 [0.350, 0.582], mean action: 36.700 [7.000, 79.000], mean observation: 3.153 [-1.467, 10.324], loss: 1.542845, mae: 5.025902, mean_q: 5.233408
 64444/100000: episode: 6577, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.086, mean reward: 0.409 [0.320, 0.517], mean action: 30.300 [7.000, 95.000], mean observation: 3.155 [-2.039, 10.317], loss: 1.077973, mae: 5.023846, mean_q: 5.234488
 64454/100000: episode: 6578, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 3.982, mean reward: 0.398 [0.345, 0.510], mean action: 18.100 [1.000, 78.000], mean observation: 3.170 [-1.095, 10.303], loss: 1.511288, mae: 5.025329, mean_q: 5.229147
 64464/100000: episode: 6579, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.103, mean reward: 0.410 [0.353, 0.490], mean action: 33.800 [3.000, 82.000], mean observation: 3.157 [-1.010, 10.486], loss: 1.106349, mae: 5.023754, mean_q: 5.229534
 64474/100000: episode: 6580, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.557, mean reward: 0.456 [0.389, 0.541], mean action: 28.600 [3.000, 101.000], mean observation: 3.151 [-1.039, 10.276], loss: 1.329899, mae: 5.024504, mean_q: 5.231795
 64484/100000: episode: 6581, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.963, mean reward: 0.396 [0.331, 0.469], mean action: 21.900 [3.000, 69.000], mean observation: 3.165 [-1.484, 10.348], loss: 0.735620, mae: 5.022326, mean_q: 5.231727
 64494/100000: episode: 6582, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.150, mean reward: 0.415 [0.336, 0.470], mean action: 27.200 [3.000, 82.000], mean observation: 3.150 [-1.586, 10.305], loss: 1.294621, mae: 5.024745, mean_q: 5.231230
 64504/100000: episode: 6583, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 3.768, mean reward: 0.377 [0.346, 0.408], mean action: 28.800 [3.000, 89.000], mean observation: 3.158 [-1.182, 10.250], loss: 1.393854, mae: 5.025072, mean_q: 5.233054
 64514/100000: episode: 6584, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.987, mean reward: 0.399 [0.348, 0.502], mean action: 21.200 [2.000, 84.000], mean observation: 3.157 [-1.311, 10.370], loss: 1.002493, mae: 5.023418, mean_q: 5.235079
 64524/100000: episode: 6585, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.012, mean reward: 0.401 [0.327, 0.448], mean action: 29.800 [3.000, 69.000], mean observation: 3.155 [-1.104, 10.281], loss: 1.355291, mae: 5.024629, mean_q: 5.236711
 64534/100000: episode: 6586, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.402, mean reward: 0.440 [0.340, 0.540], mean action: 39.600 [3.000, 92.000], mean observation: 3.157 [-1.823, 10.253], loss: 1.003132, mae: 5.023317, mean_q: 5.235819
 64544/100000: episode: 6587, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 3.698, mean reward: 0.370 [0.308, 0.470], mean action: 22.600 [3.000, 98.000], mean observation: 3.157 [-2.117, 10.372], loss: 1.341723, mae: 5.024708, mean_q: 5.235427
 64554/100000: episode: 6588, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 3.868, mean reward: 0.387 [0.309, 0.498], mean action: 28.600 [2.000, 80.000], mean observation: 3.159 [-1.165, 10.298], loss: 1.111075, mae: 5.024070, mean_q: 5.236785
 64564/100000: episode: 6589, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 3.772, mean reward: 0.377 [0.295, 0.455], mean action: 27.500 [3.000, 67.000], mean observation: 3.152 [-1.599, 10.312], loss: 1.286152, mae: 5.024612, mean_q: 5.237277
 64574/100000: episode: 6590, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.318, mean reward: 0.432 [0.322, 0.591], mean action: 46.100 [25.000, 100.000], mean observation: 3.163 [-1.125, 10.262], loss: 1.041731, mae: 5.023653, mean_q: 5.234802
 64584/100000: episode: 6591, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.160, mean reward: 0.416 [0.370, 0.470], mean action: 39.600 [11.000, 100.000], mean observation: 3.160 [-1.229, 10.421], loss: 1.169738, mae: 5.024250, mean_q: 5.235271
 64594/100000: episode: 6592, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.324, mean reward: 0.432 [0.414, 0.536], mean action: 56.000 [25.000, 101.000], mean observation: 3.152 [-1.787, 10.315], loss: 1.334259, mae: 5.024853, mean_q: 5.237074
 64604/100000: episode: 6593, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.292, mean reward: 0.429 [0.358, 0.484], mean action: 44.300 [25.000, 96.000], mean observation: 3.153 [-1.328, 10.325], loss: 1.502156, mae: 5.025434, mean_q: 5.236939
 64614/100000: episode: 6594, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.426, mean reward: 0.443 [0.338, 0.540], mean action: 40.500 [20.000, 99.000], mean observation: 3.158 [-1.783, 10.403], loss: 1.496982, mae: 5.024958, mean_q: 5.235798
 64624/100000: episode: 6595, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.988, mean reward: 0.499 [0.499, 0.499], mean action: 44.500 [10.000, 94.000], mean observation: 3.167 [-1.316, 10.497], loss: 1.173237, mae: 5.023293, mean_q: 5.231746
 64634/100000: episode: 6596, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.867, mean reward: 0.387 [0.319, 0.441], mean action: 28.500 [12.000, 87.000], mean observation: 3.154 [-1.802, 10.259], loss: 1.295901, mae: 5.023456, mean_q: 5.230390
 64644/100000: episode: 6597, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.356, mean reward: 0.436 [0.331, 0.466], mean action: 48.400 [3.000, 97.000], mean observation: 3.152 [-1.612, 10.225], loss: 0.871077, mae: 5.022086, mean_q: 5.229228
 64645/100000: episode: 6598, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 40.000 [40.000, 40.000], mean observation: 3.142 [-1.513, 10.100], loss: 1.504506, mae: 5.024261, mean_q: 5.229856
 64655/100000: episode: 6599, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.188, mean reward: 0.419 [0.339, 0.513], mean action: 40.300 [3.000, 93.000], mean observation: 3.156 [-0.907, 10.443], loss: 1.377114, mae: 5.024085, mean_q: 5.230861
 64665/100000: episode: 6600, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.289, mean reward: 0.429 [0.372, 0.442], mean action: 47.700 [3.000, 98.000], mean observation: 3.160 [-1.456, 10.256], loss: 0.992687, mae: 5.022940, mean_q: 5.233359
 64675/100000: episode: 6601, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.594, mean reward: 0.459 [0.434, 0.572], mean action: 30.200 [3.000, 75.000], mean observation: 3.156 [-1.375, 10.517], loss: 1.028220, mae: 5.023021, mean_q: 5.235932
 64685/100000: episode: 6602, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.263, mean reward: 0.426 [0.270, 0.548], mean action: 24.900 [3.000, 82.000], mean observation: 3.146 [-1.349, 10.227], loss: 1.388787, mae: 5.024558, mean_q: 5.237795
 64695/100000: episode: 6603, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.156, mean reward: 0.416 [0.320, 0.500], mean action: 20.700 [3.000, 93.000], mean observation: 3.155 [-1.594, 10.397], loss: 1.291830, mae: 5.024320, mean_q: 5.236366
 64705/100000: episode: 6604, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.012, mean reward: 0.401 [0.336, 0.494], mean action: 17.600 [3.000, 91.000], mean observation: 3.159 [-1.023, 10.377], loss: 1.137186, mae: 5.023660, mean_q: 5.235868
 64715/100000: episode: 6605, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.485, mean reward: 0.448 [0.327, 0.572], mean action: 36.500 [0.000, 93.000], mean observation: 3.158 [-0.983, 10.205], loss: 1.310539, mae: 5.024488, mean_q: 5.236516
 64725/100000: episode: 6606, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.085, mean reward: 0.408 [0.314, 0.563], mean action: 25.100 [2.000, 90.000], mean observation: 3.150 [-1.179, 10.328], loss: 1.213764, mae: 5.023976, mean_q: 5.231775
 64735/100000: episode: 6607, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.210, mean reward: 0.421 [0.369, 0.462], mean action: 19.000 [3.000, 56.000], mean observation: 3.158 [-1.266, 10.227], loss: 0.557719, mae: 5.021317, mean_q: 5.229712
 64745/100000: episode: 6608, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.841, mean reward: 0.384 [0.320, 0.431], mean action: 24.200 [3.000, 101.000], mean observation: 3.158 [-1.589, 10.379], loss: 1.268146, mae: 5.024239, mean_q: 5.230894
 64755/100000: episode: 6609, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 3.859, mean reward: 0.386 [0.343, 0.466], mean action: 31.000 [3.000, 98.000], mean observation: 3.153 [-1.310, 10.270], loss: 0.921610, mae: 5.023044, mean_q: 5.231374
 64765/100000: episode: 6610, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.339, mean reward: 0.434 [0.324, 0.511], mean action: 12.700 [3.000, 57.000], mean observation: 3.154 [-2.098, 10.307], loss: 1.072957, mae: 5.023896, mean_q: 5.230657
 64775/100000: episode: 6611, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.090, mean reward: 0.409 [0.329, 0.537], mean action: 32.900 [3.000, 87.000], mean observation: 3.156 [-1.738, 10.311], loss: 1.133749, mae: 5.024342, mean_q: 5.229978
 64785/100000: episode: 6612, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.280, mean reward: 0.428 [0.314, 0.534], mean action: 27.200 [3.000, 82.000], mean observation: 3.156 [-1.075, 10.334], loss: 0.999223, mae: 5.023963, mean_q: 5.228876
 64795/100000: episode: 6613, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.096, mean reward: 0.410 [0.369, 0.533], mean action: 37.800 [3.000, 96.000], mean observation: 3.157 [-1.365, 10.366], loss: 1.441092, mae: 5.026071, mean_q: 5.229207
 64805/100000: episode: 6614, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.194, mean reward: 0.419 [0.330, 0.457], mean action: 47.000 [3.000, 101.000], mean observation: 3.152 [-1.261, 10.234], loss: 1.358657, mae: 5.025660, mean_q: 5.226844
 64815/100000: episode: 6615, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.969, mean reward: 0.397 [0.388, 0.452], mean action: 42.900 [21.000, 89.000], mean observation: 3.161 [-1.122, 10.424], loss: 1.147937, mae: 5.024971, mean_q: 5.229745
 64825/100000: episode: 6616, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.948, mean reward: 0.395 [0.326, 0.450], mean action: 34.300 [10.000, 79.000], mean observation: 3.152 [-1.103, 10.415], loss: 0.746165, mae: 5.023465, mean_q: 5.232925
 64835/100000: episode: 6617, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.370, mean reward: 0.437 [0.398, 0.533], mean action: 26.500 [10.000, 44.000], mean observation: 3.160 [-1.130, 10.512], loss: 1.149440, mae: 5.025206, mean_q: 5.236106
 64845/100000: episode: 6618, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.288, mean reward: 0.429 [0.407, 0.555], mean action: 47.700 [25.000, 98.000], mean observation: 3.156 [-1.551, 10.314], loss: 0.921708, mae: 5.024786, mean_q: 5.235091
 64855/100000: episode: 6619, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.145, mean reward: 0.415 [0.278, 0.498], mean action: 30.300 [14.000, 60.000], mean observation: 3.157 [-1.576, 10.320], loss: 1.051941, mae: 5.025837, mean_q: 5.234554
 64865/100000: episode: 6620, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.962, mean reward: 0.396 [0.345, 0.481], mean action: 35.400 [25.000, 78.000], mean observation: 3.153 [-1.234, 10.310], loss: 1.037723, mae: 5.026489, mean_q: 5.234120
 64875/100000: episode: 6621, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.878, mean reward: 0.388 [0.330, 0.533], mean action: 32.100 [8.000, 77.000], mean observation: 3.160 [-1.029, 10.378], loss: 1.178617, mae: 5.027259, mean_q: 5.234496
 64885/100000: episode: 6622, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.290, mean reward: 0.429 [0.376, 0.519], mean action: 30.500 [9.000, 79.000], mean observation: 3.148 [-1.278, 10.505], loss: 1.208309, mae: 5.027749, mean_q: 5.235723
 64895/100000: episode: 6623, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.389, mean reward: 0.439 [0.363, 0.506], mean action: 32.200 [0.000, 88.000], mean observation: 3.169 [-2.068, 10.363], loss: 1.075269, mae: 5.027207, mean_q: 5.237447
 64905/100000: episode: 6624, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.055, mean reward: 0.406 [0.339, 0.484], mean action: 36.200 [4.000, 96.000], mean observation: 3.155 [-1.723, 10.294], loss: 1.321803, mae: 5.028307, mean_q: 5.237026
 64915/100000: episode: 6625, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 3.997, mean reward: 0.400 [0.334, 0.462], mean action: 30.500 [7.000, 74.000], mean observation: 3.160 [-1.304, 10.348], loss: 1.113475, mae: 5.027476, mean_q: 5.234614
 64925/100000: episode: 6626, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 4.195, mean reward: 0.420 [0.415, 0.443], mean action: 28.200 [7.000, 58.000], mean observation: 3.152 [-2.332, 10.269], loss: 1.391984, mae: 5.028628, mean_q: 5.235983
 64935/100000: episode: 6627, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.235, mean reward: 0.424 [0.361, 0.477], mean action: 33.700 [7.000, 85.000], mean observation: 3.166 [-0.884, 10.419], loss: 1.273109, mae: 5.028030, mean_q: 5.238001
 64945/100000: episode: 6628, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.448, mean reward: 0.445 [0.434, 0.508], mean action: 14.900 [7.000, 35.000], mean observation: 3.163 [-1.124, 10.288], loss: 1.010628, mae: 5.026973, mean_q: 5.238041
 64955/100000: episode: 6629, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.105, mean reward: 0.411 [0.296, 0.476], mean action: 29.800 [7.000, 91.000], mean observation: 3.149 [-1.145, 10.431], loss: 1.057584, mae: 5.027171, mean_q: 5.234983
 64965/100000: episode: 6630, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.470, mean reward: 0.447 [0.378, 0.475], mean action: 46.400 [7.000, 96.000], mean observation: 3.144 [-1.365, 10.228], loss: 1.106047, mae: 5.027479, mean_q: 5.235196
 64975/100000: episode: 6631, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.490, mean reward: 0.449 [0.410, 0.527], mean action: 36.200 [7.000, 76.000], mean observation: 3.149 [-1.811, 10.261], loss: 1.349622, mae: 5.028598, mean_q: 5.237097
 64985/100000: episode: 6632, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.175, mean reward: 0.418 [0.337, 0.547], mean action: 27.000 [5.000, 88.000], mean observation: 3.150 [-1.983, 10.268], loss: 1.406945, mae: 5.028642, mean_q: 5.239256
 64995/100000: episode: 6633, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.129, mean reward: 0.413 [0.356, 0.455], mean action: 35.700 [7.000, 92.000], mean observation: 3.159 [-1.411, 10.287], loss: 1.128379, mae: 5.027312, mean_q: 5.237502
 65005/100000: episode: 6634, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.051, mean reward: 0.405 [0.312, 0.478], mean action: 47.700 [25.000, 95.000], mean observation: 3.153 [-1.996, 10.311], loss: 1.319644, mae: 5.027779, mean_q: 5.234458
 65015/100000: episode: 6635, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 4.111, mean reward: 0.411 [0.411, 0.413], mean action: 58.000 [25.000, 101.000], mean observation: 3.145 [-0.806, 10.339], loss: 1.036692, mae: 5.026498, mean_q: 5.230949
 65025/100000: episode: 6636, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 3.975, mean reward: 0.398 [0.305, 0.494], mean action: 26.000 [5.000, 76.000], mean observation: 3.164 [-1.744, 10.431], loss: 1.219873, mae: 5.027542, mean_q: 5.227186
 65035/100000: episode: 6637, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.197, mean reward: 0.420 [0.395, 0.500], mean action: 52.800 [8.000, 88.000], mean observation: 3.166 [-1.154, 10.198], loss: 1.062073, mae: 5.026977, mean_q: 5.223007
 65045/100000: episode: 6638, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.878, mean reward: 0.388 [0.326, 0.426], mean action: 43.500 [25.000, 91.000], mean observation: 3.164 [-1.223, 10.353], loss: 1.278236, mae: 5.028005, mean_q: 5.220956
 65055/100000: episode: 6639, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.077, mean reward: 0.408 [0.342, 0.562], mean action: 28.200 [13.000, 72.000], mean observation: 3.161 [-1.106, 10.335], loss: 1.334152, mae: 5.028323, mean_q: 5.221326
 65065/100000: episode: 6640, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.176, mean reward: 0.418 [0.300, 0.493], mean action: 49.800 [4.000, 101.000], mean observation: 3.147 [-1.817, 10.280], loss: 1.509254, mae: 5.028781, mean_q: 5.222145
 65075/100000: episode: 6641, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 5.683, mean reward: 0.568 [0.568, 0.568], mean action: 35.200 [9.000, 79.000], mean observation: 3.159 [-1.246, 10.180], loss: 1.245974, mae: 5.027207, mean_q: 5.220443
 65085/100000: episode: 6642, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.804, mean reward: 0.380 [0.295, 0.484], mean action: 43.400 [20.000, 94.000], mean observation: 3.152 [-1.611, 10.391], loss: 1.361667, mae: 5.027480, mean_q: 5.219405
 65095/100000: episode: 6643, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.712, mean reward: 0.471 [0.443, 0.520], mean action: 40.700 [7.000, 95.000], mean observation: 3.151 [-1.510, 10.310], loss: 0.927029, mae: 5.025916, mean_q: 5.217994
 65105/100000: episode: 6644, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.117, mean reward: 0.412 [0.385, 0.478], mean action: 47.100 [25.000, 99.000], mean observation: 3.168 [-1.400, 10.391], loss: 1.349062, mae: 5.027472, mean_q: 5.219236
 65115/100000: episode: 6645, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.636, mean reward: 0.464 [0.463, 0.467], mean action: 48.800 [25.000, 93.000], mean observation: 3.162 [-1.292, 10.332], loss: 1.198267, mae: 5.026724, mean_q: 5.219021
 65125/100000: episode: 6646, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 3.989, mean reward: 0.399 [0.365, 0.427], mean action: 20.500 [0.000, 26.000], mean observation: 3.158 [-1.627, 10.362], loss: 1.119695, mae: 5.026324, mean_q: 5.219325
 65135/100000: episode: 6647, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.935, mean reward: 0.394 [0.279, 0.493], mean action: 38.000 [14.000, 87.000], mean observation: 3.159 [-1.731, 10.448], loss: 1.157611, mae: 5.026572, mean_q: 5.220738
 65145/100000: episode: 6648, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.608, mean reward: 0.461 [0.402, 0.519], mean action: 39.300 [25.000, 88.000], mean observation: 3.145 [-1.498, 10.353], loss: 1.388141, mae: 5.027307, mean_q: 5.220023
 65155/100000: episode: 6649, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.442, mean reward: 0.444 [0.391, 0.500], mean action: 33.700 [5.000, 100.000], mean observation: 3.160 [-1.230, 10.338], loss: 1.234159, mae: 5.026155, mean_q: 5.218583
 65165/100000: episode: 6650, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.898, mean reward: 0.390 [0.334, 0.428], mean action: 47.100 [19.000, 101.000], mean observation: 3.151 [-0.967, 10.471], loss: 1.347918, mae: 5.026550, mean_q: 5.219378
 65175/100000: episode: 6651, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.918, mean reward: 0.392 [0.336, 0.557], mean action: 47.100 [10.000, 99.000], mean observation: 3.153 [-1.177, 10.314], loss: 1.579986, mae: 5.027125, mean_q: 5.221376
 65185/100000: episode: 6652, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.306, mean reward: 0.431 [0.341, 0.521], mean action: 39.400 [12.000, 94.000], mean observation: 3.162 [-2.359, 10.484], loss: 1.001066, mae: 5.024363, mean_q: 5.219869
 65195/100000: episode: 6653, duration: 0.217s, episode steps: 10, steps per second: 46, episode reward: 3.873, mean reward: 0.387 [0.296, 0.444], mean action: 20.400 [3.000, 68.000], mean observation: 3.158 [-1.420, 10.287], loss: 1.375146, mae: 5.025508, mean_q: 5.217310
 65205/100000: episode: 6654, duration: 0.220s, episode steps: 10, steps per second: 45, episode reward: 3.867, mean reward: 0.387 [0.276, 0.470], mean action: 16.100 [3.000, 70.000], mean observation: 3.150 [-1.498, 10.308], loss: 1.196948, mae: 5.024724, mean_q: 5.217658
 65215/100000: episode: 6655, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.680, mean reward: 0.368 [0.278, 0.475], mean action: 32.300 [3.000, 91.000], mean observation: 3.158 [-1.489, 10.204], loss: 1.345782, mae: 5.025125, mean_q: 5.216515
 65225/100000: episode: 6656, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.026, mean reward: 0.403 [0.340, 0.474], mean action: 34.100 [5.000, 88.000], mean observation: 3.159 [-1.093, 10.233], loss: 1.137969, mae: 5.024344, mean_q: 5.214338
 65235/100000: episode: 6657, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.729, mean reward: 0.373 [0.304, 0.445], mean action: 34.700 [14.000, 65.000], mean observation: 3.147 [-1.474, 10.364], loss: 1.261589, mae: 5.025016, mean_q: 5.213177
 65245/100000: episode: 6658, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.776, mean reward: 0.478 [0.320, 0.500], mean action: 27.000 [1.000, 51.000], mean observation: 3.135 [-1.262, 10.236], loss: 1.222002, mae: 5.024691, mean_q: 5.213067
 65255/100000: episode: 6659, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.788, mean reward: 0.379 [0.277, 0.520], mean action: 41.000 [6.000, 83.000], mean observation: 3.151 [-1.815, 10.457], loss: 1.305691, mae: 5.024906, mean_q: 5.213277
 65265/100000: episode: 6660, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.084, mean reward: 0.408 [0.312, 0.555], mean action: 16.400 [3.000, 43.000], mean observation: 3.158 [-1.468, 10.200], loss: 1.256781, mae: 5.024637, mean_q: 5.206885
 65275/100000: episode: 6661, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.929, mean reward: 0.393 [0.368, 0.474], mean action: 58.300 [3.000, 101.000], mean observation: 3.160 [-1.027, 10.246], loss: 0.896930, mae: 5.023261, mean_q: 5.203374
 65285/100000: episode: 6662, duration: 0.105s, episode steps: 10, steps per second: 96, episode reward: 3.876, mean reward: 0.388 [0.339, 0.426], mean action: 78.000 [23.000, 101.000], mean observation: 3.146 [-1.224, 10.278], loss: 1.074847, mae: 5.024251, mean_q: 5.204050
 65295/100000: episode: 6663, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.646, mean reward: 0.465 [0.389, 0.549], mean action: 62.400 [6.000, 101.000], mean observation: 3.161 [-1.213, 10.200], loss: 1.088561, mae: 5.024483, mean_q: 5.204314
 65305/100000: episode: 6664, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.002, mean reward: 0.400 [0.386, 0.462], mean action: 43.600 [10.000, 87.000], mean observation: 3.144 [-1.962, 10.315], loss: 1.376003, mae: 5.025502, mean_q: 5.204805
 65309/100000: episode: 6665, duration: 0.084s, episode steps: 4, steps per second: 48, episode reward: 11.295, mean reward: 2.824 [0.423, 10.000], mean action: 38.250 [14.000, 63.000], mean observation: 3.160 [-0.826, 10.436], loss: 0.952605, mae: 5.023738, mean_q: 5.205948
 65319/100000: episode: 6666, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.160, mean reward: 0.416 [0.335, 0.510], mean action: 28.900 [0.000, 38.000], mean observation: 3.154 [-1.643, 10.390], loss: 1.247237, mae: 5.024804, mean_q: 5.207273
 65329/100000: episode: 6667, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.333, mean reward: 0.433 [0.323, 0.567], mean action: 39.700 [5.000, 77.000], mean observation: 3.156 [-1.343, 10.305], loss: 1.083583, mae: 5.024262, mean_q: 5.209439
 65339/100000: episode: 6668, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.023, mean reward: 0.402 [0.325, 0.479], mean action: 38.400 [14.000, 64.000], mean observation: 3.154 [-1.869, 10.329], loss: 1.139788, mae: 5.024057, mean_q: 5.211246
 65349/100000: episode: 6669, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.945, mean reward: 0.395 [0.291, 0.428], mean action: 38.200 [4.000, 84.000], mean observation: 3.155 [-1.028, 10.203], loss: 1.164983, mae: 5.024415, mean_q: 5.212726
 65359/100000: episode: 6670, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.931, mean reward: 0.393 [0.289, 0.468], mean action: 44.700 [38.000, 80.000], mean observation: 3.168 [-1.014, 10.252], loss: 1.136691, mae: 5.024446, mean_q: 5.213837
 65360/100000: episode: 6671, duration: 0.029s, episode steps: 1, steps per second: 34, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 15.000 [15.000, 15.000], mean observation: 3.153 [-0.844, 10.340], loss: 1.799052, mae: 5.027390, mean_q: 5.213567
 65370/100000: episode: 6672, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.591, mean reward: 0.459 [0.394, 0.530], mean action: 48.100 [8.000, 88.000], mean observation: 3.147 [-1.496, 10.261], loss: 1.060155, mae: 5.024209, mean_q: 5.212320
 65378/100000: episode: 6673, duration: 0.135s, episode steps: 8, steps per second: 59, episode reward: 13.021, mean reward: 1.628 [0.427, 10.000], mean action: 42.000 [13.000, 78.000], mean observation: 3.149 [-1.396, 10.269], loss: 1.349685, mae: 5.025434, mean_q: 5.208786
 65388/100000: episode: 6674, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.188, mean reward: 0.419 [0.325, 0.551], mean action: 51.800 [5.000, 95.000], mean observation: 3.165 [-1.934, 10.473], loss: 1.265323, mae: 5.025080, mean_q: 5.203272
 65398/100000: episode: 6675, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.941, mean reward: 0.394 [0.340, 0.425], mean action: 64.000 [24.000, 101.000], mean observation: 3.166 [-0.886, 10.351], loss: 1.419521, mae: 5.025465, mean_q: 5.199021
 65408/100000: episode: 6676, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.409, mean reward: 0.441 [0.427, 0.504], mean action: 64.800 [3.000, 101.000], mean observation: 3.165 [-1.132, 10.299], loss: 1.201838, mae: 5.024774, mean_q: 5.199691
 65418/100000: episode: 6677, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.974, mean reward: 0.397 [0.380, 0.499], mean action: 65.400 [8.000, 101.000], mean observation: 3.155 [-1.651, 10.222], loss: 1.044690, mae: 5.024081, mean_q: 5.200268
 65428/100000: episode: 6678, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.024, mean reward: 0.402 [0.376, 0.410], mean action: 73.800 [23.000, 101.000], mean observation: 3.156 [-1.170, 10.282], loss: 1.141889, mae: 5.024515, mean_q: 5.201509
 65438/100000: episode: 6679, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.670, mean reward: 0.367 [0.326, 0.457], mean action: 45.900 [11.000, 101.000], mean observation: 3.159 [-1.398, 10.328], loss: 1.269484, mae: 5.025252, mean_q: 5.203571
 65448/100000: episode: 6680, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.156, mean reward: 0.416 [0.399, 0.515], mean action: 41.900 [15.000, 83.000], mean observation: 3.152 [-1.198, 10.400], loss: 1.358666, mae: 5.025630, mean_q: 5.205759
 65458/100000: episode: 6681, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.929, mean reward: 0.393 [0.328, 0.452], mean action: 64.800 [38.000, 101.000], mean observation: 3.158 [-1.391, 10.358], loss: 1.303357, mae: 5.025107, mean_q: 5.206863
 65468/100000: episode: 6682, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.690, mean reward: 0.469 [0.375, 0.560], mean action: 46.300 [17.000, 87.000], mean observation: 3.155 [-1.289, 10.406], loss: 1.479654, mae: 5.025462, mean_q: 5.205522
 65475/100000: episode: 6683, duration: 0.131s, episode steps: 7, steps per second: 53, episode reward: 12.337, mean reward: 1.762 [0.349, 10.000], mean action: 44.000 [38.000, 57.000], mean observation: 3.162 [-1.777, 10.358], loss: 1.282523, mae: 5.024329, mean_q: 5.204921
 65485/100000: episode: 6684, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.991, mean reward: 0.399 [0.321, 0.478], mean action: 40.500 [5.000, 101.000], mean observation: 3.154 [-2.168, 10.329], loss: 1.723018, mae: 5.025808, mean_q: 5.205131
 65495/100000: episode: 6685, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.156, mean reward: 0.416 [0.313, 0.486], mean action: 38.000 [38.000, 38.000], mean observation: 3.152 [-1.702, 10.453], loss: 1.029661, mae: 5.023052, mean_q: 5.205785
 65505/100000: episode: 6686, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.192, mean reward: 0.419 [0.408, 0.472], mean action: 36.800 [13.000, 50.000], mean observation: 3.165 [-1.231, 10.277], loss: 1.377479, mae: 5.024102, mean_q: 5.207914
 65515/100000: episode: 6687, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.898, mean reward: 0.390 [0.325, 0.484], mean action: 47.200 [37.000, 79.000], mean observation: 3.150 [-2.053, 10.346], loss: 0.887474, mae: 5.022171, mean_q: 5.210290
 65519/100000: episode: 6688, duration: 0.086s, episode steps: 4, steps per second: 47, episode reward: 11.378, mean reward: 2.844 [0.459, 10.000], mean action: 30.250 [4.000, 41.000], mean observation: 3.161 [-1.621, 10.398], loss: 1.490867, mae: 5.024703, mean_q: 5.211720
 65529/100000: episode: 6689, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.354, mean reward: 0.435 [0.371, 0.553], mean action: 40.300 [7.000, 100.000], mean observation: 3.153 [-0.825, 10.286], loss: 1.500288, mae: 5.024399, mean_q: 5.210371
 65539/100000: episode: 6690, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.645, mean reward: 0.364 [0.315, 0.422], mean action: 37.200 [2.000, 54.000], mean observation: 3.159 [-1.076, 10.304], loss: 1.347094, mae: 5.023671, mean_q: 5.210130
 65549/100000: episode: 6691, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.152, mean reward: 0.415 [0.366, 0.534], mean action: 46.000 [38.000, 92.000], mean observation: 3.161 [-1.141, 10.405], loss: 1.125038, mae: 5.022720, mean_q: 5.206470
 65559/100000: episode: 6692, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.517, mean reward: 0.452 [0.373, 0.547], mean action: 37.500 [2.000, 101.000], mean observation: 3.153 [-1.050, 10.294], loss: 0.985964, mae: 5.022372, mean_q: 5.198740
 65569/100000: episode: 6693, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.920, mean reward: 0.392 [0.359, 0.468], mean action: 53.700 [23.000, 99.000], mean observation: 3.161 [-1.255, 10.299], loss: 1.258758, mae: 5.024027, mean_q: 5.196884
 65579/100000: episode: 6694, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.328, mean reward: 0.433 [0.409, 0.568], mean action: 51.400 [38.000, 81.000], mean observation: 3.153 [-2.060, 10.331], loss: 1.369963, mae: 5.024240, mean_q: 5.195836
 65589/100000: episode: 6695, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.159, mean reward: 0.416 [0.416, 0.416], mean action: 78.600 [8.000, 101.000], mean observation: 3.163 [-1.384, 10.258], loss: 1.145667, mae: 5.023327, mean_q: 5.195058
 65599/100000: episode: 6696, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.238, mean reward: 0.424 [0.416, 0.441], mean action: 74.500 [15.000, 101.000], mean observation: 3.156 [-0.774, 10.244], loss: 1.141967, mae: 5.023508, mean_q: 5.196297
 65609/100000: episode: 6697, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.400, mean reward: 0.440 [0.386, 0.545], mean action: 64.300 [4.000, 101.000], mean observation: 3.144 [-1.007, 10.381], loss: 1.030570, mae: 5.022979, mean_q: 5.196859
 65619/100000: episode: 6698, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.116, mean reward: 0.412 [0.325, 0.522], mean action: 61.800 [2.000, 101.000], mean observation: 3.175 [-1.201, 10.331], loss: 1.397782, mae: 5.024366, mean_q: 5.197037
 65629/100000: episode: 6699, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.056, mean reward: 0.406 [0.369, 0.468], mean action: 66.200 [25.000, 101.000], mean observation: 3.157 [-1.532, 10.245], loss: 1.227527, mae: 5.023389, mean_q: 5.197726
 65639/100000: episode: 6700, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.359, mean reward: 0.436 [0.377, 0.503], mean action: 47.800 [2.000, 99.000], mean observation: 3.157 [-1.079, 10.412], loss: 1.280046, mae: 5.023683, mean_q: 5.199621
 65649/100000: episode: 6701, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.226, mean reward: 0.423 [0.414, 0.503], mean action: 70.900 [11.000, 101.000], mean observation: 3.165 [-1.466, 10.270], loss: 1.288336, mae: 5.023449, mean_q: 5.199275
 65659/100000: episode: 6702, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.000, mean reward: 0.400 [0.333, 0.453], mean action: 60.400 [1.000, 101.000], mean observation: 3.163 [-1.446, 10.363], loss: 1.393223, mae: 5.023591, mean_q: 5.200885
 65669/100000: episode: 6703, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.169, mean reward: 0.417 [0.397, 0.457], mean action: 71.500 [18.000, 101.000], mean observation: 3.159 [-1.352, 10.365], loss: 1.132646, mae: 5.022464, mean_q: 5.202681
 65679/100000: episode: 6704, duration: 0.096s, episode steps: 10, steps per second: 104, episode reward: 3.674, mean reward: 0.367 [0.358, 0.387], mean action: 86.800 [34.000, 101.000], mean observation: 3.148 [-0.900, 10.168], loss: 1.365841, mae: 5.023200, mean_q: 5.204404
 65689/100000: episode: 6705, duration: 0.108s, episode steps: 10, steps per second: 92, episode reward: 3.895, mean reward: 0.389 [0.381, 0.425], mean action: 82.600 [8.000, 101.000], mean observation: 3.149 [-2.195, 10.202], loss: 0.789322, mae: 5.020988, mean_q: 5.206237
 65699/100000: episode: 6706, duration: 0.096s, episode steps: 10, steps per second: 104, episode reward: 3.578, mean reward: 0.358 [0.347, 0.459], mean action: 89.300 [6.000, 101.000], mean observation: 3.164 [-1.221, 10.124], loss: 1.334427, mae: 5.023293, mean_q: 5.208071
 65709/100000: episode: 6707, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.719, mean reward: 0.472 [0.465, 0.475], mean action: 59.300 [2.000, 101.000], mean observation: 3.162 [-0.801, 10.213], loss: 0.999783, mae: 5.022217, mean_q: 5.209978
 65719/100000: episode: 6708, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.874, mean reward: 0.387 [0.334, 0.559], mean action: 73.200 [3.000, 101.000], mean observation: 3.169 [-1.222, 10.542], loss: 1.445679, mae: 5.024095, mean_q: 5.212490
 65729/100000: episode: 6709, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 4.236, mean reward: 0.424 [0.382, 0.476], mean action: 78.100 [22.000, 101.000], mean observation: 3.162 [-1.137, 10.490], loss: 1.097554, mae: 5.022891, mean_q: 5.210351
 65739/100000: episode: 6710, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.904, mean reward: 0.490 [0.490, 0.490], mean action: 55.100 [19.000, 101.000], mean observation: 3.153 [-1.531, 10.298], loss: 1.052770, mae: 5.022801, mean_q: 5.210414
 65749/100000: episode: 6711, duration: 0.102s, episode steps: 10, steps per second: 98, episode reward: 3.390, mean reward: 0.339 [0.313, 0.383], mean action: 84.800 [4.000, 101.000], mean observation: 3.163 [-1.237, 10.252], loss: 1.058915, mae: 5.022788, mean_q: 5.211082
 65759/100000: episode: 6712, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 4.388, mean reward: 0.439 [0.425, 0.440], mean action: 83.400 [26.000, 101.000], mean observation: 3.145 [-1.439, 10.157], loss: 1.361326, mae: 5.024041, mean_q: 5.211443
 65769/100000: episode: 6713, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.573, mean reward: 0.457 [0.359, 0.572], mean action: 79.700 [4.000, 101.000], mean observation: 3.150 [-1.193, 10.196], loss: 0.924689, mae: 5.022217, mean_q: 5.212718
 65779/100000: episode: 6714, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 4.640, mean reward: 0.464 [0.464, 0.464], mean action: 72.400 [40.000, 101.000], mean observation: 3.171 [-1.748, 10.277], loss: 1.240998, mae: 5.023499, mean_q: 5.211016
 65789/100000: episode: 6715, duration: 0.093s, episode steps: 10, steps per second: 108, episode reward: 3.334, mean reward: 0.333 [0.327, 0.343], mean action: 95.000 [45.000, 101.000], mean observation: 3.152 [-1.137, 10.175], loss: 1.336247, mae: 5.023618, mean_q: 5.205201
 65799/100000: episode: 6716, duration: 0.099s, episode steps: 10, steps per second: 101, episode reward: 4.294, mean reward: 0.429 [0.391, 0.583], mean action: 85.000 [17.000, 101.000], mean observation: 3.158 [-1.584, 10.572], loss: 1.200905, mae: 5.023144, mean_q: 5.196719
 65809/100000: episode: 6717, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.916, mean reward: 0.392 [0.338, 0.461], mean action: 66.500 [23.000, 101.000], mean observation: 3.163 [-1.188, 10.371], loss: 1.132955, mae: 5.022730, mean_q: 5.191262
 65819/100000: episode: 6718, duration: 0.086s, episode steps: 10, steps per second: 117, episode reward: 3.383, mean reward: 0.338 [0.330, 0.358], mean action: 94.400 [41.000, 101.000], mean observation: 3.156 [-1.069, 10.209], loss: 1.496956, mae: 5.024066, mean_q: 5.190755
 65829/100000: episode: 6719, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 4.469, mean reward: 0.447 [0.447, 0.447], mean action: 80.700 [22.000, 101.000], mean observation: 3.168 [-0.744, 10.378], loss: 1.409447, mae: 5.023668, mean_q: 5.194400
 65839/100000: episode: 6720, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.159, mean reward: 0.416 [0.383, 0.497], mean action: 60.900 [6.000, 101.000], mean observation: 3.154 [-1.648, 10.265], loss: 1.130780, mae: 5.022649, mean_q: 5.197262
 65849/100000: episode: 6721, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.167, mean reward: 0.417 [0.417, 0.417], mean action: 76.600 [36.000, 101.000], mean observation: 3.155 [-1.038, 10.204], loss: 1.084521, mae: 5.022278, mean_q: 5.197249
 65859/100000: episode: 6722, duration: 0.123s, episode steps: 10, steps per second: 82, episode reward: 3.794, mean reward: 0.379 [0.328, 0.426], mean action: 71.900 [12.000, 101.000], mean observation: 3.171 [-1.032, 10.287], loss: 0.963857, mae: 5.022253, mean_q: 5.198409
 65869/100000: episode: 6723, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 3.859, mean reward: 0.386 [0.332, 0.467], mean action: 77.400 [8.000, 101.000], mean observation: 3.145 [-2.197, 10.226], loss: 1.332743, mae: 5.023588, mean_q: 5.197595
 65879/100000: episode: 6724, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.659, mean reward: 0.466 [0.403, 0.525], mean action: 77.700 [16.000, 101.000], mean observation: 3.162 [-1.668, 10.512], loss: 1.198774, mae: 5.022918, mean_q: 5.194656
 65889/100000: episode: 6725, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.031, mean reward: 0.403 [0.360, 0.525], mean action: 65.100 [6.000, 101.000], mean observation: 3.170 [-1.348, 10.504], loss: 1.048458, mae: 5.022573, mean_q: 5.194293
 65899/100000: episode: 6726, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.114, mean reward: 0.411 [0.400, 0.429], mean action: 73.600 [3.000, 101.000], mean observation: 3.140 [-0.733, 10.139], loss: 1.111753, mae: 5.022582, mean_q: 5.192609
 65909/100000: episode: 6727, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.397, mean reward: 0.440 [0.335, 0.493], mean action: 55.600 [2.000, 101.000], mean observation: 3.166 [-1.293, 10.333], loss: 1.410570, mae: 5.023687, mean_q: 5.190860
 65919/100000: episode: 6728, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 3.712, mean reward: 0.371 [0.323, 0.430], mean action: 83.800 [36.000, 101.000], mean observation: 3.156 [-2.200, 10.309], loss: 1.279668, mae: 5.023109, mean_q: 5.192015
 65929/100000: episode: 6729, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 3.675, mean reward: 0.367 [0.315, 0.459], mean action: 67.600 [10.000, 101.000], mean observation: 3.146 [-1.336, 10.278], loss: 0.893625, mae: 5.021622, mean_q: 5.194967
 65939/100000: episode: 6730, duration: 0.089s, episode steps: 10, steps per second: 113, episode reward: 4.118, mean reward: 0.412 [0.412, 0.412], mean action: 91.500 [57.000, 101.000], mean observation: 3.150 [-1.527, 10.229], loss: 1.496058, mae: 5.024051, mean_q: 5.197745
 65949/100000: episode: 6731, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.602, mean reward: 0.460 [0.414, 0.504], mean action: 73.900 [1.000, 101.000], mean observation: 3.149 [-0.938, 10.350], loss: 1.199227, mae: 5.022650, mean_q: 5.199724
 65959/100000: episode: 6732, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.953, mean reward: 0.395 [0.346, 0.488], mean action: 65.500 [1.000, 101.000], mean observation: 3.164 [-1.984, 10.437], loss: 1.567396, mae: 5.024068, mean_q: 5.202696
 65969/100000: episode: 6733, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.708, mean reward: 0.471 [0.471, 0.471], mean action: 77.800 [29.000, 101.000], mean observation: 3.151 [-1.580, 10.270], loss: 1.121059, mae: 5.022139, mean_q: 5.204589
 65979/100000: episode: 6734, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.987, mean reward: 0.399 [0.346, 0.552], mean action: 73.300 [32.000, 101.000], mean observation: 3.171 [-0.999, 10.415], loss: 0.953314, mae: 5.021242, mean_q: 5.206138
 65989/100000: episode: 6735, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 3.264, mean reward: 0.326 [0.313, 0.371], mean action: 87.300 [52.000, 101.000], mean observation: 3.160 [-0.876, 10.289], loss: 1.379373, mae: 5.023071, mean_q: 5.205771
 65999/100000: episode: 6736, duration: 0.092s, episode steps: 10, steps per second: 109, episode reward: 4.472, mean reward: 0.447 [0.440, 0.460], mean action: 87.100 [31.000, 101.000], mean observation: 3.141 [-1.025, 10.350], loss: 1.270852, mae: 5.022677, mean_q: 5.200688
 66009/100000: episode: 6737, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 4.426, mean reward: 0.443 [0.359, 0.493], mean action: 73.800 [9.000, 101.000], mean observation: 3.152 [-0.731, 10.480], loss: 1.053658, mae: 5.021698, mean_q: 5.197820
 66019/100000: episode: 6738, duration: 0.092s, episode steps: 10, steps per second: 109, episode reward: 3.443, mean reward: 0.344 [0.332, 0.454], mean action: 91.600 [7.000, 101.000], mean observation: 3.158 [-1.305, 10.240], loss: 1.099797, mae: 5.021999, mean_q: 5.197910
 66029/100000: episode: 6739, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 5.219, mean reward: 0.522 [0.393, 0.561], mean action: 65.400 [5.000, 101.000], mean observation: 3.148 [-1.492, 10.268], loss: 1.373206, mae: 5.023216, mean_q: 5.199149
 66039/100000: episode: 6740, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 3.895, mean reward: 0.390 [0.373, 0.422], mean action: 74.000 [2.000, 101.000], mean observation: 3.167 [-1.481, 10.289], loss: 1.195365, mae: 5.022492, mean_q: 5.199445
 66049/100000: episode: 6741, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 4.433, mean reward: 0.443 [0.390, 0.479], mean action: 75.100 [28.000, 101.000], mean observation: 3.165 [-1.379, 10.158], loss: 1.244207, mae: 5.022658, mean_q: 5.196286
 66059/100000: episode: 6742, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 4.468, mean reward: 0.447 [0.407, 0.463], mean action: 78.400 [23.000, 101.000], mean observation: 3.160 [-0.929, 10.309], loss: 1.426480, mae: 5.023226, mean_q: 5.195787
 66069/100000: episode: 6743, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.120, mean reward: 0.412 [0.333, 0.504], mean action: 57.600 [1.000, 101.000], mean observation: 3.155 [-1.625, 10.345], loss: 1.299295, mae: 5.022626, mean_q: 5.196248
 66079/100000: episode: 6744, duration: 0.093s, episode steps: 10, steps per second: 108, episode reward: 4.172, mean reward: 0.417 [0.417, 0.417], mean action: 97.900 [73.000, 101.000], mean observation: 3.162 [-0.825, 10.196], loss: 1.428119, mae: 5.022821, mean_q: 5.196562
 66089/100000: episode: 6745, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.719, mean reward: 0.472 [0.438, 0.506], mean action: 68.800 [27.000, 84.000], mean observation: 3.153 [-1.195, 10.457], loss: 1.289222, mae: 5.021852, mean_q: 5.194772
 66099/100000: episode: 6746, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.141, mean reward: 0.414 [0.332, 0.472], mean action: 66.700 [11.000, 101.000], mean observation: 3.148 [-0.860, 10.344], loss: 1.237643, mae: 5.021350, mean_q: 5.193987
 66109/100000: episode: 6747, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.604, mean reward: 0.460 [0.448, 0.490], mean action: 67.400 [14.000, 85.000], mean observation: 3.150 [-1.100, 10.323], loss: 1.532558, mae: 5.022489, mean_q: 5.192189
 66119/100000: episode: 6748, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.990, mean reward: 0.399 [0.322, 0.539], mean action: 68.500 [14.000, 101.000], mean observation: 3.150 [-1.385, 10.426], loss: 1.001276, mae: 5.019955, mean_q: 5.186616
 66129/100000: episode: 6749, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.376, mean reward: 0.438 [0.345, 0.454], mean action: 45.000 [2.000, 89.000], mean observation: 3.139 [-1.618, 10.319], loss: 1.059111, mae: 5.019979, mean_q: 5.185760
 66137/100000: episode: 6750, duration: 0.124s, episode steps: 8, steps per second: 64, episode reward: 12.552, mean reward: 1.569 [0.339, 10.000], mean action: 54.250 [31.000, 88.000], mean observation: 3.158 [-1.241, 10.167], loss: 1.133435, mae: 5.020402, mean_q: 5.186758
 66147/100000: episode: 6751, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.456, mean reward: 0.446 [0.360, 0.514], mean action: 32.900 [9.000, 69.000], mean observation: 3.154 [-1.915, 10.313], loss: 1.433814, mae: 5.021626, mean_q: 5.187545
 66157/100000: episode: 6752, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.003, mean reward: 0.400 [0.313, 0.452], mean action: 44.300 [5.000, 87.000], mean observation: 3.163 [-1.246, 10.347], loss: 1.526476, mae: 5.021846, mean_q: 5.189370
 66167/100000: episode: 6753, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.244, mean reward: 0.424 [0.379, 0.501], mean action: 53.500 [31.000, 99.000], mean observation: 3.171 [-1.541, 10.367], loss: 1.276562, mae: 5.020504, mean_q: 5.192385
 66177/100000: episode: 6754, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.130, mean reward: 0.413 [0.404, 0.458], mean action: 48.700 [33.000, 100.000], mean observation: 3.144 [-1.387, 10.352], loss: 1.048449, mae: 5.019652, mean_q: 5.194807
 66187/100000: episode: 6755, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.001, mean reward: 0.400 [0.359, 0.508], mean action: 36.900 [7.000, 58.000], mean observation: 3.139 [-1.694, 10.424], loss: 1.507856, mae: 5.021220, mean_q: 5.196255
 66197/100000: episode: 6756, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.207, mean reward: 0.421 [0.368, 0.441], mean action: 54.500 [4.000, 99.000], mean observation: 3.165 [-0.880, 10.191], loss: 0.965822, mae: 5.019071, mean_q: 5.198039
 66207/100000: episode: 6757, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.778, mean reward: 0.378 [0.330, 0.511], mean action: 43.500 [32.000, 69.000], mean observation: 3.150 [-1.103, 10.341], loss: 1.200413, mae: 5.020149, mean_q: 5.197726
 66217/100000: episode: 6758, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.619, mean reward: 0.362 [0.316, 0.417], mean action: 39.300 [15.000, 74.000], mean observation: 3.140 [-1.510, 10.239], loss: 1.229224, mae: 5.020469, mean_q: 5.196894
 66227/100000: episode: 6759, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.438, mean reward: 0.444 [0.432, 0.465], mean action: 58.100 [38.000, 92.000], mean observation: 3.147 [-1.041, 10.221], loss: 1.582461, mae: 5.021841, mean_q: 5.192864
 66237/100000: episode: 6760, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.623, mean reward: 0.362 [0.290, 0.494], mean action: 60.400 [50.000, 83.000], mean observation: 3.156 [-1.120, 10.213], loss: 1.563257, mae: 5.021611, mean_q: 5.193155
 66247/100000: episode: 6761, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.941, mean reward: 0.394 [0.323, 0.502], mean action: 44.400 [26.000, 50.000], mean observation: 3.169 [-1.031, 10.308], loss: 0.960469, mae: 5.019019, mean_q: 5.194094
 66257/100000: episode: 6762, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.055, mean reward: 0.405 [0.322, 0.521], mean action: 41.800 [6.000, 52.000], mean observation: 3.156 [-1.714, 10.404], loss: 0.879420, mae: 5.018715, mean_q: 5.193655
 66267/100000: episode: 6763, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.842, mean reward: 0.384 [0.278, 0.516], mean action: 41.000 [8.000, 80.000], mean observation: 3.154 [-2.073, 10.370], loss: 1.183640, mae: 5.020200, mean_q: 5.191879
 66277/100000: episode: 6764, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.963, mean reward: 0.496 [0.435, 0.558], mean action: 56.400 [6.000, 62.000], mean observation: 3.165 [-0.799, 10.298], loss: 1.163530, mae: 5.020203, mean_q: 5.192420
 66287/100000: episode: 6765, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.213, mean reward: 0.421 [0.294, 0.527], mean action: 57.000 [3.000, 97.000], mean observation: 3.156 [-1.974, 10.258], loss: 1.185128, mae: 5.020473, mean_q: 5.193793
 66297/100000: episode: 6766, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.702, mean reward: 0.370 [0.268, 0.442], mean action: 56.600 [10.000, 91.000], mean observation: 3.154 [-1.200, 10.417], loss: 1.056384, mae: 5.019885, mean_q: 5.195192
 66307/100000: episode: 6767, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 3.825, mean reward: 0.382 [0.329, 0.414], mean action: 45.100 [0.000, 95.000], mean observation: 3.155 [-1.333, 10.297], loss: 1.325248, mae: 5.020997, mean_q: 5.196804
 66317/100000: episode: 6768, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 3.980, mean reward: 0.398 [0.347, 0.465], mean action: 60.700 [24.000, 101.000], mean observation: 3.156 [-1.659, 10.239], loss: 1.376722, mae: 5.020895, mean_q: 5.196181
 66327/100000: episode: 6769, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.022, mean reward: 0.402 [0.359, 0.532], mean action: 46.500 [8.000, 101.000], mean observation: 3.145 [-1.274, 10.279], loss: 1.266223, mae: 5.020568, mean_q: 5.190425
 66337/100000: episode: 6770, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.070, mean reward: 0.407 [0.313, 0.521], mean action: 24.100 [5.000, 101.000], mean observation: 3.156 [-1.157, 10.314], loss: 1.198766, mae: 5.020331, mean_q: 5.189551
 66347/100000: episode: 6771, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.443, mean reward: 0.444 [0.437, 0.457], mean action: 66.800 [8.000, 96.000], mean observation: 3.143 [-0.928, 10.275], loss: 1.205441, mae: 5.020424, mean_q: 5.189124
 66357/100000: episode: 6772, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.130, mean reward: 0.413 [0.322, 0.491], mean action: 35.200 [7.000, 83.000], mean observation: 3.166 [-1.500, 10.438], loss: 0.958865, mae: 5.019603, mean_q: 5.190852
 66367/100000: episode: 6773, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.159, mean reward: 0.416 [0.317, 0.554], mean action: 37.900 [8.000, 92.000], mean observation: 3.163 [-1.224, 10.299], loss: 0.987296, mae: 5.019840, mean_q: 5.193624
 66377/100000: episode: 6774, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.113, mean reward: 0.411 [0.367, 0.473], mean action: 29.800 [8.000, 53.000], mean observation: 3.153 [-1.970, 10.374], loss: 1.053367, mae: 5.020249, mean_q: 5.196125
 66387/100000: episode: 6775, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.165, mean reward: 0.416 [0.357, 0.502], mean action: 49.500 [0.000, 95.000], mean observation: 3.150 [-1.063, 10.266], loss: 1.078577, mae: 5.020538, mean_q: 5.197817
 66397/100000: episode: 6776, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.287, mean reward: 0.429 [0.374, 0.507], mean action: 48.300 [23.000, 97.000], mean observation: 3.148 [-2.126, 10.303], loss: 1.122976, mae: 5.021001, mean_q: 5.199737
 66407/100000: episode: 6777, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.853, mean reward: 0.385 [0.311, 0.492], mean action: 27.100 [7.000, 71.000], mean observation: 3.160 [-1.234, 10.321], loss: 0.923275, mae: 5.020501, mean_q: 5.200616
 66417/100000: episode: 6778, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.104, mean reward: 0.410 [0.333, 0.480], mean action: 26.300 [8.000, 76.000], mean observation: 3.146 [-1.165, 10.320], loss: 1.006584, mae: 5.021052, mean_q: 5.202604
 66427/100000: episode: 6779, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.037, mean reward: 0.404 [0.308, 0.564], mean action: 17.600 [4.000, 73.000], mean observation: 3.157 [-1.681, 10.264], loss: 1.189200, mae: 5.022196, mean_q: 5.204566
 66437/100000: episode: 6780, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.330, mean reward: 0.433 [0.325, 0.487], mean action: 28.900 [8.000, 100.000], mean observation: 3.161 [-1.824, 10.360], loss: 1.327299, mae: 5.022714, mean_q: 5.207199
 66447/100000: episode: 6781, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.048, mean reward: 0.405 [0.345, 0.544], mean action: 19.100 [8.000, 98.000], mean observation: 3.158 [-1.148, 10.401], loss: 1.065754, mae: 5.021517, mean_q: 5.207319
 66457/100000: episode: 6782, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.163, mean reward: 0.416 [0.362, 0.518], mean action: 24.500 [8.000, 93.000], mean observation: 3.155 [-1.686, 10.415], loss: 1.137752, mae: 5.021646, mean_q: 5.202438
 66467/100000: episode: 6783, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.159, mean reward: 0.416 [0.336, 0.492], mean action: 67.100 [38.000, 83.000], mean observation: 3.150 [-1.642, 10.284], loss: 1.194357, mae: 5.021739, mean_q: 5.202375
 66477/100000: episode: 6784, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 3.762, mean reward: 0.376 [0.268, 0.416], mean action: 20.700 [5.000, 68.000], mean observation: 3.152 [-1.641, 10.296], loss: 1.271436, mae: 5.022284, mean_q: 5.202338
 66487/100000: episode: 6785, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.140, mean reward: 0.414 [0.384, 0.495], mean action: 52.100 [8.000, 92.000], mean observation: 3.161 [-1.620, 10.255], loss: 1.004632, mae: 5.021101, mean_q: 5.200583
 66497/100000: episode: 6786, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.038, mean reward: 0.404 [0.328, 0.502], mean action: 48.600 [23.000, 69.000], mean observation: 3.156 [-1.425, 10.243], loss: 1.273595, mae: 5.022348, mean_q: 5.199051
 66507/100000: episode: 6787, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 3.952, mean reward: 0.395 [0.325, 0.443], mean action: 24.800 [8.000, 100.000], mean observation: 3.153 [-1.233, 10.382], loss: 1.373287, mae: 5.022952, mean_q: 5.197438
 66517/100000: episode: 6788, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.926, mean reward: 0.393 [0.322, 0.499], mean action: 31.400 [8.000, 84.000], mean observation: 3.149 [-1.614, 10.518], loss: 1.263866, mae: 5.022541, mean_q: 5.198889
 66527/100000: episode: 6789, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 4.322, mean reward: 0.432 [0.379, 0.523], mean action: 20.400 [8.000, 73.000], mean observation: 3.147 [-1.588, 10.224], loss: 1.378871, mae: 5.023004, mean_q: 5.198160
 66537/100000: episode: 6790, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.104, mean reward: 0.410 [0.346, 0.444], mean action: 14.500 [8.000, 41.000], mean observation: 3.149 [-1.608, 10.287], loss: 1.487426, mae: 5.023137, mean_q: 5.194354
 66547/100000: episode: 6791, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.755, mean reward: 0.375 [0.314, 0.469], mean action: 34.200 [8.000, 85.000], mean observation: 3.162 [-1.094, 10.353], loss: 1.127626, mae: 5.021283, mean_q: 5.193853
 66557/100000: episode: 6792, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.299, mean reward: 0.430 [0.331, 0.564], mean action: 23.500 [1.000, 79.000], mean observation: 3.144 [-1.206, 10.320], loss: 1.629257, mae: 5.022996, mean_q: 5.195035
 66567/100000: episode: 6793, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.252, mean reward: 0.425 [0.333, 0.522], mean action: 22.000 [8.000, 93.000], mean observation: 3.159 [-1.340, 10.439], loss: 1.328264, mae: 5.021400, mean_q: 5.192595
 66577/100000: episode: 6794, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.140, mean reward: 0.414 [0.330, 0.572], mean action: 68.000 [14.000, 86.000], mean observation: 3.162 [-1.224, 10.437], loss: 1.164740, mae: 5.020571, mean_q: 5.191297
 66587/100000: episode: 6795, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.343, mean reward: 0.434 [0.326, 0.510], mean action: 60.400 [19.000, 83.000], mean observation: 3.155 [-1.591, 10.252], loss: 1.156787, mae: 5.020718, mean_q: 5.190989
 66597/100000: episode: 6796, duration: 0.236s, episode steps: 10, steps per second: 42, episode reward: 4.008, mean reward: 0.401 [0.337, 0.490], mean action: 11.200 [3.000, 28.000], mean observation: 3.155 [-1.173, 10.419], loss: 1.375727, mae: 5.021681, mean_q: 5.191356
 66607/100000: episode: 6797, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.104, mean reward: 0.410 [0.362, 0.501], mean action: 25.600 [8.000, 57.000], mean observation: 3.153 [-2.079, 10.414], loss: 1.231586, mae: 5.021016, mean_q: 5.192840
 66617/100000: episode: 6798, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.114, mean reward: 0.411 [0.299, 0.496], mean action: 33.900 [8.000, 86.000], mean observation: 3.158 [-1.403, 10.405], loss: 1.131093, mae: 5.020862, mean_q: 5.194182
 66627/100000: episode: 6799, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.752, mean reward: 0.375 [0.315, 0.471], mean action: 30.600 [5.000, 76.000], mean observation: 3.161 [-1.301, 10.321], loss: 1.051706, mae: 5.020560, mean_q: 5.195159
 66637/100000: episode: 6800, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.974, mean reward: 0.397 [0.333, 0.496], mean action: 33.200 [8.000, 90.000], mean observation: 3.161 [-1.251, 10.381], loss: 1.364847, mae: 5.021830, mean_q: 5.193776
 66647/100000: episode: 6801, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.351, mean reward: 0.435 [0.357, 0.492], mean action: 26.800 [3.000, 73.000], mean observation: 3.149 [-2.031, 10.329], loss: 1.311420, mae: 5.021668, mean_q: 5.194319
 66657/100000: episode: 6802, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 3.881, mean reward: 0.388 [0.358, 0.425], mean action: 18.300 [8.000, 86.000], mean observation: 3.148 [-1.395, 10.389], loss: 1.032113, mae: 5.020456, mean_q: 5.196691
 66667/100000: episode: 6803, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.810, mean reward: 0.381 [0.331, 0.423], mean action: 17.600 [8.000, 66.000], mean observation: 3.165 [-1.102, 10.430], loss: 1.619019, mae: 5.022796, mean_q: 5.196095
 66677/100000: episode: 6804, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 3.870, mean reward: 0.387 [0.367, 0.417], mean action: 67.400 [33.000, 100.000], mean observation: 3.168 [-1.238, 10.286], loss: 1.194455, mae: 5.020874, mean_q: 5.196717
 66687/100000: episode: 6805, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.846, mean reward: 0.385 [0.339, 0.538], mean action: 49.100 [1.000, 85.000], mean observation: 3.152 [-1.186, 10.243], loss: 0.867585, mae: 5.019443, mean_q: 5.198423
 66697/100000: episode: 6806, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.579, mean reward: 0.458 [0.361, 0.541], mean action: 37.500 [16.000, 101.000], mean observation: 3.151 [-2.653, 10.435], loss: 0.948875, mae: 5.019713, mean_q: 5.201756
 66707/100000: episode: 6807, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.218, mean reward: 0.422 [0.308, 0.501], mean action: 30.600 [16.000, 67.000], mean observation: 3.151 [-1.343, 10.313], loss: 1.383486, mae: 5.021947, mean_q: 5.203520
 66717/100000: episode: 6808, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.391, mean reward: 0.439 [0.376, 0.556], mean action: 16.400 [5.000, 35.000], mean observation: 3.159 [-1.679, 10.326], loss: 1.485770, mae: 5.022148, mean_q: 5.204700
 66727/100000: episode: 6809, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.144, mean reward: 0.414 [0.357, 0.494], mean action: 48.500 [16.000, 96.000], mean observation: 3.163 [-1.237, 10.204], loss: 1.304324, mae: 5.021119, mean_q: 5.206111
 66737/100000: episode: 6810, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.216, mean reward: 0.422 [0.241, 0.544], mean action: 35.900 [5.000, 82.000], mean observation: 3.158 [-1.309, 10.268], loss: 1.705803, mae: 5.022419, mean_q: 5.209146
 66747/100000: episode: 6811, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.989, mean reward: 0.399 [0.356, 0.456], mean action: 31.600 [0.000, 77.000], mean observation: 3.154 [-1.712, 10.202], loss: 1.432038, mae: 5.021174, mean_q: 5.214522
 66757/100000: episode: 6812, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.069, mean reward: 0.407 [0.321, 0.584], mean action: 32.400 [1.000, 93.000], mean observation: 3.164 [-1.202, 10.294], loss: 1.492063, mae: 5.021163, mean_q: 5.216486
 66767/100000: episode: 6813, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.210, mean reward: 0.421 [0.367, 0.533], mean action: 26.900 [0.000, 100.000], mean observation: 3.163 [-2.246, 10.291], loss: 1.326947, mae: 5.020231, mean_q: 5.218112
 66777/100000: episode: 6814, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.902, mean reward: 0.390 [0.335, 0.472], mean action: 36.700 [0.000, 90.000], mean observation: 3.160 [-1.346, 10.340], loss: 0.960352, mae: 5.018598, mean_q: 5.219804
 66787/100000: episode: 6815, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.984, mean reward: 0.398 [0.385, 0.440], mean action: 40.600 [15.000, 96.000], mean observation: 3.168 [-1.326, 10.351], loss: 1.375393, mae: 5.020231, mean_q: 5.220554
 66797/100000: episode: 6816, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.214, mean reward: 0.421 [0.362, 0.548], mean action: 28.500 [7.000, 70.000], mean observation: 3.153 [-1.077, 10.285], loss: 1.609384, mae: 5.021292, mean_q: 5.221113
 66807/100000: episode: 6817, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.855, mean reward: 0.485 [0.485, 0.485], mean action: 27.300 [16.000, 93.000], mean observation: 3.167 [-1.884, 10.262], loss: 1.093544, mae: 5.018981, mean_q: 5.221673
 66817/100000: episode: 6818, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.004, mean reward: 0.400 [0.339, 0.466], mean action: 21.600 [9.000, 76.000], mean observation: 3.154 [-1.455, 10.388], loss: 1.186637, mae: 5.019144, mean_q: 5.222415
 66827/100000: episode: 6819, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.436, mean reward: 0.444 [0.336, 0.504], mean action: 51.900 [16.000, 96.000], mean observation: 3.147 [-1.979, 10.346], loss: 1.180990, mae: 5.019287, mean_q: 5.224523
 66837/100000: episode: 6820, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.210, mean reward: 0.421 [0.382, 0.576], mean action: 25.300 [16.000, 100.000], mean observation: 3.160 [-1.524, 10.363], loss: 1.416527, mae: 5.020336, mean_q: 5.225798
 66847/100000: episode: 6821, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.116, mean reward: 0.412 [0.332, 0.532], mean action: 40.000 [16.000, 89.000], mean observation: 3.147 [-1.193, 10.184], loss: 1.206138, mae: 5.019531, mean_q: 5.227158
 66857/100000: episode: 6822, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.145, mean reward: 0.415 [0.358, 0.496], mean action: 30.500 [3.000, 83.000], mean observation: 3.158 [-1.379, 10.341], loss: 1.109154, mae: 5.019363, mean_q: 5.224564
 66867/100000: episode: 6823, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.844, mean reward: 0.384 [0.374, 0.395], mean action: 30.800 [7.000, 93.000], mean observation: 3.156 [-1.229, 10.302], loss: 1.334113, mae: 5.020223, mean_q: 5.222126
 66877/100000: episode: 6824, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.658, mean reward: 0.466 [0.378, 0.572], mean action: 35.400 [16.000, 100.000], mean observation: 3.165 [-1.908, 10.243], loss: 0.930660, mae: 5.018725, mean_q: 5.221946
 66878/100000: episode: 6825, duration: 0.022s, episode steps: 1, steps per second: 45, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 79.000 [79.000, 79.000], mean observation: 3.172 [-0.208, 10.239], loss: 1.449405, mae: 5.020850, mean_q: 5.222506
 66888/100000: episode: 6826, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.279, mean reward: 0.428 [0.324, 0.510], mean action: 27.700 [2.000, 61.000], mean observation: 3.155 [-0.922, 10.305], loss: 1.627094, mae: 5.021398, mean_q: 5.223084
 66898/100000: episode: 6827, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.149, mean reward: 0.415 [0.360, 0.500], mean action: 35.500 [3.000, 96.000], mean observation: 3.155 [-1.602, 10.286], loss: 1.255412, mae: 5.019829, mean_q: 5.224406
 66908/100000: episode: 6828, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.197, mean reward: 0.420 [0.373, 0.469], mean action: 27.800 [0.000, 97.000], mean observation: 3.153 [-1.183, 10.333], loss: 1.372779, mae: 5.020380, mean_q: 5.226568
 66918/100000: episode: 6829, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.041, mean reward: 0.404 [0.344, 0.480], mean action: 21.600 [1.000, 66.000], mean observation: 3.154 [-1.326, 10.391], loss: 1.186200, mae: 5.019475, mean_q: 5.224486
 66928/100000: episode: 6830, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.417, mean reward: 0.442 [0.353, 0.576], mean action: 27.200 [12.000, 55.000], mean observation: 3.165 [-2.105, 10.457], loss: 0.982281, mae: 5.018665, mean_q: 5.223025
 66938/100000: episode: 6831, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.084, mean reward: 0.408 [0.347, 0.521], mean action: 47.300 [7.000, 96.000], mean observation: 3.160 [-1.337, 10.336], loss: 0.702453, mae: 5.017681, mean_q: 5.224536
 66948/100000: episode: 6832, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.927, mean reward: 0.393 [0.300, 0.491], mean action: 39.700 [2.000, 100.000], mean observation: 3.160 [-1.283, 10.134], loss: 1.168764, mae: 5.020178, mean_q: 5.226981
 66958/100000: episode: 6833, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.998, mean reward: 0.400 [0.383, 0.447], mean action: 49.900 [16.000, 86.000], mean observation: 3.150 [-1.527, 10.313], loss: 1.373860, mae: 5.021166, mean_q: 5.224669
 66968/100000: episode: 6834, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.744, mean reward: 0.374 [0.265, 0.490], mean action: 45.900 [8.000, 101.000], mean observation: 3.151 [-1.146, 10.363], loss: 0.803382, mae: 5.018992, mean_q: 5.221340
 66973/100000: episode: 6835, duration: 0.105s, episode steps: 5, steps per second: 48, episode reward: 11.685, mean reward: 2.337 [0.409, 10.000], mean action: 31.600 [16.000, 62.000], mean observation: 3.161 [-1.725, 10.348], loss: 1.132422, mae: 5.020531, mean_q: 5.218106
 66983/100000: episode: 6836, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.885, mean reward: 0.388 [0.261, 0.491], mean action: 40.300 [16.000, 96.000], mean observation: 3.160 [-2.517, 10.359], loss: 1.108455, mae: 5.020785, mean_q: 5.217719
 66993/100000: episode: 6837, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.817, mean reward: 0.382 [0.331, 0.494], mean action: 28.100 [14.000, 77.000], mean observation: 3.152 [-1.129, 10.355], loss: 1.332703, mae: 5.021903, mean_q: 5.215612
 67003/100000: episode: 6838, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.918, mean reward: 0.392 [0.315, 0.519], mean action: 25.000 [15.000, 56.000], mean observation: 3.148 [-1.657, 10.377], loss: 1.414776, mae: 5.022446, mean_q: 5.213133
 67013/100000: episode: 6839, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.844, mean reward: 0.384 [0.372, 0.454], mean action: 50.200 [4.000, 87.000], mean observation: 3.154 [-1.806, 10.293], loss: 0.798770, mae: 5.019936, mean_q: 5.212144
 67023/100000: episode: 6840, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.936, mean reward: 0.394 [0.318, 0.536], mean action: 39.400 [0.000, 91.000], mean observation: 3.158 [-1.962, 10.262], loss: 1.510235, mae: 5.022880, mean_q: 5.208565
 67033/100000: episode: 6841, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.189, mean reward: 0.419 [0.356, 0.509], mean action: 30.400 [16.000, 77.000], mean observation: 3.153 [-2.080, 10.464], loss: 0.855378, mae: 5.020183, mean_q: 5.208416
 67043/100000: episode: 6842, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.022, mean reward: 0.402 [0.341, 0.480], mean action: 37.900 [11.000, 90.000], mean observation: 3.152 [-1.158, 10.317], loss: 1.655313, mae: 5.023457, mean_q: 5.209004
 67047/100000: episode: 6843, duration: 0.099s, episode steps: 4, steps per second: 41, episode reward: 11.080, mean reward: 2.770 [0.347, 10.000], mean action: 15.750 [0.000, 31.000], mean observation: 3.161 [-0.903, 10.471], loss: 1.311323, mae: 5.022006, mean_q: 5.205950
 67057/100000: episode: 6844, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 3.491, mean reward: 0.349 [0.331, 0.395], mean action: 75.000 [16.000, 98.000], mean observation: 3.167 [-1.374, 10.262], loss: 0.921330, mae: 5.020312, mean_q: 5.200755
 67067/100000: episode: 6845, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 3.783, mean reward: 0.378 [0.357, 0.450], mean action: 73.800 [24.000, 92.000], mean observation: 3.142 [-0.608, 10.292], loss: 1.332909, mae: 5.022044, mean_q: 5.200016
 67077/100000: episode: 6846, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.950, mean reward: 0.395 [0.321, 0.482], mean action: 69.400 [12.000, 84.000], mean observation: 3.158 [-1.447, 10.350], loss: 1.330804, mae: 5.022103, mean_q: 5.200853
 67087/100000: episode: 6847, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.537, mean reward: 0.454 [0.452, 0.465], mean action: 67.500 [11.000, 83.000], mean observation: 3.155 [-0.813, 10.528], loss: 1.268091, mae: 5.021958, mean_q: 5.202055
 67097/100000: episode: 6848, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.769, mean reward: 0.377 [0.313, 0.490], mean action: 65.500 [7.000, 88.000], mean observation: 3.162 [-0.993, 10.439], loss: 1.329810, mae: 5.021827, mean_q: 5.203334
 67107/100000: episode: 6849, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.489, mean reward: 0.449 [0.430, 0.526], mean action: 72.000 [22.000, 101.000], mean observation: 3.145 [-1.481, 10.359], loss: 1.332701, mae: 5.021686, mean_q: 5.204534
 67117/100000: episode: 6850, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.690, mean reward: 0.369 [0.340, 0.470], mean action: 77.900 [39.000, 83.000], mean observation: 3.147 [-1.981, 10.307], loss: 0.962814, mae: 5.020153, mean_q: 5.203496
 67127/100000: episode: 6851, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.914, mean reward: 0.391 [0.342, 0.422], mean action: 62.000 [0.000, 86.000], mean observation: 3.164 [-1.028, 10.335], loss: 1.137490, mae: 5.020669, mean_q: 5.201651
 67137/100000: episode: 6852, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.025, mean reward: 0.402 [0.306, 0.518], mean action: 31.500 [8.000, 101.000], mean observation: 3.165 [-1.064, 10.346], loss: 1.152727, mae: 5.021111, mean_q: 5.200796
 67147/100000: episode: 6853, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.917, mean reward: 0.392 [0.344, 0.459], mean action: 37.400 [8.000, 96.000], mean observation: 3.158 [-1.427, 10.315], loss: 1.056036, mae: 5.020781, mean_q: 5.202336
 67157/100000: episode: 6854, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.417, mean reward: 0.442 [0.323, 0.573], mean action: 34.500 [5.000, 94.000], mean observation: 3.158 [-1.999, 10.273], loss: 1.642201, mae: 5.023059, mean_q: 5.204186
 67167/100000: episode: 6855, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.512, mean reward: 0.451 [0.330, 0.522], mean action: 26.300 [8.000, 87.000], mean observation: 3.152 [-1.394, 10.324], loss: 1.027186, mae: 5.020466, mean_q: 5.202870
 67177/100000: episode: 6856, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.906, mean reward: 0.391 [0.349, 0.480], mean action: 75.800 [40.000, 83.000], mean observation: 3.153 [-1.258, 10.290], loss: 1.329121, mae: 5.021525, mean_q: 5.203122
 67187/100000: episode: 6857, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 3.869, mean reward: 0.387 [0.383, 0.402], mean action: 73.800 [23.000, 88.000], mean observation: 3.162 [-0.815, 10.278], loss: 1.455244, mae: 5.021955, mean_q: 5.205780
 67197/100000: episode: 6858, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.740, mean reward: 0.374 [0.357, 0.416], mean action: 72.800 [17.000, 87.000], mean observation: 3.142 [-1.352, 10.347], loss: 1.126133, mae: 5.020480, mean_q: 5.208854
 67207/100000: episode: 6859, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.686, mean reward: 0.469 [0.355, 0.543], mean action: 63.400 [4.000, 83.000], mean observation: 3.150 [-1.474, 10.224], loss: 1.480280, mae: 5.022057, mean_q: 5.209181
 67217/100000: episode: 6860, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.708, mean reward: 0.471 [0.372, 0.581], mean action: 62.600 [14.000, 88.000], mean observation: 3.164 [-2.388, 10.394], loss: 1.197664, mae: 5.021038, mean_q: 5.209268
 67227/100000: episode: 6861, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.942, mean reward: 0.394 [0.318, 0.521], mean action: 56.900 [10.000, 97.000], mean observation: 3.158 [-0.810, 10.456], loss: 1.227459, mae: 5.020926, mean_q: 5.209766
 67237/100000: episode: 6862, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.061, mean reward: 0.406 [0.320, 0.456], mean action: 59.700 [23.000, 99.000], mean observation: 3.170 [-1.438, 10.373], loss: 1.220640, mae: 5.021046, mean_q: 5.210859
 67247/100000: episode: 6863, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.000, mean reward: 0.400 [0.400, 0.400], mean action: 84.800 [83.000, 101.000], mean observation: 3.157 [-1.776, 10.301], loss: 1.408228, mae: 5.021813, mean_q: 5.212227
 67257/100000: episode: 6864, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.160, mean reward: 0.416 [0.319, 0.460], mean action: 53.600 [5.000, 83.000], mean observation: 3.146 [-1.138, 10.347], loss: 1.290745, mae: 5.021222, mean_q: 5.213585
 67267/100000: episode: 6865, duration: 0.096s, episode steps: 10, steps per second: 105, episode reward: 3.658, mean reward: 0.366 [0.364, 0.378], mean action: 89.200 [83.000, 101.000], mean observation: 3.156 [-0.666, 10.352], loss: 1.298668, mae: 5.021230, mean_q: 5.213243
 67277/100000: episode: 6866, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 3.530, mean reward: 0.353 [0.326, 0.405], mean action: 57.000 [8.000, 90.000], mean observation: 3.151 [-1.515, 10.238], loss: 0.817572, mae: 5.019122, mean_q: 5.214635
 67287/100000: episode: 6867, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.269, mean reward: 0.427 [0.352, 0.532], mean action: 59.800 [18.000, 83.000], mean observation: 3.156 [-1.556, 10.270], loss: 1.805876, mae: 5.023201, mean_q: 5.216256
 67295/100000: episode: 6868, duration: 0.103s, episode steps: 8, steps per second: 78, episode reward: 13.927, mean reward: 1.741 [0.561, 10.000], mean action: 64.625 [7.000, 83.000], mean observation: 3.157 [-1.430, 10.261], loss: 1.301370, mae: 5.020819, mean_q: 5.214281
 67305/100000: episode: 6869, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 3.990, mean reward: 0.399 [0.369, 0.409], mean action: 71.000 [13.000, 83.000], mean observation: 3.139 [-0.971, 10.221], loss: 1.168910, mae: 5.019755, mean_q: 5.211959
 67315/100000: episode: 6870, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.997, mean reward: 0.400 [0.371, 0.444], mean action: 54.600 [5.000, 95.000], mean observation: 3.150 [-1.262, 10.370], loss: 1.421197, mae: 5.020577, mean_q: 5.210317
 67325/100000: episode: 6871, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.323, mean reward: 0.432 [0.415, 0.437], mean action: 68.400 [6.000, 83.000], mean observation: 3.168 [-1.226, 10.359], loss: 1.238226, mae: 5.019803, mean_q: 5.211134
 67335/100000: episode: 6872, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.978, mean reward: 0.398 [0.349, 0.475], mean action: 50.600 [19.000, 101.000], mean observation: 3.157 [-2.008, 10.323], loss: 1.355169, mae: 5.019912, mean_q: 5.210648
 67345/100000: episode: 6873, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.746, mean reward: 0.375 [0.302, 0.493], mean action: 45.700 [14.000, 82.000], mean observation: 3.151 [-1.557, 10.320], loss: 1.068302, mae: 5.018608, mean_q: 5.212617
 67355/100000: episode: 6874, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.121, mean reward: 0.412 [0.347, 0.518], mean action: 55.300 [2.000, 89.000], mean observation: 3.172 [-1.863, 10.343], loss: 1.438888, mae: 5.020342, mean_q: 5.213531
 67365/100000: episode: 6875, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.435, mean reward: 0.443 [0.380, 0.485], mean action: 45.500 [8.000, 99.000], mean observation: 3.156 [-2.111, 10.368], loss: 1.095175, mae: 5.019223, mean_q: 5.213295
 67366/100000: episode: 6876, duration: 0.031s, episode steps: 1, steps per second: 33, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 50.000 [50.000, 50.000], mean observation: 3.155 [-0.771, 10.258], loss: 0.457046, mae: 5.016804, mean_q: 5.213451
 67376/100000: episode: 6877, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.164, mean reward: 0.416 [0.341, 0.425], mean action: 44.300 [0.000, 82.000], mean observation: 3.161 [-1.392, 10.298], loss: 1.207379, mae: 5.019850, mean_q: 5.213521
 67386/100000: episode: 6878, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.902, mean reward: 0.390 [0.325, 0.425], mean action: 43.700 [4.000, 80.000], mean observation: 3.165 [-1.585, 10.354], loss: 1.440848, mae: 5.020503, mean_q: 5.213658
 67396/100000: episode: 6879, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.432, mean reward: 0.443 [0.397, 0.472], mean action: 55.600 [35.000, 86.000], mean observation: 3.169 [-0.888, 10.338], loss: 1.393533, mae: 5.020164, mean_q: 5.213864
 67406/100000: episode: 6880, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.212, mean reward: 0.421 [0.420, 0.431], mean action: 53.100 [50.000, 81.000], mean observation: 3.158 [-1.727, 10.273], loss: 1.195432, mae: 5.019298, mean_q: 5.214599
 67416/100000: episode: 6881, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.113, mean reward: 0.411 [0.328, 0.438], mean action: 51.600 [10.000, 84.000], mean observation: 3.157 [-2.059, 10.313], loss: 1.517320, mae: 5.020399, mean_q: 5.216311
 67426/100000: episode: 6882, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 5.395, mean reward: 0.539 [0.526, 0.545], mean action: 57.300 [1.000, 101.000], mean observation: 3.177 [-1.310, 10.289], loss: 0.971569, mae: 5.018240, mean_q: 5.218251
 67436/100000: episode: 6883, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.215, mean reward: 0.421 [0.341, 0.461], mean action: 43.000 [3.000, 97.000], mean observation: 3.146 [-1.687, 10.413], loss: 0.985481, mae: 5.018686, mean_q: 5.219733
 67446/100000: episode: 6884, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.997, mean reward: 0.400 [0.338, 0.475], mean action: 53.200 [3.000, 87.000], mean observation: 3.154 [-1.155, 10.289], loss: 0.973432, mae: 5.018785, mean_q: 5.220964
 67456/100000: episode: 6885, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.880, mean reward: 0.388 [0.329, 0.496], mean action: 55.000 [13.000, 95.000], mean observation: 3.164 [-1.481, 10.419], loss: 1.237635, mae: 5.020144, mean_q: 5.222074
 67466/100000: episode: 6886, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.864, mean reward: 0.386 [0.335, 0.458], mean action: 59.400 [17.000, 96.000], mean observation: 3.157 [-1.804, 10.345], loss: 0.895845, mae: 5.019017, mean_q: 5.223387
 67476/100000: episode: 6887, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.210, mean reward: 0.421 [0.359, 0.497], mean action: 36.500 [0.000, 75.000], mean observation: 3.149 [-0.795, 10.256], loss: 1.143883, mae: 5.020263, mean_q: 5.225793
 67486/100000: episode: 6888, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.554, mean reward: 0.355 [0.302, 0.420], mean action: 58.600 [31.000, 85.000], mean observation: 3.144 [-1.173, 10.222], loss: 1.781151, mae: 5.023035, mean_q: 5.227021
 67496/100000: episode: 6889, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.747, mean reward: 0.375 [0.345, 0.435], mean action: 39.800 [10.000, 95.000], mean observation: 3.155 [-1.304, 10.249], loss: 1.164815, mae: 5.020391, mean_q: 5.226752
 67506/100000: episode: 6890, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.299, mean reward: 0.430 [0.406, 0.538], mean action: 37.400 [16.000, 88.000], mean observation: 3.146 [-1.605, 10.489], loss: 0.664642, mae: 5.018552, mean_q: 5.228116
 67509/100000: episode: 6891, duration: 0.049s, episode steps: 3, steps per second: 61, episode reward: 10.914, mean reward: 3.638 [0.456, 10.000], mean action: 59.000 [16.000, 91.000], mean observation: 3.176 [-1.000, 10.229], loss: 1.725753, mae: 5.022999, mean_q: 5.229258
 67519/100000: episode: 6892, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.824, mean reward: 0.382 [0.339, 0.448], mean action: 37.800 [16.000, 99.000], mean observation: 3.161 [-2.105, 10.327], loss: 1.184479, mae: 5.020862, mean_q: 5.230037
 67529/100000: episode: 6893, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.122, mean reward: 0.412 [0.366, 0.475], mean action: 30.500 [16.000, 76.000], mean observation: 3.144 [-1.449, 10.423], loss: 1.207812, mae: 5.021019, mean_q: 5.230660
 67539/100000: episode: 6894, duration: 0.222s, episode steps: 10, steps per second: 45, episode reward: 4.230, mean reward: 0.423 [0.345, 0.497], mean action: 29.300 [16.000, 69.000], mean observation: 3.145 [-1.351, 10.362], loss: 1.430949, mae: 5.021848, mean_q: 5.232822
 67549/100000: episode: 6895, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.280, mean reward: 0.428 [0.376, 0.487], mean action: 34.100 [1.000, 95.000], mean observation: 3.157 [-1.144, 10.249], loss: 1.413969, mae: 5.021707, mean_q: 5.235271
 67559/100000: episode: 6896, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.804, mean reward: 0.380 [0.316, 0.452], mean action: 27.200 [16.000, 64.000], mean observation: 3.149 [-1.349, 10.277], loss: 1.270369, mae: 5.020920, mean_q: 5.237373
 67569/100000: episode: 6897, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.110, mean reward: 0.411 [0.297, 0.500], mean action: 25.000 [16.000, 64.000], mean observation: 3.147 [-1.394, 10.367], loss: 1.325870, mae: 5.021217, mean_q: 5.238927
 67579/100000: episode: 6898, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 3.984, mean reward: 0.398 [0.320, 0.473], mean action: 23.900 [15.000, 96.000], mean observation: 3.142 [-1.147, 10.348], loss: 1.196524, mae: 5.020861, mean_q: 5.240561
 67589/100000: episode: 6899, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.310, mean reward: 0.431 [0.364, 0.460], mean action: 22.600 [3.000, 53.000], mean observation: 3.154 [-1.276, 10.326], loss: 1.207736, mae: 5.020896, mean_q: 5.242302
 67599/100000: episode: 6900, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.904, mean reward: 0.390 [0.348, 0.432], mean action: 29.100 [14.000, 91.000], mean observation: 3.155 [-0.931, 10.223], loss: 1.139188, mae: 5.020606, mean_q: 5.243749
 67609/100000: episode: 6901, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 3.636, mean reward: 0.364 [0.296, 0.468], mean action: 31.200 [8.000, 100.000], mean observation: 3.160 [-1.105, 10.331], loss: 1.284160, mae: 5.021430, mean_q: 5.245809
 67619/100000: episode: 6902, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.704, mean reward: 0.470 [0.361, 0.566], mean action: 25.400 [14.000, 61.000], mean observation: 3.155 [-1.824, 10.264], loss: 1.144319, mae: 5.020882, mean_q: 5.247354
 67629/100000: episode: 6903, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 5.610, mean reward: 0.561 [0.561, 0.561], mean action: 32.800 [16.000, 70.000], mean observation: 3.132 [-1.702, 10.249], loss: 1.420200, mae: 5.022095, mean_q: 5.249098
 67639/100000: episode: 6904, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.157, mean reward: 0.416 [0.386, 0.438], mean action: 29.100 [12.000, 75.000], mean observation: 3.147 [-1.193, 10.320], loss: 0.957699, mae: 5.020107, mean_q: 5.251809
 67649/100000: episode: 6905, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.862, mean reward: 0.386 [0.304, 0.454], mean action: 30.200 [5.000, 74.000], mean observation: 3.150 [-1.793, 10.347], loss: 1.513470, mae: 5.022650, mean_q: 5.253591
 67659/100000: episode: 6906, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.594, mean reward: 0.459 [0.343, 0.562], mean action: 51.200 [1.000, 101.000], mean observation: 3.151 [-1.416, 10.420], loss: 1.200948, mae: 5.021469, mean_q: 5.252264
 67669/100000: episode: 6907, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.357, mean reward: 0.436 [0.432, 0.469], mean action: 32.400 [16.000, 95.000], mean observation: 3.158 [-1.647, 10.355], loss: 1.048696, mae: 5.021004, mean_q: 5.255147
 67679/100000: episode: 6908, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.963, mean reward: 0.396 [0.345, 0.444], mean action: 35.300 [5.000, 100.000], mean observation: 3.147 [-1.771, 10.265], loss: 1.119906, mae: 5.021507, mean_q: 5.263558
 67689/100000: episode: 6909, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.279, mean reward: 0.428 [0.317, 0.590], mean action: 20.900 [6.000, 55.000], mean observation: 3.143 [-1.920, 10.275], loss: 1.137625, mae: 5.021912, mean_q: 5.263638
 67699/100000: episode: 6910, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.021, mean reward: 0.402 [0.327, 0.533], mean action: 33.200 [7.000, 99.000], mean observation: 3.152 [-1.150, 10.370], loss: 1.341174, mae: 5.022715, mean_q: 5.265404
 67709/100000: episode: 6911, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.142, mean reward: 0.414 [0.400, 0.480], mean action: 38.300 [8.000, 77.000], mean observation: 3.154 [-1.476, 10.364], loss: 1.107757, mae: 5.021863, mean_q: 5.268296
 67719/100000: episode: 6912, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.315, mean reward: 0.431 [0.309, 0.525], mean action: 45.500 [6.000, 89.000], mean observation: 3.158 [-1.444, 10.501], loss: 1.151249, mae: 5.022077, mean_q: 5.266479
 67729/100000: episode: 6913, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.169, mean reward: 0.417 [0.339, 0.475], mean action: 25.600 [7.000, 83.000], mean observation: 3.153 [-1.921, 10.341], loss: 1.160077, mae: 5.022069, mean_q: 5.265611
 67739/100000: episode: 6914, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.172, mean reward: 0.417 [0.293, 0.515], mean action: 33.900 [0.000, 84.000], mean observation: 3.160 [-1.306, 10.424], loss: 1.623267, mae: 5.023817, mean_q: 5.265479
 67749/100000: episode: 6915, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.095, mean reward: 0.409 [0.347, 0.464], mean action: 19.800 [14.000, 56.000], mean observation: 3.156 [-1.298, 10.291], loss: 1.311703, mae: 5.022658, mean_q: 5.259061
 67759/100000: episode: 6916, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.159, mean reward: 0.416 [0.342, 0.493], mean action: 27.000 [1.000, 65.000], mean observation: 3.148 [-1.310, 10.386], loss: 1.265597, mae: 5.022466, mean_q: 5.256865
 67769/100000: episode: 6917, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.005, mean reward: 0.400 [0.379, 0.446], mean action: 35.300 [1.000, 80.000], mean observation: 3.154 [-1.197, 10.316], loss: 1.361098, mae: 5.022933, mean_q: 5.255744
 67779/100000: episode: 6918, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.962, mean reward: 0.396 [0.346, 0.473], mean action: 30.900 [16.000, 83.000], mean observation: 3.151 [-1.539, 10.265], loss: 1.037321, mae: 5.021476, mean_q: 5.253730
 67789/100000: episode: 6919, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.203, mean reward: 0.420 [0.338, 0.507], mean action: 34.500 [8.000, 93.000], mean observation: 3.157 [-1.264, 10.441], loss: 1.290062, mae: 5.022867, mean_q: 5.253890
 67799/100000: episode: 6920, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.063, mean reward: 0.406 [0.335, 0.452], mean action: 44.500 [10.000, 96.000], mean observation: 3.151 [-1.230, 10.340], loss: 1.360932, mae: 5.023021, mean_q: 5.254306
 67809/100000: episode: 6921, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.179, mean reward: 0.418 [0.335, 0.496], mean action: 26.800 [15.000, 52.000], mean observation: 3.151 [-1.367, 10.510], loss: 1.185018, mae: 5.022404, mean_q: 5.255786
 67819/100000: episode: 6922, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.026, mean reward: 0.403 [0.319, 0.485], mean action: 29.900 [16.000, 74.000], mean observation: 3.164 [-1.381, 10.430], loss: 1.189510, mae: 5.022636, mean_q: 5.255093
 67829/100000: episode: 6923, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.146, mean reward: 0.415 [0.321, 0.478], mean action: 39.600 [1.000, 98.000], mean observation: 3.154 [-1.400, 10.430], loss: 1.388679, mae: 5.023435, mean_q: 5.252711
 67839/100000: episode: 6924, duration: 0.224s, episode steps: 10, steps per second: 45, episode reward: 4.323, mean reward: 0.432 [0.376, 0.593], mean action: 24.800 [1.000, 67.000], mean observation: 3.156 [-1.547, 10.480], loss: 1.231003, mae: 5.022994, mean_q: 5.253098
 67849/100000: episode: 6925, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.700, mean reward: 0.370 [0.323, 0.416], mean action: 33.700 [5.000, 101.000], mean observation: 3.159 [-1.710, 10.350], loss: 1.355514, mae: 5.023421, mean_q: 5.254584
 67859/100000: episode: 6926, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.306, mean reward: 0.431 [0.382, 0.471], mean action: 34.200 [15.000, 100.000], mean observation: 3.168 [-1.483, 10.556], loss: 1.221970, mae: 5.023407, mean_q: 5.257212
 67869/100000: episode: 6927, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.328, mean reward: 0.433 [0.379, 0.505], mean action: 33.400 [11.000, 89.000], mean observation: 3.153 [-1.486, 10.414], loss: 1.153596, mae: 5.023031, mean_q: 5.259938
 67879/100000: episode: 6928, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.182, mean reward: 0.418 [0.342, 0.559], mean action: 37.600 [10.000, 88.000], mean observation: 3.158 [-1.614, 10.371], loss: 0.947323, mae: 5.022779, mean_q: 5.263370
 67889/100000: episode: 6929, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.097, mean reward: 0.410 [0.358, 0.455], mean action: 44.100 [0.000, 97.000], mean observation: 3.164 [-1.295, 10.253], loss: 1.346814, mae: 5.024550, mean_q: 5.266435
 67899/100000: episode: 6930, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.050, mean reward: 0.405 [0.344, 0.490], mean action: 21.900 [0.000, 57.000], mean observation: 3.157 [-1.186, 10.285], loss: 0.996751, mae: 5.023405, mean_q: 5.269477
 67909/100000: episode: 6931, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.922, mean reward: 0.392 [0.345, 0.433], mean action: 29.900 [16.000, 59.000], mean observation: 3.168 [-1.449, 10.373], loss: 1.327998, mae: 5.025086, mean_q: 5.271811
 67919/100000: episode: 6932, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.015, mean reward: 0.402 [0.350, 0.480], mean action: 45.900 [8.000, 93.000], mean observation: 3.163 [-1.517, 10.288], loss: 1.407024, mae: 5.025579, mean_q: 5.274280
 67929/100000: episode: 6933, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.843, mean reward: 0.384 [0.288, 0.523], mean action: 20.600 [1.000, 71.000], mean observation: 3.159 [-1.557, 10.244], loss: 1.159046, mae: 5.024561, mean_q: 5.276416
 67939/100000: episode: 6934, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.213, mean reward: 0.421 [0.302, 0.551], mean action: 22.600 [8.000, 60.000], mean observation: 3.151 [-1.829, 10.448], loss: 1.199313, mae: 5.025096, mean_q: 5.278366
 67949/100000: episode: 6935, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.077, mean reward: 0.408 [0.339, 0.507], mean action: 39.300 [16.000, 77.000], mean observation: 3.142 [-1.766, 10.299], loss: 1.061380, mae: 5.024389, mean_q: 5.276561
 67959/100000: episode: 6936, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 3.838, mean reward: 0.384 [0.295, 0.469], mean action: 28.500 [12.000, 67.000], mean observation: 3.162 [-1.150, 10.256], loss: 1.205536, mae: 5.025405, mean_q: 5.271526
 67969/100000: episode: 6937, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.944, mean reward: 0.394 [0.328, 0.462], mean action: 30.200 [3.000, 96.000], mean observation: 3.154 [-1.336, 10.243], loss: 1.055598, mae: 5.025092, mean_q: 5.271135
 67979/100000: episode: 6938, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.780, mean reward: 0.378 [0.318, 0.434], mean action: 31.000 [16.000, 96.000], mean observation: 3.156 [-0.982, 10.324], loss: 1.425695, mae: 5.026793, mean_q: 5.272721
 67986/100000: episode: 6939, duration: 0.134s, episode steps: 7, steps per second: 52, episode reward: 12.287, mean reward: 1.755 [0.282, 10.000], mean action: 29.571 [16.000, 86.000], mean observation: 3.161 [-0.690, 10.348], loss: 1.160369, mae: 5.025741, mean_q: 5.274414
 67996/100000: episode: 6940, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.894, mean reward: 0.389 [0.340, 0.477], mean action: 37.600 [16.000, 101.000], mean observation: 3.138 [-1.153, 10.265], loss: 1.109739, mae: 5.025749, mean_q: 5.276359
 68006/100000: episode: 6941, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 13.687, mean reward: 1.369 [0.355, 10.000], mean action: 39.100 [6.000, 98.000], mean observation: 3.161 [-1.910, 10.305], loss: 1.663503, mae: 5.027986, mean_q: 5.276353
 68016/100000: episode: 6942, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.618, mean reward: 0.362 [0.298, 0.467], mean action: 46.200 [16.000, 99.000], mean observation: 3.162 [-0.832, 10.357], loss: 1.370325, mae: 5.026718, mean_q: 5.274471
 68026/100000: episode: 6943, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.341, mean reward: 0.434 [0.377, 0.536], mean action: 34.700 [3.000, 86.000], mean observation: 3.164 [-1.423, 10.409], loss: 1.125627, mae: 5.025782, mean_q: 5.274591
 68036/100000: episode: 6944, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.675, mean reward: 0.468 [0.373, 0.554], mean action: 36.200 [12.000, 95.000], mean observation: 3.161 [-0.995, 10.319], loss: 1.508070, mae: 5.027414, mean_q: 5.276093
 68046/100000: episode: 6945, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.230, mean reward: 0.423 [0.355, 0.500], mean action: 41.400 [10.000, 81.000], mean observation: 3.154 [-1.153, 10.328], loss: 1.265670, mae: 5.026389, mean_q: 5.278581
 68056/100000: episode: 6946, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.441, mean reward: 0.444 [0.359, 0.564], mean action: 21.600 [3.000, 76.000], mean observation: 3.153 [-1.286, 10.269], loss: 1.498638, mae: 5.027272, mean_q: 5.276613
 68066/100000: episode: 6947, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 13.671, mean reward: 1.367 [0.305, 10.000], mean action: 15.600 [12.000, 16.000], mean observation: 3.157 [-1.683, 10.627], loss: 1.071741, mae: 5.025551, mean_q: 5.276641
 68076/100000: episode: 6948, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.693, mean reward: 0.469 [0.444, 0.558], mean action: 41.400 [14.000, 81.000], mean observation: 3.151 [-1.274, 10.476], loss: 1.355184, mae: 5.027064, mean_q: 5.274398
 68086/100000: episode: 6949, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.749, mean reward: 0.375 [0.293, 0.447], mean action: 47.200 [3.000, 96.000], mean observation: 3.154 [-1.119, 10.330], loss: 1.396146, mae: 5.027198, mean_q: 5.274795
 68096/100000: episode: 6950, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.269, mean reward: 0.427 [0.398, 0.503], mean action: 35.500 [16.000, 90.000], mean observation: 3.155 [-1.671, 10.442], loss: 1.226293, mae: 5.026449, mean_q: 5.280220
 68106/100000: episode: 6951, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.945, mean reward: 0.394 [0.356, 0.451], mean action: 34.000 [6.000, 97.000], mean observation: 3.163 [-1.420, 10.320], loss: 1.134390, mae: 5.026490, mean_q: 5.286612
 68116/100000: episode: 6952, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.178, mean reward: 0.418 [0.390, 0.492], mean action: 48.900 [16.000, 100.000], mean observation: 3.152 [-1.083, 10.460], loss: 1.514403, mae: 5.028318, mean_q: 5.286479
 68126/100000: episode: 6953, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.115, mean reward: 0.412 [0.347, 0.478], mean action: 34.800 [4.000, 93.000], mean observation: 3.156 [-2.000, 10.274], loss: 1.352841, mae: 5.027764, mean_q: 5.286375
 68136/100000: episode: 6954, duration: 0.215s, episode steps: 10, steps per second: 47, episode reward: 4.159, mean reward: 0.416 [0.363, 0.472], mean action: 22.400 [16.000, 70.000], mean observation: 3.155 [-1.925, 10.353], loss: 0.926118, mae: 5.026358, mean_q: 5.288203
 68146/100000: episode: 6955, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.434, mean reward: 0.443 [0.367, 0.553], mean action: 38.900 [8.000, 96.000], mean observation: 3.163 [-1.352, 10.618], loss: 1.327602, mae: 5.028354, mean_q: 5.289948
 68156/100000: episode: 6956, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.756, mean reward: 0.476 [0.371, 0.570], mean action: 37.800 [16.000, 101.000], mean observation: 3.169 [-1.597, 10.473], loss: 1.320900, mae: 5.028522, mean_q: 5.291531
 68166/100000: episode: 6957, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.043, mean reward: 0.404 [0.369, 0.485], mean action: 35.900 [3.000, 89.000], mean observation: 3.162 [-1.583, 10.353], loss: 1.181665, mae: 5.028299, mean_q: 5.293428
 68168/100000: episode: 6958, duration: 0.056s, episode steps: 2, steps per second: 36, episode reward: 10.358, mean reward: 5.179 [0.358, 10.000], mean action: 57.500 [16.000, 99.000], mean observation: 3.146 [-1.128, 10.357], loss: 1.986813, mae: 5.031279, mean_q: 5.294350
 68178/100000: episode: 6959, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.099, mean reward: 0.410 [0.357, 0.435], mean action: 24.600 [12.000, 60.000], mean observation: 3.155 [-1.122, 10.350], loss: 1.172015, mae: 5.028141, mean_q: 5.295489
 68188/100000: episode: 6960, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.127, mean reward: 0.413 [0.386, 0.452], mean action: 44.000 [0.000, 99.000], mean observation: 3.159 [-1.487, 10.433], loss: 1.145995, mae: 5.028353, mean_q: 5.297951
 68198/100000: episode: 6961, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 13.580, mean reward: 1.358 [0.365, 10.000], mean action: 26.200 [0.000, 79.000], mean observation: 3.159 [-1.565, 10.339], loss: 1.095684, mae: 5.028442, mean_q: 5.301610
 68208/100000: episode: 6962, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.965, mean reward: 0.396 [0.352, 0.582], mean action: 29.100 [0.000, 78.000], mean observation: 3.150 [-1.428, 10.244], loss: 1.290447, mae: 5.029650, mean_q: 5.303546
 68218/100000: episode: 6963, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.350, mean reward: 0.435 [0.378, 0.555], mean action: 22.400 [16.000, 71.000], mean observation: 3.157 [-1.743, 10.309], loss: 1.254982, mae: 5.029623, mean_q: 5.294209
 68228/100000: episode: 6964, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.907, mean reward: 0.391 [0.363, 0.418], mean action: 26.400 [9.000, 98.000], mean observation: 3.161 [-1.505, 10.219], loss: 1.242810, mae: 5.029885, mean_q: 5.286710
 68238/100000: episode: 6965, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.332, mean reward: 0.433 [0.410, 0.517], mean action: 24.400 [16.000, 100.000], mean observation: 3.150 [-1.049, 10.443], loss: 1.149573, mae: 5.029916, mean_q: 5.277326
 68248/100000: episode: 6966, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.919, mean reward: 0.392 [0.321, 0.526], mean action: 42.200 [8.000, 73.000], mean observation: 3.157 [-1.325, 10.432], loss: 1.284858, mae: 5.030761, mean_q: 5.273479
 68258/100000: episode: 6967, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.905, mean reward: 0.390 [0.329, 0.444], mean action: 23.800 [0.000, 70.000], mean observation: 3.149 [-1.230, 10.277], loss: 1.316232, mae: 5.031074, mean_q: 5.270602
 68268/100000: episode: 6968, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.133, mean reward: 0.413 [0.370, 0.563], mean action: 33.600 [5.000, 90.000], mean observation: 3.157 [-1.895, 10.355], loss: 1.308108, mae: 5.031391, mean_q: 5.271075
 68278/100000: episode: 6969, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.020, mean reward: 0.402 [0.336, 0.480], mean action: 30.200 [0.000, 94.000], mean observation: 3.147 [-1.250, 10.368], loss: 1.118199, mae: 5.031018, mean_q: 5.271338
 68288/100000: episode: 6970, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.686, mean reward: 0.369 [0.329, 0.452], mean action: 30.700 [2.000, 101.000], mean observation: 3.153 [-1.105, 10.398], loss: 1.062119, mae: 5.031343, mean_q: 5.269736
 68298/100000: episode: 6971, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.795, mean reward: 0.379 [0.323, 0.448], mean action: 34.900 [16.000, 91.000], mean observation: 3.152 [-1.433, 10.531], loss: 1.431268, mae: 5.033282, mean_q: 5.271382
 68308/100000: episode: 6972, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.405, mean reward: 0.440 [0.368, 0.554], mean action: 33.400 [15.000, 86.000], mean observation: 3.155 [-1.634, 10.410], loss: 1.278838, mae: 5.032955, mean_q: 5.273594
 68318/100000: episode: 6973, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.949, mean reward: 0.395 [0.350, 0.487], mean action: 31.600 [0.000, 86.000], mean observation: 3.162 [-1.469, 10.203], loss: 1.390096, mae: 5.033408, mean_q: 5.275947
 68328/100000: episode: 6974, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.217, mean reward: 0.422 [0.376, 0.479], mean action: 19.400 [9.000, 57.000], mean observation: 3.160 [-1.192, 10.475], loss: 1.207753, mae: 5.032892, mean_q: 5.277796
 68338/100000: episode: 6975, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.984, mean reward: 0.398 [0.335, 0.590], mean action: 35.000 [1.000, 97.000], mean observation: 3.160 [-1.242, 10.444], loss: 1.039497, mae: 5.032111, mean_q: 5.276742
 68348/100000: episode: 6976, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.001, mean reward: 0.400 [0.322, 0.549], mean action: 34.800 [8.000, 81.000], mean observation: 3.162 [-1.717, 10.375], loss: 0.951034, mae: 5.032369, mean_q: 5.277398
 68358/100000: episode: 6977, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.186, mean reward: 0.419 [0.349, 0.521], mean action: 29.300 [16.000, 69.000], mean observation: 3.160 [-1.367, 10.308], loss: 1.448703, mae: 5.034479, mean_q: 5.273034
 68368/100000: episode: 6978, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.229, mean reward: 0.423 [0.353, 0.555], mean action: 25.200 [6.000, 87.000], mean observation: 3.156 [-1.780, 10.315], loss: 1.399237, mae: 5.034693, mean_q: 5.271519
 68378/100000: episode: 6979, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 3.769, mean reward: 0.377 [0.334, 0.461], mean action: 32.900 [2.000, 85.000], mean observation: 3.154 [-1.302, 10.372], loss: 1.436974, mae: 5.034923, mean_q: 5.272659
 68388/100000: episode: 6980, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.206, mean reward: 0.421 [0.380, 0.492], mean action: 37.200 [16.000, 79.000], mean observation: 3.152 [-1.542, 10.300], loss: 1.270034, mae: 5.033945, mean_q: 5.271990
 68398/100000: episode: 6981, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 3.847, mean reward: 0.385 [0.307, 0.503], mean action: 31.800 [16.000, 70.000], mean observation: 3.164 [-1.159, 10.283], loss: 1.347066, mae: 5.034441, mean_q: 5.271111
 68408/100000: episode: 6982, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.770, mean reward: 0.377 [0.364, 0.415], mean action: 47.000 [16.000, 88.000], mean observation: 3.162 [-0.996, 10.223], loss: 1.266097, mae: 5.034105, mean_q: 5.271062
 68418/100000: episode: 6983, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.055, mean reward: 0.405 [0.330, 0.560], mean action: 35.500 [16.000, 96.000], mean observation: 3.149 [-1.326, 10.352], loss: 1.186926, mae: 5.033903, mean_q: 5.267826
 68428/100000: episode: 6984, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.896, mean reward: 0.390 [0.349, 0.413], mean action: 42.300 [16.000, 101.000], mean observation: 3.159 [-1.919, 10.395], loss: 1.337414, mae: 5.034240, mean_q: 5.263238
 68438/100000: episode: 6985, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.847, mean reward: 0.385 [0.343, 0.446], mean action: 42.000 [16.000, 99.000], mean observation: 3.163 [-1.422, 10.284], loss: 1.599210, mae: 5.035097, mean_q: 5.258683
 68448/100000: episode: 6986, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 3.985, mean reward: 0.398 [0.357, 0.478], mean action: 23.500 [0.000, 88.000], mean observation: 3.155 [-1.217, 10.369], loss: 1.138040, mae: 5.033275, mean_q: 5.255809
 68458/100000: episode: 6987, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.973, mean reward: 0.397 [0.325, 0.461], mean action: 20.900 [1.000, 63.000], mean observation: 3.160 [-1.569, 10.326], loss: 0.748703, mae: 5.031852, mean_q: 5.254913
 68468/100000: episode: 6988, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.830, mean reward: 0.383 [0.317, 0.500], mean action: 31.500 [9.000, 83.000], mean observation: 3.155 [-1.503, 10.455], loss: 1.031612, mae: 5.033373, mean_q: 5.255601
 68478/100000: episode: 6989, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 5.242, mean reward: 0.524 [0.517, 0.542], mean action: 38.400 [16.000, 82.000], mean observation: 3.164 [-1.662, 10.206], loss: 1.262446, mae: 5.034820, mean_q: 5.250590
 68488/100000: episode: 6990, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.019, mean reward: 0.402 [0.364, 0.452], mean action: 25.000 [16.000, 56.000], mean observation: 3.162 [-1.415, 10.295], loss: 1.435014, mae: 5.035460, mean_q: 5.245084
 68498/100000: episode: 6991, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.998, mean reward: 0.400 [0.328, 0.443], mean action: 31.200 [16.000, 94.000], mean observation: 3.161 [-1.015, 10.468], loss: 1.344345, mae: 5.035404, mean_q: 5.245697
 68508/100000: episode: 6992, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.110, mean reward: 0.411 [0.342, 0.493], mean action: 22.900 [13.000, 67.000], mean observation: 3.153 [-1.074, 10.421], loss: 1.085348, mae: 5.034551, mean_q: 5.245737
 68518/100000: episode: 6993, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.635, mean reward: 0.464 [0.463, 0.464], mean action: 40.200 [14.000, 87.000], mean observation: 3.155 [-1.191, 10.366], loss: 1.569300, mae: 5.036515, mean_q: 5.241984
 68528/100000: episode: 6994, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.893, mean reward: 0.389 [0.325, 0.458], mean action: 29.300 [3.000, 97.000], mean observation: 3.141 [-1.207, 10.213], loss: 1.023800, mae: 5.034557, mean_q: 5.242955
 68538/100000: episode: 6995, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.619, mean reward: 0.462 [0.434, 0.569], mean action: 19.500 [16.000, 51.000], mean observation: 3.160 [-1.342, 10.283], loss: 1.357919, mae: 5.036150, mean_q: 5.245304
 68540/100000: episode: 6996, duration: 0.050s, episode steps: 2, steps per second: 40, episode reward: 10.395, mean reward: 5.198 [0.395, 10.000], mean action: 23.000 [16.000, 30.000], mean observation: 3.172 [-1.308, 10.337], loss: 1.490680, mae: 5.036725, mean_q: 5.245842
 68550/100000: episode: 6997, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 3.750, mean reward: 0.375 [0.333, 0.410], mean action: 32.800 [16.000, 84.000], mean observation: 3.157 [-1.212, 10.331], loss: 1.023769, mae: 5.034973, mean_q: 5.243361
 68560/100000: episode: 6998, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.199, mean reward: 0.420 [0.363, 0.489], mean action: 53.300 [9.000, 98.000], mean observation: 3.155 [-1.223, 10.319], loss: 1.359268, mae: 5.036551, mean_q: 5.244630
 68570/100000: episode: 6999, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.776, mean reward: 0.478 [0.346, 0.597], mean action: 31.200 [16.000, 79.000], mean observation: 3.152 [-1.515, 10.488], loss: 1.328061, mae: 5.036429, mean_q: 5.247042
 68580/100000: episode: 7000, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.770, mean reward: 0.377 [0.273, 0.475], mean action: 36.800 [16.000, 99.000], mean observation: 3.153 [-1.925, 10.277], loss: 1.355017, mae: 5.036760, mean_q: 5.248550
 68590/100000: episode: 7001, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.246, mean reward: 0.425 [0.424, 0.433], mean action: 29.900 [16.000, 101.000], mean observation: 3.167 [-1.232, 10.327], loss: 1.178764, mae: 5.036122, mean_q: 5.249017
 68600/100000: episode: 7002, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.021, mean reward: 0.402 [0.377, 0.473], mean action: 45.900 [16.000, 91.000], mean observation: 3.160 [-1.489, 10.268], loss: 1.185918, mae: 5.036211, mean_q: 5.246835
 68610/100000: episode: 7003, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.753, mean reward: 0.375 [0.302, 0.430], mean action: 51.700 [16.000, 101.000], mean observation: 3.149 [-1.242, 10.312], loss: 0.951111, mae: 5.035381, mean_q: 5.244252
 68620/100000: episode: 7004, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.012, mean reward: 0.401 [0.334, 0.481], mean action: 34.300 [10.000, 97.000], mean observation: 3.154 [-1.224, 10.290], loss: 1.219319, mae: 5.036569, mean_q: 5.240704
 68630/100000: episode: 7005, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.415, mean reward: 0.441 [0.369, 0.534], mean action: 28.600 [10.000, 86.000], mean observation: 3.151 [-1.690, 10.407], loss: 1.805264, mae: 5.038527, mean_q: 5.239872
 68640/100000: episode: 7006, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.975, mean reward: 0.398 [0.352, 0.446], mean action: 42.100 [16.000, 79.000], mean observation: 3.161 [-1.527, 10.390], loss: 1.519911, mae: 5.037112, mean_q: 5.242169
 68650/100000: episode: 7007, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.577, mean reward: 0.358 [0.311, 0.427], mean action: 48.200 [16.000, 93.000], mean observation: 3.162 [-0.906, 10.320], loss: 1.416384, mae: 5.036451, mean_q: 5.243873
 68660/100000: episode: 7008, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.675, mean reward: 0.468 [0.454, 0.528], mean action: 28.400 [16.000, 77.000], mean observation: 3.149 [-1.160, 10.547], loss: 1.351165, mae: 5.035825, mean_q: 5.244151
 68670/100000: episode: 7009, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.417, mean reward: 0.442 [0.345, 0.558], mean action: 33.000 [8.000, 83.000], mean observation: 3.148 [-0.811, 10.309], loss: 1.191599, mae: 5.035082, mean_q: 5.246301
 68680/100000: episode: 7010, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.274, mean reward: 0.427 [0.382, 0.560], mean action: 36.200 [16.000, 88.000], mean observation: 3.155 [-1.135, 10.391], loss: 1.075500, mae: 5.034815, mean_q: 5.248734
 68690/100000: episode: 7011, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.412, mean reward: 0.441 [0.347, 0.521], mean action: 47.900 [5.000, 97.000], mean observation: 3.157 [-0.879, 10.359], loss: 1.158324, mae: 5.035285, mean_q: 5.250010
 68700/100000: episode: 7012, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.457, mean reward: 0.446 [0.377, 0.650], mean action: 41.400 [6.000, 100.000], mean observation: 3.165 [-1.071, 10.441], loss: 1.216066, mae: 5.035464, mean_q: 5.249630
 68710/100000: episode: 7013, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.994, mean reward: 0.399 [0.338, 0.430], mean action: 43.700 [11.000, 94.000], mean observation: 3.155 [-1.497, 10.246], loss: 1.302710, mae: 5.036056, mean_q: 5.246403
 68720/100000: episode: 7014, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.406, mean reward: 0.441 [0.334, 0.544], mean action: 49.200 [16.000, 91.000], mean observation: 3.162 [-1.130, 10.254], loss: 1.186386, mae: 5.035620, mean_q: 5.244801
 68730/100000: episode: 7015, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.006, mean reward: 0.401 [0.325, 0.440], mean action: 27.600 [13.000, 92.000], mean observation: 3.143 [-1.229, 10.323], loss: 1.015900, mae: 5.035113, mean_q: 5.242969
 68740/100000: episode: 7016, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.310, mean reward: 0.431 [0.333, 0.549], mean action: 19.800 [3.000, 52.000], mean observation: 3.162 [-0.966, 10.373], loss: 1.612087, mae: 5.037707, mean_q: 5.239364
 68750/100000: episode: 7017, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.033, mean reward: 0.403 [0.307, 0.495], mean action: 19.500 [16.000, 37.000], mean observation: 3.167 [-1.766, 10.288], loss: 1.273546, mae: 5.036292, mean_q: 5.237697
 68760/100000: episode: 7018, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.048, mean reward: 0.405 [0.366, 0.450], mean action: 38.600 [16.000, 86.000], mean observation: 3.158 [-2.139, 10.324], loss: 1.222474, mae: 5.036115, mean_q: 5.237346
 68770/100000: episode: 7019, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.149, mean reward: 0.415 [0.351, 0.500], mean action: 34.000 [5.000, 91.000], mean observation: 3.163 [-1.758, 10.354], loss: 0.898782, mae: 5.034914, mean_q: 5.235694
 68780/100000: episode: 7020, duration: 0.220s, episode steps: 10, steps per second: 45, episode reward: 4.366, mean reward: 0.437 [0.309, 0.592], mean action: 17.700 [13.000, 30.000], mean observation: 3.147 [-2.314, 10.161], loss: 1.227903, mae: 5.036350, mean_q: 5.235085
 68790/100000: episode: 7021, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.688, mean reward: 0.369 [0.298, 0.446], mean action: 43.400 [16.000, 95.000], mean observation: 3.155 [-1.332, 10.239], loss: 1.280500, mae: 5.036650, mean_q: 5.233241
 68800/100000: episode: 7022, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.031, mean reward: 0.403 [0.338, 0.591], mean action: 48.800 [16.000, 96.000], mean observation: 3.146 [-1.569, 10.283], loss: 1.329936, mae: 5.036838, mean_q: 5.233653
 68810/100000: episode: 7023, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.148, mean reward: 0.415 [0.341, 0.476], mean action: 25.400 [4.000, 89.000], mean observation: 3.165 [-1.426, 10.211], loss: 1.186905, mae: 5.036440, mean_q: 5.229806
 68820/100000: episode: 7024, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.275, mean reward: 0.427 [0.352, 0.533], mean action: 27.800 [16.000, 84.000], mean observation: 3.165 [-1.656, 10.548], loss: 1.521746, mae: 5.037596, mean_q: 5.228591
 68830/100000: episode: 7025, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.400, mean reward: 0.440 [0.371, 0.530], mean action: 27.900 [0.000, 83.000], mean observation: 3.155 [-1.459, 10.516], loss: 1.250316, mae: 5.036275, mean_q: 5.230046
 68840/100000: episode: 7026, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.102, mean reward: 0.410 [0.309, 0.500], mean action: 28.000 [16.000, 89.000], mean observation: 3.155 [-1.317, 10.468], loss: 1.189775, mae: 5.036147, mean_q: 5.231472
 68850/100000: episode: 7027, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.578, mean reward: 0.358 [0.309, 0.435], mean action: 60.200 [16.000, 100.000], mean observation: 3.149 [-1.392, 10.296], loss: 1.474538, mae: 5.037221, mean_q: 5.230978
 68860/100000: episode: 7028, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.912, mean reward: 0.391 [0.338, 0.419], mean action: 18.200 [12.000, 44.000], mean observation: 3.160 [-1.512, 10.335], loss: 1.274443, mae: 5.036284, mean_q: 5.228729
 68870/100000: episode: 7029, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.264, mean reward: 0.426 [0.356, 0.548], mean action: 19.300 [1.000, 47.000], mean observation: 3.156 [-1.590, 10.285], loss: 0.933513, mae: 5.035081, mean_q: 5.225139
 68880/100000: episode: 7030, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.023, mean reward: 0.402 [0.316, 0.454], mean action: 25.300 [16.000, 66.000], mean observation: 3.156 [-1.069, 10.398], loss: 1.205042, mae: 5.036302, mean_q: 5.227307
 68890/100000: episode: 7031, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.417, mean reward: 0.442 [0.405, 0.482], mean action: 49.600 [16.000, 83.000], mean observation: 3.148 [-1.457, 10.280], loss: 1.378385, mae: 5.037157, mean_q: 5.227208
 68900/100000: episode: 7032, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.139, mean reward: 0.414 [0.323, 0.485], mean action: 23.400 [12.000, 88.000], mean observation: 3.155 [-1.252, 10.383], loss: 0.871032, mae: 5.035184, mean_q: 5.227856
 68910/100000: episode: 7033, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.642, mean reward: 0.464 [0.418, 0.506], mean action: 30.800 [10.000, 96.000], mean observation: 3.146 [-1.847, 10.325], loss: 0.838826, mae: 5.035460, mean_q: 5.227829
 68920/100000: episode: 7034, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.206, mean reward: 0.421 [0.310, 0.507], mean action: 26.700 [16.000, 52.000], mean observation: 3.156 [-1.738, 10.318], loss: 1.278631, mae: 5.037495, mean_q: 5.225188
 68930/100000: episode: 7035, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 5.368, mean reward: 0.537 [0.536, 0.544], mean action: 24.900 [14.000, 92.000], mean observation: 3.156 [-1.649, 10.501], loss: 1.318063, mae: 5.038048, mean_q: 5.221593
 68940/100000: episode: 7036, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.280, mean reward: 0.428 [0.360, 0.501], mean action: 47.700 [1.000, 81.000], mean observation: 3.150 [-1.029, 10.301], loss: 0.998397, mae: 5.036831, mean_q: 5.218503
 68941/100000: episode: 7037, duration: 0.030s, episode steps: 1, steps per second: 33, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 50.000 [50.000, 50.000], mean observation: 3.152 [-1.496, 10.307], loss: 1.789892, mae: 5.039396, mean_q: 5.218049
 68951/100000: episode: 7038, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.499, mean reward: 0.350 [0.287, 0.428], mean action: 46.700 [5.000, 97.000], mean observation: 3.158 [-1.120, 10.332], loss: 1.516731, mae: 5.038566, mean_q: 5.218312
 68961/100000: episode: 7039, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.978, mean reward: 0.398 [0.376, 0.416], mean action: 47.900 [12.000, 98.000], mean observation: 3.161 [-1.076, 10.443], loss: 0.936890, mae: 5.036583, mean_q: 5.219075
 68971/100000: episode: 7040, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.084, mean reward: 0.408 [0.390, 0.557], mean action: 50.900 [7.000, 96.000], mean observation: 3.156 [-1.562, 10.293], loss: 1.317462, mae: 5.037753, mean_q: 5.218759
 68981/100000: episode: 7041, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.899, mean reward: 0.490 [0.396, 0.513], mean action: 70.800 [2.000, 83.000], mean observation: 3.171 [-1.159, 10.317], loss: 1.097156, mae: 5.036819, mean_q: 5.216433
 68991/100000: episode: 7042, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.860, mean reward: 0.386 [0.355, 0.405], mean action: 56.700 [16.000, 95.000], mean observation: 3.151 [-1.147, 10.299], loss: 1.003475, mae: 5.036300, mean_q: 5.216299
 69001/100000: episode: 7043, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.047, mean reward: 0.405 [0.369, 0.485], mean action: 37.500 [16.000, 84.000], mean observation: 3.160 [-1.538, 10.280], loss: 1.202422, mae: 5.037505, mean_q: 5.214610
 69011/100000: episode: 7044, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.649, mean reward: 0.365 [0.326, 0.409], mean action: 43.200 [16.000, 83.000], mean observation: 3.158 [-1.240, 10.346], loss: 1.200232, mae: 5.037303, mean_q: 5.212680
 69021/100000: episode: 7045, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.007, mean reward: 0.401 [0.361, 0.466], mean action: 60.500 [6.000, 85.000], mean observation: 3.161 [-1.675, 10.258], loss: 1.500050, mae: 5.038373, mean_q: 5.213859
 69031/100000: episode: 7046, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.098, mean reward: 0.410 [0.362, 0.542], mean action: 63.500 [14.000, 101.000], mean observation: 3.158 [-1.110, 10.321], loss: 1.385291, mae: 5.037672, mean_q: 5.215762
 69041/100000: episode: 7047, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.797, mean reward: 0.480 [0.330, 0.533], mean action: 63.900 [1.000, 93.000], mean observation: 3.156 [-1.092, 10.326], loss: 0.883377, mae: 5.035575, mean_q: 5.217636
 69051/100000: episode: 7048, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.316, mean reward: 0.432 [0.401, 0.452], mean action: 69.400 [43.000, 83.000], mean observation: 3.145 [-1.339, 10.243], loss: 1.173393, mae: 5.036701, mean_q: 5.219051
 69061/100000: episode: 7049, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.077, mean reward: 0.408 [0.383, 0.487], mean action: 70.400 [12.000, 98.000], mean observation: 3.157 [-1.077, 10.282], loss: 0.869146, mae: 5.035787, mean_q: 5.220403
 69071/100000: episode: 7050, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.168, mean reward: 0.417 [0.382, 0.473], mean action: 66.700 [4.000, 95.000], mean observation: 3.144 [-1.408, 10.410], loss: 1.367618, mae: 5.038015, mean_q: 5.220849
 69081/100000: episode: 7051, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.617, mean reward: 0.362 [0.343, 0.415], mean action: 66.200 [9.000, 83.000], mean observation: 3.156 [-1.271, 10.337], loss: 1.409069, mae: 5.037979, mean_q: 5.219040
 69091/100000: episode: 7052, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 3.704, mean reward: 0.370 [0.347, 0.471], mean action: 75.700 [32.000, 96.000], mean observation: 3.154 [-1.719, 10.299], loss: 1.006061, mae: 5.036298, mean_q: 5.217309
 69101/100000: episode: 7053, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 3.606, mean reward: 0.361 [0.246, 0.542], mean action: 75.400 [39.000, 86.000], mean observation: 3.164 [-1.152, 10.237], loss: 1.195472, mae: 5.036757, mean_q: 5.217220
 69111/100000: episode: 7054, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.263, mean reward: 0.426 [0.419, 0.493], mean action: 71.100 [18.000, 83.000], mean observation: 3.162 [-1.221, 10.347], loss: 1.181335, mae: 5.036746, mean_q: 5.217700
 69121/100000: episode: 7055, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.986, mean reward: 0.499 [0.399, 0.538], mean action: 65.300 [11.000, 91.000], mean observation: 3.150 [-1.667, 10.310], loss: 1.055091, mae: 5.036282, mean_q: 5.218253
 69131/100000: episode: 7056, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.788, mean reward: 0.379 [0.362, 0.431], mean action: 61.500 [17.000, 83.000], mean observation: 3.158 [-1.038, 10.259], loss: 1.352177, mae: 5.037541, mean_q: 5.218932
 69141/100000: episode: 7057, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.715, mean reward: 0.471 [0.376, 0.519], mean action: 77.100 [23.000, 100.000], mean observation: 3.157 [-2.008, 10.226], loss: 0.760948, mae: 5.035447, mean_q: 5.220345
 69151/100000: episode: 7058, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.060, mean reward: 0.406 [0.293, 0.518], mean action: 58.300 [2.000, 91.000], mean observation: 3.166 [-1.519, 10.344], loss: 1.110131, mae: 5.036912, mean_q: 5.221911
 69161/100000: episode: 7059, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.354, mean reward: 0.435 [0.430, 0.486], mean action: 65.500 [18.000, 88.000], mean observation: 3.148 [-0.956, 10.221], loss: 1.149127, mae: 5.037477, mean_q: 5.223542
 69171/100000: episode: 7060, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.752, mean reward: 0.375 [0.345, 0.417], mean action: 59.400 [6.000, 83.000], mean observation: 3.155 [-1.289, 10.286], loss: 1.049672, mae: 5.037308, mean_q: 5.222285
 69181/100000: episode: 7061, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.669, mean reward: 0.467 [0.290, 0.528], mean action: 70.200 [32.000, 99.000], mean observation: 3.157 [-1.950, 10.249], loss: 1.293893, mae: 5.038281, mean_q: 5.222436
 69191/100000: episode: 7062, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.619, mean reward: 0.362 [0.358, 0.390], mean action: 71.900 [10.000, 96.000], mean observation: 3.137 [-0.996, 10.290], loss: 1.099554, mae: 5.037382, mean_q: 5.223326
 69201/100000: episode: 7063, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 3.831, mean reward: 0.383 [0.357, 0.444], mean action: 76.600 [52.000, 99.000], mean observation: 3.148 [-0.958, 10.218], loss: 1.420210, mae: 5.038623, mean_q: 5.223957
 69211/100000: episode: 7064, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.283, mean reward: 0.428 [0.405, 0.475], mean action: 65.500 [2.000, 99.000], mean observation: 3.150 [-1.530, 10.213], loss: 1.304916, mae: 5.038152, mean_q: 5.218952
 69221/100000: episode: 7065, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.262, mean reward: 0.426 [0.366, 0.599], mean action: 48.500 [0.000, 97.000], mean observation: 3.160 [-1.267, 10.339], loss: 1.310565, mae: 5.037979, mean_q: 5.214229
 69231/100000: episode: 7066, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.091, mean reward: 0.409 [0.349, 0.512], mean action: 43.100 [14.000, 68.000], mean observation: 3.163 [-1.125, 10.172], loss: 1.053206, mae: 5.037011, mean_q: 5.212577
 69241/100000: episode: 7067, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.929, mean reward: 0.393 [0.332, 0.546], mean action: 41.100 [1.000, 74.000], mean observation: 3.161 [-1.837, 10.379], loss: 0.967919, mae: 5.036745, mean_q: 5.213017
 69251/100000: episode: 7068, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.821, mean reward: 0.482 [0.451, 0.492], mean action: 44.300 [6.000, 81.000], mean observation: 3.166 [-1.177, 10.404], loss: 1.261907, mae: 5.037951, mean_q: 5.213886
 69261/100000: episode: 7069, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.958, mean reward: 0.496 [0.449, 0.565], mean action: 37.200 [8.000, 76.000], mean observation: 3.178 [-1.981, 10.380], loss: 1.051560, mae: 5.037016, mean_q: 5.210735
 69271/100000: episode: 7070, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.904, mean reward: 0.390 [0.322, 0.448], mean action: 47.000 [4.000, 97.000], mean observation: 3.149 [-1.383, 10.493], loss: 1.194993, mae: 5.037602, mean_q: 5.209872
 69281/100000: episode: 7071, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.991, mean reward: 0.399 [0.382, 0.423], mean action: 81.300 [58.000, 101.000], mean observation: 3.154 [-0.753, 10.350], loss: 1.311644, mae: 5.038058, mean_q: 5.210291
 69291/100000: episode: 7072, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.560, mean reward: 0.356 [0.320, 0.370], mean action: 53.400 [16.000, 85.000], mean observation: 3.157 [-1.627, 10.302], loss: 1.163404, mae: 5.037240, mean_q: 5.207883
 69301/100000: episode: 7073, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.353, mean reward: 0.435 [0.356, 0.593], mean action: 26.000 [16.000, 65.000], mean observation: 3.165 [-1.102, 10.297], loss: 1.167344, mae: 5.037165, mean_q: 5.205972
 69311/100000: episode: 7074, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.803, mean reward: 0.380 [0.322, 0.473], mean action: 29.800 [16.000, 98.000], mean observation: 3.159 [-1.089, 10.310], loss: 1.303991, mae: 5.037548, mean_q: 5.207045
 69321/100000: episode: 7075, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.756, mean reward: 0.376 [0.316, 0.498], mean action: 40.400 [16.000, 100.000], mean observation: 3.154 [-1.616, 10.243], loss: 0.971559, mae: 5.036517, mean_q: 5.208302
 69331/100000: episode: 7076, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.983, mean reward: 0.398 [0.362, 0.469], mean action: 38.300 [16.000, 86.000], mean observation: 3.153 [-1.491, 10.397], loss: 0.950554, mae: 5.036591, mean_q: 5.205880
 69341/100000: episode: 7077, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.314, mean reward: 0.431 [0.387, 0.515], mean action: 61.700 [8.000, 83.000], mean observation: 3.142 [-2.055, 10.286], loss: 1.014534, mae: 5.037181, mean_q: 5.206185
 69351/100000: episode: 7078, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.161, mean reward: 0.416 [0.402, 0.456], mean action: 49.200 [2.000, 100.000], mean observation: 3.146 [-1.458, 10.467], loss: 1.140692, mae: 5.037958, mean_q: 5.207830
 69361/100000: episode: 7079, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.766, mean reward: 0.377 [0.339, 0.427], mean action: 58.000 [23.000, 83.000], mean observation: 3.171 [-1.524, 10.478], loss: 0.892133, mae: 5.037249, mean_q: 5.209343
 69371/100000: episode: 7080, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.178, mean reward: 0.418 [0.374, 0.508], mean action: 65.600 [1.000, 83.000], mean observation: 3.170 [-0.981, 10.285], loss: 1.420789, mae: 5.039197, mean_q: 5.211409
 69381/100000: episode: 7081, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.461, mean reward: 0.346 [0.321, 0.431], mean action: 60.500 [14.000, 89.000], mean observation: 3.162 [-1.067, 10.285], loss: 1.243644, mae: 5.038836, mean_q: 5.213620
 69391/100000: episode: 7082, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 3.767, mean reward: 0.377 [0.362, 0.417], mean action: 77.400 [26.000, 84.000], mean observation: 3.166 [-1.334, 10.308], loss: 1.529487, mae: 5.039927, mean_q: 5.215788
 69401/100000: episode: 7083, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.359, mean reward: 0.436 [0.436, 0.436], mean action: 76.900 [43.000, 83.000], mean observation: 3.150 [-1.608, 10.281], loss: 1.237720, mae: 5.038510, mean_q: 5.217199
 69411/100000: episode: 7084, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.273, mean reward: 0.427 [0.352, 0.486], mean action: 61.900 [12.000, 83.000], mean observation: 3.167 [-1.050, 10.367], loss: 1.056803, mae: 5.037752, mean_q: 5.214494
 69421/100000: episode: 7085, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.869, mean reward: 0.387 [0.310, 0.421], mean action: 65.200 [0.000, 98.000], mean observation: 3.168 [-0.907, 10.390], loss: 1.227462, mae: 5.038363, mean_q: 5.213735
 69431/100000: episode: 7086, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.069, mean reward: 0.407 [0.373, 0.501], mean action: 62.000 [6.000, 83.000], mean observation: 3.157 [-1.037, 10.240], loss: 1.321836, mae: 5.038795, mean_q: 5.215463
 69441/100000: episode: 7087, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.289, mean reward: 0.429 [0.404, 0.446], mean action: 82.700 [35.000, 99.000], mean observation: 3.158 [-1.226, 10.332], loss: 0.906099, mae: 5.036911, mean_q: 5.218378
 69451/100000: episode: 7088, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.335, mean reward: 0.433 [0.382, 0.535], mean action: 71.100 [22.000, 94.000], mean observation: 3.155 [-1.290, 10.446], loss: 1.202476, mae: 5.038389, mean_q: 5.222465
 69461/100000: episode: 7089, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 4.100, mean reward: 0.410 [0.384, 0.445], mean action: 67.200 [13.000, 83.000], mean observation: 3.161 [-1.283, 10.283], loss: 1.221826, mae: 5.038529, mean_q: 5.225816
 69471/100000: episode: 7090, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 4.212, mean reward: 0.421 [0.364, 0.453], mean action: 76.800 [30.000, 101.000], mean observation: 3.163 [-1.081, 10.333], loss: 1.374646, mae: 5.038725, mean_q: 5.226030
 69481/100000: episode: 7091, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.398, mean reward: 0.440 [0.358, 0.588], mean action: 51.900 [1.000, 99.000], mean observation: 3.147 [-2.352, 10.311], loss: 0.898229, mae: 5.036859, mean_q: 5.226928
 69491/100000: episode: 7092, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.752, mean reward: 0.375 [0.285, 0.436], mean action: 59.100 [12.000, 83.000], mean observation: 3.159 [-0.921, 10.387], loss: 1.396404, mae: 5.038743, mean_q: 5.223351
 69501/100000: episode: 7093, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.308, mean reward: 0.431 [0.398, 0.455], mean action: 64.600 [13.000, 96.000], mean observation: 3.158 [-1.633, 10.365], loss: 1.068249, mae: 5.037356, mean_q: 5.221411
 69511/100000: episode: 7094, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.535, mean reward: 0.353 [0.310, 0.425], mean action: 69.500 [3.000, 85.000], mean observation: 3.154 [-1.279, 10.282], loss: 1.083891, mae: 5.037551, mean_q: 5.220175
 69521/100000: episode: 7095, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.104, mean reward: 0.410 [0.324, 0.474], mean action: 58.900 [5.000, 98.000], mean observation: 3.158 [-1.279, 10.357], loss: 1.565379, mae: 5.039584, mean_q: 5.216931
 69531/100000: episode: 7096, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.087, mean reward: 0.409 [0.339, 0.445], mean action: 68.000 [30.000, 83.000], mean observation: 3.155 [-1.215, 10.273], loss: 1.204781, mae: 5.038113, mean_q: 5.213926
 69541/100000: episode: 7097, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.606, mean reward: 0.461 [0.461, 0.461], mean action: 70.200 [33.000, 83.000], mean observation: 3.133 [-1.022, 10.250], loss: 0.984309, mae: 5.037205, mean_q: 5.213306
 69551/100000: episode: 7098, duration: 0.121s, episode steps: 10, steps per second: 82, episode reward: 4.127, mean reward: 0.413 [0.396, 0.532], mean action: 77.700 [40.000, 93.000], mean observation: 3.142 [-1.785, 10.295], loss: 1.105236, mae: 5.037583, mean_q: 5.211436
 69561/100000: episode: 7099, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.874, mean reward: 0.387 [0.346, 0.487], mean action: 71.000 [3.000, 83.000], mean observation: 3.161 [-1.396, 10.338], loss: 1.109464, mae: 5.037791, mean_q: 5.210124
 69571/100000: episode: 7100, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 3.861, mean reward: 0.386 [0.328, 0.419], mean action: 62.600 [43.000, 100.000], mean observation: 3.153 [-1.722, 10.228], loss: 1.060934, mae: 5.037828, mean_q: 5.208996
 69581/100000: episode: 7101, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.876, mean reward: 0.388 [0.344, 0.505], mean action: 52.200 [4.000, 98.000], mean observation: 3.167 [-1.341, 10.292], loss: 1.166162, mae: 5.038065, mean_q: 5.211665
 69591/100000: episode: 7102, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.097, mean reward: 0.410 [0.366, 0.512], mean action: 36.800 [11.000, 43.000], mean observation: 3.149 [-2.028, 10.239], loss: 1.324114, mae: 5.038400, mean_q: 5.211742
 69601/100000: episode: 7103, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.174, mean reward: 0.417 [0.355, 0.506], mean action: 42.200 [3.000, 97.000], mean observation: 3.153 [-0.773, 10.429], loss: 1.191993, mae: 5.037755, mean_q: 5.212752
 69611/100000: episode: 7104, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.077, mean reward: 0.408 [0.299, 0.518], mean action: 44.000 [6.000, 97.000], mean observation: 3.157 [-1.029, 10.419], loss: 1.096648, mae: 5.037289, mean_q: 5.212121
 69621/100000: episode: 7105, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.740, mean reward: 0.374 [0.325, 0.439], mean action: 40.800 [0.000, 98.000], mean observation: 3.150 [-1.876, 10.243], loss: 0.916283, mae: 5.036717, mean_q: 5.214054
 69631/100000: episode: 7106, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.210, mean reward: 0.421 [0.367, 0.482], mean action: 44.100 [11.000, 96.000], mean observation: 3.160 [-1.070, 10.267], loss: 0.817890, mae: 5.036696, mean_q: 5.216551
 69641/100000: episode: 7107, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.384, mean reward: 0.438 [0.368, 0.556], mean action: 20.800 [16.000, 39.000], mean observation: 3.158 [-1.347, 10.333], loss: 1.110580, mae: 5.038386, mean_q: 5.218728
 69651/100000: episode: 7108, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 3.610, mean reward: 0.361 [0.297, 0.404], mean action: 34.500 [13.000, 94.000], mean observation: 3.158 [-1.591, 10.382], loss: 1.273291, mae: 5.039186, mean_q: 5.216638
 69661/100000: episode: 7109, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.916, mean reward: 0.392 [0.357, 0.459], mean action: 60.200 [16.000, 97.000], mean observation: 3.166 [-1.606, 10.346], loss: 0.985781, mae: 5.037943, mean_q: 5.216957
 69668/100000: episode: 7110, duration: 0.118s, episode steps: 7, steps per second: 59, episode reward: 12.458, mean reward: 1.780 [0.356, 10.000], mean action: 39.571 [16.000, 58.000], mean observation: 3.157 [-1.586, 10.328], loss: 1.456119, mae: 5.039813, mean_q: 5.218133
 69678/100000: episode: 7111, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.246, mean reward: 0.425 [0.337, 0.472], mean action: 38.200 [16.000, 77.000], mean observation: 3.148 [-1.213, 10.270], loss: 0.906604, mae: 5.037430, mean_q: 5.219010
 69688/100000: episode: 7112, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.169, mean reward: 0.417 [0.342, 0.519], mean action: 29.200 [0.000, 84.000], mean observation: 3.145 [-1.689, 10.447], loss: 1.366388, mae: 5.039021, mean_q: 5.218769
 69698/100000: episode: 7113, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.107, mean reward: 0.411 [0.346, 0.478], mean action: 33.000 [16.000, 101.000], mean observation: 3.159 [-1.500, 10.333], loss: 1.039057, mae: 5.037511, mean_q: 5.215588
 69708/100000: episode: 7114, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.147, mean reward: 0.415 [0.320, 0.539], mean action: 52.700 [9.000, 97.000], mean observation: 3.153 [-1.291, 10.310], loss: 1.575056, mae: 5.039479, mean_q: 5.214589
 69718/100000: episode: 7115, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.779, mean reward: 0.378 [0.325, 0.415], mean action: 27.500 [16.000, 43.000], mean observation: 3.157 [-1.111, 10.314], loss: 1.374793, mae: 5.038259, mean_q: 5.215481
 69728/100000: episode: 7116, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.308, mean reward: 0.431 [0.349, 0.520], mean action: 26.500 [13.000, 89.000], mean observation: 3.157 [-1.651, 10.383], loss: 1.383965, mae: 5.038106, mean_q: 5.217517
 69738/100000: episode: 7117, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.046, mean reward: 0.405 [0.352, 0.508], mean action: 47.300 [16.000, 85.000], mean observation: 3.158 [-1.476, 10.318], loss: 1.091284, mae: 5.036835, mean_q: 5.217335
 69742/100000: episode: 7118, duration: 0.099s, episode steps: 4, steps per second: 41, episode reward: 11.462, mean reward: 2.866 [0.486, 10.000], mean action: 34.250 [16.000, 89.000], mean observation: 3.163 [-1.858, 10.353], loss: 1.709478, mae: 5.039105, mean_q: 5.217267
 69752/100000: episode: 7119, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.323, mean reward: 0.432 [0.367, 0.467], mean action: 34.700 [16.000, 99.000], mean observation: 3.149 [-1.213, 10.346], loss: 1.059461, mae: 5.036296, mean_q: 5.219131
 69762/100000: episode: 7120, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.097, mean reward: 0.410 [0.360, 0.464], mean action: 31.500 [0.000, 101.000], mean observation: 3.162 [-1.061, 10.467], loss: 0.999428, mae: 5.035975, mean_q: 5.222648
 69772/100000: episode: 7121, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.157, mean reward: 0.416 [0.339, 0.517], mean action: 23.600 [6.000, 68.000], mean observation: 3.162 [-1.658, 10.412], loss: 1.193290, mae: 5.037033, mean_q: 5.225020
 69782/100000: episode: 7122, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.608, mean reward: 0.461 [0.392, 0.500], mean action: 33.500 [8.000, 91.000], mean observation: 3.154 [-1.375, 10.536], loss: 1.424315, mae: 5.037827, mean_q: 5.228368
 69792/100000: episode: 7123, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.788, mean reward: 0.379 [0.331, 0.461], mean action: 27.800 [6.000, 96.000], mean observation: 3.148 [-1.644, 10.296], loss: 1.386085, mae: 5.037473, mean_q: 5.230105
 69802/100000: episode: 7124, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.845, mean reward: 0.385 [0.348, 0.443], mean action: 39.000 [5.000, 101.000], mean observation: 3.164 [-1.396, 10.437], loss: 1.010160, mae: 5.035895, mean_q: 5.231917
 69812/100000: episode: 7125, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.793, mean reward: 0.379 [0.307, 0.488], mean action: 32.200 [7.000, 91.000], mean observation: 3.162 [-2.681, 10.313], loss: 1.252650, mae: 5.036945, mean_q: 5.233301
 69822/100000: episode: 7126, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.997, mean reward: 0.400 [0.326, 0.479], mean action: 23.600 [4.000, 100.000], mean observation: 3.167 [-1.551, 10.314], loss: 0.963226, mae: 5.035929, mean_q: 5.233067
 69832/100000: episode: 7127, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.269, mean reward: 0.427 [0.344, 0.508], mean action: 30.800 [16.000, 74.000], mean observation: 3.162 [-1.193, 10.296], loss: 1.454523, mae: 5.037878, mean_q: 5.235123
 69842/100000: episode: 7128, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.376, mean reward: 0.438 [0.310, 0.550], mean action: 26.300 [5.000, 70.000], mean observation: 3.155 [-1.638, 10.270], loss: 1.068396, mae: 5.036417, mean_q: 5.232675
 69852/100000: episode: 7129, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.859, mean reward: 0.386 [0.317, 0.458], mean action: 26.800 [16.000, 75.000], mean observation: 3.157 [-1.637, 10.324], loss: 1.395938, mae: 5.037667, mean_q: 5.230237
 69862/100000: episode: 7130, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.172, mean reward: 0.417 [0.407, 0.498], mean action: 44.100 [16.000, 100.000], mean observation: 3.152 [-2.210, 10.209], loss: 1.404025, mae: 5.037504, mean_q: 5.230685
 69872/100000: episode: 7131, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.991, mean reward: 0.399 [0.358, 0.450], mean action: 28.500 [16.000, 99.000], mean observation: 3.149 [-1.560, 10.239], loss: 1.423082, mae: 5.037275, mean_q: 5.230588
 69882/100000: episode: 7132, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.255, mean reward: 0.426 [0.376, 0.496], mean action: 41.300 [3.000, 96.000], mean observation: 3.150 [-1.434, 10.338], loss: 1.028545, mae: 5.035569, mean_q: 5.230037
 69892/100000: episode: 7133, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.135, mean reward: 0.413 [0.362, 0.549], mean action: 49.000 [13.000, 101.000], mean observation: 3.154 [-1.730, 10.191], loss: 1.269671, mae: 5.036457, mean_q: 5.228877
 69902/100000: episode: 7134, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.235, mean reward: 0.423 [0.356, 0.497], mean action: 27.100 [16.000, 77.000], mean observation: 3.161 [-1.556, 10.318], loss: 1.199185, mae: 5.036129, mean_q: 5.230021
 69912/100000: episode: 7135, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.282, mean reward: 0.428 [0.344, 0.515], mean action: 30.200 [16.000, 73.000], mean observation: 3.157 [-1.631, 10.326], loss: 1.211079, mae: 5.036077, mean_q: 5.230607
 69922/100000: episode: 7136, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.196, mean reward: 0.420 [0.399, 0.514], mean action: 39.600 [7.000, 92.000], mean observation: 3.169 [-1.233, 10.412], loss: 1.269464, mae: 5.036280, mean_q: 5.229762
 69932/100000: episode: 7137, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.225, mean reward: 0.422 [0.334, 0.499], mean action: 38.000 [16.000, 95.000], mean observation: 3.146 [-1.316, 10.241], loss: 1.091576, mae: 5.035657, mean_q: 5.230052
 69942/100000: episode: 7138, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 5.150, mean reward: 0.515 [0.515, 0.515], mean action: 27.900 [16.000, 72.000], mean observation: 3.170 [-1.634, 10.331], loss: 1.028894, mae: 5.035486, mean_q: 5.232311
 69952/100000: episode: 7139, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 4.436, mean reward: 0.444 [0.329, 0.551], mean action: 30.100 [3.000, 97.000], mean observation: 3.154 [-1.460, 10.269], loss: 0.835966, mae: 5.034811, mean_q: 5.231619
 69962/100000: episode: 7140, duration: 0.224s, episode steps: 10, steps per second: 45, episode reward: 4.462, mean reward: 0.446 [0.377, 0.513], mean action: 23.100 [1.000, 66.000], mean observation: 3.152 [-1.197, 10.329], loss: 1.256417, mae: 5.036469, mean_q: 5.230590
 69972/100000: episode: 7141, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.084, mean reward: 0.408 [0.287, 0.512], mean action: 32.900 [0.000, 95.000], mean observation: 3.151 [-1.490, 10.352], loss: 0.996335, mae: 5.035565, mean_q: 5.227668
 69982/100000: episode: 7142, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.072, mean reward: 0.407 [0.344, 0.492], mean action: 32.400 [16.000, 74.000], mean observation: 3.157 [-1.444, 10.530], loss: 1.044801, mae: 5.036130, mean_q: 5.226206
 69992/100000: episode: 7143, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.021, mean reward: 0.402 [0.317, 0.540], mean action: 45.400 [16.000, 93.000], mean observation: 3.161 [-1.115, 10.407], loss: 1.309906, mae: 5.037204, mean_q: 5.225616
 70002/100000: episode: 7144, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.554, mean reward: 0.455 [0.446, 0.542], mean action: 26.800 [16.000, 66.000], mean observation: 3.166 [-1.848, 10.389], loss: 1.300570, mae: 5.037248, mean_q: 5.224514
 70012/100000: episode: 7145, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.151, mean reward: 0.415 [0.305, 0.466], mean action: 38.000 [8.000, 96.000], mean observation: 3.155 [-1.489, 10.217], loss: 1.282254, mae: 5.036977, mean_q: 5.222013
 70022/100000: episode: 7146, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.250, mean reward: 0.425 [0.357, 0.451], mean action: 25.800 [0.000, 80.000], mean observation: 3.155 [-1.397, 10.386], loss: 1.026065, mae: 5.035886, mean_q: 5.221654
 70032/100000: episode: 7147, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.701, mean reward: 0.470 [0.362, 0.505], mean action: 25.000 [6.000, 67.000], mean observation: 3.161 [-1.171, 10.309], loss: 1.168751, mae: 5.036487, mean_q: 5.217638
 70042/100000: episode: 7148, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.093, mean reward: 0.409 [0.278, 0.556], mean action: 35.600 [16.000, 78.000], mean observation: 3.164 [-1.199, 10.287], loss: 1.259648, mae: 5.037141, mean_q: 5.213727
 70046/100000: episode: 7149, duration: 0.101s, episode steps: 4, steps per second: 40, episode reward: 11.251, mean reward: 2.813 [0.359, 10.000], mean action: 16.000 [16.000, 16.000], mean observation: 3.152 [-1.097, 10.184], loss: 0.962659, mae: 5.035704, mean_q: 5.213161
 70056/100000: episode: 7150, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.159, mean reward: 0.416 [0.355, 0.480], mean action: 23.700 [0.000, 58.000], mean observation: 3.161 [-1.470, 10.415], loss: 1.281331, mae: 5.036953, mean_q: 5.212055
 70066/100000: episode: 7151, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.181, mean reward: 0.418 [0.363, 0.518], mean action: 49.700 [10.000, 96.000], mean observation: 3.160 [-1.326, 10.319], loss: 1.373703, mae: 5.037413, mean_q: 5.203842
 70076/100000: episode: 7152, duration: 0.127s, episode steps: 10, steps per second: 78, episode reward: 3.628, mean reward: 0.363 [0.315, 0.407], mean action: 69.700 [20.000, 100.000], mean observation: 3.159 [-1.478, 10.389], loss: 1.218001, mae: 5.036750, mean_q: 5.204327
 70086/100000: episode: 7153, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.722, mean reward: 0.372 [0.342, 0.399], mean action: 49.400 [33.000, 75.000], mean observation: 3.149 [-2.362, 10.216], loss: 0.970333, mae: 5.035705, mean_q: 5.205478
 70096/100000: episode: 7154, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.363, mean reward: 0.436 [0.356, 0.460], mean action: 54.400 [30.000, 86.000], mean observation: 3.161 [-1.383, 10.267], loss: 1.103039, mae: 5.036128, mean_q: 5.206914
 70106/100000: episode: 7155, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.445, mean reward: 0.444 [0.410, 0.565], mean action: 45.200 [9.000, 76.000], mean observation: 3.160 [-1.336, 10.396], loss: 1.375527, mae: 5.037405, mean_q: 5.208249
 70116/100000: episode: 7156, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.964, mean reward: 0.496 [0.496, 0.496], mean action: 55.800 [40.000, 76.000], mean observation: 3.128 [-1.353, 10.252], loss: 1.105589, mae: 5.036386, mean_q: 5.209691
 70126/100000: episode: 7157, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.870, mean reward: 0.387 [0.357, 0.488], mean action: 56.600 [30.000, 88.000], mean observation: 3.135 [-1.584, 10.317], loss: 0.925609, mae: 5.035873, mean_q: 5.210782
 70136/100000: episode: 7158, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.319, mean reward: 0.432 [0.414, 0.517], mean action: 49.500 [20.000, 99.000], mean observation: 3.159 [-1.180, 10.360], loss: 1.171574, mae: 5.036830, mean_q: 5.212198
 70146/100000: episode: 7159, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.163, mean reward: 0.416 [0.370, 0.544], mean action: 60.700 [11.000, 92.000], mean observation: 3.153 [-1.567, 10.273], loss: 1.188947, mae: 5.036890, mean_q: 5.213472
 70156/100000: episode: 7160, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.192, mean reward: 0.419 [0.370, 0.477], mean action: 47.700 [9.000, 97.000], mean observation: 3.151 [-1.572, 10.306], loss: 1.200272, mae: 5.036801, mean_q: 5.211966
 70166/100000: episode: 7161, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.280, mean reward: 0.428 [0.386, 0.514], mean action: 51.100 [31.000, 76.000], mean observation: 3.147 [-2.139, 10.280], loss: 1.518331, mae: 5.037834, mean_q: 5.208842
 70176/100000: episode: 7162, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.284, mean reward: 0.428 [0.322, 0.543], mean action: 59.000 [13.000, 94.000], mean observation: 3.154 [-1.548, 10.265], loss: 1.261214, mae: 5.036854, mean_q: 5.205311
 70186/100000: episode: 7163, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.032, mean reward: 0.403 [0.398, 0.448], mean action: 49.900 [23.000, 93.000], mean observation: 3.141 [-1.963, 10.299], loss: 1.305433, mae: 5.036729, mean_q: 5.201357
 70196/100000: episode: 7164, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.153, mean reward: 0.415 [0.373, 0.475], mean action: 59.900 [6.000, 99.000], mean observation: 3.159 [-1.669, 10.385], loss: 1.322656, mae: 5.036498, mean_q: 5.201496
 70206/100000: episode: 7165, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.909, mean reward: 0.391 [0.361, 0.448], mean action: 51.100 [8.000, 96.000], mean observation: 3.153 [-1.228, 10.365], loss: 1.525825, mae: 5.037141, mean_q: 5.203175
 70216/100000: episode: 7166, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 3.824, mean reward: 0.382 [0.319, 0.484], mean action: 61.100 [51.000, 85.000], mean observation: 3.147 [-1.816, 10.346], loss: 1.261896, mae: 5.035463, mean_q: 5.203977
 70226/100000: episode: 7167, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.701, mean reward: 0.370 [0.333, 0.427], mean action: 59.400 [4.000, 95.000], mean observation: 3.158 [-1.331, 10.459], loss: 0.991797, mae: 5.034247, mean_q: 5.204586
 70236/100000: episode: 7168, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.919, mean reward: 0.392 [0.355, 0.570], mean action: 55.500 [24.000, 90.000], mean observation: 3.152 [-1.238, 10.290], loss: 1.125410, mae: 5.034564, mean_q: 5.205204
 70246/100000: episode: 7169, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.030, mean reward: 0.403 [0.360, 0.473], mean action: 53.400 [22.000, 100.000], mean observation: 3.159 [-1.179, 10.194], loss: 1.372872, mae: 5.035623, mean_q: 5.206439
 70256/100000: episode: 7170, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.977, mean reward: 0.398 [0.336, 0.431], mean action: 26.600 [1.000, 90.000], mean observation: 3.153 [-1.433, 10.302], loss: 1.366786, mae: 5.035296, mean_q: 5.205606
 70266/100000: episode: 7171, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.160, mean reward: 0.416 [0.365, 0.536], mean action: 52.800 [4.000, 93.000], mean observation: 3.156 [-1.605, 10.302], loss: 1.420134, mae: 5.035048, mean_q: 5.206262
 70276/100000: episode: 7172, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.501, mean reward: 0.450 [0.361, 0.460], mean action: 45.500 [4.000, 85.000], mean observation: 3.161 [-1.893, 10.337], loss: 1.396146, mae: 5.034579, mean_q: 5.206185
 70286/100000: episode: 7173, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.871, mean reward: 0.387 [0.346, 0.421], mean action: 38.100 [4.000, 93.000], mean observation: 3.154 [-1.297, 10.231], loss: 1.243380, mae: 5.033467, mean_q: 5.207442
 70296/100000: episode: 7174, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.865, mean reward: 0.487 [0.405, 0.499], mean action: 44.900 [3.000, 88.000], mean observation: 3.148 [-1.368, 10.356], loss: 1.643077, mae: 5.034594, mean_q: 5.209278
 70306/100000: episode: 7175, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 3.940, mean reward: 0.394 [0.357, 0.456], mean action: 53.200 [30.000, 86.000], mean observation: 3.153 [-1.137, 10.302], loss: 1.209618, mae: 5.032718, mean_q: 5.212898
 70316/100000: episode: 7176, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.352, mean reward: 0.435 [0.434, 0.445], mean action: 53.600 [18.000, 85.000], mean observation: 3.155 [-2.303, 10.311], loss: 1.556121, mae: 5.033536, mean_q: 5.214756
 70326/100000: episode: 7177, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.706, mean reward: 0.371 [0.325, 0.492], mean action: 51.000 [51.000, 51.000], mean observation: 3.156 [-1.990, 10.283], loss: 1.138268, mae: 5.031531, mean_q: 5.217247
 70336/100000: episode: 7178, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 3.507, mean reward: 0.351 [0.309, 0.401], mean action: 32.700 [1.000, 51.000], mean observation: 3.154 [-1.240, 10.256], loss: 1.283661, mae: 5.031805, mean_q: 5.219819
 70346/100000: episode: 7179, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.833, mean reward: 0.383 [0.360, 0.482], mean action: 51.000 [13.000, 81.000], mean observation: 3.159 [-1.614, 10.309], loss: 1.310885, mae: 5.031737, mean_q: 5.221705
 70356/100000: episode: 7180, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.170, mean reward: 0.417 [0.393, 0.478], mean action: 46.700 [27.000, 78.000], mean observation: 3.159 [-1.176, 10.404], loss: 1.206324, mae: 5.031437, mean_q: 5.223779
 70366/100000: episode: 7181, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.902, mean reward: 0.390 [0.301, 0.488], mean action: 47.700 [17.000, 88.000], mean observation: 3.156 [-1.513, 10.326], loss: 1.026425, mae: 5.030896, mean_q: 5.225817
 70376/100000: episode: 7182, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.434, mean reward: 0.443 [0.440, 0.470], mean action: 56.900 [51.000, 87.000], mean observation: 3.173 [-1.529, 10.284], loss: 1.376271, mae: 5.032027, mean_q: 5.227891
 70386/100000: episode: 7183, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.445, mean reward: 0.445 [0.343, 0.506], mean action: 33.200 [0.000, 82.000], mean observation: 3.156 [-1.905, 10.270], loss: 1.027048, mae: 5.030831, mean_q: 5.229247
 70396/100000: episode: 7184, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.212, mean reward: 0.421 [0.349, 0.516], mean action: 52.400 [11.000, 99.000], mean observation: 3.189 [-1.321, 10.472], loss: 1.137732, mae: 5.031374, mean_q: 5.231125
 70406/100000: episode: 7185, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.792, mean reward: 0.379 [0.349, 0.513], mean action: 52.300 [43.000, 64.000], mean observation: 3.160 [-2.304, 10.309], loss: 1.340536, mae: 5.032008, mean_q: 5.233542
 70416/100000: episode: 7186, duration: 0.131s, episode steps: 10, steps per second: 77, episode reward: 3.848, mean reward: 0.385 [0.337, 0.432], mean action: 62.100 [2.000, 101.000], mean observation: 3.166 [-1.880, 10.319], loss: 1.476293, mae: 5.032499, mean_q: 5.235686
 70426/100000: episode: 7187, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.874, mean reward: 0.387 [0.366, 0.482], mean action: 55.600 [30.000, 90.000], mean observation: 3.157 [-0.925, 10.234], loss: 1.201323, mae: 5.031022, mean_q: 5.238048
 70436/100000: episode: 7188, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 3.952, mean reward: 0.395 [0.344, 0.442], mean action: 59.900 [30.000, 93.000], mean observation: 3.166 [-2.062, 10.324], loss: 1.044309, mae: 5.030499, mean_q: 5.240210
 70446/100000: episode: 7189, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.331, mean reward: 0.433 [0.417, 0.498], mean action: 49.800 [11.000, 92.000], mean observation: 3.161 [-1.140, 10.490], loss: 1.197019, mae: 5.031457, mean_q: 5.241441
 70456/100000: episode: 7190, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.077, mean reward: 0.408 [0.292, 0.504], mean action: 64.100 [46.000, 100.000], mean observation: 3.160 [-0.804, 10.275], loss: 1.548139, mae: 5.032932, mean_q: 5.240298
 70466/100000: episode: 7191, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.578, mean reward: 0.458 [0.338, 0.506], mean action: 38.000 [12.000, 51.000], mean observation: 3.148 [-0.991, 10.359], loss: 0.998238, mae: 5.030651, mean_q: 5.238946
 70476/100000: episode: 7192, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.868, mean reward: 0.387 [0.353, 0.450], mean action: 55.600 [16.000, 98.000], mean observation: 3.149 [-1.154, 10.253], loss: 1.069471, mae: 5.030902, mean_q: 5.234841
 70486/100000: episode: 7193, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.872, mean reward: 0.487 [0.487, 0.487], mean action: 47.800 [11.000, 100.000], mean observation: 3.154 [-1.384, 10.480], loss: 1.374684, mae: 5.032372, mean_q: 5.226349
 70496/100000: episode: 7194, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.936, mean reward: 0.394 [0.354, 0.476], mean action: 43.000 [1.000, 89.000], mean observation: 3.160 [-1.633, 10.327], loss: 1.244820, mae: 5.031775, mean_q: 5.222331
 70506/100000: episode: 7195, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.830, mean reward: 0.383 [0.334, 0.510], mean action: 48.700 [33.000, 51.000], mean observation: 3.149 [-1.589, 10.218], loss: 1.263552, mae: 5.031667, mean_q: 5.221478
 70516/100000: episode: 7196, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.519, mean reward: 0.352 [0.312, 0.432], mean action: 54.700 [11.000, 91.000], mean observation: 3.162 [-1.115, 10.289], loss: 1.287123, mae: 5.031722, mean_q: 5.221307
 70526/100000: episode: 7197, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.905, mean reward: 0.390 [0.387, 0.415], mean action: 45.900 [12.000, 60.000], mean observation: 3.156 [-0.822, 10.354], loss: 1.519194, mae: 5.032436, mean_q: 5.218036
 70536/100000: episode: 7198, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.164, mean reward: 0.416 [0.388, 0.446], mean action: 44.000 [5.000, 97.000], mean observation: 3.162 [-2.509, 10.295], loss: 1.187463, mae: 5.030818, mean_q: 5.216708
 70546/100000: episode: 7199, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.709, mean reward: 0.471 [0.338, 0.512], mean action: 48.300 [8.000, 76.000], mean observation: 3.151 [-1.441, 10.270], loss: 1.407074, mae: 5.031439, mean_q: 5.214134
 70556/100000: episode: 7200, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.573, mean reward: 0.457 [0.414, 0.513], mean action: 55.000 [10.000, 95.000], mean observation: 3.148 [-1.666, 10.303], loss: 1.275776, mae: 5.030578, mean_q: 5.213922
 70566/100000: episode: 7201, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.007, mean reward: 0.401 [0.305, 0.543], mean action: 40.800 [0.000, 65.000], mean observation: 3.154 [-2.217, 10.513], loss: 1.199611, mae: 5.030231, mean_q: 5.211010
 70576/100000: episode: 7202, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.517, mean reward: 0.452 [0.361, 0.504], mean action: 59.100 [32.000, 95.000], mean observation: 3.165 [-1.595, 10.332], loss: 1.212363, mae: 5.030317, mean_q: 5.210095
 70586/100000: episode: 7203, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.018, mean reward: 0.402 [0.350, 0.463], mean action: 65.100 [28.000, 93.000], mean observation: 3.146 [-1.900, 10.303], loss: 1.251146, mae: 5.030253, mean_q: 5.207070
 70596/100000: episode: 7204, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 6.156, mean reward: 0.616 [0.616, 0.616], mean action: 51.800 [3.000, 88.000], mean observation: 3.164 [-1.604, 10.332], loss: 1.415509, mae: 5.030946, mean_q: 5.202001
 70606/100000: episode: 7205, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.028, mean reward: 0.403 [0.326, 0.516], mean action: 39.100 [0.000, 90.000], mean observation: 3.154 [-1.431, 10.368], loss: 0.754570, mae: 5.028382, mean_q: 5.200686
 70616/100000: episode: 7206, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.895, mean reward: 0.389 [0.293, 0.540], mean action: 35.900 [0.000, 92.000], mean observation: 3.161 [-1.628, 10.462], loss: 1.318063, mae: 5.030618, mean_q: 5.203532
 70626/100000: episode: 7207, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.157, mean reward: 0.416 [0.317, 0.480], mean action: 31.400 [0.000, 95.000], mean observation: 3.153 [-1.795, 10.282], loss: 1.508465, mae: 5.031340, mean_q: 5.206916
 70636/100000: episode: 7208, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.733, mean reward: 0.373 [0.304, 0.633], mean action: 50.700 [0.000, 101.000], mean observation: 3.161 [-1.734, 10.340], loss: 1.130721, mae: 5.029315, mean_q: 5.207826
 70646/100000: episode: 7209, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.279, mean reward: 0.428 [0.338, 0.581], mean action: 29.800 [0.000, 95.000], mean observation: 3.160 [-1.427, 10.262], loss: 1.025270, mae: 5.028852, mean_q: 5.209410
 70656/100000: episode: 7210, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 3.876, mean reward: 0.388 [0.275, 0.443], mean action: 11.900 [0.000, 90.000], mean observation: 3.155 [-1.107, 10.370], loss: 1.336288, mae: 5.030059, mean_q: 5.213316
 70666/100000: episode: 7211, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.275, mean reward: 0.428 [0.326, 0.516], mean action: 11.300 [0.000, 43.000], mean observation: 3.148 [-1.459, 10.177], loss: 1.230653, mae: 5.029521, mean_q: 5.215241
 70676/100000: episode: 7212, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.252, mean reward: 0.425 [0.363, 0.488], mean action: 33.500 [0.000, 99.000], mean observation: 3.157 [-1.497, 10.384], loss: 0.895736, mae: 5.028288, mean_q: 5.214324
 70686/100000: episode: 7213, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.186, mean reward: 0.419 [0.347, 0.464], mean action: 56.500 [0.000, 101.000], mean observation: 3.151 [-1.540, 10.102], loss: 1.169593, mae: 5.029534, mean_q: 5.214435
 70696/100000: episode: 7214, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.192, mean reward: 0.419 [0.338, 0.524], mean action: 21.600 [0.000, 93.000], mean observation: 3.152 [-1.850, 10.321], loss: 1.365892, mae: 5.030513, mean_q: 5.213894
 70706/100000: episode: 7215, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.366, mean reward: 0.437 [0.350, 0.588], mean action: 22.400 [0.000, 66.000], mean observation: 3.152 [-2.195, 10.345], loss: 1.064418, mae: 5.029117, mean_q: 5.211124
 70716/100000: episode: 7216, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.936, mean reward: 0.394 [0.293, 0.528], mean action: 13.800 [0.000, 57.000], mean observation: 3.162 [-1.601, 10.299], loss: 1.374666, mae: 5.030099, mean_q: 5.211251
 70723/100000: episode: 7217, duration: 0.123s, episode steps: 7, steps per second: 57, episode reward: 12.566, mean reward: 1.795 [0.359, 10.000], mean action: 38.429 [0.000, 78.000], mean observation: 3.156 [-1.009, 10.395], loss: 1.736243, mae: 5.031165, mean_q: 5.212546
 70733/100000: episode: 7218, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.901, mean reward: 0.390 [0.322, 0.447], mean action: 30.000 [0.000, 90.000], mean observation: 3.158 [-1.158, 10.289], loss: 1.131626, mae: 5.028561, mean_q: 5.215183
 70743/100000: episode: 7219, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.147, mean reward: 0.415 [0.347, 0.490], mean action: 19.100 [0.000, 100.000], mean observation: 3.150 [-1.734, 10.334], loss: 1.309551, mae: 5.029122, mean_q: 5.218003
 70753/100000: episode: 7220, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.231, mean reward: 0.423 [0.388, 0.496], mean action: 29.100 [0.000, 84.000], mean observation: 3.159 [-1.089, 10.496], loss: 1.106668, mae: 5.028426, mean_q: 5.218401
 70763/100000: episode: 7221, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.503, mean reward: 0.450 [0.374, 0.502], mean action: 27.200 [0.000, 88.000], mean observation: 3.150 [-1.282, 10.239], loss: 1.331531, mae: 5.029269, mean_q: 5.212809
 70773/100000: episode: 7222, duration: 0.244s, episode steps: 10, steps per second: 41, episode reward: 4.018, mean reward: 0.402 [0.300, 0.472], mean action: 10.900 [0.000, 99.000], mean observation: 3.164 [-1.226, 10.471], loss: 1.238281, mae: 5.028737, mean_q: 5.209709
 70783/100000: episode: 7223, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.223, mean reward: 0.422 [0.383, 0.495], mean action: 21.500 [0.000, 100.000], mean observation: 3.151 [-2.054, 10.287], loss: 1.390337, mae: 5.029124, mean_q: 5.207456
 70793/100000: episode: 7224, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.236, mean reward: 0.424 [0.345, 0.528], mean action: 44.400 [0.000, 86.000], mean observation: 3.146 [-1.249, 10.277], loss: 1.275753, mae: 5.028403, mean_q: 5.206912
 70803/100000: episode: 7225, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.894, mean reward: 0.389 [0.322, 0.455], mean action: 57.800 [29.000, 99.000], mean observation: 3.170 [-1.735, 10.310], loss: 0.989263, mae: 5.027095, mean_q: 5.209071
 70813/100000: episode: 7226, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.602, mean reward: 0.460 [0.389, 0.508], mean action: 46.100 [7.000, 99.000], mean observation: 3.162 [-1.391, 10.306], loss: 0.952138, mae: 5.026752, mean_q: 5.210753
 70814/100000: episode: 7227, duration: 0.044s, episode steps: 1, steps per second: 23, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 5.000 [5.000, 5.000], mean observation: 3.163 [-0.512, 10.459], loss: 0.441226, mae: 5.025202, mean_q: 5.212049
 70824/100000: episode: 7228, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.019, mean reward: 0.402 [0.291, 0.489], mean action: 54.100 [48.000, 84.000], mean observation: 3.163 [-2.125, 10.239], loss: 1.339259, mae: 5.028531, mean_q: 5.213002
 70834/100000: episode: 7229, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.218, mean reward: 0.422 [0.375, 0.485], mean action: 46.500 [10.000, 65.000], mean observation: 3.159 [-2.422, 10.369], loss: 1.316436, mae: 5.028507, mean_q: 5.214242
 70844/100000: episode: 7230, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.844, mean reward: 0.384 [0.341, 0.448], mean action: 45.800 [11.000, 82.000], mean observation: 3.161 [-1.436, 10.244], loss: 1.169350, mae: 5.027901, mean_q: 5.215745
 70854/100000: episode: 7231, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.618, mean reward: 0.462 [0.451, 0.518], mean action: 55.400 [23.000, 81.000], mean observation: 3.144 [-0.964, 10.308], loss: 1.345816, mae: 5.028599, mean_q: 5.218112
 70864/100000: episode: 7232, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.749, mean reward: 0.375 [0.346, 0.528], mean action: 46.600 [16.000, 50.000], mean observation: 3.143 [-1.799, 10.262], loss: 1.279792, mae: 5.028361, mean_q: 5.220325
 70874/100000: episode: 7233, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.078, mean reward: 0.408 [0.348, 0.507], mean action: 54.000 [22.000, 85.000], mean observation: 3.160 [-0.945, 10.215], loss: 1.170898, mae: 5.027753, mean_q: 5.222063
 70884/100000: episode: 7234, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.906, mean reward: 0.391 [0.368, 0.416], mean action: 57.500 [48.000, 95.000], mean observation: 3.157 [-1.575, 10.314], loss: 1.431668, mae: 5.028710, mean_q: 5.223699
 70891/100000: episode: 7235, duration: 0.133s, episode steps: 7, steps per second: 53, episode reward: 12.481, mean reward: 1.783 [0.361, 10.000], mean action: 37.571 [16.000, 50.000], mean observation: 3.170 [-1.305, 10.224], loss: 1.282320, mae: 5.027833, mean_q: 5.223662
 70901/100000: episode: 7236, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.921, mean reward: 0.392 [0.324, 0.465], mean action: 57.100 [18.000, 96.000], mean observation: 3.141 [-1.198, 10.299], loss: 1.199358, mae: 5.027477, mean_q: 5.221267
 70911/100000: episode: 7237, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.359, mean reward: 0.436 [0.343, 0.538], mean action: 39.000 [2.000, 58.000], mean observation: 3.150 [-1.845, 10.500], loss: 1.111300, mae: 5.026987, mean_q: 5.218331
 70921/100000: episode: 7238, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.635, mean reward: 0.464 [0.387, 0.596], mean action: 45.100 [1.000, 95.000], mean observation: 3.162 [-1.985, 10.265], loss: 1.274837, mae: 5.027423, mean_q: 5.219819
 70931/100000: episode: 7239, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.581, mean reward: 0.458 [0.409, 0.594], mean action: 50.200 [14.000, 88.000], mean observation: 3.158 [-1.035, 10.256], loss: 1.187717, mae: 5.027175, mean_q: 5.223449
 70941/100000: episode: 7240, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.066, mean reward: 0.407 [0.324, 0.491], mean action: 49.200 [2.000, 92.000], mean observation: 3.155 [-1.886, 10.256], loss: 1.175375, mae: 5.027194, mean_q: 5.225788
 70951/100000: episode: 7241, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.123, mean reward: 0.412 [0.333, 0.468], mean action: 47.500 [11.000, 98.000], mean observation: 3.164 [-1.758, 10.302], loss: 1.475391, mae: 5.028240, mean_q: 5.227518
 70961/100000: episode: 7242, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.379, mean reward: 0.438 [0.396, 0.500], mean action: 56.500 [16.000, 89.000], mean observation: 3.158 [-0.888, 10.287], loss: 1.282537, mae: 5.027504, mean_q: 5.228920
 70971/100000: episode: 7243, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.954, mean reward: 0.395 [0.340, 0.581], mean action: 64.100 [50.000, 101.000], mean observation: 3.153 [-1.375, 10.300], loss: 0.957849, mae: 5.026369, mean_q: 5.230089
 70981/100000: episode: 7244, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.007, mean reward: 0.401 [0.354, 0.443], mean action: 53.000 [5.000, 98.000], mean observation: 3.159 [-1.014, 10.184], loss: 1.319051, mae: 5.027797, mean_q: 5.231485
 70991/100000: episode: 7245, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.395, mean reward: 0.439 [0.334, 0.506], mean action: 40.800 [3.000, 87.000], mean observation: 3.168 [-1.970, 10.371], loss: 1.329973, mae: 5.027779, mean_q: 5.232997
 70999/100000: episode: 7246, duration: 0.136s, episode steps: 8, steps per second: 59, episode reward: 12.755, mean reward: 1.594 [0.345, 10.000], mean action: 46.375 [11.000, 101.000], mean observation: 3.171 [-1.536, 10.321], loss: 1.226231, mae: 5.026928, mean_q: 5.234523
 71009/100000: episode: 7247, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 3.513, mean reward: 0.351 [0.308, 0.468], mean action: 38.400 [1.000, 50.000], mean observation: 3.161 [-1.431, 10.217], loss: 1.118546, mae: 5.026650, mean_q: 5.235722
 71019/100000: episode: 7248, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.757, mean reward: 0.376 [0.291, 0.553], mean action: 51.700 [50.000, 67.000], mean observation: 3.154 [-1.215, 10.380], loss: 1.139544, mae: 5.026755, mean_q: 5.231479
 71029/100000: episode: 7249, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.102, mean reward: 0.410 [0.345, 0.546], mean action: 46.200 [9.000, 97.000], mean observation: 3.164 [-1.395, 10.203], loss: 1.179177, mae: 5.026965, mean_q: 5.229476
 71039/100000: episode: 7250, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.153, mean reward: 0.415 [0.284, 0.508], mean action: 45.100 [3.000, 79.000], mean observation: 3.161 [-1.178, 10.322], loss: 1.000487, mae: 5.026364, mean_q: 5.229364
 71049/100000: episode: 7251, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.188, mean reward: 0.419 [0.366, 0.491], mean action: 45.500 [0.000, 93.000], mean observation: 3.150 [-1.457, 10.399], loss: 1.108109, mae: 5.027258, mean_q: 5.231677
 71059/100000: episode: 7252, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.155, mean reward: 0.416 [0.384, 0.484], mean action: 41.900 [17.000, 71.000], mean observation: 3.148 [-2.224, 10.243], loss: 1.084635, mae: 5.027467, mean_q: 5.228209
 71069/100000: episode: 7253, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.567, mean reward: 0.457 [0.350, 0.596], mean action: 51.400 [10.000, 93.000], mean observation: 3.168 [-1.618, 10.241], loss: 1.105113, mae: 5.027902, mean_q: 5.225477
 71079/100000: episode: 7254, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.540, mean reward: 0.454 [0.431, 0.523], mean action: 60.800 [33.000, 93.000], mean observation: 3.166 [-1.136, 10.530], loss: 1.275528, mae: 5.028662, mean_q: 5.222674
 71089/100000: episode: 7255, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.071, mean reward: 0.407 [0.356, 0.449], mean action: 39.000 [10.000, 71.000], mean observation: 3.161 [-1.252, 10.306], loss: 1.316438, mae: 5.028859, mean_q: 5.222177
 71099/100000: episode: 7256, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.918, mean reward: 0.392 [0.372, 0.469], mean action: 45.400 [2.000, 79.000], mean observation: 3.143 [-0.973, 10.277], loss: 1.121478, mae: 5.028213, mean_q: 5.222883
 71109/100000: episode: 7257, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.047, mean reward: 0.405 [0.349, 0.539], mean action: 45.100 [11.000, 67.000], mean observation: 3.159 [-1.091, 10.320], loss: 1.462774, mae: 5.029717, mean_q: 5.224425
 71119/100000: episode: 7258, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.864, mean reward: 0.386 [0.361, 0.517], mean action: 65.600 [50.000, 100.000], mean observation: 3.139 [-1.438, 10.287], loss: 1.114954, mae: 5.028294, mean_q: 5.226072
 71129/100000: episode: 7259, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.889, mean reward: 0.389 [0.309, 0.499], mean action: 43.500 [1.000, 68.000], mean observation: 3.150 [-1.618, 10.177], loss: 1.112895, mae: 5.028638, mean_q: 5.228183
 71139/100000: episode: 7260, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.269, mean reward: 0.427 [0.385, 0.510], mean action: 47.100 [0.000, 70.000], mean observation: 3.163 [-1.803, 10.372], loss: 1.186173, mae: 5.029151, mean_q: 5.230549
 71149/100000: episode: 7261, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.815, mean reward: 0.381 [0.339, 0.478], mean action: 53.100 [2.000, 98.000], mean observation: 3.153 [-1.712, 10.360], loss: 1.239594, mae: 5.029428, mean_q: 5.232158
 71159/100000: episode: 7262, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.318, mean reward: 0.432 [0.429, 0.443], mean action: 37.000 [7.000, 81.000], mean observation: 3.157 [-2.284, 10.449], loss: 0.914298, mae: 5.028107, mean_q: 5.233651
 71169/100000: episode: 7263, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.714, mean reward: 0.371 [0.345, 0.503], mean action: 53.500 [46.000, 93.000], mean observation: 3.151 [-0.869, 10.196], loss: 1.345437, mae: 5.030332, mean_q: 5.232662
 71179/100000: episode: 7264, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.794, mean reward: 0.379 [0.321, 0.501], mean action: 49.800 [7.000, 83.000], mean observation: 3.149 [-1.715, 10.276], loss: 1.404235, mae: 5.030282, mean_q: 5.230591
 71189/100000: episode: 7265, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.000, mean reward: 0.400 [0.326, 0.483], mean action: 53.200 [14.000, 86.000], mean observation: 3.153 [-1.068, 10.276], loss: 1.373292, mae: 5.029871, mean_q: 5.226818
 71195/100000: episode: 7266, duration: 0.106s, episode steps: 6, steps per second: 57, episode reward: 12.123, mean reward: 2.021 [0.414, 10.000], mean action: 50.333 [50.000, 52.000], mean observation: 3.164 [-1.248, 10.291], loss: 1.191492, mae: 5.028827, mean_q: 5.225276
 71205/100000: episode: 7267, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.869, mean reward: 0.387 [0.331, 0.482], mean action: 52.800 [50.000, 70.000], mean observation: 3.150 [-1.072, 10.341], loss: 1.248719, mae: 5.028983, mean_q: 5.225688
 71215/100000: episode: 7268, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.049, mean reward: 0.405 [0.323, 0.486], mean action: 63.400 [50.000, 94.000], mean observation: 3.152 [-1.671, 10.283], loss: 1.293997, mae: 5.029039, mean_q: 5.226922
 71225/100000: episode: 7269, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.096, mean reward: 0.410 [0.364, 0.490], mean action: 40.200 [5.000, 97.000], mean observation: 3.150 [-1.852, 10.300], loss: 1.142168, mae: 5.028421, mean_q: 5.228467
 71235/100000: episode: 7270, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.612, mean reward: 0.361 [0.337, 0.401], mean action: 61.600 [34.000, 100.000], mean observation: 3.159 [-1.499, 10.236], loss: 1.281485, mae: 5.028824, mean_q: 5.229940
 71245/100000: episode: 7271, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.738, mean reward: 0.474 [0.436, 0.508], mean action: 58.300 [27.000, 91.000], mean observation: 3.166 [-0.857, 10.255], loss: 1.262495, mae: 5.028657, mean_q: 5.231836
 71255/100000: episode: 7272, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.338, mean reward: 0.434 [0.388, 0.563], mean action: 43.100 [10.000, 78.000], mean observation: 3.163 [-1.939, 10.292], loss: 1.432065, mae: 5.029376, mean_q: 5.233762
 71265/100000: episode: 7273, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.948, mean reward: 0.395 [0.309, 0.450], mean action: 49.800 [15.000, 78.000], mean observation: 3.165 [-1.142, 10.385], loss: 1.465421, mae: 5.029289, mean_q: 5.233981
 71275/100000: episode: 7274, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.244, mean reward: 0.424 [0.413, 0.488], mean action: 57.500 [25.000, 101.000], mean observation: 3.142 [-1.933, 10.413], loss: 1.124555, mae: 5.027891, mean_q: 5.232407
 71285/100000: episode: 7275, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.103, mean reward: 0.410 [0.348, 0.471], mean action: 56.100 [47.000, 95.000], mean observation: 3.161 [-1.287, 10.311], loss: 1.475499, mae: 5.029239, mean_q: 5.233218
 71295/100000: episode: 7276, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.855, mean reward: 0.385 [0.332, 0.483], mean action: 48.100 [18.000, 88.000], mean observation: 3.152 [-1.384, 10.233], loss: 1.153863, mae: 5.027936, mean_q: 5.233816
 71305/100000: episode: 7277, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.884, mean reward: 0.388 [0.356, 0.446], mean action: 40.800 [9.000, 50.000], mean observation: 3.160 [-1.145, 10.415], loss: 1.484059, mae: 5.029222, mean_q: 5.233700
 71315/100000: episode: 7278, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.705, mean reward: 0.470 [0.353, 0.579], mean action: 48.100 [2.000, 84.000], mean observation: 3.142 [-1.281, 10.299], loss: 1.310870, mae: 5.028359, mean_q: 5.230712
 71325/100000: episode: 7279, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.355, mean reward: 0.436 [0.393, 0.462], mean action: 50.900 [12.000, 95.000], mean observation: 3.154 [-1.831, 10.427], loss: 1.384586, mae: 5.028794, mean_q: 5.228147
 71335/100000: episode: 7280, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.012, mean reward: 0.401 [0.310, 0.543], mean action: 50.900 [9.000, 89.000], mean observation: 3.159 [-1.814, 10.237], loss: 1.285652, mae: 5.028328, mean_q: 5.228883
 71345/100000: episode: 7281, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.669, mean reward: 0.367 [0.315, 0.464], mean action: 51.200 [6.000, 98.000], mean observation: 3.159 [-1.076, 10.390], loss: 1.433396, mae: 5.028848, mean_q: 5.228187
 71355/100000: episode: 7282, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.429, mean reward: 0.343 [0.285, 0.425], mean action: 53.700 [47.000, 77.000], mean observation: 3.162 [-1.271, 10.302], loss: 1.024171, mae: 5.027003, mean_q: 5.229115
 71365/100000: episode: 7283, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.470, mean reward: 0.447 [0.394, 0.504], mean action: 52.700 [17.000, 99.000], mean observation: 3.146 [-1.534, 10.254], loss: 1.395405, mae: 5.028508, mean_q: 5.230243
 71375/100000: episode: 7284, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.896, mean reward: 0.390 [0.357, 0.464], mean action: 58.700 [40.000, 86.000], mean observation: 3.169 [-2.124, 10.235], loss: 1.092662, mae: 5.027319, mean_q: 5.229396
 71385/100000: episode: 7285, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.890, mean reward: 0.389 [0.376, 0.418], mean action: 58.800 [23.000, 89.000], mean observation: 3.170 [-0.949, 10.262], loss: 1.235446, mae: 5.027583, mean_q: 5.227691
 71395/100000: episode: 7286, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.039, mean reward: 0.404 [0.372, 0.462], mean action: 53.800 [15.000, 91.000], mean observation: 3.151 [-0.853, 10.299], loss: 1.146385, mae: 5.027123, mean_q: 5.223746
 71405/100000: episode: 7287, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.902, mean reward: 0.390 [0.349, 0.467], mean action: 51.400 [4.000, 91.000], mean observation: 3.149 [-1.366, 10.426], loss: 1.205293, mae: 5.027441, mean_q: 5.218692
 71415/100000: episode: 7288, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.337, mean reward: 0.434 [0.406, 0.518], mean action: 47.500 [11.000, 96.000], mean observation: 3.168 [-1.484, 10.254], loss: 1.362318, mae: 5.027957, mean_q: 5.219438
 71425/100000: episode: 7289, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.030, mean reward: 0.403 [0.379, 0.443], mean action: 42.200 [2.000, 73.000], mean observation: 3.158 [-1.583, 10.271], loss: 1.201960, mae: 5.027146, mean_q: 5.221107
 71435/100000: episode: 7290, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.106, mean reward: 0.411 [0.375, 0.538], mean action: 59.700 [24.000, 95.000], mean observation: 3.145 [-2.400, 10.352], loss: 1.242845, mae: 5.027062, mean_q: 5.222220
 71445/100000: episode: 7291, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.189, mean reward: 0.419 [0.410, 0.476], mean action: 47.300 [39.000, 50.000], mean observation: 3.177 [-1.042, 10.268], loss: 1.054643, mae: 5.026437, mean_q: 5.220466
 71455/100000: episode: 7292, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.451, mean reward: 0.445 [0.424, 0.511], mean action: 49.000 [4.000, 99.000], mean observation: 3.157 [-1.734, 10.321], loss: 1.199418, mae: 5.026917, mean_q: 5.220181
 71465/100000: episode: 7293, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.064, mean reward: 0.406 [0.382, 0.427], mean action: 56.500 [18.000, 98.000], mean observation: 3.140 [-1.268, 10.469], loss: 1.107582, mae: 5.026651, mean_q: 5.217963
 71475/100000: episode: 7294, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.747, mean reward: 0.375 [0.278, 0.474], mean action: 51.300 [41.000, 71.000], mean observation: 3.156 [-0.917, 10.305], loss: 1.268501, mae: 5.027315, mean_q: 5.217994
 71485/100000: episode: 7295, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 3.710, mean reward: 0.371 [0.242, 0.493], mean action: 42.900 [21.000, 73.000], mean observation: 3.159 [-1.649, 10.402], loss: 0.954219, mae: 5.026212, mean_q: 5.217766
 71495/100000: episode: 7296, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.241, mean reward: 0.424 [0.354, 0.486], mean action: 54.000 [11.000, 100.000], mean observation: 3.146 [-1.487, 10.298], loss: 1.237904, mae: 5.027740, mean_q: 5.217168
 71505/100000: episode: 7297, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.225, mean reward: 0.422 [0.389, 0.493], mean action: 58.300 [25.000, 81.000], mean observation: 3.162 [-1.335, 10.366], loss: 0.937474, mae: 5.026484, mean_q: 5.218368
 71515/100000: episode: 7298, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.858, mean reward: 0.386 [0.340, 0.430], mean action: 48.900 [5.000, 90.000], mean observation: 3.148 [-1.062, 10.514], loss: 1.031679, mae: 5.027102, mean_q: 5.220429
 71525/100000: episode: 7299, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.889, mean reward: 0.389 [0.348, 0.512], mean action: 63.700 [48.000, 96.000], mean observation: 3.159 [-1.029, 10.219], loss: 1.287143, mae: 5.028268, mean_q: 5.224061
 71535/100000: episode: 7300, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.229, mean reward: 0.423 [0.420, 0.431], mean action: 44.300 [7.000, 64.000], mean observation: 3.156 [-1.140, 10.376], loss: 1.066272, mae: 5.027988, mean_q: 5.230431
 71545/100000: episode: 7301, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.518, mean reward: 0.352 [0.313, 0.435], mean action: 44.400 [8.000, 93.000], mean observation: 3.153 [-0.925, 10.293], loss: 1.068959, mae: 5.027949, mean_q: 5.230654
 71555/100000: episode: 7302, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.391, mean reward: 0.439 [0.307, 0.491], mean action: 57.200 [31.000, 88.000], mean observation: 3.153 [-1.568, 10.228], loss: 0.998825, mae: 5.027807, mean_q: 5.229041
 71565/100000: episode: 7303, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.421, mean reward: 0.442 [0.333, 0.520], mean action: 50.100 [0.000, 79.000], mean observation: 3.152 [-2.062, 10.243], loss: 1.069537, mae: 5.028255, mean_q: 5.227791
 71575/100000: episode: 7304, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.121, mean reward: 0.412 [0.327, 0.462], mean action: 51.000 [21.000, 87.000], mean observation: 3.171 [-1.734, 10.204], loss: 1.404503, mae: 5.029515, mean_q: 5.227320
 71585/100000: episode: 7305, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.144, mean reward: 0.414 [0.404, 0.426], mean action: 47.500 [3.000, 84.000], mean observation: 3.155 [-1.092, 10.303], loss: 0.895222, mae: 5.027373, mean_q: 5.228495
 71595/100000: episode: 7306, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.573, mean reward: 0.357 [0.299, 0.499], mean action: 50.200 [2.000, 93.000], mean observation: 3.149 [-1.718, 10.383], loss: 1.415233, mae: 5.029351, mean_q: 5.230420
 71605/100000: episode: 7307, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.996, mean reward: 0.400 [0.357, 0.503], mean action: 40.200 [7.000, 73.000], mean observation: 3.148 [-1.361, 10.307], loss: 1.165694, mae: 5.028571, mean_q: 5.229385
 71615/100000: episode: 7308, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.521, mean reward: 0.352 [0.330, 0.406], mean action: 54.600 [50.000, 83.000], mean observation: 3.166 [-1.024, 10.351], loss: 1.344020, mae: 5.029082, mean_q: 5.225988
 71625/100000: episode: 7309, duration: 0.153s, episode steps: 10, steps per second: 66, episode reward: 3.813, mean reward: 0.381 [0.329, 0.476], mean action: 46.000 [12.000, 79.000], mean observation: 3.161 [-1.545, 10.392], loss: 1.411009, mae: 5.029183, mean_q: 5.222754
 71635/100000: episode: 7310, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.024, mean reward: 0.402 [0.287, 0.526], mean action: 57.100 [50.000, 90.000], mean observation: 3.162 [-1.239, 10.296], loss: 1.566934, mae: 5.029680, mean_q: 5.222899
 71645/100000: episode: 7311, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.344, mean reward: 0.434 [0.374, 0.530], mean action: 49.600 [5.000, 95.000], mean observation: 3.166 [-1.490, 10.408], loss: 1.147749, mae: 5.027924, mean_q: 5.224261
 71655/100000: episode: 7312, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.770, mean reward: 0.477 [0.463, 0.499], mean action: 52.700 [19.000, 98.000], mean observation: 3.161 [-0.970, 10.332], loss: 1.164409, mae: 5.028015, mean_q: 5.226491
 71665/100000: episode: 7313, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.382, mean reward: 0.438 [0.276, 0.469], mean action: 47.700 [1.000, 93.000], mean observation: 3.159 [-1.362, 10.301], loss: 0.833963, mae: 5.027095, mean_q: 5.229501
 71675/100000: episode: 7314, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.325, mean reward: 0.433 [0.392, 0.510], mean action: 52.000 [20.000, 78.000], mean observation: 3.152 [-1.176, 10.441], loss: 1.523592, mae: 5.030007, mean_q: 5.231691
 71685/100000: episode: 7315, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.978, mean reward: 0.398 [0.344, 0.439], mean action: 46.800 [5.000, 85.000], mean observation: 3.158 [-1.370, 10.263], loss: 1.010653, mae: 5.027929, mean_q: 5.230864
 71695/100000: episode: 7316, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.965, mean reward: 0.397 [0.361, 0.495], mean action: 55.100 [14.000, 99.000], mean observation: 3.158 [-1.339, 10.330], loss: 1.311703, mae: 5.029159, mean_q: 5.226069
 71705/100000: episode: 7317, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.826, mean reward: 0.483 [0.388, 0.493], mean action: 50.500 [14.000, 72.000], mean observation: 3.163 [-0.847, 10.198], loss: 1.425024, mae: 5.029582, mean_q: 5.223253
 71715/100000: episode: 7318, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.983, mean reward: 0.398 [0.332, 0.495], mean action: 44.700 [7.000, 77.000], mean observation: 3.163 [-2.152, 10.163], loss: 1.526680, mae: 5.029800, mean_q: 5.217457
 71725/100000: episode: 7319, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.612, mean reward: 0.461 [0.439, 0.482], mean action: 50.800 [27.000, 78.000], mean observation: 3.161 [-1.115, 10.426], loss: 1.111823, mae: 5.028147, mean_q: 5.216056
 71735/100000: episode: 7320, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.930, mean reward: 0.393 [0.350, 0.470], mean action: 54.100 [6.000, 94.000], mean observation: 3.149 [-1.551, 10.277], loss: 1.687504, mae: 5.030487, mean_q: 5.219117
 71745/100000: episode: 7321, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.750, mean reward: 0.375 [0.355, 0.464], mean action: 55.600 [47.000, 87.000], mean observation: 3.151 [-1.340, 10.308], loss: 0.832283, mae: 5.027066, mean_q: 5.223246
 71755/100000: episode: 7322, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.114, mean reward: 0.411 [0.350, 0.445], mean action: 43.900 [20.000, 59.000], mean observation: 3.150 [-1.245, 10.383], loss: 1.114826, mae: 5.028201, mean_q: 5.226470
 71765/100000: episode: 7323, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.869, mean reward: 0.487 [0.395, 0.575], mean action: 54.900 [2.000, 96.000], mean observation: 3.151 [-1.705, 10.231], loss: 1.200997, mae: 5.028651, mean_q: 5.229428
 71775/100000: episode: 7324, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.040, mean reward: 0.404 [0.381, 0.479], mean action: 48.500 [30.000, 62.000], mean observation: 3.171 [-1.712, 10.247], loss: 1.375111, mae: 5.029541, mean_q: 5.227049
 71785/100000: episode: 7325, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.571, mean reward: 0.457 [0.420, 0.562], mean action: 53.200 [3.000, 92.000], mean observation: 3.160 [-1.056, 10.292], loss: 1.337323, mae: 5.029371, mean_q: 5.217826
 71795/100000: episode: 7326, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.790, mean reward: 0.379 [0.321, 0.538], mean action: 49.400 [9.000, 71.000], mean observation: 3.159 [-1.298, 10.173], loss: 1.161672, mae: 5.028487, mean_q: 5.215482
 71805/100000: episode: 7327, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.893, mean reward: 0.389 [0.335, 0.475], mean action: 40.300 [1.000, 92.000], mean observation: 3.144 [-1.484, 10.379], loss: 1.346345, mae: 5.029328, mean_q: 5.215158
 71815/100000: episode: 7328, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.615, mean reward: 0.361 [0.322, 0.423], mean action: 47.000 [10.000, 66.000], mean observation: 3.157 [-1.038, 10.465], loss: 1.516445, mae: 5.029508, mean_q: 5.210891
 71825/100000: episode: 7329, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.343, mean reward: 0.434 [0.403, 0.495], mean action: 74.100 [50.000, 99.000], mean observation: 3.140 [-1.004, 10.360], loss: 1.243083, mae: 5.028526, mean_q: 5.208591
 71835/100000: episode: 7330, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.960, mean reward: 0.396 [0.353, 0.443], mean action: 59.100 [35.000, 92.000], mean observation: 3.151 [-1.213, 10.306], loss: 1.328967, mae: 5.028779, mean_q: 5.208109
 71845/100000: episode: 7331, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.327, mean reward: 0.433 [0.415, 0.476], mean action: 51.300 [37.000, 63.000], mean observation: 3.148 [-1.247, 10.424], loss: 1.590063, mae: 5.029675, mean_q: 5.209649
 71855/100000: episode: 7332, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.474, mean reward: 0.447 [0.366, 0.548], mean action: 50.300 [23.000, 81.000], mean observation: 3.158 [-1.154, 10.334], loss: 1.069771, mae: 5.027436, mean_q: 5.209651
 71865/100000: episode: 7333, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.134, mean reward: 0.413 [0.383, 0.534], mean action: 46.400 [2.000, 73.000], mean observation: 3.173 [-1.353, 10.434], loss: 1.271593, mae: 5.028193, mean_q: 5.210063
 71875/100000: episode: 7334, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.121, mean reward: 0.412 [0.373, 0.493], mean action: 53.300 [11.000, 90.000], mean observation: 3.151 [-1.223, 10.311], loss: 0.815407, mae: 5.026538, mean_q: 5.211497
 71885/100000: episode: 7335, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.806, mean reward: 0.381 [0.300, 0.460], mean action: 43.500 [18.000, 50.000], mean observation: 3.157 [-1.812, 10.290], loss: 0.930109, mae: 5.027316, mean_q: 5.213624
 71895/100000: episode: 7336, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.074, mean reward: 0.407 [0.320, 0.509], mean action: 53.000 [13.000, 88.000], mean observation: 3.163 [-1.446, 10.427], loss: 1.004143, mae: 5.028020, mean_q: 5.215518
 71905/100000: episode: 7337, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.964, mean reward: 0.396 [0.393, 0.413], mean action: 50.000 [50.000, 50.000], mean observation: 3.158 [-1.359, 10.357], loss: 1.346475, mae: 5.029578, mean_q: 5.215440
 71915/100000: episode: 7338, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.905, mean reward: 0.390 [0.324, 0.448], mean action: 40.400 [6.000, 50.000], mean observation: 3.153 [-1.720, 10.461], loss: 0.974934, mae: 5.028278, mean_q: 5.212251
 71925/100000: episode: 7339, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.268, mean reward: 0.427 [0.310, 0.453], mean action: 61.200 [28.000, 97.000], mean observation: 3.160 [-2.198, 10.302], loss: 1.235547, mae: 5.029530, mean_q: 5.212047
 71935/100000: episode: 7340, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.105, mean reward: 0.411 [0.338, 0.455], mean action: 43.400 [4.000, 65.000], mean observation: 3.161 [-1.626, 10.355], loss: 1.166977, mae: 5.029160, mean_q: 5.213023
 71945/100000: episode: 7341, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.898, mean reward: 0.490 [0.418, 0.516], mean action: 47.000 [3.000, 99.000], mean observation: 3.157 [-1.725, 10.303], loss: 1.581439, mae: 5.030679, mean_q: 5.214865
 71955/100000: episode: 7342, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.516, mean reward: 0.452 [0.443, 0.532], mean action: 47.300 [23.000, 50.000], mean observation: 3.154 [-2.990, 10.274], loss: 1.102479, mae: 5.028579, mean_q: 5.216478
 71965/100000: episode: 7343, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.375, mean reward: 0.437 [0.420, 0.503], mean action: 52.100 [22.000, 100.000], mean observation: 3.155 [-1.525, 10.327], loss: 1.241124, mae: 5.028998, mean_q: 5.215989
 71972/100000: episode: 7344, duration: 0.098s, episode steps: 7, steps per second: 71, episode reward: 12.279, mean reward: 1.754 [0.376, 10.000], mean action: 63.143 [26.000, 101.000], mean observation: 3.165 [-1.082, 10.250], loss: 1.249589, mae: 5.029040, mean_q: 5.214736
 71982/100000: episode: 7345, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.832, mean reward: 0.383 [0.316, 0.454], mean action: 50.900 [15.000, 94.000], mean observation: 3.157 [-1.359, 10.221], loss: 1.103858, mae: 5.028791, mean_q: 5.214447
 71992/100000: episode: 7346, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.686, mean reward: 0.369 [0.329, 0.504], mean action: 52.700 [50.000, 75.000], mean observation: 3.156 [-1.132, 10.293], loss: 1.331854, mae: 5.029798, mean_q: 5.214187
 72002/100000: episode: 7347, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.858, mean reward: 0.386 [0.317, 0.521], mean action: 44.900 [5.000, 50.000], mean observation: 3.158 [-1.767, 10.451], loss: 1.575406, mae: 5.030711, mean_q: 5.211660
 72012/100000: episode: 7348, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.135, mean reward: 0.413 [0.339, 0.565], mean action: 47.800 [17.000, 72.000], mean observation: 3.147 [-1.567, 10.348], loss: 0.866807, mae: 5.027925, mean_q: 5.210092
 72022/100000: episode: 7349, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.470, mean reward: 0.447 [0.447, 0.447], mean action: 59.300 [50.000, 90.000], mean observation: 3.153 [-1.735, 10.277], loss: 1.092656, mae: 5.028783, mean_q: 5.208787
 72032/100000: episode: 7350, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.244, mean reward: 0.424 [0.381, 0.536], mean action: 60.500 [37.000, 96.000], mean observation: 3.146 [-0.654, 10.433], loss: 1.229371, mae: 5.029325, mean_q: 5.208288
 72042/100000: episode: 7351, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.789, mean reward: 0.479 [0.400, 0.488], mean action: 45.200 [1.000, 76.000], mean observation: 3.151 [-1.148, 10.329], loss: 0.847319, mae: 5.027983, mean_q: 5.207345
 72052/100000: episode: 7352, duration: 0.153s, episode steps: 10, steps per second: 66, episode reward: 3.901, mean reward: 0.390 [0.329, 0.457], mean action: 50.300 [9.000, 91.000], mean observation: 3.155 [-1.159, 10.320], loss: 1.309197, mae: 5.030069, mean_q: 5.208807
 72062/100000: episode: 7353, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.280, mean reward: 0.428 [0.314, 0.523], mean action: 50.500 [9.000, 97.000], mean observation: 3.154 [-1.114, 10.276], loss: 1.019847, mae: 5.028883, mean_q: 5.210759
 72072/100000: episode: 7354, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.062, mean reward: 0.406 [0.384, 0.517], mean action: 52.100 [42.000, 79.000], mean observation: 3.143 [-1.123, 10.297], loss: 0.892533, mae: 5.028718, mean_q: 5.213063
 72082/100000: episode: 7355, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.615, mean reward: 0.362 [0.285, 0.439], mean action: 46.400 [3.000, 84.000], mean observation: 3.151 [-1.470, 10.288], loss: 1.202136, mae: 5.030145, mean_q: 5.215157
 72092/100000: episode: 7356, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.067, mean reward: 0.407 [0.400, 0.463], mean action: 52.700 [12.000, 93.000], mean observation: 3.164 [-1.946, 10.276], loss: 1.251311, mae: 5.030513, mean_q: 5.215519
 72102/100000: episode: 7357, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.724, mean reward: 0.472 [0.472, 0.472], mean action: 63.100 [38.000, 98.000], mean observation: 3.155 [-1.427, 10.308], loss: 1.379410, mae: 5.031419, mean_q: 5.217258
 72112/100000: episode: 7358, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.982, mean reward: 0.398 [0.341, 0.515], mean action: 49.000 [15.000, 101.000], mean observation: 3.166 [-2.583, 10.286], loss: 1.238094, mae: 5.030886, mean_q: 5.218641
 72122/100000: episode: 7359, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.287, mean reward: 0.429 [0.404, 0.474], mean action: 59.100 [32.000, 101.000], mean observation: 3.140 [-1.533, 10.406], loss: 0.956989, mae: 5.029757, mean_q: 5.216475
 72132/100000: episode: 7360, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.020, mean reward: 0.402 [0.298, 0.540], mean action: 47.100 [21.000, 82.000], mean observation: 3.164 [-1.845, 10.216], loss: 1.415223, mae: 5.031595, mean_q: 5.216550
 72142/100000: episode: 7361, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.092, mean reward: 0.409 [0.359, 0.506], mean action: 47.000 [6.000, 84.000], mean observation: 3.147 [-1.306, 10.297], loss: 1.039297, mae: 5.030310, mean_q: 5.215280
 72152/100000: episode: 7362, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.152, mean reward: 0.415 [0.392, 0.524], mean action: 57.600 [14.000, 98.000], mean observation: 3.148 [-1.989, 10.237], loss: 1.214653, mae: 5.031005, mean_q: 5.215390
 72162/100000: episode: 7363, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.492, mean reward: 0.449 [0.361, 0.492], mean action: 39.600 [1.000, 72.000], mean observation: 3.142 [-1.551, 10.307], loss: 1.310063, mae: 5.031644, mean_q: 5.216171
 72172/100000: episode: 7364, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.655, mean reward: 0.465 [0.464, 0.475], mean action: 51.400 [37.000, 90.000], mean observation: 3.149 [-1.448, 10.333], loss: 1.408278, mae: 5.032073, mean_q: 5.217837
 72182/100000: episode: 7365, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.805, mean reward: 0.481 [0.357, 0.508], mean action: 61.400 [36.000, 101.000], mean observation: 3.157 [-1.352, 10.413], loss: 1.107169, mae: 5.030991, mean_q: 5.219473
 72192/100000: episode: 7366, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.243, mean reward: 0.424 [0.364, 0.516], mean action: 51.000 [5.000, 95.000], mean observation: 3.158 [-0.969, 10.268], loss: 1.075729, mae: 5.030775, mean_q: 5.221191
 72202/100000: episode: 7367, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.067, mean reward: 0.407 [0.397, 0.449], mean action: 48.400 [6.000, 85.000], mean observation: 3.152 [-2.217, 10.209], loss: 0.895900, mae: 5.030397, mean_q: 5.223723
 72212/100000: episode: 7368, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.699, mean reward: 0.470 [0.465, 0.513], mean action: 54.300 [50.000, 88.000], mean observation: 3.142 [-1.605, 10.510], loss: 1.180002, mae: 5.031793, mean_q: 5.224306
 72222/100000: episode: 7369, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.415, mean reward: 0.442 [0.321, 0.511], mean action: 50.200 [1.000, 97.000], mean observation: 3.153 [-1.745, 10.421], loss: 1.101026, mae: 5.031566, mean_q: 5.222669
 72232/100000: episode: 7370, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.930, mean reward: 0.393 [0.308, 0.488], mean action: 34.400 [3.000, 54.000], mean observation: 3.148 [-2.145, 10.201], loss: 1.266835, mae: 5.032188, mean_q: 5.219489
 72242/100000: episode: 7371, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.981, mean reward: 0.398 [0.392, 0.423], mean action: 46.300 [18.000, 76.000], mean observation: 3.162 [-1.702, 10.298], loss: 1.305731, mae: 5.032569, mean_q: 5.219312
 72252/100000: episode: 7372, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.977, mean reward: 0.398 [0.314, 0.503], mean action: 48.100 [16.000, 72.000], mean observation: 3.167 [-2.469, 10.315], loss: 1.006391, mae: 5.031462, mean_q: 5.219814
 72262/100000: episode: 7373, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.896, mean reward: 0.390 [0.363, 0.444], mean action: 58.000 [19.000, 95.000], mean observation: 3.159 [-1.336, 10.422], loss: 1.211801, mae: 5.032308, mean_q: 5.219061
 72272/100000: episode: 7374, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 5.073, mean reward: 0.507 [0.507, 0.507], mean action: 56.200 [5.000, 90.000], mean observation: 3.147 [-1.111, 10.368], loss: 1.028624, mae: 5.031812, mean_q: 5.214698
 72282/100000: episode: 7375, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.864, mean reward: 0.386 [0.326, 0.595], mean action: 51.200 [14.000, 98.000], mean observation: 3.162 [-2.237, 10.317], loss: 1.240827, mae: 5.032934, mean_q: 5.213576
 72292/100000: episode: 7376, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.990, mean reward: 0.399 [0.307, 0.456], mean action: 48.600 [5.000, 95.000], mean observation: 3.148 [-1.437, 10.260], loss: 1.429896, mae: 5.033640, mean_q: 5.215204
 72302/100000: episode: 7377, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.265, mean reward: 0.427 [0.362, 0.501], mean action: 68.100 [45.000, 99.000], mean observation: 3.166 [-1.354, 10.432], loss: 1.787166, mae: 5.034925, mean_q: 5.216417
 72308/100000: episode: 7378, duration: 0.087s, episode steps: 6, steps per second: 69, episode reward: 12.012, mean reward: 2.002 [0.402, 10.000], mean action: 65.000 [50.000, 92.000], mean observation: 3.172 [-1.378, 10.327], loss: 0.902909, mae: 5.031208, mean_q: 5.216056
 72318/100000: episode: 7379, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.774, mean reward: 0.377 [0.354, 0.403], mean action: 50.100 [35.000, 65.000], mean observation: 3.152 [-1.391, 10.423], loss: 1.376923, mae: 5.032938, mean_q: 5.216198
 72328/100000: episode: 7380, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.278, mean reward: 0.428 [0.321, 0.509], mean action: 58.000 [29.000, 98.000], mean observation: 3.168 [-1.738, 10.339], loss: 1.046303, mae: 5.031651, mean_q: 5.214797
 72338/100000: episode: 7381, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.997, mean reward: 0.400 [0.357, 0.517], mean action: 48.200 [6.000, 90.000], mean observation: 3.147 [-1.395, 10.262], loss: 1.268458, mae: 5.032599, mean_q: 5.214967
 72348/100000: episode: 7382, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 3.708, mean reward: 0.371 [0.339, 0.420], mean action: 40.800 [1.000, 50.000], mean observation: 3.148 [-1.604, 10.380], loss: 1.138924, mae: 5.032273, mean_q: 5.212497
 72358/100000: episode: 7383, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.647, mean reward: 0.365 [0.301, 0.403], mean action: 65.900 [28.000, 96.000], mean observation: 3.160 [-1.203, 10.284], loss: 1.143838, mae: 5.032233, mean_q: 5.211737
 72368/100000: episode: 7384, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.318, mean reward: 0.332 [0.315, 0.385], mean action: 78.600 [70.000, 81.000], mean observation: 3.147 [-0.710, 10.300], loss: 1.369940, mae: 5.033162, mean_q: 5.213593
 72378/100000: episode: 7385, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.916, mean reward: 0.392 [0.320, 0.492], mean action: 66.000 [19.000, 97.000], mean observation: 3.155 [-1.031, 10.433], loss: 1.269739, mae: 5.032694, mean_q: 5.216516
 72388/100000: episode: 7386, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.740, mean reward: 0.474 [0.395, 0.511], mean action: 64.100 [0.000, 96.000], mean observation: 3.136 [-1.091, 10.292], loss: 1.126779, mae: 5.032491, mean_q: 5.217959
 72398/100000: episode: 7387, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.784, mean reward: 0.378 [0.309, 0.420], mean action: 61.800 [0.000, 95.000], mean observation: 3.156 [-1.728, 10.317], loss: 1.588229, mae: 5.034358, mean_q: 5.218886
 72408/100000: episode: 7388, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.242, mean reward: 0.424 [0.374, 0.468], mean action: 69.600 [41.000, 81.000], mean observation: 3.153 [-0.931, 10.343], loss: 0.885288, mae: 5.031550, mean_q: 5.220601
 72418/100000: episode: 7389, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.992, mean reward: 0.399 [0.354, 0.448], mean action: 63.900 [35.000, 81.000], mean observation: 3.159 [-1.823, 10.324], loss: 1.040228, mae: 5.032298, mean_q: 5.222911
 72428/100000: episode: 7390, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.384, mean reward: 0.438 [0.420, 0.462], mean action: 63.900 [3.000, 97.000], mean observation: 3.155 [-1.831, 10.288], loss: 1.064493, mae: 5.032643, mean_q: 5.223838
 72438/100000: episode: 7391, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 3.782, mean reward: 0.378 [0.318, 0.503], mean action: 73.600 [4.000, 101.000], mean observation: 3.147 [-1.226, 10.250], loss: 0.959046, mae: 5.032386, mean_q: 5.221154
 72448/100000: episode: 7392, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.520, mean reward: 0.452 [0.343, 0.464], mean action: 67.700 [24.000, 83.000], mean observation: 3.157 [-1.132, 10.272], loss: 1.538487, mae: 5.034721, mean_q: 5.217874
 72453/100000: episode: 7393, duration: 0.083s, episode steps: 5, steps per second: 60, episode reward: 11.342, mean reward: 2.268 [0.335, 10.000], mean action: 71.200 [56.000, 75.000], mean observation: 3.153 [-1.226, 10.193], loss: 1.526273, mae: 5.034478, mean_q: 5.217127
 72463/100000: episode: 7394, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.760, mean reward: 0.376 [0.350, 0.441], mean action: 58.000 [12.000, 98.000], mean observation: 3.157 [-1.692, 10.358], loss: 1.310650, mae: 5.033554, mean_q: 5.217736
 72473/100000: episode: 7395, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.134, mean reward: 0.413 [0.410, 0.443], mean action: 65.300 [36.000, 85.000], mean observation: 3.149 [-1.317, 10.225], loss: 1.296675, mae: 5.033494, mean_q: 5.216983
 72483/100000: episode: 7396, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 3.613, mean reward: 0.361 [0.340, 0.500], mean action: 76.500 [37.000, 101.000], mean observation: 3.156 [-1.889, 10.329], loss: 1.023944, mae: 5.032175, mean_q: 5.212743
 72493/100000: episode: 7397, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.633, mean reward: 0.463 [0.385, 0.538], mean action: 65.900 [7.000, 81.000], mean observation: 3.159 [-1.315, 10.327], loss: 1.207240, mae: 5.032994, mean_q: 5.212583
 72503/100000: episode: 7398, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 5.071, mean reward: 0.507 [0.475, 0.511], mean action: 69.700 [5.000, 93.000], mean observation: 3.148 [-2.047, 10.294], loss: 1.159581, mae: 5.032521, mean_q: 5.214179
 72513/100000: episode: 7399, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.320, mean reward: 0.432 [0.402, 0.452], mean action: 71.500 [3.000, 101.000], mean observation: 3.155 [-1.183, 10.351], loss: 1.248159, mae: 5.033064, mean_q: 5.215141
 72523/100000: episode: 7400, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 5.666, mean reward: 0.567 [0.567, 0.567], mean action: 72.300 [25.000, 87.000], mean observation: 3.138 [-2.036, 10.268], loss: 1.440296, mae: 5.033817, mean_q: 5.216916
 72533/100000: episode: 7401, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.689, mean reward: 0.369 [0.295, 0.451], mean action: 68.600 [1.000, 101.000], mean observation: 3.150 [-0.953, 10.419], loss: 1.266617, mae: 5.032992, mean_q: 5.219338
 72543/100000: episode: 7402, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.559, mean reward: 0.456 [0.355, 0.586], mean action: 30.600 [2.000, 81.000], mean observation: 3.154 [-1.468, 10.283], loss: 1.210036, mae: 5.032510, mean_q: 5.221074
 72553/100000: episode: 7403, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.622, mean reward: 0.362 [0.350, 0.424], mean action: 57.000 [14.000, 81.000], mean observation: 3.144 [-1.516, 10.351], loss: 1.070063, mae: 5.031699, mean_q: 5.216886
 72563/100000: episode: 7404, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.292, mean reward: 0.429 [0.372, 0.519], mean action: 58.900 [0.000, 101.000], mean observation: 3.160 [-0.894, 10.415], loss: 1.182609, mae: 5.031848, mean_q: 5.215324
 72573/100000: episode: 7405, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.131, mean reward: 0.413 [0.334, 0.505], mean action: 61.800 [2.000, 81.000], mean observation: 3.156 [-0.904, 10.423], loss: 1.678101, mae: 5.033794, mean_q: 5.216723
 72583/100000: episode: 7406, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.293, mean reward: 0.429 [0.340, 0.563], mean action: 44.200 [3.000, 78.000], mean observation: 3.154 [-0.990, 10.238], loss: 1.036494, mae: 5.030869, mean_q: 5.214363
 72593/100000: episode: 7407, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.019, mean reward: 0.402 [0.352, 0.452], mean action: 72.400 [1.000, 99.000], mean observation: 3.161 [-1.468, 10.305], loss: 1.444031, mae: 5.032393, mean_q: 5.213361
 72603/100000: episode: 7408, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 5.354, mean reward: 0.535 [0.428, 0.587], mean action: 59.800 [0.000, 99.000], mean observation: 3.159 [-1.351, 10.372], loss: 1.131481, mae: 5.030961, mean_q: 5.215258
 72613/100000: episode: 7409, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.803, mean reward: 0.380 [0.363, 0.466], mean action: 76.800 [15.000, 99.000], mean observation: 3.153 [-1.554, 10.388], loss: 1.176862, mae: 5.030868, mean_q: 5.216604
 72623/100000: episode: 7410, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.460, mean reward: 0.446 [0.385, 0.526], mean action: 72.800 [14.000, 99.000], mean observation: 3.139 [-1.593, 10.224], loss: 1.264643, mae: 5.031238, mean_q: 5.218055
 72633/100000: episode: 7411, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.301, mean reward: 0.430 [0.407, 0.483], mean action: 72.900 [11.000, 101.000], mean observation: 3.164 [-0.950, 10.362], loss: 1.022027, mae: 5.030165, mean_q: 5.219755
 72643/100000: episode: 7412, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.024, mean reward: 0.402 [0.308, 0.480], mean action: 81.100 [38.000, 99.000], mean observation: 3.140 [-1.499, 10.184], loss: 1.410239, mae: 5.031682, mean_q: 5.221013
 72653/100000: episode: 7413, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 4.470, mean reward: 0.447 [0.429, 0.487], mean action: 66.200 [25.000, 99.000], mean observation: 3.167 [-1.010, 10.478], loss: 0.998086, mae: 5.029818, mean_q: 5.221980
 72663/100000: episode: 7414, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.958, mean reward: 0.496 [0.377, 0.564], mean action: 59.800 [5.000, 99.000], mean observation: 3.131 [-1.204, 10.380], loss: 1.238223, mae: 5.030857, mean_q: 5.223336
 72673/100000: episode: 7415, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.225, mean reward: 0.422 [0.402, 0.481], mean action: 70.100 [25.000, 99.000], mean observation: 3.166 [-0.984, 10.297], loss: 1.178673, mae: 5.030723, mean_q: 5.224273
 72683/100000: episode: 7416, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.985, mean reward: 0.398 [0.391, 0.444], mean action: 62.600 [6.000, 99.000], mean observation: 3.154 [-1.315, 10.285], loss: 1.037077, mae: 5.030326, mean_q: 5.225669
 72693/100000: episode: 7417, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 3.986, mean reward: 0.399 [0.367, 0.445], mean action: 88.000 [18.000, 99.000], mean observation: 3.178 [-0.692, 10.354], loss: 0.971616, mae: 5.030334, mean_q: 5.226451
 72695/100000: episode: 7418, duration: 0.050s, episode steps: 2, steps per second: 40, episode reward: 10.333, mean reward: 5.166 [0.333, 10.000], mean action: 58.500 [50.000, 67.000], mean observation: 3.152 [-1.198, 10.287], loss: 1.305355, mae: 5.031751, mean_q: 5.226940
 72705/100000: episode: 7419, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.193, mean reward: 0.419 [0.390, 0.521], mean action: 66.500 [14.000, 99.000], mean observation: 3.156 [-0.978, 10.300], loss: 1.287298, mae: 5.031909, mean_q: 5.227776
 72715/100000: episode: 7420, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.949, mean reward: 0.395 [0.320, 0.435], mean action: 85.000 [20.000, 101.000], mean observation: 3.160 [-1.255, 10.280], loss: 1.162037, mae: 5.031716, mean_q: 5.229033
 72725/100000: episode: 7421, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.077, mean reward: 0.408 [0.363, 0.486], mean action: 56.600 [9.000, 99.000], mean observation: 3.174 [-1.577, 10.283], loss: 1.493876, mae: 5.032894, mean_q: 5.230303
 72735/100000: episode: 7422, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.216, mean reward: 0.422 [0.407, 0.550], mean action: 81.300 [37.000, 99.000], mean observation: 3.158 [-1.591, 10.261], loss: 1.556486, mae: 5.032895, mean_q: 5.232020
 72745/100000: episode: 7423, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.744, mean reward: 0.374 [0.319, 0.559], mean action: 78.100 [11.000, 99.000], mean observation: 3.149 [-1.052, 10.448], loss: 1.069603, mae: 5.030888, mean_q: 5.232942
 72755/100000: episode: 7424, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.153, mean reward: 0.415 [0.405, 0.431], mean action: 80.400 [14.000, 99.000], mean observation: 3.157 [-1.403, 10.320], loss: 1.749276, mae: 5.033354, mean_q: 5.233746
 72765/100000: episode: 7425, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.476, mean reward: 0.448 [0.442, 0.475], mean action: 64.900 [19.000, 99.000], mean observation: 3.151 [-2.088, 10.314], loss: 1.270532, mae: 5.031313, mean_q: 5.234475
 72769/100000: episode: 7426, duration: 0.077s, episode steps: 4, steps per second: 52, episode reward: 11.230, mean reward: 2.808 [0.366, 10.000], mean action: 28.500 [7.000, 79.000], mean observation: 3.163 [-1.454, 10.326], loss: 0.968447, mae: 5.030174, mean_q: 5.235611
 72779/100000: episode: 7427, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.068, mean reward: 0.407 [0.388, 0.470], mean action: 58.400 [9.000, 99.000], mean observation: 3.141 [-1.273, 10.247], loss: 1.326916, mae: 5.031297, mean_q: 5.236872
 72789/100000: episode: 7428, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 4.330, mean reward: 0.433 [0.388, 0.471], mean action: 89.800 [10.000, 99.000], mean observation: 3.160 [-1.759, 10.450], loss: 1.039310, mae: 5.030261, mean_q: 5.238526
 72799/100000: episode: 7429, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 4.234, mean reward: 0.423 [0.423, 0.423], mean action: 83.300 [42.000, 99.000], mean observation: 3.163 [-0.706, 10.345], loss: 1.306153, mae: 5.031362, mean_q: 5.239376
 72809/100000: episode: 7430, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.990, mean reward: 0.399 [0.308, 0.496], mean action: 76.900 [16.000, 99.000], mean observation: 3.160 [-0.999, 10.495], loss: 1.424406, mae: 5.031646, mean_q: 5.239611
 72819/100000: episode: 7431, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.428, mean reward: 0.443 [0.393, 0.511], mean action: 65.800 [6.000, 99.000], mean observation: 3.152 [-1.241, 10.326], loss: 1.342363, mae: 5.031184, mean_q: 5.232506
 72829/100000: episode: 7432, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.235, mean reward: 0.423 [0.304, 0.513], mean action: 58.200 [10.000, 96.000], mean observation: 3.159 [-1.349, 10.509], loss: 1.555968, mae: 5.031641, mean_q: 5.226598
 72839/100000: episode: 7433, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.185, mean reward: 0.418 [0.390, 0.455], mean action: 67.400 [25.000, 92.000], mean observation: 3.169 [-1.576, 10.401], loss: 1.370217, mae: 5.030658, mean_q: 5.223654
 72849/100000: episode: 7434, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.830, mean reward: 0.483 [0.475, 0.555], mean action: 62.600 [37.000, 75.000], mean observation: 3.150 [-1.119, 10.251], loss: 1.233170, mae: 5.029976, mean_q: 5.224796
 72859/100000: episode: 7435, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.074, mean reward: 0.407 [0.337, 0.450], mean action: 66.400 [17.000, 85.000], mean observation: 3.161 [-1.344, 10.293], loss: 1.025786, mae: 5.029250, mean_q: 5.227046
 72869/100000: episode: 7436, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.168, mean reward: 0.417 [0.382, 0.472], mean action: 69.200 [15.000, 99.000], mean observation: 3.166 [-1.187, 10.238], loss: 1.184454, mae: 5.030044, mean_q: 5.227767
 72879/100000: episode: 7437, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 3.863, mean reward: 0.386 [0.386, 0.386], mean action: 95.800 [81.000, 99.000], mean observation: 3.163 [-0.964, 10.321], loss: 1.077027, mae: 5.029846, mean_q: 5.227376
 72889/100000: episode: 7438, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.868, mean reward: 0.387 [0.340, 0.471], mean action: 78.300 [2.000, 99.000], mean observation: 3.158 [-0.761, 10.348], loss: 1.208990, mae: 5.030746, mean_q: 5.227980
 72899/100000: episode: 7439, duration: 0.118s, episode steps: 10, steps per second: 84, episode reward: 4.202, mean reward: 0.420 [0.359, 0.507], mean action: 76.500 [3.000, 99.000], mean observation: 3.151 [-1.380, 10.361], loss: 1.166821, mae: 5.030787, mean_q: 5.228483
 72909/100000: episode: 7440, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.135, mean reward: 0.413 [0.369, 0.450], mean action: 69.400 [26.000, 101.000], mean observation: 3.155 [-1.278, 10.294], loss: 1.315061, mae: 5.031501, mean_q: 5.229312
 72919/100000: episode: 7441, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 3.814, mean reward: 0.381 [0.284, 0.443], mean action: 83.100 [40.000, 99.000], mean observation: 3.165 [-1.102, 10.412], loss: 1.333541, mae: 5.031800, mean_q: 5.230314
 72929/100000: episode: 7442, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 4.099, mean reward: 0.410 [0.401, 0.486], mean action: 80.600 [15.000, 101.000], mean observation: 3.139 [-1.801, 10.309], loss: 1.552140, mae: 5.032325, mean_q: 5.226471
 72939/100000: episode: 7443, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.110, mean reward: 0.411 [0.335, 0.499], mean action: 62.000 [8.000, 99.000], mean observation: 3.159 [-1.162, 10.485], loss: 1.308223, mae: 5.031055, mean_q: 5.219499
 72949/100000: episode: 7444, duration: 0.108s, episode steps: 10, steps per second: 92, episode reward: 4.552, mean reward: 0.455 [0.446, 0.492], mean action: 84.100 [12.000, 99.000], mean observation: 3.154 [-0.967, 10.311], loss: 1.211355, mae: 5.030458, mean_q: 5.218247
 72959/100000: episode: 7445, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.997, mean reward: 0.500 [0.500, 0.500], mean action: 78.500 [44.000, 100.000], mean observation: 3.168 [-1.297, 10.270], loss: 1.453282, mae: 5.031365, mean_q: 5.219840
 72969/100000: episode: 7446, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.078, mean reward: 0.408 [0.372, 0.443], mean action: 68.200 [0.000, 99.000], mean observation: 3.150 [-1.613, 10.517], loss: 1.097265, mae: 5.029893, mean_q: 5.221328
 72979/100000: episode: 7447, duration: 0.096s, episode steps: 10, steps per second: 104, episode reward: 3.831, mean reward: 0.383 [0.380, 0.414], mean action: 97.300 [82.000, 99.000], mean observation: 3.146 [-1.308, 10.295], loss: 1.054361, mae: 5.029645, mean_q: 5.223545
 72989/100000: episode: 7448, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.241, mean reward: 0.424 [0.404, 0.434], mean action: 82.400 [30.000, 99.000], mean observation: 3.161 [-1.717, 10.263], loss: 1.207553, mae: 5.030499, mean_q: 5.225472
 72999/100000: episode: 7449, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.827, mean reward: 0.383 [0.351, 0.464], mean action: 72.900 [17.000, 99.000], mean observation: 3.147 [-1.074, 10.423], loss: 1.349858, mae: 5.031161, mean_q: 5.227172
 73009/100000: episode: 7450, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 3.964, mean reward: 0.396 [0.396, 0.396], mean action: 84.900 [49.000, 99.000], mean observation: 3.144 [-0.841, 10.379], loss: 1.554687, mae: 5.031808, mean_q: 5.228950
 73019/100000: episode: 7451, duration: 0.108s, episode steps: 10, steps per second: 92, episode reward: 4.544, mean reward: 0.454 [0.454, 0.454], mean action: 93.200 [45.000, 101.000], mean observation: 3.166 [-1.647, 10.389], loss: 1.611750, mae: 5.031912, mean_q: 5.231123
 73029/100000: episode: 7452, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 4.179, mean reward: 0.418 [0.355, 0.504], mean action: 72.500 [22.000, 99.000], mean observation: 3.154 [-1.586, 10.395], loss: 1.119926, mae: 5.029727, mean_q: 5.232049
 73039/100000: episode: 7453, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.314, mean reward: 0.431 [0.413, 0.492], mean action: 58.100 [15.000, 99.000], mean observation: 3.160 [-1.429, 10.351], loss: 1.038041, mae: 5.029349, mean_q: 5.233159
 73049/100000: episode: 7454, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 3.951, mean reward: 0.395 [0.336, 0.464], mean action: 72.700 [4.000, 99.000], mean observation: 3.154 [-1.625, 10.455], loss: 1.525960, mae: 5.031251, mean_q: 5.232626
 73059/100000: episode: 7455, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.566, mean reward: 0.457 [0.413, 0.564], mean action: 64.800 [4.000, 99.000], mean observation: 3.161 [-1.008, 10.272], loss: 1.132942, mae: 5.029804, mean_q: 5.228785
 73069/100000: episode: 7456, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.299, mean reward: 0.430 [0.357, 0.461], mean action: 82.600 [5.000, 99.000], mean observation: 3.165 [-0.915, 10.426], loss: 1.415642, mae: 5.030786, mean_q: 5.228734
 73074/100000: episode: 7457, duration: 0.067s, episode steps: 5, steps per second: 75, episode reward: 11.935, mean reward: 2.387 [0.471, 10.000], mean action: 83.000 [57.000, 99.000], mean observation: 3.137 [-1.226, 10.280], loss: 0.996639, mae: 5.029244, mean_q: 5.230210
 73084/100000: episode: 7458, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.251, mean reward: 0.425 [0.416, 0.511], mean action: 83.200 [47.000, 99.000], mean observation: 3.155 [-1.212, 10.205], loss: 1.428718, mae: 5.031005, mean_q: 5.229379
 73094/100000: episode: 7459, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 3.617, mean reward: 0.362 [0.325, 0.393], mean action: 79.100 [8.000, 99.000], mean observation: 3.153 [-1.096, 10.230], loss: 1.379806, mae: 5.030908, mean_q: 5.226933
 73104/100000: episode: 7460, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 3.814, mean reward: 0.381 [0.323, 0.436], mean action: 75.700 [5.000, 99.000], mean observation: 3.151 [-1.632, 10.301], loss: 0.698952, mae: 5.028228, mean_q: 5.226392
 73114/100000: episode: 7461, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.913, mean reward: 0.391 [0.311, 0.472], mean action: 74.700 [9.000, 99.000], mean observation: 3.151 [-1.667, 10.468], loss: 1.266824, mae: 5.030645, mean_q: 5.223788
 73124/100000: episode: 7462, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 4.071, mean reward: 0.407 [0.398, 0.429], mean action: 81.300 [30.000, 99.000], mean observation: 3.158 [-1.756, 10.292], loss: 1.147463, mae: 5.030322, mean_q: 5.225181
 73134/100000: episode: 7463, duration: 0.127s, episode steps: 10, steps per second: 78, episode reward: 3.662, mean reward: 0.366 [0.302, 0.530], mean action: 69.300 [13.000, 99.000], mean observation: 3.150 [-1.455, 10.308], loss: 1.087128, mae: 5.030179, mean_q: 5.225383
 73144/100000: episode: 7464, duration: 0.096s, episode steps: 10, steps per second: 104, episode reward: 3.880, mean reward: 0.388 [0.327, 0.573], mean action: 88.500 [54.000, 99.000], mean observation: 3.173 [-0.807, 10.385], loss: 1.373115, mae: 5.031422, mean_q: 5.222718
 73154/100000: episode: 7465, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.917, mean reward: 0.392 [0.346, 0.430], mean action: 68.400 [20.000, 99.000], mean observation: 3.160 [-1.317, 10.268], loss: 0.974868, mae: 5.029967, mean_q: 5.218522
 73164/100000: episode: 7466, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.484, mean reward: 0.348 [0.329, 0.396], mean action: 59.800 [8.000, 75.000], mean observation: 3.143 [-1.119, 10.223], loss: 1.166785, mae: 5.030786, mean_q: 5.217158
 73174/100000: episode: 7467, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.425, mean reward: 0.443 [0.432, 0.534], mean action: 68.000 [39.000, 99.000], mean observation: 3.135 [-1.793, 10.435], loss: 1.103852, mae: 5.030766, mean_q: 5.217915
 73184/100000: episode: 7468, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 3.335, mean reward: 0.333 [0.326, 0.383], mean action: 73.800 [45.000, 98.000], mean observation: 3.172 [-1.889, 10.374], loss: 1.412033, mae: 5.031774, mean_q: 5.219696
 73194/100000: episode: 7469, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.768, mean reward: 0.377 [0.327, 0.427], mean action: 73.400 [37.000, 86.000], mean observation: 3.161 [-1.649, 10.347], loss: 1.486159, mae: 5.032052, mean_q: 5.220707
 73204/100000: episode: 7470, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.007, mean reward: 0.401 [0.360, 0.465], mean action: 54.800 [1.000, 94.000], mean observation: 3.159 [-0.720, 10.386], loss: 1.675132, mae: 5.032351, mean_q: 5.218968
 73214/100000: episode: 7471, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.373, mean reward: 0.437 [0.356, 0.475], mean action: 57.000 [9.000, 75.000], mean observation: 3.174 [-1.213, 10.281], loss: 1.305312, mae: 5.030177, mean_q: 5.216927
 73224/100000: episode: 7472, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 5.347, mean reward: 0.535 [0.535, 0.535], mean action: 74.300 [62.000, 83.000], mean observation: 3.155 [-1.574, 10.237], loss: 1.437192, mae: 5.030214, mean_q: 5.215671
 73234/100000: episode: 7473, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.568, mean reward: 0.457 [0.306, 0.535], mean action: 71.600 [27.000, 93.000], mean observation: 3.139 [-0.838, 10.304], loss: 1.205779, mae: 5.029130, mean_q: 5.216465
 73244/100000: episode: 7474, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.776, mean reward: 0.378 [0.343, 0.418], mean action: 55.300 [4.000, 85.000], mean observation: 3.155 [-1.038, 10.313], loss: 1.473047, mae: 5.030007, mean_q: 5.218588
 73254/100000: episode: 7475, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.142, mean reward: 0.414 [0.378, 0.456], mean action: 59.500 [17.000, 79.000], mean observation: 3.166 [-1.458, 10.372], loss: 0.981111, mae: 5.027786, mean_q: 5.220579
 73264/100000: episode: 7476, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 4.143, mean reward: 0.414 [0.397, 0.435], mean action: 75.200 [13.000, 99.000], mean observation: 3.145 [-1.025, 10.343], loss: 1.107531, mae: 5.028253, mean_q: 5.222023
 73274/100000: episode: 7477, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.888, mean reward: 0.389 [0.307, 0.494], mean action: 64.600 [25.000, 86.000], mean observation: 3.157 [-1.801, 10.300], loss: 1.142890, mae: 5.028502, mean_q: 5.222699
 73284/100000: episode: 7478, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.477, mean reward: 0.448 [0.408, 0.502], mean action: 60.800 [4.000, 101.000], mean observation: 3.148 [-1.266, 10.294], loss: 1.272293, mae: 5.029154, mean_q: 5.219391
 73294/100000: episode: 7479, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.158, mean reward: 0.416 [0.327, 0.510], mean action: 58.300 [6.000, 99.000], mean observation: 3.153 [-1.609, 10.337], loss: 1.129820, mae: 5.028651, mean_q: 5.218451
 73304/100000: episode: 7480, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.875, mean reward: 0.388 [0.356, 0.485], mean action: 71.000 [40.000, 92.000], mean observation: 3.152 [-1.439, 10.330], loss: 1.159195, mae: 5.028822, mean_q: 5.219014
 73314/100000: episode: 7481, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.628, mean reward: 0.463 [0.393, 0.561], mean action: 31.000 [2.000, 75.000], mean observation: 3.160 [-1.064, 10.446], loss: 1.048570, mae: 5.028454, mean_q: 5.220855
 73324/100000: episode: 7482, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.610, mean reward: 0.361 [0.333, 0.483], mean action: 74.300 [44.000, 99.000], mean observation: 3.152 [-0.726, 10.289], loss: 0.988137, mae: 5.028506, mean_q: 5.222325
 73334/100000: episode: 7483, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.434, mean reward: 0.343 [0.314, 0.374], mean action: 71.000 [46.000, 75.000], mean observation: 3.161 [-1.056, 10.367], loss: 1.020459, mae: 5.028950, mean_q: 5.222960
 73344/100000: episode: 7484, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.733, mean reward: 0.373 [0.364, 0.442], mean action: 57.600 [13.000, 75.000], mean observation: 3.165 [-1.529, 10.304], loss: 1.209093, mae: 5.029754, mean_q: 5.224380
 73354/100000: episode: 7485, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.879, mean reward: 0.388 [0.329, 0.543], mean action: 64.900 [14.000, 101.000], mean observation: 3.144 [-1.312, 10.166], loss: 1.101274, mae: 5.029500, mean_q: 5.224050
 73364/100000: episode: 7486, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.840, mean reward: 0.384 [0.321, 0.555], mean action: 48.500 [12.000, 78.000], mean observation: 3.148 [-1.286, 10.391], loss: 1.269430, mae: 5.030242, mean_q: 5.222082
 73374/100000: episode: 7487, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.252, mean reward: 0.425 [0.333, 0.468], mean action: 60.400 [12.000, 85.000], mean observation: 3.146 [-1.947, 10.312], loss: 1.559464, mae: 5.031315, mean_q: 5.220583
 73384/100000: episode: 7488, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 13.485, mean reward: 1.349 [0.342, 10.000], mean action: 65.500 [23.000, 99.000], mean observation: 3.151 [-1.167, 10.215], loss: 1.483342, mae: 5.030753, mean_q: 5.215079
 73394/100000: episode: 7489, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.207, mean reward: 0.421 [0.318, 0.473], mean action: 68.500 [39.000, 98.000], mean observation: 3.161 [-1.548, 10.298], loss: 1.509953, mae: 5.030662, mean_q: 5.212810
 73404/100000: episode: 7490, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.004, mean reward: 0.400 [0.342, 0.470], mean action: 45.200 [9.000, 75.000], mean observation: 3.153 [-1.611, 10.474], loss: 1.185784, mae: 5.029155, mean_q: 5.212921
 73414/100000: episode: 7491, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.078, mean reward: 0.408 [0.322, 0.493], mean action: 51.900 [5.000, 75.000], mean observation: 3.173 [-1.306, 10.290], loss: 1.238632, mae: 5.029487, mean_q: 5.214881
 73424/100000: episode: 7492, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 5.007, mean reward: 0.501 [0.501, 0.501], mean action: 67.400 [39.000, 80.000], mean observation: 3.153 [-1.483, 10.331], loss: 1.325802, mae: 5.029727, mean_q: 5.212171
 73434/100000: episode: 7493, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.441, mean reward: 0.444 [0.379, 0.512], mean action: 46.300 [0.000, 75.000], mean observation: 3.160 [-1.025, 10.373], loss: 1.160246, mae: 5.029258, mean_q: 5.206391
 73444/100000: episode: 7494, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.880, mean reward: 0.388 [0.344, 0.493], mean action: 54.200 [27.000, 65.000], mean observation: 3.163 [-0.995, 10.391], loss: 1.371933, mae: 5.030037, mean_q: 5.203558
 73454/100000: episode: 7495, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.570, mean reward: 0.357 [0.314, 0.395], mean action: 49.000 [0.000, 98.000], mean observation: 3.156 [-1.734, 10.246], loss: 1.205401, mae: 5.029506, mean_q: 5.202613
 73464/100000: episode: 7496, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.127, mean reward: 0.413 [0.318, 0.590], mean action: 59.500 [21.000, 101.000], mean observation: 3.149 [-1.575, 10.189], loss: 1.162922, mae: 5.029436, mean_q: 5.203297
 73474/100000: episode: 7497, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.217, mean reward: 0.422 [0.358, 0.566], mean action: 55.800 [16.000, 95.000], mean observation: 3.150 [-1.421, 10.317], loss: 1.300881, mae: 5.030147, mean_q: 5.203545
 73484/100000: episode: 7498, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.810, mean reward: 0.381 [0.379, 0.400], mean action: 65.000 [59.000, 100.000], mean observation: 3.168 [-1.748, 10.318], loss: 1.202158, mae: 5.029841, mean_q: 5.201566
 73494/100000: episode: 7499, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.772, mean reward: 0.377 [0.321, 0.417], mean action: 68.100 [53.000, 86.000], mean observation: 3.163 [-1.300, 10.284], loss: 1.360662, mae: 5.030396, mean_q: 5.201791
 73504/100000: episode: 7500, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.203, mean reward: 0.420 [0.420, 0.421], mean action: 41.200 [2.000, 59.000], mean observation: 3.148 [-1.295, 10.256], loss: 1.415808, mae: 5.030303, mean_q: 5.201827
 73514/100000: episode: 7501, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.884, mean reward: 0.388 [0.347, 0.517], mean action: 55.500 [30.000, 69.000], mean observation: 3.147 [-1.696, 10.314], loss: 1.058552, mae: 5.028819, mean_q: 5.196232
 73524/100000: episode: 7502, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.954, mean reward: 0.395 [0.393, 0.415], mean action: 51.500 [11.000, 59.000], mean observation: 3.154 [-1.835, 10.277], loss: 1.457397, mae: 5.030250, mean_q: 5.194146
 73534/100000: episode: 7503, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.162, mean reward: 0.416 [0.346, 0.502], mean action: 58.400 [24.000, 72.000], mean observation: 3.161 [-0.995, 10.354], loss: 0.860485, mae: 5.027725, mean_q: 5.194831
 73544/100000: episode: 7504, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.421, mean reward: 0.342 [0.312, 0.400], mean action: 60.900 [27.000, 95.000], mean observation: 3.156 [-1.246, 10.233], loss: 1.089794, mae: 5.028514, mean_q: 5.196925
 73554/100000: episode: 7505, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.899, mean reward: 0.390 [0.343, 0.477], mean action: 57.300 [8.000, 95.000], mean observation: 3.157 [-1.421, 10.320], loss: 1.128968, mae: 5.028672, mean_q: 5.198840
 73564/100000: episode: 7506, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.882, mean reward: 0.388 [0.362, 0.437], mean action: 63.600 [32.000, 86.000], mean observation: 3.170 [-1.030, 10.306], loss: 1.033716, mae: 5.028489, mean_q: 5.199708
 73574/100000: episode: 7507, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.744, mean reward: 0.374 [0.370, 0.418], mean action: 70.600 [45.000, 98.000], mean observation: 3.148 [-1.372, 10.285], loss: 1.090805, mae: 5.029107, mean_q: 5.196373
 73584/100000: episode: 7508, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 3.974, mean reward: 0.397 [0.359, 0.465], mean action: 72.900 [7.000, 87.000], mean observation: 3.135 [-2.038, 10.225], loss: 1.057887, mae: 5.029236, mean_q: 5.195544
 73594/100000: episode: 7509, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.598, mean reward: 0.460 [0.375, 0.485], mean action: 73.600 [21.000, 98.000], mean observation: 3.164 [-2.474, 10.379], loss: 1.453847, mae: 5.030894, mean_q: 5.196519
 73604/100000: episode: 7510, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.865, mean reward: 0.386 [0.354, 0.417], mean action: 57.300 [19.000, 86.000], mean observation: 3.161 [-1.819, 10.351], loss: 1.031698, mae: 5.028998, mean_q: 5.197317
 73614/100000: episode: 7511, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 3.984, mean reward: 0.398 [0.344, 0.473], mean action: 74.400 [50.000, 92.000], mean observation: 3.149 [-2.621, 10.351], loss: 1.424295, mae: 5.030528, mean_q: 5.198640
 73624/100000: episode: 7512, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.331, mean reward: 0.433 [0.410, 0.524], mean action: 70.200 [27.000, 98.000], mean observation: 3.157 [-0.830, 10.216], loss: 1.332668, mae: 5.029822, mean_q: 5.200102
 73634/100000: episode: 7513, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.927, mean reward: 0.393 [0.358, 0.468], mean action: 66.900 [33.000, 86.000], mean observation: 3.152 [-0.942, 10.413], loss: 1.355053, mae: 5.029530, mean_q: 5.201444
 73644/100000: episode: 7514, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 3.732, mean reward: 0.373 [0.301, 0.456], mean action: 72.200 [21.000, 85.000], mean observation: 3.155 [-1.539, 10.332], loss: 1.699466, mae: 5.030485, mean_q: 5.202407
 73654/100000: episode: 7515, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.296, mean reward: 0.430 [0.377, 0.512], mean action: 75.400 [57.000, 84.000], mean observation: 3.153 [-1.200, 10.281], loss: 1.440009, mae: 5.028856, mean_q: 5.202111
 73664/100000: episode: 7516, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.389, mean reward: 0.439 [0.350, 0.563], mean action: 63.400 [10.000, 90.000], mean observation: 3.151 [-1.183, 10.305], loss: 1.204122, mae: 5.027300, mean_q: 5.197591
 73674/100000: episode: 7517, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.746, mean reward: 0.375 [0.359, 0.410], mean action: 66.400 [10.000, 96.000], mean observation: 3.157 [-1.206, 10.317], loss: 0.998564, mae: 5.026454, mean_q: 5.196486
 73684/100000: episode: 7518, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.405, mean reward: 0.341 [0.321, 0.441], mean action: 80.900 [26.000, 101.000], mean observation: 3.150 [-1.612, 10.140], loss: 1.086776, mae: 5.027072, mean_q: 5.197722
 73694/100000: episode: 7519, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.091, mean reward: 0.409 [0.373, 0.442], mean action: 70.600 [7.000, 101.000], mean observation: 3.148 [-1.394, 10.281], loss: 1.092628, mae: 5.027226, mean_q: 5.200231
 73704/100000: episode: 7520, duration: 0.113s, episode steps: 10, steps per second: 88, episode reward: 4.016, mean reward: 0.402 [0.341, 0.508], mean action: 72.200 [4.000, 101.000], mean observation: 3.160 [-1.023, 10.429], loss: 1.015579, mae: 5.027002, mean_q: 5.202734
 73714/100000: episode: 7521, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.948, mean reward: 0.395 [0.337, 0.438], mean action: 75.700 [11.000, 101.000], mean observation: 3.152 [-1.303, 10.274], loss: 1.096058, mae: 5.027275, mean_q: 5.204309
 73724/100000: episode: 7522, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 4.448, mean reward: 0.445 [0.426, 0.470], mean action: 80.100 [8.000, 101.000], mean observation: 3.141 [-1.566, 10.417], loss: 1.230843, mae: 5.028108, mean_q: 5.205218
 73734/100000: episode: 7523, duration: 0.102s, episode steps: 10, steps per second: 98, episode reward: 3.692, mean reward: 0.369 [0.369, 0.369], mean action: 93.700 [50.000, 101.000], mean observation: 3.153 [-1.390, 10.149], loss: 1.003879, mae: 5.027560, mean_q: 5.206749
 73744/100000: episode: 7524, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 3.735, mean reward: 0.374 [0.347, 0.463], mean action: 69.000 [23.000, 101.000], mean observation: 3.152 [-1.282, 10.206], loss: 1.355300, mae: 5.028909, mean_q: 5.208530
 73754/100000: episode: 7525, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 3.524, mean reward: 0.352 [0.336, 0.421], mean action: 72.600 [21.000, 101.000], mean observation: 3.158 [-1.477, 10.446], loss: 1.308855, mae: 5.028931, mean_q: 5.210342
 73764/100000: episode: 7526, duration: 0.107s, episode steps: 10, steps per second: 93, episode reward: 4.163, mean reward: 0.416 [0.350, 0.482], mean action: 81.100 [6.000, 101.000], mean observation: 3.154 [-1.043, 10.479], loss: 1.238691, mae: 5.028594, mean_q: 5.210453
 73774/100000: episode: 7527, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.131, mean reward: 0.413 [0.396, 0.453], mean action: 66.900 [21.000, 101.000], mean observation: 3.160 [-1.166, 10.211], loss: 1.335412, mae: 5.028780, mean_q: 5.207150
 73784/100000: episode: 7528, duration: 0.093s, episode steps: 10, steps per second: 108, episode reward: 4.535, mean reward: 0.453 [0.453, 0.453], mean action: 92.800 [57.000, 101.000], mean observation: 3.155 [-1.251, 10.236], loss: 1.227581, mae: 5.028433, mean_q: 5.205382
 73794/100000: episode: 7529, duration: 0.088s, episode steps: 10, steps per second: 113, episode reward: 4.481, mean reward: 0.448 [0.412, 0.457], mean action: 96.500 [62.000, 101.000], mean observation: 3.157 [-0.909, 10.232], loss: 0.970158, mae: 5.027500, mean_q: 5.203343
 73804/100000: episode: 7530, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 4.124, mean reward: 0.412 [0.381, 0.462], mean action: 75.100 [4.000, 101.000], mean observation: 3.153 [-0.986, 10.280], loss: 1.426663, mae: 5.029280, mean_q: 5.201207
 73814/100000: episode: 7531, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.583, mean reward: 0.458 [0.405, 0.493], mean action: 74.100 [3.000, 101.000], mean observation: 3.142 [-1.958, 10.198], loss: 1.173451, mae: 5.028345, mean_q: 5.199735
 73824/100000: episode: 7532, duration: 0.113s, episode steps: 10, steps per second: 88, episode reward: 4.455, mean reward: 0.445 [0.373, 0.466], mean action: 84.400 [33.000, 101.000], mean observation: 3.167 [-0.612, 10.335], loss: 1.107451, mae: 5.028119, mean_q: 5.200236
 73834/100000: episode: 7533, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.510, mean reward: 0.451 [0.357, 0.523], mean action: 73.300 [9.000, 101.000], mean observation: 3.156 [-1.655, 10.475], loss: 1.537320, mae: 5.029681, mean_q: 5.201640
 73844/100000: episode: 7534, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.616, mean reward: 0.362 [0.308, 0.504], mean action: 61.300 [13.000, 101.000], mean observation: 3.152 [-1.229, 10.251], loss: 1.311534, mae: 5.028493, mean_q: 5.200009
 73854/100000: episode: 7535, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.888, mean reward: 0.389 [0.369, 0.409], mean action: 79.100 [35.000, 101.000], mean observation: 3.163 [-0.952, 10.360], loss: 1.250483, mae: 5.028186, mean_q: 5.195444
 73864/100000: episode: 7536, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.263, mean reward: 0.426 [0.358, 0.466], mean action: 80.300 [48.000, 101.000], mean observation: 3.140 [-0.953, 10.280], loss: 1.546737, mae: 5.029063, mean_q: 5.191739
 73874/100000: episode: 7537, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.753, mean reward: 0.375 [0.294, 0.455], mean action: 58.400 [27.000, 85.000], mean observation: 3.167 [-1.455, 10.343], loss: 0.934575, mae: 5.026143, mean_q: 5.191332
 73884/100000: episode: 7538, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.785, mean reward: 0.379 [0.333, 0.497], mean action: 54.400 [4.000, 87.000], mean observation: 3.146 [-1.369, 10.312], loss: 1.166920, mae: 5.027015, mean_q: 5.191884
 73894/100000: episode: 7539, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.812, mean reward: 0.381 [0.327, 0.465], mean action: 36.400 [1.000, 77.000], mean observation: 3.155 [-1.162, 10.489], loss: 1.063591, mae: 5.026582, mean_q: 5.191977
 73904/100000: episode: 7540, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.558, mean reward: 0.356 [0.306, 0.437], mean action: 66.800 [13.000, 96.000], mean observation: 3.157 [-1.959, 10.223], loss: 1.382196, mae: 5.028057, mean_q: 5.192348
 73914/100000: episode: 7541, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.273, mean reward: 0.427 [0.297, 0.461], mean action: 59.100 [1.000, 77.000], mean observation: 3.152 [-1.309, 10.375], loss: 0.963204, mae: 5.026329, mean_q: 5.193254
 73924/100000: episode: 7542, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.091, mean reward: 0.409 [0.338, 0.487], mean action: 70.300 [0.000, 99.000], mean observation: 3.142 [-1.385, 10.393], loss: 1.163114, mae: 5.027329, mean_q: 5.194807
 73934/100000: episode: 7543, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.955, mean reward: 0.395 [0.319, 0.472], mean action: 61.200 [14.000, 92.000], mean observation: 3.156 [-1.151, 10.322], loss: 1.334983, mae: 5.027768, mean_q: 5.196245
 73944/100000: episode: 7544, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 4.344, mean reward: 0.434 [0.399, 0.483], mean action: 70.800 [15.000, 77.000], mean observation: 3.174 [-1.947, 10.268], loss: 1.090277, mae: 5.026685, mean_q: 5.197617
 73954/100000: episode: 7545, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.419, mean reward: 0.442 [0.374, 0.500], mean action: 66.600 [10.000, 99.000], mean observation: 3.147 [-1.801, 10.250], loss: 1.238909, mae: 5.027577, mean_q: 5.199849
 73964/100000: episode: 7546, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.158, mean reward: 0.416 [0.364, 0.518], mean action: 72.100 [28.000, 77.000], mean observation: 3.155 [-1.447, 10.306], loss: 1.273179, mae: 5.027377, mean_q: 5.196781
 73974/100000: episode: 7547, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.794, mean reward: 0.379 [0.320, 0.443], mean action: 51.800 [10.000, 89.000], mean observation: 3.153 [-1.881, 10.310], loss: 1.462209, mae: 5.028029, mean_q: 5.194455
 73984/100000: episode: 7548, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.095, mean reward: 0.409 [0.399, 0.434], mean action: 55.300 [11.000, 81.000], mean observation: 3.169 [-0.969, 10.416], loss: 1.285787, mae: 5.026889, mean_q: 5.193930
 73994/100000: episode: 7549, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.581, mean reward: 0.358 [0.321, 0.412], mean action: 49.100 [4.000, 93.000], mean observation: 3.151 [-0.963, 10.300], loss: 1.201119, mae: 5.026311, mean_q: 5.194944
 74004/100000: episode: 7550, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.849, mean reward: 0.385 [0.319, 0.428], mean action: 61.600 [2.000, 94.000], mean observation: 3.142 [-2.470, 10.236], loss: 1.119299, mae: 5.025719, mean_q: 5.195390
 74014/100000: episode: 7551, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.084, mean reward: 0.408 [0.359, 0.440], mean action: 56.900 [4.000, 99.000], mean observation: 3.150 [-1.845, 10.245], loss: 1.568187, mae: 5.027223, mean_q: 5.196315
 74024/100000: episode: 7552, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.996, mean reward: 0.400 [0.316, 0.477], mean action: 41.000 [0.000, 74.000], mean observation: 3.149 [-1.350, 10.339], loss: 1.132586, mae: 5.024993, mean_q: 5.198003
 74034/100000: episode: 7553, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 4.121, mean reward: 0.412 [0.320, 0.469], mean action: 28.400 [21.000, 58.000], mean observation: 3.141 [-2.041, 10.246], loss: 1.600921, mae: 5.026755, mean_q: 5.203734
 74044/100000: episode: 7554, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.926, mean reward: 0.393 [0.326, 0.503], mean action: 36.900 [12.000, 83.000], mean observation: 3.168 [-1.095, 10.300], loss: 1.227719, mae: 5.025167, mean_q: 5.205991
 74054/100000: episode: 7555, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.061, mean reward: 0.406 [0.350, 0.527], mean action: 30.600 [21.000, 69.000], mean observation: 3.156 [-1.314, 10.473], loss: 1.360340, mae: 5.025220, mean_q: 5.207106
 74064/100000: episode: 7556, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.913, mean reward: 0.391 [0.330, 0.454], mean action: 40.400 [3.000, 91.000], mean observation: 3.152 [-1.160, 10.333], loss: 1.119142, mae: 5.023965, mean_q: 5.208001
 74074/100000: episode: 7557, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.160, mean reward: 0.416 [0.394, 0.451], mean action: 34.200 [21.000, 100.000], mean observation: 3.152 [-1.098, 10.285], loss: 1.110366, mae: 5.023767, mean_q: 5.209255
 74084/100000: episode: 7558, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.260, mean reward: 0.426 [0.332, 0.532], mean action: 40.100 [7.000, 100.000], mean observation: 3.155 [-1.829, 10.518], loss: 1.439645, mae: 5.025128, mean_q: 5.210361
 74094/100000: episode: 7559, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.882, mean reward: 0.488 [0.379, 0.548], mean action: 35.700 [13.000, 79.000], mean observation: 3.142 [-1.332, 10.388], loss: 1.199124, mae: 5.024172, mean_q: 5.211264
 74104/100000: episode: 7560, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 4.056, mean reward: 0.406 [0.297, 0.496], mean action: 15.800 [0.000, 21.000], mean observation: 3.153 [-1.833, 10.437], loss: 1.504234, mae: 5.025056, mean_q: 5.212150
 74114/100000: episode: 7561, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.276, mean reward: 0.428 [0.388, 0.451], mean action: 32.500 [14.000, 84.000], mean observation: 3.153 [-1.663, 10.293], loss: 1.548893, mae: 5.025024, mean_q: 5.212436
 74124/100000: episode: 7562, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.008, mean reward: 0.401 [0.314, 0.530], mean action: 28.800 [8.000, 61.000], mean observation: 3.154 [-1.570, 10.434], loss: 1.136541, mae: 5.022943, mean_q: 5.212558
 74134/100000: episode: 7563, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.155, mean reward: 0.415 [0.348, 0.458], mean action: 53.700 [9.000, 98.000], mean observation: 3.157 [-1.397, 10.296], loss: 1.372955, mae: 5.023733, mean_q: 5.213926
 74144/100000: episode: 7564, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.024, mean reward: 0.402 [0.313, 0.465], mean action: 28.800 [8.000, 74.000], mean observation: 3.150 [-1.234, 10.256], loss: 1.181107, mae: 5.022908, mean_q: 5.215549
 74154/100000: episode: 7565, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.234, mean reward: 0.423 [0.353, 0.587], mean action: 39.200 [21.000, 100.000], mean observation: 3.154 [-1.531, 10.384], loss: 1.479952, mae: 5.023725, mean_q: 5.216086
 74164/100000: episode: 7566, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.040, mean reward: 0.404 [0.353, 0.463], mean action: 32.800 [4.000, 90.000], mean observation: 3.165 [-1.054, 10.556], loss: 0.927953, mae: 5.021623, mean_q: 5.212701
 74174/100000: episode: 7567, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.135, mean reward: 0.413 [0.356, 0.433], mean action: 39.900 [4.000, 85.000], mean observation: 3.138 [-1.055, 10.377], loss: 1.074471, mae: 5.022468, mean_q: 5.211959
 74175/100000: episode: 7568, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 80.000 [80.000, 80.000], mean observation: 3.119 [-0.671, 10.100], loss: 0.784559, mae: 5.021264, mean_q: 5.212425
 74185/100000: episode: 7569, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.041, mean reward: 0.404 [0.276, 0.516], mean action: 34.000 [21.000, 87.000], mean observation: 3.154 [-2.028, 10.343], loss: 1.499144, mae: 5.024106, mean_q: 5.213900
 74195/100000: episode: 7570, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.216, mean reward: 0.422 [0.365, 0.495], mean action: 35.400 [10.000, 85.000], mean observation: 3.152 [-1.241, 10.384], loss: 1.405893, mae: 5.023425, mean_q: 5.216854
 74205/100000: episode: 7571, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.207, mean reward: 0.421 [0.308, 0.528], mean action: 34.400 [8.000, 95.000], mean observation: 3.149 [-1.938, 10.408], loss: 1.307870, mae: 5.022783, mean_q: 5.218164
 74215/100000: episode: 7572, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.614, mean reward: 0.361 [0.322, 0.404], mean action: 34.200 [9.000, 71.000], mean observation: 3.154 [-1.377, 10.355], loss: 1.371213, mae: 5.022576, mean_q: 5.219313
 74225/100000: episode: 7573, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.837, mean reward: 0.384 [0.336, 0.431], mean action: 43.400 [21.000, 95.000], mean observation: 3.153 [-1.608, 10.264], loss: 0.871892, mae: 5.020615, mean_q: 5.220448
 74235/100000: episode: 7574, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.297, mean reward: 0.430 [0.387, 0.548], mean action: 40.400 [10.000, 87.000], mean observation: 3.162 [-1.746, 10.424], loss: 1.535257, mae: 5.023345, mean_q: 5.221584
 74245/100000: episode: 7575, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.055, mean reward: 0.405 [0.340, 0.473], mean action: 29.300 [6.000, 53.000], mean observation: 3.152 [-1.355, 10.263], loss: 1.359546, mae: 5.022406, mean_q: 5.223335
 74255/100000: episode: 7576, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.376, mean reward: 0.438 [0.362, 0.534], mean action: 26.700 [18.000, 46.000], mean observation: 3.145 [-1.226, 10.177], loss: 0.824427, mae: 5.020551, mean_q: 5.224998
 74265/100000: episode: 7577, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.351, mean reward: 0.435 [0.414, 0.495], mean action: 32.800 [21.000, 76.000], mean observation: 3.169 [-1.463, 10.254], loss: 1.268698, mae: 5.022597, mean_q: 5.231018
 74275/100000: episode: 7578, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.192, mean reward: 0.419 [0.398, 0.513], mean action: 41.900 [0.000, 88.000], mean observation: 3.162 [-1.461, 10.311], loss: 1.072423, mae: 5.021926, mean_q: 5.230396
 74283/100000: episode: 7579, duration: 0.139s, episode steps: 8, steps per second: 58, episode reward: 12.819, mean reward: 1.602 [0.364, 10.000], mean action: 34.750 [4.000, 92.000], mean observation: 3.169 [-1.198, 10.356], loss: 0.961857, mae: 5.021651, mean_q: 5.229346
 74293/100000: episode: 7580, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 3.894, mean reward: 0.389 [0.333, 0.472], mean action: 23.500 [1.000, 69.000], mean observation: 3.151 [-1.349, 10.433], loss: 1.144222, mae: 5.022549, mean_q: 5.229992
 74303/100000: episode: 7581, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.884, mean reward: 0.488 [0.412, 0.553], mean action: 38.700 [21.000, 88.000], mean observation: 3.164 [-1.263, 10.260], loss: 1.346526, mae: 5.023594, mean_q: 5.231828
 74313/100000: episode: 7582, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.432, mean reward: 0.443 [0.419, 0.523], mean action: 31.900 [10.000, 93.000], mean observation: 3.153 [-1.376, 10.283], loss: 1.032308, mae: 5.022479, mean_q: 5.233777
 74323/100000: episode: 7583, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.073, mean reward: 0.407 [0.347, 0.456], mean action: 34.500 [5.000, 73.000], mean observation: 3.158 [-1.300, 10.424], loss: 1.379452, mae: 5.024019, mean_q: 5.235742
 74333/100000: episode: 7584, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.779, mean reward: 0.478 [0.478, 0.481], mean action: 39.900 [21.000, 95.000], mean observation: 3.160 [-1.657, 10.259], loss: 1.314951, mae: 5.023686, mean_q: 5.233655
 74336/100000: episode: 7585, duration: 0.066s, episode steps: 3, steps per second: 46, episode reward: 10.750, mean reward: 3.583 [0.365, 10.000], mean action: 42.667 [21.000, 78.000], mean observation: 3.165 [-1.950, 10.605], loss: 1.352556, mae: 5.024141, mean_q: 5.234104
 74346/100000: episode: 7586, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.008, mean reward: 0.401 [0.324, 0.490], mean action: 31.500 [21.000, 94.000], mean observation: 3.155 [-1.367, 10.422], loss: 1.354931, mae: 5.023925, mean_q: 5.238068
 74356/100000: episode: 7587, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.121, mean reward: 0.412 [0.386, 0.471], mean action: 29.500 [1.000, 101.000], mean observation: 3.151 [-1.331, 10.362], loss: 1.455160, mae: 5.024154, mean_q: 5.240419
 74366/100000: episode: 7588, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.243, mean reward: 0.424 [0.408, 0.500], mean action: 48.900 [21.000, 91.000], mean observation: 3.161 [-2.549, 10.265], loss: 1.234065, mae: 5.023269, mean_q: 5.237597
 74376/100000: episode: 7589, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.909, mean reward: 0.391 [0.374, 0.447], mean action: 33.900 [21.000, 86.000], mean observation: 3.158 [-1.271, 10.330], loss: 1.090615, mae: 5.022838, mean_q: 5.237099
 74386/100000: episode: 7590, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.882, mean reward: 0.388 [0.329, 0.541], mean action: 34.900 [16.000, 88.000], mean observation: 3.158 [-1.373, 10.373], loss: 1.303512, mae: 5.023888, mean_q: 5.238433
 74396/100000: episode: 7591, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.222, mean reward: 0.422 [0.376, 0.492], mean action: 32.300 [15.000, 88.000], mean observation: 3.165 [-1.442, 10.167], loss: 1.370716, mae: 5.024187, mean_q: 5.236273
 74406/100000: episode: 7592, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.636, mean reward: 0.364 [0.288, 0.410], mean action: 54.600 [16.000, 101.000], mean observation: 3.158 [-1.432, 10.289], loss: 1.151834, mae: 5.023274, mean_q: 5.234639
 74416/100000: episode: 7593, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.986, mean reward: 0.399 [0.328, 0.524], mean action: 23.300 [16.000, 44.000], mean observation: 3.155 [-1.537, 10.399], loss: 1.489967, mae: 5.024433, mean_q: 5.230433
 74426/100000: episode: 7594, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.875, mean reward: 0.388 [0.369, 0.446], mean action: 34.600 [4.000, 68.000], mean observation: 3.162 [-1.081, 10.438], loss: 1.294685, mae: 5.023644, mean_q: 5.225116
 74436/100000: episode: 7595, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.833, mean reward: 0.383 [0.292, 0.453], mean action: 35.500 [1.000, 80.000], mean observation: 3.156 [-1.610, 10.270], loss: 1.323146, mae: 5.023758, mean_q: 5.225110
 74446/100000: episode: 7596, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.147, mean reward: 0.415 [0.299, 0.507], mean action: 27.700 [8.000, 55.000], mean observation: 3.165 [-1.393, 10.353], loss: 1.342064, mae: 5.023702, mean_q: 5.226932
 74456/100000: episode: 7597, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 3.901, mean reward: 0.390 [0.353, 0.465], mean action: 30.700 [17.000, 83.000], mean observation: 3.151 [-1.633, 10.300], loss: 1.358875, mae: 5.023794, mean_q: 5.227542
 74466/100000: episode: 7598, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.317, mean reward: 0.432 [0.382, 0.480], mean action: 32.700 [7.000, 84.000], mean observation: 3.153 [-1.854, 10.387], loss: 1.450617, mae: 5.024215, mean_q: 5.224775
 74476/100000: episode: 7599, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.700, mean reward: 0.370 [0.332, 0.432], mean action: 46.000 [8.000, 101.000], mean observation: 3.151 [-1.525, 10.354], loss: 1.189205, mae: 5.023281, mean_q: 5.221373
 74486/100000: episode: 7600, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.006, mean reward: 0.401 [0.347, 0.510], mean action: 37.500 [21.000, 80.000], mean observation: 3.163 [-1.984, 10.570], loss: 1.101507, mae: 5.022855, mean_q: 5.218667
 74496/100000: episode: 7601, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.117, mean reward: 0.412 [0.386, 0.484], mean action: 26.900 [12.000, 55.000], mean observation: 3.153 [-1.500, 10.362], loss: 1.476905, mae: 5.024190, mean_q: 5.214760
 74506/100000: episode: 7602, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.092, mean reward: 0.409 [0.394, 0.473], mean action: 30.100 [5.000, 89.000], mean observation: 3.162 [-1.109, 10.253], loss: 1.475743, mae: 5.023847, mean_q: 5.212221
 74509/100000: episode: 7603, duration: 0.064s, episode steps: 3, steps per second: 47, episode reward: 10.694, mean reward: 3.565 [0.337, 10.000], mean action: 21.000 [21.000, 21.000], mean observation: 3.165 [-0.899, 10.133], loss: 0.813401, mae: 5.020993, mean_q: 5.211736
 74519/100000: episode: 7604, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.990, mean reward: 0.399 [0.295, 0.481], mean action: 47.800 [21.000, 101.000], mean observation: 3.151 [-1.661, 10.315], loss: 1.050351, mae: 5.021842, mean_q: 5.212070
 74529/100000: episode: 7605, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.134, mean reward: 0.413 [0.342, 0.536], mean action: 36.500 [6.000, 83.000], mean observation: 3.137 [-1.599, 10.457], loss: 1.402879, mae: 5.023221, mean_q: 5.214171
 74539/100000: episode: 7606, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.135, mean reward: 0.414 [0.295, 0.540], mean action: 42.600 [21.000, 96.000], mean observation: 3.160 [-1.629, 10.304], loss: 1.441599, mae: 5.023465, mean_q: 5.220155
 74549/100000: episode: 7607, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.255, mean reward: 0.425 [0.347, 0.562], mean action: 17.700 [0.000, 40.000], mean observation: 3.155 [-2.008, 10.346], loss: 1.149794, mae: 5.022412, mean_q: 5.225037
 74559/100000: episode: 7608, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.152, mean reward: 0.415 [0.353, 0.546], mean action: 34.000 [21.000, 93.000], mean observation: 3.165 [-1.590, 10.234], loss: 1.298966, mae: 5.023080, mean_q: 5.225174
 74569/100000: episode: 7609, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.372, mean reward: 0.437 [0.370, 0.522], mean action: 34.100 [9.000, 83.000], mean observation: 3.158 [-1.066, 10.413], loss: 1.256047, mae: 5.023101, mean_q: 5.223159
 74579/100000: episode: 7610, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.126, mean reward: 0.413 [0.409, 0.444], mean action: 47.200 [21.000, 100.000], mean observation: 3.161 [-1.505, 10.367], loss: 1.483721, mae: 5.023938, mean_q: 5.220619
 74589/100000: episode: 7611, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.068, mean reward: 0.407 [0.277, 0.522], mean action: 30.000 [8.000, 74.000], mean observation: 3.156 [-2.636, 10.296], loss: 1.280455, mae: 5.022662, mean_q: 5.216679
 74599/100000: episode: 7612, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.048, mean reward: 0.405 [0.311, 0.553], mean action: 32.900 [3.000, 73.000], mean observation: 3.161 [-1.559, 10.345], loss: 1.426262, mae: 5.023034, mean_q: 5.215120
 74609/100000: episode: 7613, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.714, mean reward: 0.371 [0.336, 0.447], mean action: 33.500 [3.000, 91.000], mean observation: 3.157 [-0.972, 10.315], loss: 1.057261, mae: 5.021103, mean_q: 5.216083
 74619/100000: episode: 7614, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.990, mean reward: 0.399 [0.336, 0.514], mean action: 36.400 [21.000, 96.000], mean observation: 3.162 [-1.644, 10.351], loss: 1.018924, mae: 5.021072, mean_q: 5.218116
 74629/100000: episode: 7615, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.849, mean reward: 0.385 [0.318, 0.438], mean action: 37.800 [0.000, 89.000], mean observation: 3.159 [-0.920, 10.362], loss: 1.389544, mae: 5.022996, mean_q: 5.219769
 74639/100000: episode: 7616, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.168, mean reward: 0.417 [0.392, 0.490], mean action: 29.700 [21.000, 92.000], mean observation: 3.159 [-2.438, 10.297], loss: 1.107307, mae: 5.021960, mean_q: 5.221290
 74649/100000: episode: 7617, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.892, mean reward: 0.389 [0.332, 0.499], mean action: 40.000 [0.000, 90.000], mean observation: 3.149 [-1.463, 10.379], loss: 1.402974, mae: 5.023181, mean_q: 5.223955
 74659/100000: episode: 7618, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.289, mean reward: 0.429 [0.324, 0.530], mean action: 35.600 [4.000, 79.000], mean observation: 3.169 [-1.397, 10.270], loss: 1.162773, mae: 5.022013, mean_q: 5.226583
 74669/100000: episode: 7619, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.179, mean reward: 0.418 [0.375, 0.452], mean action: 26.600 [1.000, 79.000], mean observation: 3.156 [-1.224, 10.298], loss: 1.320273, mae: 5.022511, mean_q: 5.229153
 74679/100000: episode: 7620, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.319, mean reward: 0.432 [0.301, 0.574], mean action: 33.900 [0.000, 100.000], mean observation: 3.166 [-2.194, 10.645], loss: 1.094294, mae: 5.021461, mean_q: 5.231663
 74689/100000: episode: 7621, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.321, mean reward: 0.432 [0.378, 0.485], mean action: 36.400 [0.000, 94.000], mean observation: 3.161 [-1.411, 10.337], loss: 1.254893, mae: 5.022109, mean_q: 5.233569
 74699/100000: episode: 7622, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.048, mean reward: 0.405 [0.334, 0.538], mean action: 29.900 [7.000, 79.000], mean observation: 3.166 [-1.096, 10.400], loss: 0.906436, mae: 5.020787, mean_q: 5.235522
 74709/100000: episode: 7623, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.872, mean reward: 0.387 [0.355, 0.419], mean action: 40.300 [21.000, 99.000], mean observation: 3.156 [-1.299, 10.367], loss: 1.053786, mae: 5.021964, mean_q: 5.236293
 74719/100000: episode: 7624, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.757, mean reward: 0.376 [0.326, 0.430], mean action: 29.200 [21.000, 85.000], mean observation: 3.157 [-1.283, 10.333], loss: 1.236540, mae: 5.022717, mean_q: 5.232320
 74729/100000: episode: 7625, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.928, mean reward: 0.393 [0.315, 0.444], mean action: 44.600 [20.000, 94.000], mean observation: 3.147 [-2.199, 10.335], loss: 1.173008, mae: 5.022376, mean_q: 5.228197
 74739/100000: episode: 7626, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.242, mean reward: 0.424 [0.353, 0.516], mean action: 27.100 [3.000, 63.000], mean observation: 3.152 [-1.845, 10.342], loss: 1.346352, mae: 5.023047, mean_q: 5.227845
 74749/100000: episode: 7627, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.036, mean reward: 0.404 [0.301, 0.501], mean action: 38.900 [4.000, 72.000], mean observation: 3.155 [-1.764, 10.418], loss: 1.375473, mae: 5.023223, mean_q: 5.225623
 74759/100000: episode: 7628, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.344, mean reward: 0.434 [0.347, 0.566], mean action: 34.100 [3.000, 74.000], mean observation: 3.155 [-1.339, 10.182], loss: 1.215395, mae: 5.022101, mean_q: 5.214988
 74769/100000: episode: 7629, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.254, mean reward: 0.425 [0.355, 0.470], mean action: 52.400 [9.000, 84.000], mean observation: 3.160 [-0.988, 10.327], loss: 1.232582, mae: 5.021936, mean_q: 5.206931
 74779/100000: episode: 7630, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.720, mean reward: 0.472 [0.469, 0.503], mean action: 47.500 [21.000, 99.000], mean observation: 3.147 [-1.111, 10.395], loss: 1.027690, mae: 5.021436, mean_q: 5.208217
 74789/100000: episode: 7631, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.829, mean reward: 0.383 [0.307, 0.448], mean action: 36.500 [17.000, 81.000], mean observation: 3.151 [-1.510, 10.287], loss: 1.414147, mae: 5.023193, mean_q: 5.211066
 74799/100000: episode: 7632, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.425, mean reward: 0.443 [0.339, 0.543], mean action: 42.100 [21.000, 88.000], mean observation: 3.168 [-1.769, 10.209], loss: 1.075450, mae: 5.021958, mean_q: 5.213480
 74809/100000: episode: 7633, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.449, mean reward: 0.445 [0.298, 0.586], mean action: 21.000 [11.000, 31.000], mean observation: 3.154 [-1.422, 10.333], loss: 1.379715, mae: 5.023407, mean_q: 5.215698
 74813/100000: episode: 7634, duration: 0.070s, episode steps: 4, steps per second: 57, episode reward: 11.019, mean reward: 2.755 [0.313, 10.000], mean action: 45.750 [1.000, 99.000], mean observation: 3.142 [-1.121, 10.229], loss: 1.390255, mae: 5.023555, mean_q: 5.217155
 74823/100000: episode: 7635, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.086, mean reward: 0.409 [0.369, 0.484], mean action: 28.000 [13.000, 81.000], mean observation: 3.150 [-1.174, 10.354], loss: 1.235195, mae: 5.022996, mean_q: 5.218432
 74833/100000: episode: 7636, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.105, mean reward: 0.411 [0.303, 0.522], mean action: 30.300 [21.000, 61.000], mean observation: 3.150 [-1.772, 10.366], loss: 1.076019, mae: 5.022405, mean_q: 5.220636
 74843/100000: episode: 7637, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.202, mean reward: 0.420 [0.402, 0.473], mean action: 44.500 [21.000, 83.000], mean observation: 3.154 [-1.081, 10.399], loss: 1.102693, mae: 5.022698, mean_q: 5.219837
 74853/100000: episode: 7638, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.211, mean reward: 0.421 [0.339, 0.478], mean action: 56.500 [9.000, 98.000], mean observation: 3.165 [-1.537, 10.446], loss: 1.145464, mae: 5.022939, mean_q: 5.218246
 74863/100000: episode: 7639, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.135, mean reward: 0.413 [0.369, 0.574], mean action: 71.100 [6.000, 98.000], mean observation: 3.157 [-2.158, 10.334], loss: 1.028867, mae: 5.022700, mean_q: 5.219274
 74873/100000: episode: 7640, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.892, mean reward: 0.489 [0.457, 0.510], mean action: 70.200 [10.000, 98.000], mean observation: 3.137 [-1.078, 10.359], loss: 1.536881, mae: 5.024827, mean_q: 5.219689
 74883/100000: episode: 7641, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.804, mean reward: 0.380 [0.335, 0.459], mean action: 86.000 [43.000, 98.000], mean observation: 3.168 [-1.178, 10.350], loss: 0.998045, mae: 5.022539, mean_q: 5.220325
 74893/100000: episode: 7642, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.719, mean reward: 0.372 [0.344, 0.419], mean action: 64.000 [8.000, 98.000], mean observation: 3.172 [-1.540, 10.378], loss: 1.255089, mae: 5.023622, mean_q: 5.219508
 74903/100000: episode: 7643, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.248, mean reward: 0.425 [0.371, 0.445], mean action: 79.600 [4.000, 98.000], mean observation: 3.168 [-1.503, 10.336], loss: 1.242276, mae: 5.023794, mean_q: 5.214869
 74913/100000: episode: 7644, duration: 0.120s, episode steps: 10, steps per second: 84, episode reward: 3.938, mean reward: 0.394 [0.346, 0.472], mean action: 80.200 [13.000, 98.000], mean observation: 3.150 [-1.452, 10.342], loss: 1.175628, mae: 5.023470, mean_q: 5.213914
 74923/100000: episode: 7645, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 4.185, mean reward: 0.419 [0.339, 0.542], mean action: 81.900 [4.000, 98.000], mean observation: 3.154 [-1.157, 10.380], loss: 1.216132, mae: 5.023637, mean_q: 5.212293
 74933/100000: episode: 7646, duration: 0.102s, episode steps: 10, steps per second: 98, episode reward: 3.892, mean reward: 0.389 [0.382, 0.418], mean action: 85.600 [51.000, 100.000], mean observation: 3.164 [-0.998, 10.267], loss: 1.334633, mae: 5.024056, mean_q: 5.207398
 74943/100000: episode: 7647, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.419, mean reward: 0.442 [0.426, 0.498], mean action: 64.100 [15.000, 86.000], mean observation: 3.157 [-1.888, 10.366], loss: 1.071128, mae: 5.022901, mean_q: 5.208084
 74953/100000: episode: 7648, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.956, mean reward: 0.396 [0.336, 0.493], mean action: 47.100 [5.000, 77.000], mean observation: 3.157 [-1.020, 10.386], loss: 1.123113, mae: 5.023458, mean_q: 5.208861
 74963/100000: episode: 7649, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.085, mean reward: 0.408 [0.377, 0.464], mean action: 73.800 [18.000, 100.000], mean observation: 3.155 [-1.054, 10.297], loss: 1.227321, mae: 5.023999, mean_q: 5.209880
 74973/100000: episode: 7650, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.082, mean reward: 0.408 [0.367, 0.469], mean action: 63.200 [22.000, 96.000], mean observation: 3.170 [-1.618, 10.237], loss: 1.189757, mae: 5.023825, mean_q: 5.209713
 74983/100000: episode: 7651, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.626, mean reward: 0.363 [0.329, 0.400], mean action: 57.300 [8.000, 88.000], mean observation: 3.163 [-1.348, 10.292], loss: 1.134931, mae: 5.023559, mean_q: 5.205974
 74993/100000: episode: 7652, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 3.854, mean reward: 0.385 [0.318, 0.460], mean action: 71.100 [10.000, 98.000], mean observation: 3.133 [-1.257, 10.223], loss: 1.594633, mae: 5.025149, mean_q: 5.205436
 75003/100000: episode: 7653, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.919, mean reward: 0.392 [0.340, 0.430], mean action: 44.500 [0.000, 93.000], mean observation: 3.145 [-1.720, 10.323], loss: 1.028562, mae: 5.022846, mean_q: 5.206099
 75013/100000: episode: 7654, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.962, mean reward: 0.396 [0.369, 0.467], mean action: 67.700 [21.000, 78.000], mean observation: 3.162 [-1.485, 10.345], loss: 1.396995, mae: 5.024113, mean_q: 5.206777
 75023/100000: episode: 7655, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.419, mean reward: 0.442 [0.413, 0.529], mean action: 64.300 [9.000, 100.000], mean observation: 3.159 [-1.601, 10.304], loss: 1.106344, mae: 5.022764, mean_q: 5.205436
 75033/100000: episode: 7656, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.954, mean reward: 0.395 [0.303, 0.419], mean action: 66.200 [14.000, 95.000], mean observation: 3.154 [-1.779, 10.339], loss: 1.241502, mae: 5.023377, mean_q: 5.203809
 75043/100000: episode: 7657, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 3.902, mean reward: 0.390 [0.328, 0.490], mean action: 66.000 [8.000, 100.000], mean observation: 3.154 [-1.293, 10.441], loss: 1.174271, mae: 5.023130, mean_q: 5.201971
 75053/100000: episode: 7658, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.815, mean reward: 0.482 [0.433, 0.530], mean action: 63.300 [26.000, 101.000], mean observation: 3.173 [-1.003, 10.253], loss: 1.412666, mae: 5.024053, mean_q: 5.196052
 75063/100000: episode: 7659, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.147, mean reward: 0.415 [0.349, 0.489], mean action: 45.600 [0.000, 97.000], mean observation: 3.156 [-0.776, 10.271], loss: 1.187142, mae: 5.022619, mean_q: 5.194227
 75066/100000: episode: 7660, duration: 0.091s, episode steps: 3, steps per second: 33, episode reward: 10.770, mean reward: 3.590 [0.322, 10.000], mean action: 1.667 [0.000, 5.000], mean observation: 3.162 [-1.563, 10.682], loss: 0.906322, mae: 5.021431, mean_q: 5.195090
 75076/100000: episode: 7661, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.031, mean reward: 0.403 [0.307, 0.508], mean action: 17.800 [0.000, 63.000], mean observation: 3.153 [-1.531, 10.329], loss: 1.557640, mae: 5.023719, mean_q: 5.195947
 75086/100000: episode: 7662, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.318, mean reward: 0.432 [0.298, 0.572], mean action: 36.800 [0.000, 95.000], mean observation: 3.162 [-1.011, 10.425], loss: 1.182498, mae: 5.021626, mean_q: 5.193780
 75096/100000: episode: 7663, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.348, mean reward: 0.435 [0.379, 0.476], mean action: 57.700 [6.000, 71.000], mean observation: 3.155 [-1.492, 10.360], loss: 1.203024, mae: 5.021402, mean_q: 5.192590
 75106/100000: episode: 7664, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.923, mean reward: 0.392 [0.391, 0.401], mean action: 58.200 [27.000, 87.000], mean observation: 3.167 [-2.265, 10.372], loss: 1.478677, mae: 5.022246, mean_q: 5.193446
 75116/100000: episode: 7665, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.587, mean reward: 0.459 [0.375, 0.498], mean action: 44.700 [5.000, 97.000], mean observation: 3.147 [-1.120, 10.335], loss: 1.344632, mae: 5.021490, mean_q: 5.194149
 75126/100000: episode: 7666, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.911, mean reward: 0.391 [0.387, 0.401], mean action: 56.600 [11.000, 87.000], mean observation: 3.149 [-1.121, 10.399], loss: 1.086128, mae: 5.019974, mean_q: 5.194581
 75136/100000: episode: 7667, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.263, mean reward: 0.426 [0.338, 0.536], mean action: 40.100 [2.000, 70.000], mean observation: 3.158 [-1.474, 10.276], loss: 1.332875, mae: 5.020895, mean_q: 5.195186
 75146/100000: episode: 7668, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 5.018, mean reward: 0.502 [0.325, 0.546], mean action: 51.900 [0.000, 99.000], mean observation: 3.174 [-1.446, 10.401], loss: 1.045675, mae: 5.019623, mean_q: 5.196442
 75156/100000: episode: 7669, duration: 0.225s, episode steps: 10, steps per second: 44, episode reward: 4.059, mean reward: 0.406 [0.309, 0.487], mean action: 7.700 [0.000, 59.000], mean observation: 3.149 [-1.408, 10.170], loss: 1.166759, mae: 5.020048, mean_q: 5.199929
 75166/100000: episode: 7670, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.297, mean reward: 0.430 [0.379, 0.527], mean action: 28.100 [0.000, 98.000], mean observation: 3.165 [-0.832, 10.378], loss: 1.318529, mae: 5.020693, mean_q: 5.201651
 75176/100000: episode: 7671, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.770, mean reward: 0.377 [0.286, 0.510], mean action: 31.000 [0.000, 99.000], mean observation: 3.155 [-1.392, 10.411], loss: 1.416129, mae: 5.020907, mean_q: 5.200667
 75186/100000: episode: 7672, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.709, mean reward: 0.371 [0.338, 0.418], mean action: 56.200 [0.000, 94.000], mean observation: 3.144 [-1.548, 10.513], loss: 1.223255, mae: 5.020165, mean_q: 5.198412
 75196/100000: episode: 7673, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.817, mean reward: 0.382 [0.346, 0.458], mean action: 60.000 [12.000, 101.000], mean observation: 3.143 [-1.362, 10.269], loss: 1.443176, mae: 5.020646, mean_q: 5.198395
 75206/100000: episode: 7674, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.084, mean reward: 0.408 [0.371, 0.455], mean action: 61.800 [8.000, 99.000], mean observation: 3.153 [-1.671, 10.498], loss: 1.202213, mae: 5.019412, mean_q: 5.197912
 75216/100000: episode: 7675, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.652, mean reward: 0.365 [0.283, 0.482], mean action: 58.500 [24.000, 70.000], mean observation: 3.163 [-1.241, 10.297], loss: 1.098201, mae: 5.018966, mean_q: 5.194697
 75226/100000: episode: 7676, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.915, mean reward: 0.391 [0.359, 0.474], mean action: 52.600 [11.000, 91.000], mean observation: 3.148 [-1.384, 10.232], loss: 0.864171, mae: 5.018193, mean_q: 5.194781
 75236/100000: episode: 7677, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.159, mean reward: 0.416 [0.367, 0.487], mean action: 60.500 [22.000, 96.000], mean observation: 3.143 [-1.297, 10.285], loss: 0.936605, mae: 5.018834, mean_q: 5.195794
 75246/100000: episode: 7678, duration: 0.105s, episode steps: 10, steps per second: 96, episode reward: 4.065, mean reward: 0.406 [0.388, 0.451], mean action: 82.800 [46.000, 98.000], mean observation: 3.182 [-1.337, 10.509], loss: 1.220787, mae: 5.020108, mean_q: 5.193899
 75256/100000: episode: 7679, duration: 0.121s, episode steps: 10, steps per second: 82, episode reward: 4.161, mean reward: 0.416 [0.385, 0.448], mean action: 71.600 [28.000, 98.000], mean observation: 3.154 [-1.269, 10.278], loss: 1.257824, mae: 5.020127, mean_q: 5.194711
 75266/100000: episode: 7680, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.159, mean reward: 0.416 [0.346, 0.475], mean action: 62.600 [0.000, 98.000], mean observation: 3.157 [-1.019, 10.268], loss: 1.124624, mae: 5.019382, mean_q: 5.193254
 75276/100000: episode: 7681, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.052, mean reward: 0.405 [0.323, 0.457], mean action: 61.300 [0.000, 98.000], mean observation: 3.165 [-1.121, 10.482], loss: 1.337706, mae: 5.020030, mean_q: 5.190103
 75286/100000: episode: 7682, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.738, mean reward: 0.474 [0.368, 0.535], mean action: 77.400 [8.000, 98.000], mean observation: 3.147 [-1.147, 10.216], loss: 1.174782, mae: 5.019244, mean_q: 5.191317
 75296/100000: episode: 7683, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 3.449, mean reward: 0.345 [0.315, 0.370], mean action: 74.900 [13.000, 98.000], mean observation: 3.151 [-0.891, 10.385], loss: 1.369070, mae: 5.019804, mean_q: 5.193455
 75306/100000: episode: 7684, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.252, mean reward: 0.425 [0.321, 0.469], mean action: 63.300 [3.000, 98.000], mean observation: 3.170 [-1.284, 10.509], loss: 0.853149, mae: 5.017877, mean_q: 5.195724
 75316/100000: episode: 7685, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.610, mean reward: 0.461 [0.457, 0.492], mean action: 73.600 [28.000, 98.000], mean observation: 3.157 [-1.056, 10.439], loss: 1.435915, mae: 5.020022, mean_q: 5.197225
 75326/100000: episode: 7686, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.036, mean reward: 0.404 [0.342, 0.421], mean action: 60.700 [0.000, 98.000], mean observation: 3.172 [-1.430, 10.291], loss: 1.336841, mae: 5.019528, mean_q: 5.198201
 75336/100000: episode: 7687, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.389, mean reward: 0.439 [0.372, 0.453], mean action: 64.600 [10.000, 98.000], mean observation: 3.151 [-1.099, 10.205], loss: 0.947624, mae: 5.018017, mean_q: 5.199918
 75346/100000: episode: 7688, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 4.287, mean reward: 0.429 [0.411, 0.478], mean action: 80.500 [16.000, 98.000], mean observation: 3.166 [-1.451, 10.471], loss: 1.355239, mae: 5.019272, mean_q: 5.201093
 75356/100000: episode: 7689, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.415, mean reward: 0.442 [0.361, 0.484], mean action: 80.400 [13.000, 98.000], mean observation: 3.162 [-1.055, 10.380], loss: 1.357783, mae: 5.019059, mean_q: 5.202061
 75366/100000: episode: 7690, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.137, mean reward: 0.414 [0.367, 0.469], mean action: 69.400 [24.000, 98.000], mean observation: 3.165 [-1.744, 10.296], loss: 1.398982, mae: 5.018975, mean_q: 5.203462
 75376/100000: episode: 7691, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 5.030, mean reward: 0.503 [0.335, 0.531], mean action: 91.700 [62.000, 98.000], mean observation: 3.159 [-1.174, 10.305], loss: 1.307060, mae: 5.018398, mean_q: 5.201640
 75386/100000: episode: 7692, duration: 0.087s, episode steps: 10, steps per second: 116, episode reward: 4.419, mean reward: 0.442 [0.441, 0.449], mean action: 91.300 [31.000, 98.000], mean observation: 3.149 [-1.419, 10.333], loss: 1.134611, mae: 5.017641, mean_q: 5.199113
 75396/100000: episode: 7693, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.686, mean reward: 0.369 [0.334, 0.475], mean action: 72.000 [2.000, 98.000], mean observation: 3.168 [-0.765, 10.350], loss: 0.857349, mae: 5.016763, mean_q: 5.199698
 75401/100000: episode: 7694, duration: 0.092s, episode steps: 5, steps per second: 55, episode reward: 11.646, mean reward: 2.329 [0.353, 10.000], mean action: 51.600 [2.000, 98.000], mean observation: 3.156 [-1.958, 10.506], loss: 1.055057, mae: 5.018098, mean_q: 5.200898
 75411/100000: episode: 7695, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.109, mean reward: 0.411 [0.404, 0.473], mean action: 82.000 [16.000, 101.000], mean observation: 3.157 [-1.413, 10.416], loss: 1.056940, mae: 5.017991, mean_q: 5.199826
 75421/100000: episode: 7696, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.042, mean reward: 0.404 [0.307, 0.496], mean action: 73.700 [3.000, 98.000], mean observation: 3.158 [-1.806, 10.319], loss: 1.080711, mae: 5.018492, mean_q: 5.197078
 75431/100000: episode: 7697, duration: 0.095s, episode steps: 10, steps per second: 105, episode reward: 3.603, mean reward: 0.360 [0.349, 0.368], mean action: 87.800 [26.000, 98.000], mean observation: 3.162 [-0.712, 10.415], loss: 1.707629, mae: 5.021027, mean_q: 5.190959
 75441/100000: episode: 7698, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.696, mean reward: 0.370 [0.327, 0.443], mean action: 69.900 [1.000, 99.000], mean observation: 3.149 [-1.203, 10.313], loss: 0.961876, mae: 5.017786, mean_q: 5.186600
 75451/100000: episode: 7699, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.946, mean reward: 0.395 [0.327, 0.483], mean action: 69.300 [27.000, 99.000], mean observation: 3.152 [-1.364, 10.286], loss: 0.825747, mae: 5.017323, mean_q: 5.187170
 75461/100000: episode: 7700, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.994, mean reward: 0.399 [0.360, 0.432], mean action: 74.900 [4.000, 98.000], mean observation: 3.151 [-1.563, 10.359], loss: 1.474042, mae: 5.019953, mean_q: 5.187912
 75471/100000: episode: 7701, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.177, mean reward: 0.418 [0.343, 0.506], mean action: 62.200 [4.000, 99.000], mean observation: 3.149 [-1.401, 10.417], loss: 1.236695, mae: 5.019099, mean_q: 5.185382
 75481/100000: episode: 7702, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.218, mean reward: 0.422 [0.346, 0.523], mean action: 74.700 [12.000, 98.000], mean observation: 3.154 [-1.137, 10.401], loss: 0.947060, mae: 5.018100, mean_q: 5.178277
 75491/100000: episode: 7703, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 4.198, mean reward: 0.420 [0.372, 0.511], mean action: 60.000 [4.000, 98.000], mean observation: 3.166 [-0.937, 10.306], loss: 1.390980, mae: 5.019868, mean_q: 5.176516
 75501/100000: episode: 7704, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 4.017, mean reward: 0.402 [0.316, 0.499], mean action: 61.800 [9.000, 98.000], mean observation: 3.155 [-1.325, 10.278], loss: 1.332704, mae: 5.019507, mean_q: 5.176500
 75511/100000: episode: 7705, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.440, mean reward: 0.444 [0.428, 0.543], mean action: 74.500 [3.000, 101.000], mean observation: 3.151 [-1.521, 10.325], loss: 1.160244, mae: 5.018915, mean_q: 5.177351
 75521/100000: episode: 7706, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.432, mean reward: 0.443 [0.397, 0.532], mean action: 64.800 [2.000, 98.000], mean observation: 3.149 [-1.297, 10.314], loss: 0.986384, mae: 5.017941, mean_q: 5.178020
 75531/100000: episode: 7707, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 3.774, mean reward: 0.377 [0.329, 0.456], mean action: 72.500 [2.000, 98.000], mean observation: 3.156 [-0.979, 10.147], loss: 1.329767, mae: 5.019380, mean_q: 5.179324
 75541/100000: episode: 7708, duration: 0.131s, episode steps: 10, steps per second: 77, episode reward: 4.389, mean reward: 0.439 [0.419, 0.541], mean action: 65.600 [28.000, 98.000], mean observation: 3.170 [-1.142, 10.319], loss: 1.422357, mae: 5.019813, mean_q: 5.181504
 75551/100000: episode: 7709, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.177, mean reward: 0.418 [0.363, 0.467], mean action: 60.000 [6.000, 98.000], mean observation: 3.153 [-1.282, 10.345], loss: 1.231166, mae: 5.018611, mean_q: 5.178976
 75561/100000: episode: 7710, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.101, mean reward: 0.410 [0.363, 0.439], mean action: 70.700 [18.000, 98.000], mean observation: 3.161 [-0.940, 10.305], loss: 1.183128, mae: 5.018326, mean_q: 5.178377
 75571/100000: episode: 7711, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.862, mean reward: 0.386 [0.371, 0.414], mean action: 54.600 [12.000, 98.000], mean observation: 3.162 [-1.474, 10.313], loss: 1.127552, mae: 5.017880, mean_q: 5.179516
 75581/100000: episode: 7712, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 5.657, mean reward: 0.566 [0.566, 0.566], mean action: 64.700 [2.000, 98.000], mean observation: 3.164 [-1.108, 10.376], loss: 1.491243, mae: 5.019184, mean_q: 5.181279
 75591/100000: episode: 7713, duration: 0.113s, episode steps: 10, steps per second: 88, episode reward: 4.078, mean reward: 0.408 [0.395, 0.460], mean action: 80.400 [0.000, 98.000], mean observation: 3.166 [-1.499, 10.246], loss: 1.492238, mae: 5.018640, mean_q: 5.182775
 75601/100000: episode: 7714, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.988, mean reward: 0.399 [0.380, 0.434], mean action: 71.300 [9.000, 98.000], mean observation: 3.161 [-1.227, 10.396], loss: 1.192254, mae: 5.016939, mean_q: 5.184304
 75611/100000: episode: 7715, duration: 0.102s, episode steps: 10, steps per second: 98, episode reward: 4.086, mean reward: 0.409 [0.361, 0.468], mean action: 87.900 [55.000, 101.000], mean observation: 3.150 [-0.954, 10.339], loss: 1.042518, mae: 5.016122, mean_q: 5.185165
 75621/100000: episode: 7716, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.969, mean reward: 0.397 [0.342, 0.451], mean action: 70.200 [6.000, 98.000], mean observation: 3.149 [-1.337, 10.366], loss: 1.086169, mae: 5.016561, mean_q: 5.185891
 75631/100000: episode: 7717, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 4.033, mean reward: 0.403 [0.401, 0.425], mean action: 84.100 [52.000, 98.000], mean observation: 3.155 [-0.935, 10.252], loss: 1.253703, mae: 5.017162, mean_q: 5.188591
 75641/100000: episode: 7718, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 3.852, mean reward: 0.385 [0.347, 0.469], mean action: 67.000 [18.000, 98.000], mean observation: 3.156 [-1.612, 10.310], loss: 1.641660, mae: 5.018411, mean_q: 5.191752
 75645/100000: episode: 7719, duration: 0.070s, episode steps: 4, steps per second: 57, episode reward: 11.231, mean reward: 2.808 [0.409, 10.000], mean action: 51.250 [32.000, 98.000], mean observation: 3.150 [-0.934, 10.402], loss: 1.572789, mae: 5.017925, mean_q: 5.193275
 75655/100000: episode: 7720, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.035, mean reward: 0.404 [0.366, 0.487], mean action: 67.400 [12.000, 98.000], mean observation: 3.144 [-1.116, 10.432], loss: 1.523225, mae: 5.017425, mean_q: 5.193933
 75665/100000: episode: 7721, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.206, mean reward: 0.421 [0.367, 0.527], mean action: 56.400 [6.000, 98.000], mean observation: 3.159 [-1.235, 10.388], loss: 1.161349, mae: 5.015530, mean_q: 5.195045
 75675/100000: episode: 7722, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.081, mean reward: 0.408 [0.377, 0.434], mean action: 65.200 [5.000, 98.000], mean observation: 3.154 [-1.795, 10.293], loss: 1.257004, mae: 5.015994, mean_q: 5.197798
 75685/100000: episode: 7723, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.344, mean reward: 0.434 [0.318, 0.581], mean action: 85.300 [47.000, 98.000], mean observation: 3.142 [-1.334, 10.363], loss: 1.095275, mae: 5.015048, mean_q: 5.196631
 75695/100000: episode: 7724, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 3.797, mean reward: 0.380 [0.344, 0.403], mean action: 82.600 [13.000, 98.000], mean observation: 3.141 [-2.017, 10.451], loss: 1.159242, mae: 5.015037, mean_q: 5.195750
 75705/100000: episode: 7725, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 3.872, mean reward: 0.387 [0.365, 0.420], mean action: 74.300 [15.000, 98.000], mean observation: 3.156 [-1.958, 10.306], loss: 1.282864, mae: 5.015396, mean_q: 5.190151
 75715/100000: episode: 7726, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.152, mean reward: 0.415 [0.326, 0.478], mean action: 71.500 [10.000, 99.000], mean observation: 3.169 [-1.026, 10.374], loss: 1.050296, mae: 5.014511, mean_q: 5.181055
 75725/100000: episode: 7727, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 4.145, mean reward: 0.414 [0.337, 0.510], mean action: 78.700 [9.000, 99.000], mean observation: 3.166 [-0.960, 10.536], loss: 1.085356, mae: 5.014625, mean_q: 5.182048
 75735/100000: episode: 7728, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.109, mean reward: 0.411 [0.325, 0.515], mean action: 80.300 [34.000, 99.000], mean observation: 3.165 [-0.948, 10.214], loss: 1.007103, mae: 5.014359, mean_q: 5.184684
 75745/100000: episode: 7729, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.982, mean reward: 0.398 [0.355, 0.494], mean action: 74.900 [7.000, 99.000], mean observation: 3.162 [-1.499, 10.353], loss: 1.490473, mae: 5.016218, mean_q: 5.187372
 75755/100000: episode: 7730, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.944, mean reward: 0.394 [0.357, 0.427], mean action: 72.800 [6.000, 99.000], mean observation: 3.160 [-1.549, 10.178], loss: 1.347198, mae: 5.015361, mean_q: 5.189314
 75765/100000: episode: 7731, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.481, mean reward: 0.448 [0.305, 0.509], mean action: 60.800 [4.000, 99.000], mean observation: 3.157 [-1.432, 10.359], loss: 1.278703, mae: 5.014740, mean_q: 5.190894
 75775/100000: episode: 7732, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.834, mean reward: 0.483 [0.398, 0.543], mean action: 64.000 [12.000, 99.000], mean observation: 3.158 [-1.740, 10.264], loss: 1.240411, mae: 5.014430, mean_q: 5.190110
 75785/100000: episode: 7733, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.897, mean reward: 0.390 [0.372, 0.412], mean action: 58.300 [16.000, 101.000], mean observation: 3.160 [-1.772, 10.296], loss: 1.004429, mae: 5.013116, mean_q: 5.188585
 75795/100000: episode: 7734, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.862, mean reward: 0.386 [0.343, 0.424], mean action: 68.500 [14.000, 99.000], mean observation: 3.162 [-1.321, 10.219], loss: 1.227421, mae: 5.014014, mean_q: 5.186615
 75805/100000: episode: 7735, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.726, mean reward: 0.373 [0.332, 0.495], mean action: 63.500 [4.000, 99.000], mean observation: 3.160 [-1.716, 10.414], loss: 0.983982, mae: 5.013064, mean_q: 5.186698
 75815/100000: episode: 7736, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.306, mean reward: 0.431 [0.380, 0.505], mean action: 75.100 [12.000, 99.000], mean observation: 3.168 [-1.229, 10.556], loss: 1.013265, mae: 5.013352, mean_q: 5.187366
 75825/100000: episode: 7737, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.566, mean reward: 0.357 [0.352, 0.402], mean action: 78.100 [4.000, 99.000], mean observation: 3.177 [-1.400, 10.272], loss: 1.331731, mae: 5.014627, mean_q: 5.186177
 75835/100000: episode: 7738, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.798, mean reward: 0.480 [0.461, 0.492], mean action: 81.100 [26.000, 100.000], mean observation: 3.163 [-1.085, 10.342], loss: 1.218108, mae: 5.013857, mean_q: 5.186965
 75838/100000: episode: 7739, duration: 0.050s, episode steps: 3, steps per second: 60, episode reward: 10.799, mean reward: 3.600 [0.399, 10.000], mean action: 80.333 [64.000, 99.000], mean observation: 3.173 [-1.126, 10.185], loss: 1.740631, mae: 5.015557, mean_q: 5.187748
 75848/100000: episode: 7740, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.579, mean reward: 0.358 [0.340, 0.384], mean action: 75.300 [9.000, 99.000], mean observation: 3.158 [-1.448, 10.285], loss: 1.469350, mae: 5.014328, mean_q: 5.187794
 75858/100000: episode: 7741, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.094, mean reward: 0.409 [0.353, 0.467], mean action: 73.400 [17.000, 99.000], mean observation: 3.148 [-1.265, 10.355], loss: 1.222802, mae: 5.013293, mean_q: 5.182167
 75868/100000: episode: 7742, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.346, mean reward: 0.435 [0.435, 0.435], mean action: 60.200 [24.000, 95.000], mean observation: 3.159 [-1.575, 10.155], loss: 1.517096, mae: 5.014251, mean_q: 5.178241
 75878/100000: episode: 7743, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.840, mean reward: 0.384 [0.331, 0.508], mean action: 42.000 [24.000, 99.000], mean observation: 3.166 [-1.472, 10.413], loss: 1.118216, mae: 5.012569, mean_q: 5.179195
 75888/100000: episode: 7744, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.013, mean reward: 0.401 [0.354, 0.457], mean action: 24.600 [16.000, 45.000], mean observation: 3.157 [-1.433, 10.342], loss: 1.564484, mae: 5.014194, mean_q: 5.181113
 75898/100000: episode: 7745, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.281, mean reward: 0.428 [0.325, 0.544], mean action: 44.500 [10.000, 101.000], mean observation: 3.165 [-1.580, 10.289], loss: 1.046853, mae: 5.011956, mean_q: 5.183497
 75908/100000: episode: 7746, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 3.706, mean reward: 0.371 [0.299, 0.464], mean action: 24.100 [0.000, 61.000], mean observation: 3.163 [-1.549, 10.303], loss: 1.291579, mae: 5.012879, mean_q: 5.186383
 75918/100000: episode: 7747, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.309, mean reward: 0.431 [0.424, 0.461], mean action: 49.500 [12.000, 97.000], mean observation: 3.156 [-1.283, 10.354], loss: 1.390634, mae: 5.012852, mean_q: 5.188202
 75928/100000: episode: 7748, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.830, mean reward: 0.383 [0.343, 0.451], mean action: 33.400 [1.000, 99.000], mean observation: 3.154 [-1.418, 10.221], loss: 1.122368, mae: 5.011806, mean_q: 5.190434
 75938/100000: episode: 7749, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.978, mean reward: 0.398 [0.280, 0.474], mean action: 38.800 [24.000, 73.000], mean observation: 3.165 [-1.526, 10.391], loss: 1.470282, mae: 5.013103, mean_q: 5.192114
 75948/100000: episode: 7750, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.561, mean reward: 0.456 [0.449, 0.489], mean action: 31.400 [22.000, 49.000], mean observation: 3.162 [-1.370, 10.325], loss: 1.182565, mae: 5.011731, mean_q: 5.192138
 75958/100000: episode: 7751, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.197, mean reward: 0.420 [0.372, 0.476], mean action: 27.500 [7.000, 94.000], mean observation: 3.165 [-1.190, 10.317], loss: 0.991069, mae: 5.010703, mean_q: 5.190459
 75968/100000: episode: 7752, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.736, mean reward: 0.374 [0.317, 0.463], mean action: 28.100 [24.000, 57.000], mean observation: 3.151 [-2.355, 10.381], loss: 0.880238, mae: 5.010589, mean_q: 5.191200
 75978/100000: episode: 7753, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 3.831, mean reward: 0.383 [0.306, 0.454], mean action: 26.700 [10.000, 65.000], mean observation: 3.162 [-1.661, 10.394], loss: 1.022832, mae: 5.011369, mean_q: 5.192517
 75988/100000: episode: 7754, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.665, mean reward: 0.366 [0.315, 0.462], mean action: 31.900 [19.000, 98.000], mean observation: 3.161 [-1.517, 10.303], loss: 1.424057, mae: 5.013156, mean_q: 5.191854
 75998/100000: episode: 7755, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.164, mean reward: 0.416 [0.361, 0.487], mean action: 27.700 [5.000, 80.000], mean observation: 3.166 [-1.284, 10.439], loss: 1.071790, mae: 5.011743, mean_q: 5.189756
 76008/100000: episode: 7756, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.214, mean reward: 0.421 [0.332, 0.499], mean action: 43.700 [24.000, 101.000], mean observation: 3.150 [-1.514, 10.433], loss: 0.894703, mae: 5.011308, mean_q: 5.189490
 76018/100000: episode: 7757, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.930, mean reward: 0.393 [0.372, 0.409], mean action: 35.400 [16.000, 96.000], mean observation: 3.152 [-1.325, 10.348], loss: 0.992076, mae: 5.012131, mean_q: 5.190207
 76028/100000: episode: 7758, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.293, mean reward: 0.429 [0.416, 0.459], mean action: 45.500 [14.000, 100.000], mean observation: 3.142 [-1.831, 10.444], loss: 0.972917, mae: 5.012143, mean_q: 5.193064
 76038/100000: episode: 7759, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.840, mean reward: 0.384 [0.301, 0.504], mean action: 31.500 [18.000, 88.000], mean observation: 3.157 [-1.508, 10.394], loss: 1.389184, mae: 5.013978, mean_q: 5.194570
 76048/100000: episode: 7760, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.432, mean reward: 0.443 [0.383, 0.495], mean action: 35.600 [5.000, 86.000], mean observation: 3.167 [-1.461, 10.336], loss: 1.145064, mae: 5.013137, mean_q: 5.191096
 76058/100000: episode: 7761, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.129, mean reward: 0.413 [0.328, 0.503], mean action: 16.100 [1.000, 45.000], mean observation: 3.157 [-1.540, 10.290], loss: 1.151361, mae: 5.012885, mean_q: 5.189758
 76068/100000: episode: 7762, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.455, mean reward: 0.446 [0.367, 0.583], mean action: 40.000 [24.000, 97.000], mean observation: 3.153 [-2.286, 10.259], loss: 1.497623, mae: 5.014061, mean_q: 5.190852
 76075/100000: episode: 7763, duration: 0.111s, episode steps: 7, steps per second: 63, episode reward: 12.889, mean reward: 1.841 [0.422, 10.000], mean action: 49.857 [19.000, 99.000], mean observation: 3.157 [-1.107, 10.358], loss: 0.799299, mae: 5.011283, mean_q: 5.192355
 76085/100000: episode: 7764, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.254, mean reward: 0.425 [0.365, 0.494], mean action: 37.100 [12.000, 91.000], mean observation: 3.153 [-1.532, 10.250], loss: 1.083302, mae: 5.012387, mean_q: 5.193660
 76095/100000: episode: 7765, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.149, mean reward: 0.415 [0.348, 0.531], mean action: 49.800 [21.000, 99.000], mean observation: 3.160 [-1.766, 10.240], loss: 1.487991, mae: 5.013949, mean_q: 5.193699
 76105/100000: episode: 7766, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.124, mean reward: 0.412 [0.363, 0.469], mean action: 73.600 [14.000, 99.000], mean observation: 3.150 [-0.951, 10.309], loss: 1.436413, mae: 5.013261, mean_q: 5.191504
 76115/100000: episode: 7767, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.415, mean reward: 0.442 [0.345, 0.502], mean action: 68.900 [7.000, 99.000], mean observation: 3.142 [-1.456, 10.235], loss: 1.050789, mae: 5.011619, mean_q: 5.189958
 76125/100000: episode: 7768, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.233, mean reward: 0.423 [0.363, 0.509], mean action: 65.000 [7.000, 99.000], mean observation: 3.161 [-1.156, 10.471], loss: 1.287616, mae: 5.012440, mean_q: 5.187456
 76135/100000: episode: 7769, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.259, mean reward: 0.426 [0.412, 0.450], mean action: 34.300 [2.000, 83.000], mean observation: 3.150 [-1.309, 10.322], loss: 1.450408, mae: 5.013231, mean_q: 5.188436
 76145/100000: episode: 7770, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.462, mean reward: 0.446 [0.371, 0.489], mean action: 58.600 [26.000, 99.000], mean observation: 3.154 [-1.581, 10.285], loss: 1.497553, mae: 5.013199, mean_q: 5.187099
 76155/100000: episode: 7771, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.332, mean reward: 0.433 [0.411, 0.484], mean action: 73.600 [13.000, 99.000], mean observation: 3.143 [-1.420, 10.257], loss: 1.261697, mae: 5.012130, mean_q: 5.187127
 76165/100000: episode: 7772, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.315, mean reward: 0.432 [0.358, 0.456], mean action: 72.600 [10.000, 99.000], mean observation: 3.151 [-1.441, 10.208], loss: 0.981480, mae: 5.010844, mean_q: 5.189074
 76175/100000: episode: 7773, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.419, mean reward: 0.442 [0.347, 0.493], mean action: 60.500 [3.000, 99.000], mean observation: 3.165 [-0.857, 10.380], loss: 1.218558, mae: 5.011792, mean_q: 5.189132
 76185/100000: episode: 7774, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.605, mean reward: 0.460 [0.423, 0.507], mean action: 73.800 [7.000, 98.000], mean observation: 3.145 [-0.646, 10.387], loss: 1.388407, mae: 5.012069, mean_q: 5.189812
 76195/100000: episode: 7775, duration: 0.094s, episode steps: 10, steps per second: 106, episode reward: 4.236, mean reward: 0.424 [0.416, 0.492], mean action: 86.900 [14.000, 98.000], mean observation: 3.154 [-1.189, 10.222], loss: 1.355089, mae: 5.011730, mean_q: 5.189311
 76205/100000: episode: 7776, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.869, mean reward: 0.487 [0.373, 0.524], mean action: 80.600 [4.000, 101.000], mean observation: 3.145 [-0.947, 10.314], loss: 1.175069, mae: 5.010668, mean_q: 5.192428
 76215/100000: episode: 7777, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 3.776, mean reward: 0.378 [0.348, 0.427], mean action: 81.300 [9.000, 98.000], mean observation: 3.151 [-1.486, 10.242], loss: 1.392947, mae: 5.011388, mean_q: 5.192881
 76225/100000: episode: 7778, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.858, mean reward: 0.386 [0.299, 0.511], mean action: 42.500 [20.000, 98.000], mean observation: 3.153 [-1.403, 10.315], loss: 1.422958, mae: 5.011266, mean_q: 5.190296
 76235/100000: episode: 7779, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.979, mean reward: 0.398 [0.335, 0.496], mean action: 42.200 [24.000, 95.000], mean observation: 3.152 [-1.807, 10.373], loss: 1.197728, mae: 5.009799, mean_q: 5.187921
 76245/100000: episode: 7780, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.168, mean reward: 0.417 [0.381, 0.521], mean action: 41.600 [24.000, 94.000], mean observation: 3.165 [-1.393, 10.456], loss: 1.437306, mae: 5.010354, mean_q: 5.188013
 76255/100000: episode: 7781, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.987, mean reward: 0.399 [0.334, 0.462], mean action: 46.600 [24.000, 98.000], mean observation: 3.146 [-1.746, 10.383], loss: 1.369189, mae: 5.009732, mean_q: 5.187467
 76265/100000: episode: 7782, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.978, mean reward: 0.398 [0.323, 0.540], mean action: 46.200 [24.000, 78.000], mean observation: 3.147 [-1.171, 10.232], loss: 0.846728, mae: 5.007562, mean_q: 5.184112
 76275/100000: episode: 7783, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.060, mean reward: 0.406 [0.343, 0.517], mean action: 48.300 [1.000, 101.000], mean observation: 3.159 [-1.519, 10.462], loss: 1.399979, mae: 5.009671, mean_q: 5.180382
 76285/100000: episode: 7784, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.301, mean reward: 0.430 [0.362, 0.501], mean action: 31.600 [6.000, 101.000], mean observation: 3.169 [-1.498, 10.382], loss: 0.851039, mae: 5.007490, mean_q: 5.176599
 76295/100000: episode: 7785, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.237, mean reward: 0.424 [0.361, 0.467], mean action: 42.500 [3.000, 90.000], mean observation: 3.157 [-1.049, 10.359], loss: 1.174558, mae: 5.008834, mean_q: 5.176517
 76305/100000: episode: 7786, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.927, mean reward: 0.393 [0.367, 0.456], mean action: 36.600 [4.000, 95.000], mean observation: 3.169 [-1.033, 10.296], loss: 1.092198, mae: 5.008785, mean_q: 5.178289
 76315/100000: episode: 7787, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.347, mean reward: 0.435 [0.424, 0.529], mean action: 43.500 [24.000, 78.000], mean observation: 3.160 [-1.554, 10.283], loss: 1.195879, mae: 5.009127, mean_q: 5.179993
 76325/100000: episode: 7788, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.987, mean reward: 0.399 [0.339, 0.495], mean action: 30.600 [17.000, 52.000], mean observation: 3.160 [-1.608, 10.239], loss: 1.083960, mae: 5.009241, mean_q: 5.179244
 76335/100000: episode: 7789, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.244, mean reward: 0.424 [0.410, 0.446], mean action: 36.500 [24.000, 96.000], mean observation: 3.140 [-1.495, 10.271], loss: 1.360050, mae: 5.010333, mean_q: 5.180016
 76345/100000: episode: 7790, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.185, mean reward: 0.419 [0.380, 0.530], mean action: 39.800 [10.000, 99.000], mean observation: 3.157 [-1.272, 10.364], loss: 1.086626, mae: 5.009274, mean_q: 5.179060
 76355/100000: episode: 7791, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.227, mean reward: 0.423 [0.298, 0.579], mean action: 32.000 [4.000, 89.000], mean observation: 3.154 [-1.659, 10.258], loss: 1.049658, mae: 5.009373, mean_q: 5.179047
 76365/100000: episode: 7792, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.127, mean reward: 0.413 [0.331, 0.528], mean action: 46.800 [20.000, 91.000], mean observation: 3.154 [-1.260, 10.434], loss: 1.375921, mae: 5.010471, mean_q: 5.179860
 76375/100000: episode: 7793, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.637, mean reward: 0.364 [0.322, 0.477], mean action: 46.500 [15.000, 92.000], mean observation: 3.172 [-1.216, 10.290], loss: 1.416894, mae: 5.010364, mean_q: 5.180368
 76385/100000: episode: 7794, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.165, mean reward: 0.417 [0.332, 0.570], mean action: 28.000 [4.000, 60.000], mean observation: 3.164 [-1.558, 10.318], loss: 1.497823, mae: 5.010421, mean_q: 5.181793
 76395/100000: episode: 7795, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.271, mean reward: 0.427 [0.339, 0.529], mean action: 42.900 [24.000, 89.000], mean observation: 3.166 [-1.935, 10.255], loss: 1.208730, mae: 5.008985, mean_q: 5.180971
 76405/100000: episode: 7796, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.466, mean reward: 0.447 [0.357, 0.552], mean action: 31.500 [0.000, 85.000], mean observation: 3.164 [-1.243, 10.392], loss: 1.191334, mae: 5.008638, mean_q: 5.179370
 76415/100000: episode: 7797, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.527, mean reward: 0.453 [0.388, 0.530], mean action: 42.600 [24.000, 91.000], mean observation: 3.161 [-1.705, 10.246], loss: 1.204323, mae: 5.008405, mean_q: 5.180529
 76425/100000: episode: 7798, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.126, mean reward: 0.413 [0.340, 0.522], mean action: 34.200 [11.000, 76.000], mean observation: 3.159 [-1.818, 10.395], loss: 1.419242, mae: 5.009349, mean_q: 5.179410
 76435/100000: episode: 7799, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.196, mean reward: 0.420 [0.345, 0.466], mean action: 55.800 [20.000, 99.000], mean observation: 3.174 [-1.546, 10.309], loss: 1.652536, mae: 5.010010, mean_q: 5.177508
 76445/100000: episode: 7800, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 3.760, mean reward: 0.376 [0.366, 0.416], mean action: 63.700 [33.000, 78.000], mean observation: 3.138 [-0.912, 10.446], loss: 1.527911, mae: 5.008817, mean_q: 5.177132
 76455/100000: episode: 7801, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.838, mean reward: 0.384 [0.315, 0.512], mean action: 63.600 [5.000, 88.000], mean observation: 3.176 [-1.296, 10.377], loss: 1.155061, mae: 5.007004, mean_q: 5.177497
 76465/100000: episode: 7802, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.134, mean reward: 0.413 [0.412, 0.427], mean action: 64.500 [50.000, 67.000], mean observation: 3.157 [-1.378, 10.425], loss: 1.251758, mae: 5.007224, mean_q: 5.178524
 76475/100000: episode: 7803, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.208, mean reward: 0.421 [0.357, 0.540], mean action: 55.500 [16.000, 101.000], mean observation: 3.162 [-0.739, 10.405], loss: 1.014663, mae: 5.006317, mean_q: 5.180462
 76485/100000: episode: 7804, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.575, mean reward: 0.358 [0.335, 0.410], mean action: 63.600 [30.000, 93.000], mean observation: 3.150 [-1.948, 10.309], loss: 1.246902, mae: 5.007035, mean_q: 5.181507
 76495/100000: episode: 7805, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.728, mean reward: 0.373 [0.310, 0.476], mean action: 53.300 [24.000, 80.000], mean observation: 3.163 [-0.873, 10.299], loss: 1.249217, mae: 5.007044, mean_q: 5.182699
 76505/100000: episode: 7806, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.919, mean reward: 0.392 [0.349, 0.440], mean action: 56.600 [19.000, 66.000], mean observation: 3.161 [-1.328, 10.251], loss: 0.947320, mae: 5.005899, mean_q: 5.184891
 76515/100000: episode: 7807, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.221, mean reward: 0.422 [0.362, 0.514], mean action: 45.300 [3.000, 81.000], mean observation: 3.158 [-1.145, 10.212], loss: 1.124802, mae: 5.006847, mean_q: 5.187040
 76525/100000: episode: 7808, duration: 0.123s, episode steps: 10, steps per second: 82, episode reward: 3.904, mean reward: 0.390 [0.355, 0.422], mean action: 61.000 [6.000, 101.000], mean observation: 3.167 [-1.474, 10.294], loss: 1.360847, mae: 5.007848, mean_q: 5.188605
 76535/100000: episode: 7809, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.665, mean reward: 0.466 [0.355, 0.506], mean action: 67.700 [31.000, 92.000], mean observation: 3.160 [-0.919, 10.321], loss: 1.039214, mae: 5.006490, mean_q: 5.190562
 76545/100000: episode: 7810, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.220, mean reward: 0.422 [0.373, 0.499], mean action: 63.600 [42.000, 83.000], mean observation: 3.155 [-1.441, 10.277], loss: 1.116401, mae: 5.006785, mean_q: 5.192210
 76555/100000: episode: 7811, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.106, mean reward: 0.411 [0.395, 0.456], mean action: 59.100 [35.000, 66.000], mean observation: 3.149 [-1.745, 10.357], loss: 1.153388, mae: 5.006837, mean_q: 5.193194
 76565/100000: episode: 7812, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.381, mean reward: 0.438 [0.436, 0.460], mean action: 65.500 [54.000, 73.000], mean observation: 3.138 [-1.602, 10.333], loss: 1.105260, mae: 5.006679, mean_q: 5.194396
 76575/100000: episode: 7813, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.489, mean reward: 0.449 [0.446, 0.477], mean action: 66.900 [48.000, 92.000], mean observation: 3.154 [-1.606, 10.226], loss: 1.155748, mae: 5.007233, mean_q: 5.195067
 76585/100000: episode: 7814, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.020, mean reward: 0.402 [0.364, 0.553], mean action: 65.900 [54.000, 74.000], mean observation: 3.154 [-1.780, 10.421], loss: 1.030380, mae: 5.006658, mean_q: 5.191981
 76595/100000: episode: 7815, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.154, mean reward: 0.415 [0.396, 0.473], mean action: 55.500 [32.000, 66.000], mean observation: 3.149 [-1.446, 10.434], loss: 1.094214, mae: 5.007295, mean_q: 5.191254
 76605/100000: episode: 7816, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.833, mean reward: 0.383 [0.319, 0.579], mean action: 64.900 [28.000, 96.000], mean observation: 3.138 [-1.950, 10.318], loss: 0.950464, mae: 5.007131, mean_q: 5.192094
 76615/100000: episode: 7817, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.235, mean reward: 0.424 [0.373, 0.499], mean action: 64.200 [50.000, 73.000], mean observation: 3.167 [-1.286, 10.276], loss: 0.981417, mae: 5.007523, mean_q: 5.193490
 76625/100000: episode: 7818, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.319, mean reward: 0.332 [0.284, 0.472], mean action: 63.000 [41.000, 69.000], mean observation: 3.163 [-1.230, 10.339], loss: 1.262463, mae: 5.008405, mean_q: 5.196359
 76635/100000: episode: 7819, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.127, mean reward: 0.413 [0.390, 0.492], mean action: 63.300 [9.000, 99.000], mean observation: 3.157 [-1.352, 10.368], loss: 1.295385, mae: 5.008404, mean_q: 5.199080
 76645/100000: episode: 7820, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.046, mean reward: 0.405 [0.386, 0.491], mean action: 61.800 [37.000, 101.000], mean observation: 3.157 [-1.119, 10.402], loss: 1.284298, mae: 5.008360, mean_q: 5.201362
 76655/100000: episode: 7821, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.385, mean reward: 0.439 [0.425, 0.493], mean action: 71.400 [66.000, 99.000], mean observation: 3.161 [-1.735, 10.242], loss: 0.994263, mae: 5.007078, mean_q: 5.204046
 76663/100000: episode: 7822, duration: 0.130s, episode steps: 8, steps per second: 61, episode reward: 13.410, mean reward: 1.676 [0.487, 10.000], mean action: 59.250 [20.000, 75.000], mean observation: 3.154 [-1.679, 10.213], loss: 1.214635, mae: 5.008243, mean_q: 5.205890
 76673/100000: episode: 7823, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.046, mean reward: 0.405 [0.388, 0.441], mean action: 58.200 [27.000, 89.000], mean observation: 3.148 [-1.136, 10.459], loss: 1.241194, mae: 5.008339, mean_q: 5.207092
 76683/100000: episode: 7824, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.175, mean reward: 0.417 [0.336, 0.463], mean action: 54.200 [15.000, 81.000], mean observation: 3.155 [-1.110, 10.334], loss: 1.195495, mae: 5.008258, mean_q: 5.208112
 76693/100000: episode: 7825, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.012, mean reward: 0.401 [0.337, 0.519], mean action: 58.300 [18.000, 66.000], mean observation: 3.153 [-1.024, 10.265], loss: 1.196985, mae: 5.008200, mean_q: 5.208938
 76703/100000: episode: 7826, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.806, mean reward: 0.481 [0.479, 0.494], mean action: 58.600 [20.000, 74.000], mean observation: 3.165 [-1.301, 10.335], loss: 0.830153, mae: 5.006895, mean_q: 5.210804
 76713/100000: episode: 7827, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.772, mean reward: 0.377 [0.307, 0.443], mean action: 59.100 [22.000, 89.000], mean observation: 3.163 [-1.194, 10.279], loss: 1.213889, mae: 5.008544, mean_q: 5.211010
 76723/100000: episode: 7828, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.036, mean reward: 0.404 [0.302, 0.489], mean action: 64.000 [25.000, 96.000], mean observation: 3.162 [-1.792, 10.445], loss: 1.060019, mae: 5.008313, mean_q: 5.209873
 76733/100000: episode: 7829, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.861, mean reward: 0.386 [0.337, 0.495], mean action: 51.500 [16.000, 93.000], mean observation: 3.160 [-1.040, 10.333], loss: 1.205272, mae: 5.009095, mean_q: 5.210227
 76743/100000: episode: 7830, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.013, mean reward: 0.401 [0.341, 0.468], mean action: 59.800 [6.000, 94.000], mean observation: 3.158 [-1.428, 10.326], loss: 1.208008, mae: 5.009287, mean_q: 5.211782
 76753/100000: episode: 7831, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.574, mean reward: 0.457 [0.418, 0.494], mean action: 65.200 [45.000, 95.000], mean observation: 3.160 [-1.685, 10.295], loss: 0.896038, mae: 5.008504, mean_q: 5.214446
 76763/100000: episode: 7832, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.214, mean reward: 0.421 [0.411, 0.471], mean action: 61.400 [22.000, 97.000], mean observation: 3.157 [-1.335, 10.277], loss: 1.197148, mae: 5.009787, mean_q: 5.217557
 76773/100000: episode: 7833, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.691, mean reward: 0.369 [0.276, 0.441], mean action: 57.700 [16.000, 72.000], mean observation: 3.160 [-1.475, 10.290], loss: 1.143620, mae: 5.009688, mean_q: 5.219583
 76783/100000: episode: 7834, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.037, mean reward: 0.404 [0.382, 0.479], mean action: 48.400 [3.000, 68.000], mean observation: 3.157 [-1.404, 10.310], loss: 1.265617, mae: 5.010176, mean_q: 5.221293
 76793/100000: episode: 7835, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.207, mean reward: 0.421 [0.381, 0.467], mean action: 63.700 [28.000, 95.000], mean observation: 3.157 [-1.702, 10.256], loss: 0.766839, mae: 5.008464, mean_q: 5.220850
 76803/100000: episode: 7836, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 5.261, mean reward: 0.526 [0.432, 0.537], mean action: 60.000 [21.000, 95.000], mean observation: 3.131 [-1.637, 10.321], loss: 1.142087, mae: 5.010530, mean_q: 5.217330
 76813/100000: episode: 7837, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.741, mean reward: 0.474 [0.474, 0.474], mean action: 74.600 [34.000, 101.000], mean observation: 3.154 [-1.233, 10.333], loss: 1.226874, mae: 5.011215, mean_q: 5.215628
 76823/100000: episode: 7838, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 3.769, mean reward: 0.377 [0.329, 0.446], mean action: 67.800 [37.000, 96.000], mean observation: 3.134 [-1.410, 10.294], loss: 1.271875, mae: 5.011254, mean_q: 5.210404
 76833/100000: episode: 7839, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.876, mean reward: 0.388 [0.283, 0.519], mean action: 57.600 [2.000, 95.000], mean observation: 3.164 [-0.882, 10.327], loss: 1.243515, mae: 5.011244, mean_q: 5.206330
 76843/100000: episode: 7840, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.929, mean reward: 0.393 [0.380, 0.432], mean action: 52.100 [3.000, 88.000], mean observation: 3.158 [-1.375, 10.328], loss: 1.026360, mae: 5.010377, mean_q: 5.206079
 76853/100000: episode: 7841, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.458, mean reward: 0.446 [0.444, 0.466], mean action: 66.400 [40.000, 93.000], mean observation: 3.162 [-1.167, 10.402], loss: 1.453583, mae: 5.012064, mean_q: 5.206840
 76863/100000: episode: 7842, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.849, mean reward: 0.385 [0.348, 0.595], mean action: 55.900 [6.000, 92.000], mean observation: 3.154 [-1.612, 10.266], loss: 0.975272, mae: 5.010127, mean_q: 5.208710
 76873/100000: episode: 7843, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.364, mean reward: 0.436 [0.359, 0.519], mean action: 51.800 [5.000, 93.000], mean observation: 3.156 [-1.018, 10.361], loss: 0.903526, mae: 5.010188, mean_q: 5.210055
 76883/100000: episode: 7844, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.197, mean reward: 0.420 [0.376, 0.497], mean action: 51.600 [16.000, 70.000], mean observation: 3.154 [-1.536, 10.409], loss: 1.073172, mae: 5.011392, mean_q: 5.212558
 76893/100000: episode: 7845, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.378, mean reward: 0.438 [0.373, 0.560], mean action: 52.000 [5.000, 101.000], mean observation: 3.163 [-1.780, 10.347], loss: 1.263896, mae: 5.012665, mean_q: 5.215089
 76903/100000: episode: 7846, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.860, mean reward: 0.386 [0.323, 0.467], mean action: 54.300 [20.000, 80.000], mean observation: 3.158 [-1.877, 10.230], loss: 1.303343, mae: 5.012889, mean_q: 5.217219
 76913/100000: episode: 7847, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.800, mean reward: 0.380 [0.332, 0.478], mean action: 46.200 [6.000, 91.000], mean observation: 3.147 [-2.133, 10.350], loss: 1.267180, mae: 5.012640, mean_q: 5.219149
 76923/100000: episode: 7848, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.806, mean reward: 0.381 [0.364, 0.464], mean action: 64.900 [45.000, 86.000], mean observation: 3.160 [-1.118, 10.406], loss: 1.102921, mae: 5.011789, mean_q: 5.219436
 76933/100000: episode: 7849, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.738, mean reward: 0.474 [0.474, 0.474], mean action: 65.000 [7.000, 97.000], mean observation: 3.144 [-1.138, 10.368], loss: 1.238380, mae: 5.012303, mean_q: 5.215761
 76943/100000: episode: 7850, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.466, mean reward: 0.447 [0.437, 0.492], mean action: 64.600 [46.000, 81.000], mean observation: 3.150 [-1.074, 10.254], loss: 1.206365, mae: 5.012199, mean_q: 5.215888
 76953/100000: episode: 7851, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.038, mean reward: 0.404 [0.318, 0.430], mean action: 66.000 [3.000, 98.000], mean observation: 3.151 [-1.501, 10.413], loss: 1.209506, mae: 5.012069, mean_q: 5.217113
 76963/100000: episode: 7852, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.669, mean reward: 0.367 [0.357, 0.372], mean action: 55.300 [2.000, 98.000], mean observation: 3.162 [-1.819, 10.251], loss: 1.232369, mae: 5.012214, mean_q: 5.217801
 76973/100000: episode: 7853, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.655, mean reward: 0.365 [0.347, 0.393], mean action: 62.900 [36.000, 70.000], mean observation: 3.144 [-1.982, 10.328], loss: 1.173986, mae: 5.012012, mean_q: 5.216377
 76983/100000: episode: 7854, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.733, mean reward: 0.373 [0.357, 0.473], mean action: 55.000 [25.000, 72.000], mean observation: 3.145 [-1.251, 10.347], loss: 1.443245, mae: 5.012970, mean_q: 5.218440
 76993/100000: episode: 7855, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.806, mean reward: 0.381 [0.345, 0.399], mean action: 51.300 [1.000, 85.000], mean observation: 3.152 [-1.259, 10.384], loss: 1.235635, mae: 5.012440, mean_q: 5.220255
 77003/100000: episode: 7856, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.541, mean reward: 0.454 [0.433, 0.462], mean action: 56.400 [1.000, 78.000], mean observation: 3.155 [-1.500, 10.334], loss: 1.168687, mae: 5.012178, mean_q: 5.218140
 77013/100000: episode: 7857, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.967, mean reward: 0.397 [0.340, 0.432], mean action: 48.000 [3.000, 84.000], mean observation: 3.157 [-1.011, 10.338], loss: 1.438551, mae: 5.013171, mean_q: 5.211370
 77023/100000: episode: 7858, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.858, mean reward: 0.386 [0.379, 0.440], mean action: 55.300 [30.000, 66.000], mean observation: 3.146 [-0.864, 10.231], loss: 1.133546, mae: 5.012057, mean_q: 5.204631
 77033/100000: episode: 7859, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.545, mean reward: 0.455 [0.455, 0.455], mean action: 64.800 [30.000, 85.000], mean observation: 3.148 [-1.369, 10.311], loss: 1.451266, mae: 5.013636, mean_q: 5.202374
 77043/100000: episode: 7860, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.917, mean reward: 0.392 [0.325, 0.441], mean action: 22.800 [3.000, 100.000], mean observation: 3.152 [-1.572, 10.365], loss: 1.288627, mae: 5.012593, mean_q: 5.199227
 77053/100000: episode: 7861, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 3.756, mean reward: 0.376 [0.272, 0.427], mean action: 24.600 [3.000, 96.000], mean observation: 3.162 [-1.480, 10.414], loss: 1.458591, mae: 5.013106, mean_q: 5.198561
 77063/100000: episode: 7862, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.386, mean reward: 0.439 [0.417, 0.587], mean action: 44.400 [3.000, 66.000], mean observation: 3.159 [-1.644, 10.232], loss: 1.260792, mae: 5.012165, mean_q: 5.199140
 77073/100000: episode: 7863, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.276, mean reward: 0.428 [0.327, 0.495], mean action: 57.800 [32.000, 73.000], mean observation: 3.139 [-0.911, 10.281], loss: 1.102782, mae: 5.011678, mean_q: 5.201232
 77083/100000: episode: 7864, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.357, mean reward: 0.336 [0.301, 0.426], mean action: 62.400 [14.000, 92.000], mean observation: 3.147 [-1.513, 10.350], loss: 1.265733, mae: 5.012290, mean_q: 5.203308
 77093/100000: episode: 7865, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.319, mean reward: 0.432 [0.325, 0.500], mean action: 68.000 [46.000, 93.000], mean observation: 3.162 [-2.039, 10.352], loss: 1.311524, mae: 5.012632, mean_q: 5.206089
 77103/100000: episode: 7866, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.181, mean reward: 0.418 [0.364, 0.475], mean action: 56.600 [9.000, 67.000], mean observation: 3.151 [-1.365, 10.267], loss: 1.103467, mae: 5.011872, mean_q: 5.206862
 77113/100000: episode: 7867, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.867, mean reward: 0.387 [0.354, 0.473], mean action: 62.200 [44.000, 66.000], mean observation: 3.160 [-1.790, 10.280], loss: 0.844097, mae: 5.011001, mean_q: 5.204396
 77123/100000: episode: 7868, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.058, mean reward: 0.406 [0.316, 0.472], mean action: 54.000 [17.000, 95.000], mean observation: 3.167 [-1.586, 10.364], loss: 1.050365, mae: 5.011993, mean_q: 5.201786
 77133/100000: episode: 7869, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.886, mean reward: 0.389 [0.340, 0.458], mean action: 48.000 [10.000, 94.000], mean observation: 3.150 [-1.709, 10.297], loss: 1.166863, mae: 5.012791, mean_q: 5.203095
 77143/100000: episode: 7870, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.669, mean reward: 0.367 [0.322, 0.456], mean action: 31.400 [6.000, 75.000], mean observation: 3.164 [-1.543, 10.312], loss: 1.302186, mae: 5.013652, mean_q: 5.204529
 77153/100000: episode: 7871, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 3.857, mean reward: 0.386 [0.332, 0.474], mean action: 31.700 [3.000, 78.000], mean observation: 3.158 [-1.792, 10.376], loss: 1.193509, mae: 5.013301, mean_q: 5.206040
 77163/100000: episode: 7872, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.426, mean reward: 0.443 [0.376, 0.518], mean action: 34.600 [16.000, 73.000], mean observation: 3.155 [-1.001, 10.402], loss: 1.577529, mae: 5.014802, mean_q: 5.207265
 77166/100000: episode: 7873, duration: 0.063s, episode steps: 3, steps per second: 47, episode reward: 10.887, mean reward: 3.629 [0.416, 10.000], mean action: 27.000 [27.000, 27.000], mean observation: 3.163 [-2.285, 10.563], loss: 1.563632, mae: 5.014701, mean_q: 5.208388
 77176/100000: episode: 7874, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.058, mean reward: 0.406 [0.276, 0.563], mean action: 40.600 [27.000, 95.000], mean observation: 3.164 [-0.818, 10.298], loss: 1.507378, mae: 5.014193, mean_q: 5.209194
 77186/100000: episode: 7875, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.947, mean reward: 0.395 [0.322, 0.437], mean action: 48.100 [16.000, 81.000], mean observation: 3.152 [-1.168, 10.235], loss: 1.437278, mae: 5.013820, mean_q: 5.210175
 77196/100000: episode: 7876, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.836, mean reward: 0.384 [0.353, 0.446], mean action: 45.400 [8.000, 100.000], mean observation: 3.150 [-1.057, 10.430], loss: 1.367775, mae: 5.013286, mean_q: 5.209708
 77206/100000: episode: 7877, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.110, mean reward: 0.411 [0.365, 0.482], mean action: 49.800 [13.000, 99.000], mean observation: 3.140 [-1.337, 10.231], loss: 1.281719, mae: 5.012899, mean_q: 5.206202
 77216/100000: episode: 7878, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.319, mean reward: 0.432 [0.409, 0.495], mean action: 42.000 [19.000, 99.000], mean observation: 3.158 [-1.259, 10.388], loss: 1.256806, mae: 5.012712, mean_q: 5.206899
 77226/100000: episode: 7879, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.884, mean reward: 0.388 [0.294, 0.439], mean action: 39.000 [13.000, 75.000], mean observation: 3.156 [-1.603, 10.391], loss: 1.006149, mae: 5.011697, mean_q: 5.208494
 77236/100000: episode: 7880, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.476, mean reward: 0.448 [0.343, 0.559], mean action: 30.100 [6.000, 65.000], mean observation: 3.146 [-1.157, 10.506], loss: 1.265059, mae: 5.012957, mean_q: 5.209503
 77246/100000: episode: 7881, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.693, mean reward: 0.369 [0.331, 0.490], mean action: 38.800 [27.000, 71.000], mean observation: 3.151 [-1.395, 10.315], loss: 1.900674, mae: 5.014989, mean_q: 5.210560
 77256/100000: episode: 7882, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.943, mean reward: 0.394 [0.351, 0.454], mean action: 33.800 [14.000, 81.000], mean observation: 3.151 [-1.350, 10.397], loss: 1.260247, mae: 5.012154, mean_q: 5.211864
 77266/100000: episode: 7883, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.354, mean reward: 0.335 [0.270, 0.393], mean action: 44.700 [27.000, 93.000], mean observation: 3.161 [-1.744, 10.258], loss: 1.476141, mae: 5.012629, mean_q: 5.213391
 77276/100000: episode: 7884, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.071, mean reward: 0.407 [0.348, 0.457], mean action: 44.200 [15.000, 94.000], mean observation: 3.148 [-1.869, 10.334], loss: 1.072366, mae: 5.010820, mean_q: 5.213954
 77286/100000: episode: 7885, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.570, mean reward: 0.357 [0.319, 0.465], mean action: 35.000 [12.000, 86.000], mean observation: 3.158 [-1.723, 10.276], loss: 1.442982, mae: 5.012215, mean_q: 5.214314
 77296/100000: episode: 7886, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.289, mean reward: 0.429 [0.357, 0.568], mean action: 45.300 [10.000, 78.000], mean observation: 3.150 [-1.268, 10.252], loss: 1.197164, mae: 5.011466, mean_q: 5.215488
 77306/100000: episode: 7887, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.885, mean reward: 0.388 [0.343, 0.531], mean action: 31.000 [5.000, 49.000], mean observation: 3.155 [-1.403, 10.387], loss: 1.343764, mae: 5.011665, mean_q: 5.213727
 77316/100000: episode: 7888, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.280, mean reward: 0.428 [0.313, 0.539], mean action: 45.300 [11.000, 94.000], mean observation: 3.147 [-1.895, 10.407], loss: 1.203301, mae: 5.011085, mean_q: 5.209023
 77326/100000: episode: 7889, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.993, mean reward: 0.399 [0.299, 0.490], mean action: 41.200 [12.000, 97.000], mean observation: 3.150 [-1.469, 10.320], loss: 1.272361, mae: 5.011329, mean_q: 5.208365
 77336/100000: episode: 7890, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.964, mean reward: 0.396 [0.387, 0.470], mean action: 30.800 [4.000, 99.000], mean observation: 3.157 [-1.373, 10.285], loss: 1.145166, mae: 5.010822, mean_q: 5.208660
 77346/100000: episode: 7891, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 3.705, mean reward: 0.371 [0.341, 0.438], mean action: 34.700 [16.000, 78.000], mean observation: 3.150 [-1.645, 10.326], loss: 1.337103, mae: 5.011551, mean_q: 5.205699
 77356/100000: episode: 7892, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.255, mean reward: 0.426 [0.325, 0.528], mean action: 37.900 [10.000, 60.000], mean observation: 3.159 [-1.900, 10.409], loss: 1.368646, mae: 5.011584, mean_q: 5.204035
 77366/100000: episode: 7893, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.637, mean reward: 0.464 [0.374, 0.589], mean action: 35.700 [2.000, 82.000], mean observation: 3.154 [-1.732, 10.413], loss: 1.281422, mae: 5.011153, mean_q: 5.203690
 77376/100000: episode: 7894, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.941, mean reward: 0.394 [0.334, 0.539], mean action: 28.500 [2.000, 91.000], mean observation: 3.159 [-1.529, 10.470], loss: 0.854314, mae: 5.009295, mean_q: 5.205379
 77386/100000: episode: 7895, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 3.895, mean reward: 0.389 [0.321, 0.460], mean action: 24.400 [2.000, 85.000], mean observation: 3.143 [-1.200, 10.237], loss: 1.277590, mae: 5.011075, mean_q: 5.205722
 77396/100000: episode: 7896, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 4.411, mean reward: 0.441 [0.371, 0.502], mean action: 12.700 [2.000, 86.000], mean observation: 3.162 [-1.042, 10.376], loss: 1.169590, mae: 5.010555, mean_q: 5.202760
 77406/100000: episode: 7897, duration: 0.217s, episode steps: 10, steps per second: 46, episode reward: 4.569, mean reward: 0.457 [0.385, 0.572], mean action: 22.300 [2.000, 85.000], mean observation: 3.153 [-1.859, 10.359], loss: 1.039318, mae: 5.010367, mean_q: 5.201060
 77416/100000: episode: 7898, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.749, mean reward: 0.375 [0.344, 0.413], mean action: 42.400 [0.000, 97.000], mean observation: 3.149 [-1.310, 10.249], loss: 1.058156, mae: 5.010657, mean_q: 5.200383
 77426/100000: episode: 7899, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.810, mean reward: 0.481 [0.356, 0.533], mean action: 32.400 [2.000, 87.000], mean observation: 3.146 [-1.868, 10.393], loss: 1.266383, mae: 5.011783, mean_q: 5.198514
 77436/100000: episode: 7900, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.303, mean reward: 0.430 [0.385, 0.557], mean action: 29.600 [1.000, 59.000], mean observation: 3.153 [-1.597, 10.399], loss: 1.282770, mae: 5.011624, mean_q: 5.199237
 77446/100000: episode: 7901, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 3.951, mean reward: 0.395 [0.365, 0.474], mean action: 20.700 [2.000, 52.000], mean observation: 3.156 [-2.197, 10.361], loss: 1.557978, mae: 5.012619, mean_q: 5.196324
 77456/100000: episode: 7902, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.473, mean reward: 0.447 [0.368, 0.562], mean action: 34.500 [2.000, 91.000], mean observation: 3.150 [-1.513, 10.363], loss: 1.184481, mae: 5.010829, mean_q: 5.194734
 77466/100000: episode: 7903, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.219, mean reward: 0.422 [0.348, 0.489], mean action: 37.800 [2.000, 97.000], mean observation: 3.144 [-1.418, 10.361], loss: 1.243250, mae: 5.010802, mean_q: 5.194601
 77476/100000: episode: 7904, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.887, mean reward: 0.389 [0.319, 0.436], mean action: 29.000 [2.000, 90.000], mean observation: 3.159 [-0.914, 10.394], loss: 1.154317, mae: 5.010324, mean_q: 5.191367
 77486/100000: episode: 7905, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.495, mean reward: 0.450 [0.311, 0.548], mean action: 34.700 [2.000, 84.000], mean observation: 3.153 [-0.998, 10.241], loss: 1.090364, mae: 5.010198, mean_q: 5.187447
 77496/100000: episode: 7906, duration: 0.228s, episode steps: 10, steps per second: 44, episode reward: 4.290, mean reward: 0.429 [0.332, 0.522], mean action: 16.400 [2.000, 38.000], mean observation: 3.149 [-2.082, 10.258], loss: 1.218788, mae: 5.010796, mean_q: 5.187050
 77506/100000: episode: 7907, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.424, mean reward: 0.442 [0.419, 0.556], mean action: 35.600 [10.000, 76.000], mean observation: 3.171 [-1.530, 10.480], loss: 1.804531, mae: 5.012826, mean_q: 5.188611
 77516/100000: episode: 7908, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.908, mean reward: 0.391 [0.332, 0.486], mean action: 30.700 [1.000, 76.000], mean observation: 3.161 [-1.808, 10.338], loss: 1.295735, mae: 5.010464, mean_q: 5.190342
 77526/100000: episode: 7909, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.519, mean reward: 0.452 [0.430, 0.505], mean action: 41.200 [5.000, 94.000], mean observation: 3.154 [-1.778, 10.307], loss: 1.119175, mae: 5.009562, mean_q: 5.190928
 77536/100000: episode: 7910, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 5.196, mean reward: 0.520 [0.520, 0.520], mean action: 40.500 [20.000, 69.000], mean observation: 3.142 [-1.402, 10.281], loss: 1.327517, mae: 5.010124, mean_q: 5.188430
 77546/100000: episode: 7911, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.358, mean reward: 0.436 [0.384, 0.493], mean action: 24.800 [3.000, 68.000], mean observation: 3.161 [-1.182, 10.252], loss: 1.105071, mae: 5.008800, mean_q: 5.187220
 77550/100000: episode: 7912, duration: 0.097s, episode steps: 4, steps per second: 41, episode reward: 11.099, mean reward: 2.775 [0.338, 10.000], mean action: 3.000 [3.000, 3.000], mean observation: 3.155 [-0.709, 10.231], loss: 1.384615, mae: 5.009949, mean_q: 5.189973
 77560/100000: episode: 7913, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.003, mean reward: 0.400 [0.350, 0.517], mean action: 28.000 [3.000, 99.000], mean observation: 3.159 [-1.547, 10.403], loss: 1.299722, mae: 5.009782, mean_q: 5.192224
 77570/100000: episode: 7914, duration: 0.224s, episode steps: 10, steps per second: 45, episode reward: 3.640, mean reward: 0.364 [0.311, 0.405], mean action: 9.800 [3.000, 58.000], mean observation: 3.154 [-1.162, 10.215], loss: 1.212882, mae: 5.009316, mean_q: 5.194409
 77580/100000: episode: 7915, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.328, mean reward: 0.433 [0.316, 0.483], mean action: 46.300 [3.000, 91.000], mean observation: 3.157 [-1.544, 10.282], loss: 1.475620, mae: 5.010115, mean_q: 5.196443
 77590/100000: episode: 7916, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 3.998, mean reward: 0.400 [0.322, 0.451], mean action: 26.900 [3.000, 93.000], mean observation: 3.147 [-1.727, 10.301], loss: 1.253610, mae: 5.008992, mean_q: 5.196100
 77600/100000: episode: 7917, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.286, mean reward: 0.429 [0.347, 0.534], mean action: 31.400 [3.000, 87.000], mean observation: 3.153 [-1.533, 10.392], loss: 1.225319, mae: 5.008774, mean_q: 5.196737
 77610/100000: episode: 7918, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.162, mean reward: 0.416 [0.363, 0.480], mean action: 29.900 [3.000, 97.000], mean observation: 3.161 [-1.537, 10.377], loss: 1.448267, mae: 5.009515, mean_q: 5.197961
 77620/100000: episode: 7919, duration: 0.233s, episode steps: 10, steps per second: 43, episode reward: 4.146, mean reward: 0.415 [0.356, 0.586], mean action: 27.900 [3.000, 99.000], mean observation: 3.154 [-1.740, 10.294], loss: 1.230058, mae: 5.008550, mean_q: 5.199663
 77621/100000: episode: 7920, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 3.000 [3.000, 3.000], mean observation: 3.172 [-0.725, 10.391], loss: 0.472945, mae: 5.006030, mean_q: 5.201154
 77631/100000: episode: 7921, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.626, mean reward: 0.363 [0.333, 0.423], mean action: 43.900 [3.000, 101.000], mean observation: 3.157 [-2.426, 10.238], loss: 1.146741, mae: 5.008369, mean_q: 5.200978
 77641/100000: episode: 7922, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.517, mean reward: 0.452 [0.378, 0.530], mean action: 40.500 [3.000, 94.000], mean observation: 3.151 [-1.759, 10.209], loss: 1.054577, mae: 5.007793, mean_q: 5.200189
 77651/100000: episode: 7923, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.996, mean reward: 0.400 [0.350, 0.521], mean action: 43.200 [3.000, 98.000], mean observation: 3.158 [-1.164, 10.357], loss: 1.211928, mae: 5.008460, mean_q: 5.195201
 77661/100000: episode: 7924, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 3.791, mean reward: 0.379 [0.326, 0.425], mean action: 13.800 [3.000, 48.000], mean observation: 3.156 [-1.234, 10.429], loss: 0.970562, mae: 5.007775, mean_q: 5.188644
 77671/100000: episode: 7925, duration: 0.233s, episode steps: 10, steps per second: 43, episode reward: 4.146, mean reward: 0.415 [0.325, 0.542], mean action: 14.800 [2.000, 40.000], mean observation: 3.161 [-1.631, 10.523], loss: 1.231440, mae: 5.009007, mean_q: 5.182929
 77681/100000: episode: 7926, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.466, mean reward: 0.447 [0.400, 0.551], mean action: 35.800 [2.000, 101.000], mean observation: 3.153 [-1.128, 10.442], loss: 1.285123, mae: 5.009061, mean_q: 5.183874
 77691/100000: episode: 7927, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.083, mean reward: 0.408 [0.350, 0.452], mean action: 32.000 [0.000, 83.000], mean observation: 3.153 [-1.373, 10.269], loss: 1.225127, mae: 5.008985, mean_q: 5.185699
 77701/100000: episode: 7928, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 4.282, mean reward: 0.428 [0.354, 0.502], mean action: 30.900 [2.000, 88.000], mean observation: 3.156 [-1.728, 10.296], loss: 1.089401, mae: 5.008459, mean_q: 5.187991
 77711/100000: episode: 7929, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.166, mean reward: 0.417 [0.318, 0.526], mean action: 26.600 [2.000, 75.000], mean observation: 3.159 [-1.055, 10.343], loss: 1.048890, mae: 5.008253, mean_q: 5.190151
 77721/100000: episode: 7930, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.771, mean reward: 0.377 [0.320, 0.485], mean action: 27.000 [2.000, 98.000], mean observation: 3.158 [-1.526, 10.242], loss: 1.360298, mae: 5.009506, mean_q: 5.192535
 77731/100000: episode: 7931, duration: 0.256s, episode steps: 10, steps per second: 39, episode reward: 4.512, mean reward: 0.451 [0.386, 0.564], mean action: 3.200 [2.000, 14.000], mean observation: 3.152 [-1.650, 10.551], loss: 1.526935, mae: 5.010171, mean_q: 5.194254
 77741/100000: episode: 7932, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.707, mean reward: 0.371 [0.287, 0.535], mean action: 48.600 [2.000, 99.000], mean observation: 3.161 [-1.061, 10.339], loss: 1.190224, mae: 5.008542, mean_q: 5.196578
 77751/100000: episode: 7933, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 3.955, mean reward: 0.396 [0.331, 0.462], mean action: 20.900 [1.000, 66.000], mean observation: 3.150 [-0.813, 10.264], loss: 1.232684, mae: 5.008537, mean_q: 5.199346
 77757/100000: episode: 7934, duration: 0.134s, episode steps: 6, steps per second: 45, episode reward: 12.289, mean reward: 2.048 [0.367, 10.000], mean action: 24.667 [2.000, 71.000], mean observation: 3.136 [-1.377, 10.311], loss: 0.996566, mae: 5.007734, mean_q: 5.201674
 77767/100000: episode: 7935, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.139, mean reward: 0.414 [0.335, 0.507], mean action: 23.000 [2.000, 92.000], mean observation: 3.153 [-1.237, 10.267], loss: 1.487306, mae: 5.009422, mean_q: 5.203526
 77777/100000: episode: 7936, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 3.991, mean reward: 0.399 [0.334, 0.469], mean action: 28.000 [2.000, 83.000], mean observation: 3.158 [-1.143, 10.308], loss: 1.023691, mae: 5.007565, mean_q: 5.205558
 77787/100000: episode: 7937, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.643, mean reward: 0.464 [0.375, 0.525], mean action: 26.400 [2.000, 98.000], mean observation: 3.165 [-1.359, 10.339], loss: 1.456053, mae: 5.009176, mean_q: 5.207209
 77797/100000: episode: 7938, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.206, mean reward: 0.421 [0.362, 0.485], mean action: 45.500 [2.000, 96.000], mean observation: 3.158 [-2.333, 10.343], loss: 1.276987, mae: 5.008399, mean_q: 5.206185
 77807/100000: episode: 7939, duration: 0.236s, episode steps: 10, steps per second: 42, episode reward: 4.361, mean reward: 0.436 [0.367, 0.494], mean action: 12.300 [1.000, 57.000], mean observation: 3.148 [-1.918, 10.369], loss: 1.257838, mae: 5.008058, mean_q: 5.203105
 77817/100000: episode: 7940, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.992, mean reward: 0.399 [0.353, 0.438], mean action: 41.100 [2.000, 99.000], mean observation: 3.155 [-1.568, 10.298], loss: 1.501402, mae: 5.008716, mean_q: 5.199143
 77827/100000: episode: 7941, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.408, mean reward: 0.441 [0.352, 0.525], mean action: 23.000 [2.000, 83.000], mean observation: 3.147 [-1.731, 10.317], loss: 1.151289, mae: 5.007314, mean_q: 5.195031
 77837/100000: episode: 7942, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.043, mean reward: 0.404 [0.344, 0.499], mean action: 22.500 [2.000, 87.000], mean observation: 3.155 [-1.422, 10.217], loss: 1.087114, mae: 5.007005, mean_q: 5.191664
 77847/100000: episode: 7943, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.114, mean reward: 0.411 [0.327, 0.475], mean action: 21.800 [2.000, 83.000], mean observation: 3.159 [-1.273, 10.310], loss: 1.270356, mae: 5.007566, mean_q: 5.191501
 77857/100000: episode: 7944, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 4.348, mean reward: 0.435 [0.299, 0.493], mean action: 21.400 [2.000, 84.000], mean observation: 3.153 [-1.633, 10.423], loss: 1.207339, mae: 5.007457, mean_q: 5.192974
 77860/100000: episode: 7945, duration: 0.074s, episode steps: 3, steps per second: 41, episode reward: 10.939, mean reward: 3.646 [0.412, 10.000], mean action: 2.000 [2.000, 2.000], mean observation: 3.162 [-1.356, 10.443], loss: 1.214984, mae: 5.007455, mean_q: 5.194042
 77870/100000: episode: 7946, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.214, mean reward: 0.421 [0.345, 0.585], mean action: 31.700 [2.000, 91.000], mean observation: 3.158 [-1.276, 10.296], loss: 1.226018, mae: 5.007359, mean_q: 5.195367
 77880/100000: episode: 7947, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.299, mean reward: 0.430 [0.373, 0.485], mean action: 21.500 [2.000, 101.000], mean observation: 3.158 [-1.178, 10.298], loss: 1.109799, mae: 5.007180, mean_q: 5.194128
 77890/100000: episode: 7948, duration: 0.225s, episode steps: 10, steps per second: 44, episode reward: 4.338, mean reward: 0.434 [0.356, 0.501], mean action: 2.800 [1.000, 11.000], mean observation: 3.155 [-1.475, 10.277], loss: 1.069762, mae: 5.006911, mean_q: 5.195075
 77900/100000: episode: 7949, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 3.894, mean reward: 0.389 [0.335, 0.417], mean action: 36.000 [2.000, 85.000], mean observation: 3.151 [-1.470, 10.229], loss: 1.377694, mae: 5.008426, mean_q: 5.197001
 77910/100000: episode: 7950, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.367, mean reward: 0.437 [0.375, 0.513], mean action: 22.800 [2.000, 89.000], mean observation: 3.156 [-1.543, 10.474], loss: 1.237594, mae: 5.007600, mean_q: 5.197409
 77920/100000: episode: 7951, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.139, mean reward: 0.414 [0.315, 0.526], mean action: 10.200 [2.000, 36.000], mean observation: 3.145 [-1.263, 10.293], loss: 0.958054, mae: 5.006442, mean_q: 5.198655
 77930/100000: episode: 7952, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.210, mean reward: 0.421 [0.421, 0.421], mean action: 24.400 [2.000, 76.000], mean observation: 3.154 [-1.613, 10.275], loss: 1.047634, mae: 5.006896, mean_q: 5.200841
 77940/100000: episode: 7953, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.915, mean reward: 0.391 [0.331, 0.466], mean action: 25.000 [2.000, 71.000], mean observation: 3.148 [-1.778, 10.334], loss: 1.242152, mae: 5.007710, mean_q: 5.203305
 77950/100000: episode: 7954, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 3.851, mean reward: 0.385 [0.318, 0.439], mean action: 13.900 [2.000, 54.000], mean observation: 3.154 [-1.972, 10.406], loss: 1.126770, mae: 5.007067, mean_q: 5.202921
 77960/100000: episode: 7955, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.062, mean reward: 0.406 [0.294, 0.501], mean action: 21.300 [2.000, 96.000], mean observation: 3.159 [-1.474, 10.380], loss: 1.426863, mae: 5.008035, mean_q: 5.199456
 77970/100000: episode: 7956, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.046, mean reward: 0.405 [0.329, 0.470], mean action: 22.700 [2.000, 72.000], mean observation: 3.161 [-1.066, 10.303], loss: 0.996913, mae: 5.006135, mean_q: 5.198533
 77980/100000: episode: 7957, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 4.410, mean reward: 0.441 [0.346, 0.576], mean action: 13.900 [2.000, 51.000], mean observation: 3.160 [-2.466, 10.242], loss: 1.360417, mae: 5.007683, mean_q: 5.197019
 77990/100000: episode: 7958, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.107, mean reward: 0.411 [0.347, 0.452], mean action: 31.200 [2.000, 100.000], mean observation: 3.143 [-1.380, 10.189], loss: 1.202783, mae: 5.007085, mean_q: 5.197413
 77991/100000: episode: 7959, duration: 0.039s, episode steps: 1, steps per second: 26, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 2.000 [2.000, 2.000], mean observation: 3.173 [-0.810, 10.228], loss: 2.126383, mae: 5.010679, mean_q: 5.197998
 78001/100000: episode: 7960, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 4.140, mean reward: 0.414 [0.341, 0.516], mean action: 2.000 [2.000, 2.000], mean observation: 3.161 [-1.323, 10.493], loss: 1.243518, mae: 5.007163, mean_q: 5.198514
 78011/100000: episode: 7961, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 3.976, mean reward: 0.398 [0.329, 0.537], mean action: 15.200 [0.000, 52.000], mean observation: 3.153 [-1.561, 10.367], loss: 1.225424, mae: 5.006865, mean_q: 5.199635
 78021/100000: episode: 7962, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.993, mean reward: 0.399 [0.331, 0.524], mean action: 37.500 [2.000, 99.000], mean observation: 3.153 [-1.477, 10.250], loss: 1.334324, mae: 5.006987, mean_q: 5.201169
 78031/100000: episode: 7963, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.366, mean reward: 0.437 [0.397, 0.575], mean action: 34.400 [2.000, 91.000], mean observation: 3.150 [-1.168, 10.421], loss: 1.293145, mae: 5.006686, mean_q: 5.199454
 78041/100000: episode: 7964, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.102, mean reward: 0.410 [0.317, 0.462], mean action: 22.700 [2.000, 99.000], mean observation: 3.167 [-1.043, 10.394], loss: 1.059732, mae: 5.005524, mean_q: 5.195078
 78051/100000: episode: 7965, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.146, mean reward: 0.415 [0.305, 0.516], mean action: 21.300 [3.000, 89.000], mean observation: 3.148 [-1.246, 10.242], loss: 1.186145, mae: 5.005887, mean_q: 5.194654
 78061/100000: episode: 7966, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.410, mean reward: 0.441 [0.324, 0.596], mean action: 32.100 [3.000, 76.000], mean observation: 3.150 [-1.306, 10.213], loss: 1.565044, mae: 5.007215, mean_q: 5.195708
 78071/100000: episode: 7967, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.971, mean reward: 0.397 [0.362, 0.439], mean action: 39.500 [0.000, 94.000], mean observation: 3.156 [-1.411, 10.313], loss: 1.197257, mae: 5.005504, mean_q: 5.195662
 78081/100000: episode: 7968, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.905, mean reward: 0.390 [0.324, 0.488], mean action: 28.900 [3.000, 94.000], mean observation: 3.154 [-1.958, 10.237], loss: 1.335031, mae: 5.005851, mean_q: 5.193741
 78091/100000: episode: 7969, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.049, mean reward: 0.405 [0.332, 0.501], mean action: 27.400 [3.000, 97.000], mean observation: 3.150 [-1.737, 10.232], loss: 1.249976, mae: 5.005230, mean_q: 5.188777
 78101/100000: episode: 7970, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.642, mean reward: 0.464 [0.298, 0.594], mean action: 25.900 [3.000, 88.000], mean observation: 3.159 [-1.432, 10.327], loss: 1.200307, mae: 5.004874, mean_q: 5.187807
 78111/100000: episode: 7971, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.943, mean reward: 0.394 [0.315, 0.477], mean action: 25.100 [3.000, 71.000], mean observation: 3.149 [-2.001, 10.227], loss: 1.122940, mae: 5.004392, mean_q: 5.188579
 78121/100000: episode: 7972, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.039, mean reward: 0.404 [0.340, 0.475], mean action: 17.000 [1.000, 58.000], mean observation: 3.150 [-1.354, 10.369], loss: 1.116246, mae: 5.004356, mean_q: 5.187322
 78131/100000: episode: 7973, duration: 0.239s, episode steps: 10, steps per second: 42, episode reward: 4.269, mean reward: 0.427 [0.370, 0.511], mean action: 21.800 [3.000, 54.000], mean observation: 3.155 [-1.701, 10.329], loss: 1.355954, mae: 5.005399, mean_q: 5.187563
 78141/100000: episode: 7974, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.079, mean reward: 0.408 [0.343, 0.456], mean action: 10.200 [3.000, 68.000], mean observation: 3.157 [-1.909, 10.406], loss: 1.122160, mae: 5.004455, mean_q: 5.186969
 78151/100000: episode: 7975, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.279, mean reward: 0.428 [0.375, 0.468], mean action: 27.200 [1.000, 84.000], mean observation: 3.159 [-1.957, 10.323], loss: 1.366447, mae: 5.005363, mean_q: 5.184360
 78159/100000: episode: 7976, duration: 0.168s, episode steps: 8, steps per second: 48, episode reward: 12.684, mean reward: 1.585 [0.359, 10.000], mean action: 36.000 [3.000, 94.000], mean observation: 3.156 [-1.146, 10.277], loss: 1.355589, mae: 5.004970, mean_q: 5.183564
 78169/100000: episode: 7977, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.005, mean reward: 0.401 [0.341, 0.517], mean action: 22.800 [3.000, 93.000], mean observation: 3.153 [-1.456, 10.336], loss: 1.259632, mae: 5.004375, mean_q: 5.181766
 78179/100000: episode: 7978, duration: 0.234s, episode steps: 10, steps per second: 43, episode reward: 4.138, mean reward: 0.414 [0.293, 0.496], mean action: 7.000 [2.000, 30.000], mean observation: 3.163 [-1.442, 10.291], loss: 1.285352, mae: 5.004138, mean_q: 5.178903
 78189/100000: episode: 7979, duration: 0.223s, episode steps: 10, steps per second: 45, episode reward: 3.881, mean reward: 0.388 [0.343, 0.442], mean action: 16.800 [2.000, 51.000], mean observation: 3.152 [-0.954, 10.315], loss: 1.043081, mae: 5.003256, mean_q: 5.180707
 78199/100000: episode: 7980, duration: 0.221s, episode steps: 10, steps per second: 45, episode reward: 3.988, mean reward: 0.399 [0.296, 0.489], mean action: 10.700 [2.000, 58.000], mean observation: 3.148 [-1.788, 10.350], loss: 1.257960, mae: 5.003923, mean_q: 5.182976
 78209/100000: episode: 7981, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.092, mean reward: 0.409 [0.329, 0.504], mean action: 16.800 [2.000, 91.000], mean observation: 3.160 [-1.355, 10.225], loss: 1.267831, mae: 5.003875, mean_q: 5.184535
 78219/100000: episode: 7982, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.515, mean reward: 0.451 [0.402, 0.561], mean action: 22.200 [2.000, 74.000], mean observation: 3.155 [-1.753, 10.369], loss: 1.224092, mae: 5.003798, mean_q: 5.183630
 78229/100000: episode: 7983, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.711, mean reward: 0.371 [0.333, 0.433], mean action: 33.600 [2.000, 99.000], mean observation: 3.164 [-0.964, 10.307], loss: 1.455078, mae: 5.004538, mean_q: 5.178852
 78239/100000: episode: 7984, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.775, mean reward: 0.377 [0.317, 0.576], mean action: 30.100 [2.000, 75.000], mean observation: 3.150 [-1.458, 10.497], loss: 1.294931, mae: 5.003734, mean_q: 5.173185
 78249/100000: episode: 7985, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.997, mean reward: 0.400 [0.339, 0.469], mean action: 9.100 [3.000, 46.000], mean observation: 3.158 [-1.898, 10.413], loss: 1.293711, mae: 5.003520, mean_q: 5.172405
 78259/100000: episode: 7986, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.976, mean reward: 0.398 [0.329, 0.483], mean action: 25.700 [3.000, 98.000], mean observation: 3.157 [-1.451, 10.243], loss: 1.202030, mae: 5.003103, mean_q: 5.172011
 78269/100000: episode: 7987, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.334, mean reward: 0.433 [0.372, 0.520], mean action: 38.100 [3.000, 96.000], mean observation: 3.157 [-1.557, 10.472], loss: 1.394566, mae: 5.003664, mean_q: 5.174623
 78279/100000: episode: 7988, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.396, mean reward: 0.440 [0.402, 0.485], mean action: 14.600 [3.000, 63.000], mean observation: 3.163 [-1.795, 10.353], loss: 1.217471, mae: 5.002697, mean_q: 5.174724
 78289/100000: episode: 7989, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.685, mean reward: 0.469 [0.360, 0.623], mean action: 14.400 [3.000, 60.000], mean observation: 3.163 [-1.563, 10.407], loss: 1.306985, mae: 5.002666, mean_q: 5.175123
 78299/100000: episode: 7990, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.034, mean reward: 0.403 [0.335, 0.478], mean action: 46.200 [14.000, 87.000], mean observation: 3.160 [-0.877, 10.394], loss: 0.953569, mae: 5.000960, mean_q: 5.174318
 78309/100000: episode: 7991, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.906, mean reward: 0.391 [0.303, 0.459], mean action: 41.800 [27.000, 65.000], mean observation: 3.154 [-1.000, 10.362], loss: 1.220728, mae: 5.001895, mean_q: 5.171992
 78319/100000: episode: 7992, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.489, mean reward: 0.449 [0.358, 0.516], mean action: 35.000 [27.000, 101.000], mean observation: 3.160 [-1.132, 10.328], loss: 1.185546, mae: 5.001770, mean_q: 5.172149
 78329/100000: episode: 7993, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.259, mean reward: 0.426 [0.326, 0.583], mean action: 37.700 [6.000, 90.000], mean observation: 3.158 [-1.353, 10.506], loss: 1.430518, mae: 5.002599, mean_q: 5.173025
 78339/100000: episode: 7994, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.520, mean reward: 0.452 [0.358, 0.489], mean action: 26.200 [4.000, 50.000], mean observation: 3.158 [-1.479, 10.409], loss: 1.054216, mae: 5.001306, mean_q: 5.174121
 78349/100000: episode: 7995, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.026, mean reward: 0.403 [0.312, 0.484], mean action: 40.100 [3.000, 93.000], mean observation: 3.155 [-1.391, 10.443], loss: 0.910786, mae: 5.000877, mean_q: 5.176090
 78359/100000: episode: 7996, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.717, mean reward: 0.372 [0.362, 0.387], mean action: 47.100 [4.000, 92.000], mean observation: 3.151 [-1.508, 10.332], loss: 1.385848, mae: 5.003030, mean_q: 5.178617
 78369/100000: episode: 7997, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 3.731, mean reward: 0.373 [0.302, 0.456], mean action: 33.400 [27.000, 58.000], mean observation: 3.164 [-1.305, 10.390], loss: 1.244736, mae: 5.002321, mean_q: 5.180823
 78379/100000: episode: 7998, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.285, mean reward: 0.429 [0.332, 0.486], mean action: 29.700 [18.000, 63.000], mean observation: 3.152 [-0.994, 10.308], loss: 1.297207, mae: 5.002300, mean_q: 5.181838
 78389/100000: episode: 7999, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.336, mean reward: 0.434 [0.352, 0.557], mean action: 31.900 [14.000, 54.000], mean observation: 3.144 [-1.223, 10.270], loss: 1.297779, mae: 5.002015, mean_q: 5.180239
 78399/100000: episode: 8000, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.448, mean reward: 0.445 [0.436, 0.496], mean action: 39.100 [20.000, 78.000], mean observation: 3.138 [-1.608, 10.275], loss: 1.169673, mae: 5.001520, mean_q: 5.177256
 78409/100000: episode: 8001, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.165, mean reward: 0.417 [0.339, 0.532], mean action: 48.500 [27.000, 88.000], mean observation: 3.159 [-1.310, 10.293], loss: 1.334489, mae: 5.002168, mean_q: 5.173312
 78419/100000: episode: 8002, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.200, mean reward: 0.420 [0.410, 0.448], mean action: 44.500 [26.000, 91.000], mean observation: 3.159 [-1.263, 10.381], loss: 1.299772, mae: 5.001596, mean_q: 5.172120
 78429/100000: episode: 8003, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.106, mean reward: 0.411 [0.321, 0.525], mean action: 38.100 [2.000, 74.000], mean observation: 3.172 [-1.475, 10.195], loss: 1.846950, mae: 5.003730, mean_q: 5.172109
 78439/100000: episode: 8004, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.332, mean reward: 0.433 [0.430, 0.441], mean action: 37.000 [3.000, 85.000], mean observation: 3.157 [-1.870, 10.201], loss: 1.250364, mae: 5.001307, mean_q: 5.169880
 78449/100000: episode: 8005, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.260, mean reward: 0.426 [0.317, 0.505], mean action: 40.500 [1.000, 100.000], mean observation: 3.149 [-2.071, 10.307], loss: 1.356125, mae: 5.001434, mean_q: 5.169466
 78450/100000: episode: 8006, duration: 0.022s, episode steps: 1, steps per second: 45, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 36.000 [36.000, 36.000], mean observation: 3.146 [-1.953, 10.276], loss: 1.071794, mae: 5.000124, mean_q: 5.169752
 78460/100000: episode: 8007, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.049, mean reward: 0.405 [0.358, 0.494], mean action: 31.400 [27.000, 71.000], mean observation: 3.161 [-1.143, 10.219], loss: 1.447353, mae: 5.001452, mean_q: 5.169746
 78470/100000: episode: 8008, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.941, mean reward: 0.394 [0.316, 0.488], mean action: 43.700 [27.000, 92.000], mean observation: 3.163 [-1.598, 10.330], loss: 1.096520, mae: 4.999824, mean_q: 5.164871
 78480/100000: episode: 8009, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.359, mean reward: 0.436 [0.319, 0.558], mean action: 25.800 [2.000, 83.000], mean observation: 3.156 [-1.497, 10.219], loss: 1.207213, mae: 5.000189, mean_q: 5.162623
 78490/100000: episode: 8010, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.205, mean reward: 0.421 [0.322, 0.504], mean action: 36.500 [2.000, 94.000], mean observation: 3.157 [-2.080, 10.326], loss: 1.348638, mae: 5.000609, mean_q: 5.164349
 78500/100000: episode: 8011, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.276, mean reward: 0.428 [0.312, 0.556], mean action: 37.900 [2.000, 82.000], mean observation: 3.154 [-2.072, 10.412], loss: 1.165040, mae: 4.999885, mean_q: 5.166247
 78510/100000: episode: 8012, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.952, mean reward: 0.495 [0.431, 0.539], mean action: 36.700 [2.000, 67.000], mean observation: 3.157 [-1.397, 10.299], loss: 1.129154, mae: 4.999817, mean_q: 5.165214
 78520/100000: episode: 8013, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.411, mean reward: 0.441 [0.436, 0.485], mean action: 43.500 [27.000, 98.000], mean observation: 3.169 [-0.926, 10.377], loss: 1.210913, mae: 5.000551, mean_q: 5.164966
 78530/100000: episode: 8014, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.402, mean reward: 0.440 [0.356, 0.570], mean action: 43.800 [11.000, 86.000], mean observation: 3.157 [-1.532, 10.397], loss: 1.173807, mae: 5.000164, mean_q: 5.161507
 78540/100000: episode: 8015, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.326, mean reward: 0.433 [0.357, 0.559], mean action: 43.200 [27.000, 69.000], mean observation: 3.164 [-1.028, 10.289], loss: 1.210300, mae: 5.000003, mean_q: 5.159945
 78550/100000: episode: 8016, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.369, mean reward: 0.437 [0.417, 0.525], mean action: 28.800 [2.000, 56.000], mean observation: 3.162 [-1.295, 10.381], loss: 1.081716, mae: 4.999547, mean_q: 5.159434
 78560/100000: episode: 8017, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.354, mean reward: 0.435 [0.389, 0.497], mean action: 42.300 [10.000, 92.000], mean observation: 3.158 [-1.377, 10.281], loss: 1.368997, mae: 5.000742, mean_q: 5.160474
 78570/100000: episode: 8018, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.209, mean reward: 0.421 [0.362, 0.497], mean action: 36.800 [4.000, 91.000], mean observation: 3.151 [-0.994, 10.425], loss: 1.522278, mae: 5.001396, mean_q: 5.161910
 78580/100000: episode: 8019, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.897, mean reward: 0.390 [0.347, 0.564], mean action: 34.700 [14.000, 99.000], mean observation: 3.156 [-1.667, 10.605], loss: 1.343641, mae: 5.000346, mean_q: 5.159822
 78590/100000: episode: 8020, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.896, mean reward: 0.390 [0.299, 0.455], mean action: 31.000 [1.000, 76.000], mean observation: 3.160 [-1.436, 10.415], loss: 1.088038, mae: 4.999005, mean_q: 5.158127
 78600/100000: episode: 8021, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.467, mean reward: 0.447 [0.373, 0.525], mean action: 30.100 [15.000, 51.000], mean observation: 3.160 [-1.236, 10.385], loss: 1.185980, mae: 4.999197, mean_q: 5.156531
 78610/100000: episode: 8022, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.238, mean reward: 0.424 [0.423, 0.428], mean action: 40.800 [27.000, 100.000], mean observation: 3.164 [-1.080, 10.218], loss: 1.238175, mae: 4.999410, mean_q: 5.158007
 78620/100000: episode: 8023, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.794, mean reward: 0.379 [0.324, 0.442], mean action: 35.900 [3.000, 98.000], mean observation: 3.155 [-1.227, 10.284], loss: 1.286433, mae: 4.999522, mean_q: 5.159568
 78630/100000: episode: 8024, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.016, mean reward: 0.402 [0.341, 0.438], mean action: 21.700 [1.000, 55.000], mean observation: 3.159 [-2.241, 10.271], loss: 1.270991, mae: 4.999549, mean_q: 5.156499
 78640/100000: episode: 8025, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.734, mean reward: 0.373 [0.337, 0.449], mean action: 59.000 [7.000, 97.000], mean observation: 3.156 [-1.627, 10.238], loss: 1.056380, mae: 4.998360, mean_q: 5.153658
 78650/100000: episode: 8026, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.288, mean reward: 0.429 [0.390, 0.497], mean action: 56.500 [31.000, 79.000], mean observation: 3.149 [-2.080, 10.342], loss: 0.876822, mae: 4.997828, mean_q: 5.154645
 78660/100000: episode: 8027, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.681, mean reward: 0.468 [0.465, 0.483], mean action: 63.300 [20.000, 99.000], mean observation: 3.143 [-1.564, 10.244], loss: 1.172098, mae: 4.999186, mean_q: 5.155083
 78670/100000: episode: 8028, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.256, mean reward: 0.426 [0.349, 0.481], mean action: 51.500 [15.000, 93.000], mean observation: 3.153 [-1.598, 10.300], loss: 1.001540, mae: 4.998652, mean_q: 5.155704
 78680/100000: episode: 8029, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.814, mean reward: 0.481 [0.481, 0.481], mean action: 56.900 [7.000, 92.000], mean observation: 3.175 [-1.528, 10.384], loss: 1.104379, mae: 4.999070, mean_q: 5.156597
 78685/100000: episode: 8030, duration: 0.102s, episode steps: 5, steps per second: 49, episode reward: 11.667, mean reward: 2.333 [0.363, 10.000], mean action: 42.200 [10.000, 63.000], mean observation: 3.141 [-1.529, 10.351], loss: 1.107597, mae: 4.999215, mean_q: 5.157457
 78695/100000: episode: 8031, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 3.738, mean reward: 0.374 [0.320, 0.427], mean action: 59.000 [18.000, 99.000], mean observation: 3.155 [-2.239, 10.361], loss: 1.099085, mae: 4.999189, mean_q: 5.158137
 78705/100000: episode: 8032, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.298, mean reward: 0.430 [0.379, 0.507], mean action: 60.000 [6.000, 87.000], mean observation: 3.162 [-1.138, 10.299], loss: 1.106465, mae: 4.999210, mean_q: 5.155914
 78715/100000: episode: 8033, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.478, mean reward: 0.348 [0.292, 0.464], mean action: 60.600 [4.000, 101.000], mean observation: 3.152 [-1.268, 10.270], loss: 1.379260, mae: 5.000316, mean_q: 5.149759
 78725/100000: episode: 8034, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.116, mean reward: 0.412 [0.294, 0.547], mean action: 32.600 [2.000, 65.000], mean observation: 3.154 [-1.852, 10.322], loss: 1.307296, mae: 4.999816, mean_q: 5.146929
 78735/100000: episode: 8035, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.587, mean reward: 0.459 [0.439, 0.572], mean action: 38.700 [10.000, 81.000], mean observation: 3.165 [-1.501, 10.485], loss: 1.099575, mae: 4.998868, mean_q: 5.145393
 78745/100000: episode: 8036, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.261, mean reward: 0.426 [0.359, 0.482], mean action: 35.500 [16.000, 87.000], mean observation: 3.171 [-1.524, 10.498], loss: 1.162400, mae: 4.999230, mean_q: 5.144285
 78755/100000: episode: 8037, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.796, mean reward: 0.380 [0.311, 0.512], mean action: 53.300 [4.000, 93.000], mean observation: 3.144 [-1.081, 10.389], loss: 1.270911, mae: 4.999694, mean_q: 5.144795
 78765/100000: episode: 8038, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.179, mean reward: 0.418 [0.323, 0.531], mean action: 27.000 [3.000, 35.000], mean observation: 3.150 [-2.074, 10.269], loss: 1.142545, mae: 4.998870, mean_q: 5.146030
 78775/100000: episode: 8039, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.668, mean reward: 0.367 [0.325, 0.440], mean action: 38.900 [27.000, 90.000], mean observation: 3.156 [-1.446, 10.353], loss: 1.546311, mae: 5.000170, mean_q: 5.147644
 78785/100000: episode: 8040, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.383, mean reward: 0.438 [0.353, 0.534], mean action: 43.900 [18.000, 86.000], mean observation: 3.171 [-1.136, 10.477], loss: 1.353391, mae: 4.999066, mean_q: 5.149083
 78795/100000: episode: 8041, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.358, mean reward: 0.436 [0.295, 0.558], mean action: 39.900 [1.000, 98.000], mean observation: 3.162 [-1.377, 10.604], loss: 0.967567, mae: 4.997456, mean_q: 5.150937
 78805/100000: episode: 8042, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.836, mean reward: 0.384 [0.316, 0.559], mean action: 49.000 [27.000, 99.000], mean observation: 3.155 [-1.477, 10.528], loss: 0.894195, mae: 4.997264, mean_q: 5.152502
 78815/100000: episode: 8043, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.343, mean reward: 0.434 [0.407, 0.533], mean action: 42.400 [26.000, 97.000], mean observation: 3.156 [-1.301, 10.272], loss: 1.182426, mae: 4.998448, mean_q: 5.154014
 78825/100000: episode: 8044, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.208, mean reward: 0.421 [0.371, 0.529], mean action: 27.400 [13.000, 44.000], mean observation: 3.141 [-1.725, 10.231], loss: 1.495842, mae: 4.999655, mean_q: 5.155580
 78835/100000: episode: 8045, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.135, mean reward: 0.414 [0.342, 0.511], mean action: 55.200 [27.000, 99.000], mean observation: 3.151 [-1.206, 10.245], loss: 1.206914, mae: 4.998196, mean_q: 5.157570
 78845/100000: episode: 8046, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.086, mean reward: 0.409 [0.373, 0.478], mean action: 33.700 [27.000, 93.000], mean observation: 3.159 [-1.594, 10.326], loss: 1.418341, mae: 4.998847, mean_q: 5.155720
 78855/100000: episode: 8047, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 13.538, mean reward: 1.354 [0.344, 10.000], mean action: 39.400 [24.000, 88.000], mean observation: 3.145 [-1.952, 10.311], loss: 1.687542, mae: 4.999503, mean_q: 5.154405
 78865/100000: episode: 8048, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.728, mean reward: 0.473 [0.472, 0.479], mean action: 31.800 [27.000, 75.000], mean observation: 3.138 [-1.282, 10.318], loss: 1.237396, mae: 4.996910, mean_q: 5.154603
 78875/100000: episode: 8049, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.916, mean reward: 0.492 [0.492, 0.492], mean action: 28.100 [13.000, 53.000], mean observation: 3.158 [-1.224, 10.438], loss: 1.383911, mae: 4.997035, mean_q: 5.155838
 78885/100000: episode: 8050, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.777, mean reward: 0.378 [0.315, 0.484], mean action: 40.300 [18.000, 101.000], mean observation: 3.155 [-1.415, 10.427], loss: 1.557608, mae: 4.997182, mean_q: 5.156989
 78895/100000: episode: 8051, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.866, mean reward: 0.387 [0.318, 0.497], mean action: 30.400 [14.000, 59.000], mean observation: 3.159 [-1.459, 10.418], loss: 1.520783, mae: 4.996464, mean_q: 5.154241
 78905/100000: episode: 8052, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.197, mean reward: 0.420 [0.399, 0.440], mean action: 30.900 [4.000, 74.000], mean observation: 3.167 [-1.350, 10.281], loss: 1.147448, mae: 4.994683, mean_q: 5.154145
 78915/100000: episode: 8053, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.513, mean reward: 0.451 [0.372, 0.524], mean action: 39.900 [1.000, 100.000], mean observation: 3.148 [-1.379, 10.402], loss: 1.086197, mae: 4.994076, mean_q: 5.154901
 78925/100000: episode: 8054, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.676, mean reward: 0.468 [0.394, 0.515], mean action: 34.000 [1.000, 95.000], mean observation: 3.147 [-1.598, 10.398], loss: 1.073199, mae: 4.994122, mean_q: 5.155986
 78935/100000: episode: 8055, duration: 0.157s, episode steps: 10, steps per second: 63, episode reward: 4.858, mean reward: 0.486 [0.374, 0.571], mean action: 45.300 [27.000, 91.000], mean observation: 3.166 [-1.860, 10.299], loss: 0.978242, mae: 4.993901, mean_q: 5.158675
 78945/100000: episode: 8056, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.911, mean reward: 0.391 [0.351, 0.495], mean action: 37.500 [27.000, 68.000], mean observation: 3.163 [-1.160, 10.328], loss: 1.444696, mae: 4.995526, mean_q: 5.162203
 78955/100000: episode: 8057, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.995, mean reward: 0.400 [0.325, 0.461], mean action: 45.700 [27.000, 92.000], mean observation: 3.159 [-1.438, 10.164], loss: 1.331525, mae: 4.995150, mean_q: 5.165261
 78965/100000: episode: 8058, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.994, mean reward: 0.399 [0.327, 0.496], mean action: 44.900 [7.000, 97.000], mean observation: 3.162 [-2.245, 10.468], loss: 1.003549, mae: 4.993501, mean_q: 5.165013
 78975/100000: episode: 8059, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.781, mean reward: 0.378 [0.335, 0.456], mean action: 48.500 [14.000, 98.000], mean observation: 3.166 [-1.502, 10.135], loss: 1.214483, mae: 4.994261, mean_q: 5.165481
 78985/100000: episode: 8060, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.891, mean reward: 0.389 [0.372, 0.440], mean action: 44.800 [21.000, 100.000], mean observation: 3.160 [-1.598, 10.467], loss: 1.223868, mae: 4.994153, mean_q: 5.163301
 78995/100000: episode: 8061, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.959, mean reward: 0.396 [0.373, 0.430], mean action: 34.800 [0.000, 94.000], mean observation: 3.147 [-1.085, 10.357], loss: 1.114189, mae: 4.993569, mean_q: 5.163286
 79005/100000: episode: 8062, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.915, mean reward: 0.391 [0.339, 0.528], mean action: 39.500 [1.000, 94.000], mean observation: 3.162 [-1.369, 10.390], loss: 1.105338, mae: 4.993471, mean_q: 5.161313
 79015/100000: episode: 8063, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.865, mean reward: 0.387 [0.332, 0.494], mean action: 48.600 [27.000, 88.000], mean observation: 3.139 [-1.489, 10.463], loss: 1.160679, mae: 4.993459, mean_q: 5.161035
 79025/100000: episode: 8064, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.877, mean reward: 0.388 [0.342, 0.457], mean action: 37.100 [13.000, 88.000], mean observation: 3.157 [-1.335, 10.316], loss: 1.015809, mae: 4.992687, mean_q: 5.163080
 79035/100000: episode: 8065, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.110, mean reward: 0.411 [0.321, 0.521], mean action: 35.500 [15.000, 101.000], mean observation: 3.160 [-1.786, 10.328], loss: 1.077365, mae: 4.993306, mean_q: 5.165580
 79045/100000: episode: 8066, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.056, mean reward: 0.406 [0.384, 0.525], mean action: 40.400 [19.000, 92.000], mean observation: 3.151 [-1.162, 10.212], loss: 1.047609, mae: 4.993493, mean_q: 5.167460
 79055/100000: episode: 8067, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.573, mean reward: 0.457 [0.402, 0.520], mean action: 38.700 [27.000, 95.000], mean observation: 3.163 [-1.368, 10.257], loss: 1.180577, mae: 4.994064, mean_q: 5.168818
 79065/100000: episode: 8068, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.786, mean reward: 0.379 [0.335, 0.505], mean action: 42.000 [27.000, 93.000], mean observation: 3.166 [-1.264, 10.367], loss: 1.418780, mae: 4.994950, mean_q: 5.164067
 79075/100000: episode: 8069, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.982, mean reward: 0.398 [0.329, 0.471], mean action: 34.300 [27.000, 90.000], mean observation: 3.159 [-1.873, 10.566], loss: 1.147475, mae: 4.993472, mean_q: 5.162903
 79085/100000: episode: 8070, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.342, mean reward: 0.434 [0.416, 0.483], mean action: 23.900 [7.000, 49.000], mean observation: 3.144 [-1.728, 10.340], loss: 1.016643, mae: 4.993110, mean_q: 5.163978
 79095/100000: episode: 8071, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.725, mean reward: 0.373 [0.309, 0.441], mean action: 41.100 [6.000, 90.000], mean observation: 3.155 [-1.571, 10.329], loss: 1.160243, mae: 4.993601, mean_q: 5.165734
 79105/100000: episode: 8072, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.093, mean reward: 0.409 [0.302, 0.530], mean action: 26.100 [12.000, 43.000], mean observation: 3.153 [-1.290, 10.328], loss: 1.552967, mae: 4.995390, mean_q: 5.166940
 79115/100000: episode: 8073, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.079, mean reward: 0.408 [0.364, 0.477], mean action: 42.600 [2.000, 95.000], mean observation: 3.171 [-1.287, 10.307], loss: 1.273757, mae: 4.993943, mean_q: 5.165376
 79125/100000: episode: 8074, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.164, mean reward: 0.416 [0.353, 0.514], mean action: 36.700 [20.000, 90.000], mean observation: 3.149 [-1.673, 10.235], loss: 1.191811, mae: 4.993320, mean_q: 5.165129
 79135/100000: episode: 8075, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.190, mean reward: 0.419 [0.264, 0.535], mean action: 34.300 [11.000, 70.000], mean observation: 3.142 [-1.789, 10.339], loss: 1.448540, mae: 4.993903, mean_q: 5.163741
 79145/100000: episode: 8076, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.041, mean reward: 0.404 [0.352, 0.525], mean action: 39.700 [2.000, 88.000], mean observation: 3.144 [-1.423, 10.365], loss: 1.179472, mae: 4.992718, mean_q: 5.159119
 79155/100000: episode: 8077, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.837, mean reward: 0.384 [0.329, 0.485], mean action: 29.200 [12.000, 64.000], mean observation: 3.155 [-1.429, 10.431], loss: 0.984801, mae: 4.991764, mean_q: 5.155611
 79165/100000: episode: 8078, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.963, mean reward: 0.396 [0.332, 0.511], mean action: 40.600 [7.000, 100.000], mean observation: 3.173 [-1.619, 10.374], loss: 1.281816, mae: 4.993014, mean_q: 5.153073
 79175/100000: episode: 8079, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.266, mean reward: 0.427 [0.363, 0.472], mean action: 64.000 [26.000, 93.000], mean observation: 3.156 [-1.961, 10.275], loss: 1.416368, mae: 4.993329, mean_q: 5.153676
 79185/100000: episode: 8080, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 4.444, mean reward: 0.444 [0.325, 0.555], mean action: 59.000 [20.000, 97.000], mean observation: 3.148 [-1.738, 10.276], loss: 1.282154, mae: 4.992599, mean_q: 5.155125
 79195/100000: episode: 8081, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.104, mean reward: 0.410 [0.330, 0.478], mean action: 56.100 [13.000, 100.000], mean observation: 3.158 [-1.042, 10.227], loss: 1.381600, mae: 4.992610, mean_q: 5.157411
 79205/100000: episode: 8082, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.796, mean reward: 0.380 [0.361, 0.421], mean action: 55.000 [22.000, 87.000], mean observation: 3.146 [-0.852, 10.297], loss: 1.133835, mae: 4.991413, mean_q: 5.158849
 79215/100000: episode: 8083, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.304, mean reward: 0.430 [0.316, 0.514], mean action: 66.800 [35.000, 88.000], mean observation: 3.147 [-0.973, 10.383], loss: 1.408199, mae: 4.992466, mean_q: 5.160654
 79225/100000: episode: 8084, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.015, mean reward: 0.402 [0.391, 0.450], mean action: 67.600 [60.000, 99.000], mean observation: 3.165 [-1.681, 10.402], loss: 1.393782, mae: 4.992200, mean_q: 5.158960
 79235/100000: episode: 8085, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.441, mean reward: 0.444 [0.391, 0.525], mean action: 57.700 [26.000, 90.000], mean observation: 3.161 [-1.181, 10.343], loss: 1.303635, mae: 4.991271, mean_q: 5.157929
 79245/100000: episode: 8086, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.659, mean reward: 0.366 [0.323, 0.413], mean action: 50.200 [10.000, 74.000], mean observation: 3.162 [-1.462, 10.222], loss: 1.100705, mae: 4.990148, mean_q: 5.151297
 79255/100000: episode: 8087, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.613, mean reward: 0.461 [0.424, 0.465], mean action: 40.100 [3.000, 67.000], mean observation: 3.160 [-1.212, 10.394], loss: 0.943267, mae: 4.989329, mean_q: 5.146438
 79265/100000: episode: 8088, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.241, mean reward: 0.424 [0.356, 0.548], mean action: 53.500 [3.000, 95.000], mean observation: 3.156 [-1.680, 10.390], loss: 0.903251, mae: 4.989522, mean_q: 5.143869
 79275/100000: episode: 8089, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.787, mean reward: 0.379 [0.335, 0.435], mean action: 67.500 [53.000, 94.000], mean observation: 3.151 [-1.011, 10.219], loss: 0.964667, mae: 4.989890, mean_q: 5.141045
 79285/100000: episode: 8090, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.888, mean reward: 0.389 [0.306, 0.458], mean action: 51.600 [9.000, 73.000], mean observation: 3.149 [-1.446, 10.334], loss: 1.118416, mae: 4.990773, mean_q: 5.140607
 79295/100000: episode: 8091, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.218, mean reward: 0.422 [0.378, 0.485], mean action: 57.100 [17.000, 101.000], mean observation: 3.153 [-1.565, 10.333], loss: 1.000517, mae: 4.990255, mean_q: 5.137795
 79305/100000: episode: 8092, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.977, mean reward: 0.398 [0.346, 0.506], mean action: 46.000 [18.000, 97.000], mean observation: 3.143 [-1.189, 10.227], loss: 1.094020, mae: 4.990520, mean_q: 5.137539
 79315/100000: episode: 8093, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.325, mean reward: 0.433 [0.338, 0.573], mean action: 44.200 [0.000, 94.000], mean observation: 3.157 [-1.308, 10.284], loss: 1.265493, mae: 4.991118, mean_q: 5.139288
 79325/100000: episode: 8094, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.975, mean reward: 0.397 [0.366, 0.468], mean action: 31.700 [5.000, 80.000], mean observation: 3.156 [-1.594, 10.359], loss: 1.404119, mae: 4.991552, mean_q: 5.141592
 79335/100000: episode: 8095, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.653, mean reward: 0.365 [0.314, 0.404], mean action: 44.400 [27.000, 101.000], mean observation: 3.167 [-1.634, 10.336], loss: 1.063077, mae: 4.989905, mean_q: 5.141000
 79345/100000: episode: 8096, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.260, mean reward: 0.426 [0.332, 0.558], mean action: 35.000 [18.000, 96.000], mean observation: 3.160 [-1.249, 10.335], loss: 1.175176, mae: 4.990154, mean_q: 5.141479
 79355/100000: episode: 8097, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.846, mean reward: 0.485 [0.333, 0.582], mean action: 44.100 [3.000, 85.000], mean observation: 3.157 [-1.212, 10.334], loss: 1.337887, mae: 4.990600, mean_q: 5.142784
 79365/100000: episode: 8098, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.173, mean reward: 0.417 [0.335, 0.507], mean action: 37.300 [16.000, 89.000], mean observation: 3.160 [-1.069, 10.537], loss: 1.141336, mae: 4.989455, mean_q: 5.144042
 79375/100000: episode: 8099, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.646, mean reward: 0.365 [0.310, 0.454], mean action: 31.800 [2.000, 94.000], mean observation: 3.157 [-1.270, 10.398], loss: 1.315351, mae: 4.989981, mean_q: 5.144623
 79385/100000: episode: 8100, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.757, mean reward: 0.476 [0.374, 0.544], mean action: 32.000 [18.000, 68.000], mean observation: 3.167 [-1.386, 10.390], loss: 1.008829, mae: 4.988633, mean_q: 5.145632
 79395/100000: episode: 8101, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.039, mean reward: 0.404 [0.361, 0.476], mean action: 36.900 [13.000, 91.000], mean observation: 3.160 [-1.025, 10.378], loss: 1.371236, mae: 4.989941, mean_q: 5.146870
 79405/100000: episode: 8102, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.131, mean reward: 0.413 [0.278, 0.505], mean action: 28.300 [18.000, 87.000], mean observation: 3.157 [-1.726, 10.344], loss: 1.604779, mae: 4.990479, mean_q: 5.148364
 79415/100000: episode: 8103, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.368, mean reward: 0.437 [0.385, 0.550], mean action: 29.000 [18.000, 78.000], mean observation: 3.152 [-1.836, 10.218], loss: 1.266499, mae: 4.988760, mean_q: 5.149469
 79423/100000: episode: 8104, duration: 0.147s, episode steps: 8, steps per second: 54, episode reward: 12.871, mean reward: 1.609 [0.358, 10.000], mean action: 27.875 [2.000, 94.000], mean observation: 3.149 [-1.412, 10.401], loss: 1.525475, mae: 4.989203, mean_q: 5.150444
 79433/100000: episode: 8105, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.035, mean reward: 0.403 [0.339, 0.459], mean action: 40.100 [8.000, 97.000], mean observation: 3.165 [-1.396, 10.413], loss: 1.018244, mae: 4.986814, mean_q: 5.150519
 79443/100000: episode: 8106, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.280, mean reward: 0.428 [0.422, 0.440], mean action: 29.200 [10.000, 60.000], mean observation: 3.151 [-1.348, 10.324], loss: 1.049132, mae: 4.986802, mean_q: 5.146549
 79453/100000: episode: 8107, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.154, mean reward: 0.415 [0.387, 0.471], mean action: 39.300 [18.000, 92.000], mean observation: 3.162 [-1.831, 10.377], loss: 1.481418, mae: 4.988425, mean_q: 5.146201
 79463/100000: episode: 8108, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.273, mean reward: 0.427 [0.366, 0.559], mean action: 51.000 [8.000, 96.000], mean observation: 3.168 [-1.383, 10.469], loss: 1.106659, mae: 4.986636, mean_q: 5.147117
 79473/100000: episode: 8109, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.861, mean reward: 0.386 [0.297, 0.438], mean action: 29.800 [7.000, 98.000], mean observation: 3.155 [-1.249, 10.303], loss: 1.256917, mae: 4.987244, mean_q: 5.149134
 79483/100000: episode: 8110, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.205, mean reward: 0.420 [0.376, 0.504], mean action: 28.700 [15.000, 92.000], mean observation: 3.159 [-1.109, 10.377], loss: 1.313460, mae: 4.987454, mean_q: 5.150361
 79493/100000: episode: 8111, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.711, mean reward: 0.371 [0.339, 0.429], mean action: 58.700 [16.000, 99.000], mean observation: 3.161 [-1.667, 10.260], loss: 1.199570, mae: 4.986989, mean_q: 5.148524
 79503/100000: episode: 8112, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.067, mean reward: 0.407 [0.340, 0.526], mean action: 44.400 [8.000, 85.000], mean observation: 3.158 [-0.901, 10.566], loss: 1.144749, mae: 4.986634, mean_q: 5.147204
 79513/100000: episode: 8113, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.239, mean reward: 0.424 [0.312, 0.493], mean action: 30.000 [8.000, 64.000], mean observation: 3.155 [-1.257, 10.364], loss: 1.439823, mae: 4.987758, mean_q: 5.147694
 79523/100000: episode: 8114, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.057, mean reward: 0.406 [0.355, 0.465], mean action: 43.000 [17.000, 92.000], mean observation: 3.160 [-1.133, 10.315], loss: 1.337623, mae: 4.987203, mean_q: 5.153300
 79533/100000: episode: 8115, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.900, mean reward: 0.390 [0.288, 0.558], mean action: 37.200 [27.000, 87.000], mean observation: 3.151 [-1.549, 10.253], loss: 1.280958, mae: 4.986807, mean_q: 5.153739
 79543/100000: episode: 8116, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.345, mean reward: 0.434 [0.355, 0.545], mean action: 31.700 [18.000, 76.000], mean observation: 3.158 [-0.967, 10.495], loss: 1.296723, mae: 4.986674, mean_q: 5.153121
 79548/100000: episode: 8117, duration: 0.096s, episode steps: 5, steps per second: 52, episode reward: 11.728, mean reward: 2.346 [0.338, 10.000], mean action: 35.600 [18.000, 94.000], mean observation: 3.153 [-1.235, 10.437], loss: 1.449464, mae: 4.987158, mean_q: 5.151808
 79558/100000: episode: 8118, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.936, mean reward: 0.394 [0.370, 0.448], mean action: 42.100 [4.000, 101.000], mean observation: 3.158 [-2.169, 10.365], loss: 1.168712, mae: 4.985795, mean_q: 5.150716
 79568/100000: episode: 8119, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.880, mean reward: 0.388 [0.340, 0.495], mean action: 42.000 [20.000, 95.000], mean observation: 3.150 [-1.291, 10.457], loss: 0.945310, mae: 4.984771, mean_q: 5.150566
 79578/100000: episode: 8120, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.027, mean reward: 0.403 [0.318, 0.486], mean action: 34.600 [2.000, 80.000], mean observation: 3.160 [-1.604, 10.279], loss: 1.300222, mae: 4.986188, mean_q: 5.149398
 79588/100000: episode: 8121, duration: 0.215s, episode steps: 10, steps per second: 47, episode reward: 3.901, mean reward: 0.390 [0.326, 0.465], mean action: 29.900 [7.000, 67.000], mean observation: 3.147 [-1.571, 10.285], loss: 1.420798, mae: 4.986554, mean_q: 5.146677
 79598/100000: episode: 8122, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.015, mean reward: 0.401 [0.306, 0.483], mean action: 36.100 [18.000, 82.000], mean observation: 3.155 [-1.764, 10.330], loss: 1.313223, mae: 4.985690, mean_q: 5.145872
 79608/100000: episode: 8123, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.085, mean reward: 0.408 [0.346, 0.550], mean action: 28.000 [8.000, 76.000], mean observation: 3.155 [-1.559, 10.386], loss: 1.118445, mae: 4.985074, mean_q: 5.147577
 79618/100000: episode: 8124, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.484, mean reward: 0.448 [0.439, 0.497], mean action: 30.900 [9.000, 67.000], mean observation: 3.162 [-1.748, 10.268], loss: 1.437233, mae: 4.986232, mean_q: 5.149165
 79628/100000: episode: 8125, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.033, mean reward: 0.403 [0.322, 0.488], mean action: 24.300 [2.000, 60.000], mean observation: 3.160 [-1.066, 10.405], loss: 1.378069, mae: 4.985515, mean_q: 5.150321
 79638/100000: episode: 8126, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.488, mean reward: 0.449 [0.363, 0.497], mean action: 55.000 [11.000, 95.000], mean observation: 3.147 [-1.201, 10.266], loss: 1.433610, mae: 4.985384, mean_q: 5.151864
 79648/100000: episode: 8127, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.727, mean reward: 0.373 [0.341, 0.422], mean action: 34.600 [3.000, 83.000], mean observation: 3.149 [-1.183, 10.317], loss: 1.244538, mae: 4.984194, mean_q: 5.153197
 79658/100000: episode: 8128, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.644, mean reward: 0.464 [0.403, 0.549], mean action: 31.900 [18.000, 95.000], mean observation: 3.172 [-1.712, 10.263], loss: 1.544122, mae: 4.984792, mean_q: 5.154387
 79668/100000: episode: 8129, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 5.096, mean reward: 0.510 [0.510, 0.510], mean action: 49.200 [18.000, 99.000], mean observation: 3.152 [-1.377, 10.331], loss: 1.081938, mae: 4.982722, mean_q: 5.156008
 79678/100000: episode: 8130, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.311, mean reward: 0.431 [0.399, 0.592], mean action: 23.000 [0.000, 86.000], mean observation: 3.153 [-1.165, 10.443], loss: 1.268818, mae: 4.983277, mean_q: 5.153522
 79688/100000: episode: 8131, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.085, mean reward: 0.409 [0.331, 0.494], mean action: 34.000 [8.000, 100.000], mean observation: 3.158 [-1.138, 10.310], loss: 1.344382, mae: 4.983401, mean_q: 5.152493
 79698/100000: episode: 8132, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 4.070, mean reward: 0.407 [0.320, 0.484], mean action: 34.100 [11.000, 82.000], mean observation: 3.154 [-1.319, 10.184], loss: 1.237878, mae: 4.983099, mean_q: 5.153649
 79708/100000: episode: 8133, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.115, mean reward: 0.411 [0.373, 0.553], mean action: 42.400 [7.000, 98.000], mean observation: 3.155 [-1.383, 10.293], loss: 1.362818, mae: 4.983224, mean_q: 5.154517
 79718/100000: episode: 8134, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.617, mean reward: 0.462 [0.390, 0.541], mean action: 43.500 [6.000, 90.000], mean observation: 3.164 [-1.106, 10.387], loss: 1.080013, mae: 4.981849, mean_q: 5.155447
 79728/100000: episode: 8135, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.621, mean reward: 0.462 [0.348, 0.534], mean action: 40.200 [18.000, 87.000], mean observation: 3.159 [-1.172, 10.398], loss: 1.153108, mae: 4.982296, mean_q: 5.156458
 79738/100000: episode: 8136, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.356, mean reward: 0.436 [0.373, 0.490], mean action: 31.400 [8.000, 94.000], mean observation: 3.150 [-1.168, 10.375], loss: 1.344581, mae: 4.983191, mean_q: 5.156966
 79748/100000: episode: 8137, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.081, mean reward: 0.408 [0.316, 0.528], mean action: 34.200 [7.000, 68.000], mean observation: 3.164 [-2.212, 10.425], loss: 1.443161, mae: 4.983840, mean_q: 5.157899
 79749/100000: episode: 8138, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 79.000 [79.000, 79.000], mean observation: 3.186 [-0.599, 10.647], loss: 2.690561, mae: 4.988431, mean_q: 5.158504
 79759/100000: episode: 8139, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.047, mean reward: 0.405 [0.355, 0.472], mean action: 33.400 [8.000, 83.000], mean observation: 3.158 [-1.005, 10.445], loss: 1.213610, mae: 4.982314, mean_q: 5.159163
 79769/100000: episode: 8140, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.921, mean reward: 0.392 [0.328, 0.446], mean action: 31.900 [18.000, 74.000], mean observation: 3.165 [-1.449, 10.390], loss: 1.192169, mae: 4.981982, mean_q: 5.160944
 79779/100000: episode: 8141, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.360, mean reward: 0.436 [0.389, 0.589], mean action: 38.300 [4.000, 100.000], mean observation: 3.157 [-2.136, 10.358], loss: 1.183262, mae: 4.981908, mean_q: 5.160435
 79789/100000: episode: 8142, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.852, mean reward: 0.385 [0.299, 0.498], mean action: 30.900 [0.000, 99.000], mean observation: 3.148 [-0.918, 10.364], loss: 1.607587, mae: 4.983603, mean_q: 5.153285
 79799/100000: episode: 8143, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.381, mean reward: 0.438 [0.369, 0.521], mean action: 41.700 [5.000, 100.000], mean observation: 3.163 [-1.661, 10.298], loss: 1.060772, mae: 4.981084, mean_q: 5.151856
 79809/100000: episode: 8144, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 3.849, mean reward: 0.385 [0.328, 0.498], mean action: 32.800 [30.000, 58.000], mean observation: 3.143 [-1.237, 10.303], loss: 1.036082, mae: 4.980748, mean_q: 5.152375
 79819/100000: episode: 8145, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.168, mean reward: 0.417 [0.374, 0.459], mean action: 23.800 [6.000, 30.000], mean observation: 3.157 [-1.499, 10.357], loss: 1.478128, mae: 4.982481, mean_q: 5.152971
 79829/100000: episode: 8146, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 3.802, mean reward: 0.380 [0.288, 0.509], mean action: 34.600 [4.000, 70.000], mean observation: 3.152 [-1.255, 10.203], loss: 1.175224, mae: 4.980933, mean_q: 5.153369
 79839/100000: episode: 8147, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.135, mean reward: 0.413 [0.402, 0.452], mean action: 40.800 [3.000, 80.000], mean observation: 3.142 [-1.943, 10.343], loss: 0.972062, mae: 4.980274, mean_q: 5.153798
 79849/100000: episode: 8148, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.895, mean reward: 0.389 [0.327, 0.504], mean action: 42.400 [13.000, 88.000], mean observation: 3.155 [-1.430, 10.309], loss: 0.902057, mae: 4.980235, mean_q: 5.155388
 79859/100000: episode: 8149, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.286, mean reward: 0.429 [0.387, 0.463], mean action: 55.100 [7.000, 97.000], mean observation: 3.155 [-1.267, 10.344], loss: 1.223885, mae: 4.981717, mean_q: 5.157239
 79869/100000: episode: 8150, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 3.919, mean reward: 0.392 [0.291, 0.476], mean action: 27.600 [0.000, 62.000], mean observation: 3.163 [-1.529, 10.339], loss: 1.094648, mae: 4.981272, mean_q: 5.158145
 79879/100000: episode: 8151, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.932, mean reward: 0.393 [0.321, 0.466], mean action: 49.600 [10.000, 100.000], mean observation: 3.160 [-0.780, 10.437], loss: 1.239865, mae: 4.981852, mean_q: 5.158799
 79889/100000: episode: 8152, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.524, mean reward: 0.352 [0.270, 0.449], mean action: 30.500 [0.000, 73.000], mean observation: 3.164 [-1.156, 10.303], loss: 1.205120, mae: 4.981629, mean_q: 5.159176
 79890/100000: episode: 8153, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 30.000 [30.000, 30.000], mean observation: 3.177 [-0.661, 10.406], loss: 0.432035, mae: 4.978889, mean_q: 5.159585
 79900/100000: episode: 8154, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 5.139, mean reward: 0.514 [0.514, 0.514], mean action: 42.700 [26.000, 95.000], mean observation: 3.137 [-1.612, 10.356], loss: 1.234265, mae: 4.981814, mean_q: 5.158403
 79910/100000: episode: 8155, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.240, mean reward: 0.424 [0.379, 0.495], mean action: 38.500 [14.000, 79.000], mean observation: 3.152 [-1.044, 10.423], loss: 1.081567, mae: 4.981033, mean_q: 5.153857
 79920/100000: episode: 8156, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.773, mean reward: 0.377 [0.276, 0.506], mean action: 40.900 [30.000, 87.000], mean observation: 3.163 [-1.444, 10.355], loss: 1.146468, mae: 4.981526, mean_q: 5.152537
 79930/100000: episode: 8157, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.856, mean reward: 0.386 [0.294, 0.458], mean action: 44.900 [5.000, 86.000], mean observation: 3.154 [-1.664, 10.271], loss: 1.155183, mae: 4.981362, mean_q: 5.154214
 79940/100000: episode: 8158, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.161, mean reward: 0.416 [0.339, 0.500], mean action: 50.500 [4.000, 94.000], mean observation: 3.143 [-1.218, 10.267], loss: 1.193493, mae: 4.981428, mean_q: 5.156952
 79950/100000: episode: 8159, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.741, mean reward: 0.374 [0.325, 0.411], mean action: 52.300 [21.000, 99.000], mean observation: 3.161 [-1.353, 10.386], loss: 1.277118, mae: 4.981660, mean_q: 5.157444
 79960/100000: episode: 8160, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.044, mean reward: 0.404 [0.387, 0.560], mean action: 52.600 [18.000, 98.000], mean observation: 3.158 [-1.577, 10.221], loss: 1.076261, mae: 4.980995, mean_q: 5.156528
 79970/100000: episode: 8161, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.690, mean reward: 0.369 [0.327, 0.416], mean action: 53.000 [35.000, 92.000], mean observation: 3.149 [-0.982, 10.218], loss: 1.234801, mae: 4.981636, mean_q: 5.156414
 79980/100000: episode: 8162, duration: 0.150s, episode steps: 10, steps per second: 66, episode reward: 4.562, mean reward: 0.456 [0.335, 0.536], mean action: 60.200 [3.000, 89.000], mean observation: 3.146 [-1.271, 10.269], loss: 1.067658, mae: 4.980875, mean_q: 5.153123
 79990/100000: episode: 8163, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.862, mean reward: 0.386 [0.301, 0.462], mean action: 62.600 [12.000, 93.000], mean observation: 3.151 [-1.631, 10.305], loss: 1.090475, mae: 4.980876, mean_q: 5.151977
 80000/100000: episode: 8164, duration: 0.150s, episode steps: 10, steps per second: 66, episode reward: 3.598, mean reward: 0.360 [0.293, 0.487], mean action: 55.400 [22.000, 98.000], mean observation: 3.150 [-1.447, 10.249], loss: 1.288273, mae: 4.981472, mean_q: 5.150975
 80010/100000: episode: 8165, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.324, mean reward: 0.432 [0.383, 0.518], mean action: 48.000 [4.000, 89.000], mean observation: 3.150 [-1.914, 10.407], loss: 1.534390, mae: 4.982368, mean_q: 5.149875
 80020/100000: episode: 8166, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.863, mean reward: 0.386 [0.351, 0.443], mean action: 51.100 [0.000, 89.000], mean observation: 3.162 [-1.055, 10.259], loss: 1.302897, mae: 4.981125, mean_q: 5.150746
 80030/100000: episode: 8167, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.532, mean reward: 0.453 [0.376, 0.484], mean action: 78.200 [38.000, 89.000], mean observation: 3.158 [-0.820, 10.216], loss: 1.072823, mae: 4.979735, mean_q: 5.154145
 80040/100000: episode: 8168, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.734, mean reward: 0.373 [0.321, 0.423], mean action: 72.000 [0.000, 89.000], mean observation: 3.150 [-1.088, 10.330], loss: 0.997383, mae: 4.979420, mean_q: 5.155713
 80050/100000: episode: 8169, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 3.824, mean reward: 0.382 [0.341, 0.486], mean action: 28.700 [8.000, 59.000], mean observation: 3.158 [-1.451, 10.305], loss: 1.092722, mae: 4.979840, mean_q: 5.154701
 80060/100000: episode: 8170, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.346, mean reward: 0.435 [0.365, 0.485], mean action: 47.000 [30.000, 93.000], mean observation: 3.168 [-1.574, 10.473], loss: 1.219903, mae: 4.980232, mean_q: 5.155517
 80070/100000: episode: 8171, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.067, mean reward: 0.407 [0.354, 0.514], mean action: 49.700 [17.000, 99.000], mean observation: 3.157 [-1.844, 10.449], loss: 1.119684, mae: 4.979914, mean_q: 5.156264
 80080/100000: episode: 8172, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.299, mean reward: 0.430 [0.361, 0.551], mean action: 45.100 [7.000, 101.000], mean observation: 3.141 [-1.896, 10.267], loss: 1.244467, mae: 4.980093, mean_q: 5.156976
 80090/100000: episode: 8173, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.086, mean reward: 0.409 [0.326, 0.442], mean action: 35.900 [24.000, 76.000], mean observation: 3.157 [-1.533, 10.311], loss: 1.312945, mae: 4.980462, mean_q: 5.158963
 80100/100000: episode: 8174, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.186, mean reward: 0.419 [0.356, 0.475], mean action: 35.900 [4.000, 92.000], mean observation: 3.156 [-0.928, 10.364], loss: 1.359910, mae: 4.980317, mean_q: 5.160486
 80110/100000: episode: 8175, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.826, mean reward: 0.383 [0.347, 0.489], mean action: 57.100 [12.000, 99.000], mean observation: 3.153 [-1.407, 10.210], loss: 1.258670, mae: 4.979865, mean_q: 5.161217
 80120/100000: episode: 8176, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.524, mean reward: 0.452 [0.451, 0.462], mean action: 40.900 [30.000, 76.000], mean observation: 3.156 [-0.935, 10.330], loss: 1.542515, mae: 4.980445, mean_q: 5.162101
 80130/100000: episode: 8177, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.808, mean reward: 0.381 [0.329, 0.444], mean action: 53.100 [30.000, 99.000], mean observation: 3.156 [-1.016, 10.461], loss: 1.516459, mae: 4.979970, mean_q: 5.163352
 80140/100000: episode: 8178, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.784, mean reward: 0.378 [0.352, 0.451], mean action: 32.300 [4.000, 75.000], mean observation: 3.158 [-0.995, 10.328], loss: 1.105234, mae: 4.978251, mean_q: 5.160908
 80149/100000: episode: 8179, duration: 0.165s, episode steps: 9, steps per second: 54, episode reward: 13.649, mean reward: 1.517 [0.438, 10.000], mean action: 43.778 [28.000, 100.000], mean observation: 3.150 [-1.494, 10.216], loss: 0.982171, mae: 4.977776, mean_q: 5.159586
 80159/100000: episode: 8180, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.965, mean reward: 0.396 [0.377, 0.429], mean action: 55.600 [13.000, 101.000], mean observation: 3.168 [-1.363, 10.303], loss: 1.476559, mae: 4.979809, mean_q: 5.159551
 80169/100000: episode: 8181, duration: 0.150s, episode steps: 10, steps per second: 66, episode reward: 4.046, mean reward: 0.405 [0.333, 0.523], mean action: 45.300 [30.000, 79.000], mean observation: 3.156 [-1.356, 10.377], loss: 1.229255, mae: 4.978761, mean_q: 5.160258
 80179/100000: episode: 8182, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.833, mean reward: 0.383 [0.281, 0.449], mean action: 32.500 [5.000, 56.000], mean observation: 3.153 [-0.899, 10.348], loss: 0.954605, mae: 4.977527, mean_q: 5.162179
 80189/100000: episode: 8183, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.074, mean reward: 0.407 [0.333, 0.472], mean action: 32.300 [15.000, 51.000], mean observation: 3.149 [-1.126, 10.146], loss: 1.294080, mae: 4.979125, mean_q: 5.165617
 80199/100000: episode: 8184, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.265, mean reward: 0.427 [0.339, 0.492], mean action: 30.300 [0.000, 78.000], mean observation: 3.150 [-1.752, 10.408], loss: 1.170822, mae: 4.978576, mean_q: 5.163866
 80209/100000: episode: 8185, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.357, mean reward: 0.436 [0.323, 0.498], mean action: 42.900 [3.000, 101.000], mean observation: 3.161 [-0.927, 10.474], loss: 1.404704, mae: 4.979471, mean_q: 5.163575
 80219/100000: episode: 8186, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.158, mean reward: 0.416 [0.347, 0.481], mean action: 43.000 [10.000, 76.000], mean observation: 3.156 [-1.468, 10.366], loss: 1.386431, mae: 4.979183, mean_q: 5.163956
 80229/100000: episode: 8187, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 5.006, mean reward: 0.501 [0.499, 0.516], mean action: 38.500 [4.000, 96.000], mean observation: 3.153 [-1.767, 10.240], loss: 1.481092, mae: 4.979181, mean_q: 5.165456
 80239/100000: episode: 8188, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.113, mean reward: 0.411 [0.369, 0.486], mean action: 50.000 [30.000, 78.000], mean observation: 3.148 [-0.802, 10.189], loss: 0.948696, mae: 4.977114, mean_q: 5.168482
 80249/100000: episode: 8189, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.122, mean reward: 0.412 [0.368, 0.520], mean action: 39.400 [3.000, 99.000], mean observation: 3.157 [-1.626, 10.293], loss: 1.182147, mae: 4.978124, mean_q: 5.171728
 80252/100000: episode: 8190, duration: 0.068s, episode steps: 3, steps per second: 44, episode reward: 10.907, mean reward: 3.636 [0.344, 10.000], mean action: 30.000 [30.000, 30.000], mean observation: 3.166 [-1.162, 10.205], loss: 1.329920, mae: 4.978868, mean_q: 5.173328
 80262/100000: episode: 8191, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.054, mean reward: 0.405 [0.369, 0.430], mean action: 49.600 [1.000, 97.000], mean observation: 3.151 [-1.838, 10.274], loss: 0.929293, mae: 4.977170, mean_q: 5.174560
 80272/100000: episode: 8192, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.940, mean reward: 0.394 [0.332, 0.446], mean action: 36.600 [9.000, 101.000], mean observation: 3.156 [-1.593, 10.280], loss: 1.185176, mae: 4.978335, mean_q: 5.175611
 80282/100000: episode: 8193, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.103, mean reward: 0.410 [0.366, 0.491], mean action: 31.600 [6.000, 89.000], mean observation: 3.153 [-1.743, 10.334], loss: 1.194271, mae: 4.978429, mean_q: 5.176965
 80292/100000: episode: 8194, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.253, mean reward: 0.425 [0.385, 0.480], mean action: 44.100 [17.000, 87.000], mean observation: 3.158 [-1.150, 10.315], loss: 1.238420, mae: 4.978875, mean_q: 5.179185
 80302/100000: episode: 8195, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.398, mean reward: 0.440 [0.363, 0.521], mean action: 56.800 [12.000, 94.000], mean observation: 3.142 [-1.224, 10.204], loss: 1.257079, mae: 4.978805, mean_q: 5.180354
 80312/100000: episode: 8196, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.038, mean reward: 0.404 [0.317, 0.516], mean action: 29.200 [5.000, 51.000], mean observation: 3.149 [-1.787, 10.403], loss: 1.122568, mae: 4.978372, mean_q: 5.182361
 80322/100000: episode: 8197, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.077, mean reward: 0.408 [0.359, 0.488], mean action: 42.700 [30.000, 90.000], mean observation: 3.168 [-1.050, 10.453], loss: 1.194186, mae: 4.978695, mean_q: 5.183978
 80332/100000: episode: 8198, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.904, mean reward: 0.390 [0.298, 0.492], mean action: 35.300 [9.000, 83.000], mean observation: 3.161 [-1.345, 10.406], loss: 1.216076, mae: 4.978791, mean_q: 5.185627
 80342/100000: episode: 8199, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.826, mean reward: 0.383 [0.337, 0.443], mean action: 52.500 [18.000, 91.000], mean observation: 3.172 [-1.500, 10.332], loss: 1.044109, mae: 4.978379, mean_q: 5.188547
 80352/100000: episode: 8200, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.055, mean reward: 0.405 [0.389, 0.441], mean action: 39.200 [5.000, 91.000], mean observation: 3.164 [-1.010, 10.331], loss: 1.361939, mae: 4.979870, mean_q: 5.187163
 80362/100000: episode: 8201, duration: 0.131s, episode steps: 10, steps per second: 77, episode reward: 4.552, mean reward: 0.455 [0.445, 0.531], mean action: 54.600 [26.000, 99.000], mean observation: 3.140 [-1.224, 10.320], loss: 1.119612, mae: 4.978555, mean_q: 5.187442
 80372/100000: episode: 8202, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.884, mean reward: 0.388 [0.349, 0.468], mean action: 38.600 [12.000, 76.000], mean observation: 3.168 [-1.451, 10.305], loss: 1.078170, mae: 4.978398, mean_q: 5.189742
 80382/100000: episode: 8203, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.208, mean reward: 0.421 [0.345, 0.513], mean action: 40.800 [30.000, 69.000], mean observation: 3.157 [-1.257, 10.304], loss: 1.015665, mae: 4.978279, mean_q: 5.192129
 80392/100000: episode: 8204, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.010, mean reward: 0.401 [0.281, 0.588], mean action: 38.900 [9.000, 97.000], mean observation: 3.151 [-1.375, 10.381], loss: 1.146098, mae: 4.979203, mean_q: 5.191262
 80402/100000: episode: 8205, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.121, mean reward: 0.412 [0.359, 0.467], mean action: 40.200 [28.000, 96.000], mean observation: 3.156 [-1.272, 10.238], loss: 1.056884, mae: 4.978990, mean_q: 5.190257
 80412/100000: episode: 8206, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.363, mean reward: 0.436 [0.355, 0.563], mean action: 36.100 [2.000, 86.000], mean observation: 3.156 [-0.976, 10.478], loss: 1.454262, mae: 4.981000, mean_q: 5.196832
 80422/100000: episode: 8207, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.948, mean reward: 0.395 [0.312, 0.546], mean action: 32.600 [23.000, 49.000], mean observation: 3.163 [-1.216, 10.275], loss: 1.224398, mae: 4.979715, mean_q: 5.197227
 80432/100000: episode: 8208, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.913, mean reward: 0.391 [0.366, 0.471], mean action: 26.400 [11.000, 47.000], mean observation: 3.164 [-1.443, 10.346], loss: 1.033344, mae: 4.978942, mean_q: 5.196761
 80442/100000: episode: 8209, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.412, mean reward: 0.441 [0.416, 0.554], mean action: 36.400 [21.000, 77.000], mean observation: 3.171 [-2.396, 10.496], loss: 1.156137, mae: 4.979695, mean_q: 5.197398
 80452/100000: episode: 8210, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.956, mean reward: 0.396 [0.316, 0.517], mean action: 31.000 [1.000, 80.000], mean observation: 3.150 [-1.608, 10.229], loss: 1.436527, mae: 4.981095, mean_q: 5.198963
 80462/100000: episode: 8211, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.215, mean reward: 0.421 [0.364, 0.479], mean action: 48.600 [30.000, 99.000], mean observation: 3.155 [-1.076, 10.398], loss: 1.476033, mae: 4.981364, mean_q: 5.200626
 80472/100000: episode: 8212, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.955, mean reward: 0.396 [0.298, 0.464], mean action: 34.600 [1.000, 87.000], mean observation: 3.153 [-1.957, 10.451], loss: 1.142590, mae: 4.980007, mean_q: 5.203170
 80482/100000: episode: 8213, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.782, mean reward: 0.378 [0.338, 0.499], mean action: 39.500 [11.000, 76.000], mean observation: 3.161 [-1.481, 10.300], loss: 1.234292, mae: 4.980597, mean_q: 5.205838
 80492/100000: episode: 8214, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.396, mean reward: 0.440 [0.417, 0.595], mean action: 31.700 [30.000, 47.000], mean observation: 3.163 [-1.502, 10.440], loss: 1.143129, mae: 4.980610, mean_q: 5.208362
 80502/100000: episode: 8215, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.135, mean reward: 0.413 [0.362, 0.552], mean action: 35.000 [6.000, 69.000], mean observation: 3.152 [-1.973, 10.314], loss: 1.139707, mae: 4.980934, mean_q: 5.210938
 80512/100000: episode: 8216, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.451, mean reward: 0.345 [0.312, 0.409], mean action: 37.000 [30.000, 91.000], mean observation: 3.159 [-1.204, 10.331], loss: 1.071665, mae: 4.980849, mean_q: 5.213210
 80522/100000: episode: 8217, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.170, mean reward: 0.417 [0.324, 0.525], mean action: 33.300 [30.000, 60.000], mean observation: 3.154 [-1.719, 10.508], loss: 0.954025, mae: 4.980762, mean_q: 5.215245
 80532/100000: episode: 8218, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.660, mean reward: 0.366 [0.321, 0.431], mean action: 28.400 [0.000, 41.000], mean observation: 3.153 [-1.532, 10.316], loss: 0.886810, mae: 4.981099, mean_q: 5.218118
 80542/100000: episode: 8219, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.141, mean reward: 0.414 [0.384, 0.506], mean action: 48.200 [15.000, 95.000], mean observation: 3.152 [-1.744, 10.229], loss: 1.093166, mae: 4.982369, mean_q: 5.220077
 80552/100000: episode: 8220, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.684, mean reward: 0.468 [0.350, 0.512], mean action: 39.100 [8.000, 73.000], mean observation: 3.157 [-1.220, 10.345], loss: 1.671920, mae: 4.984768, mean_q: 5.220953
 80562/100000: episode: 8221, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.386, mean reward: 0.439 [0.329, 0.543], mean action: 36.100 [12.000, 96.000], mean observation: 3.159 [-0.838, 10.299], loss: 1.107060, mae: 4.982537, mean_q: 5.223002
 80572/100000: episode: 8222, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.970, mean reward: 0.397 [0.347, 0.450], mean action: 41.100 [30.000, 85.000], mean observation: 3.155 [-1.445, 10.383], loss: 1.291971, mae: 4.983243, mean_q: 5.225639
 80582/100000: episode: 8223, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.619, mean reward: 0.462 [0.458, 0.494], mean action: 42.100 [15.000, 88.000], mean observation: 3.155 [-1.300, 10.481], loss: 1.402060, mae: 4.983754, mean_q: 5.227892
 80592/100000: episode: 8224, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.984, mean reward: 0.398 [0.377, 0.472], mean action: 42.000 [6.000, 101.000], mean observation: 3.160 [-1.518, 10.461], loss: 1.445702, mae: 4.983902, mean_q: 5.226819
 80602/100000: episode: 8225, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.366, mean reward: 0.437 [0.326, 0.582], mean action: 42.300 [1.000, 98.000], mean observation: 3.164 [-1.624, 10.333], loss: 0.877686, mae: 4.981927, mean_q: 5.228438
 80612/100000: episode: 8226, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 3.850, mean reward: 0.385 [0.313, 0.469], mean action: 25.300 [4.000, 30.000], mean observation: 3.156 [-1.719, 10.306], loss: 1.042122, mae: 4.982821, mean_q: 5.230928
 80622/100000: episode: 8227, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.088, mean reward: 0.409 [0.342, 0.456], mean action: 26.600 [8.000, 30.000], mean observation: 3.162 [-1.787, 10.353], loss: 1.302371, mae: 4.984374, mean_q: 5.235266
 80632/100000: episode: 8228, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.986, mean reward: 0.399 [0.328, 0.455], mean action: 33.900 [9.000, 62.000], mean observation: 3.170 [-1.319, 10.389], loss: 1.562980, mae: 4.985578, mean_q: 5.236578
 80642/100000: episode: 8229, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.234, mean reward: 0.423 [0.352, 0.564], mean action: 45.700 [8.000, 99.000], mean observation: 3.158 [-1.200, 10.390], loss: 1.137346, mae: 4.983655, mean_q: 5.235564
 80652/100000: episode: 8230, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.205, mean reward: 0.421 [0.377, 0.497], mean action: 56.900 [25.000, 100.000], mean observation: 3.137 [-1.314, 10.226], loss: 1.101906, mae: 4.983489, mean_q: 5.237440
 80662/100000: episode: 8231, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.978, mean reward: 0.398 [0.329, 0.465], mean action: 36.900 [15.000, 86.000], mean observation: 3.164 [-0.943, 10.317], loss: 1.217366, mae: 4.984042, mean_q: 5.239702
 80672/100000: episode: 8232, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.822, mean reward: 0.382 [0.353, 0.510], mean action: 42.900 [24.000, 75.000], mean observation: 3.151 [-1.310, 10.246], loss: 1.395379, mae: 4.985040, mean_q: 5.242929
 80682/100000: episode: 8233, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.999, mean reward: 0.400 [0.337, 0.553], mean action: 40.900 [30.000, 82.000], mean observation: 3.156 [-1.802, 10.237], loss: 1.528306, mae: 4.985588, mean_q: 5.245292
 80692/100000: episode: 8234, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.313, mean reward: 0.431 [0.351, 0.551], mean action: 52.700 [7.000, 97.000], mean observation: 3.160 [-1.350, 10.440], loss: 1.066755, mae: 4.983407, mean_q: 5.241605
 80702/100000: episode: 8235, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.131, mean reward: 0.413 [0.328, 0.481], mean action: 25.000 [0.000, 50.000], mean observation: 3.155 [-1.953, 10.331], loss: 1.162042, mae: 4.984080, mean_q: 5.236975
 80712/100000: episode: 8236, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.739, mean reward: 0.374 [0.322, 0.485], mean action: 45.400 [14.000, 86.000], mean observation: 3.149 [-1.464, 10.463], loss: 1.375681, mae: 4.984720, mean_q: 5.236417
 80722/100000: episode: 8237, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.253, mean reward: 0.425 [0.364, 0.502], mean action: 49.400 [19.000, 94.000], mean observation: 3.166 [-1.899, 10.295], loss: 1.069964, mae: 4.983613, mean_q: 5.238560
 80732/100000: episode: 8238, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.097, mean reward: 0.410 [0.363, 0.449], mean action: 42.200 [2.000, 86.000], mean observation: 3.154 [-1.597, 10.301], loss: 1.082267, mae: 4.983864, mean_q: 5.241800
 80742/100000: episode: 8239, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.390, mean reward: 0.439 [0.318, 0.508], mean action: 26.500 [12.000, 30.000], mean observation: 3.159 [-1.057, 10.557], loss: 1.386134, mae: 4.985175, mean_q: 5.243784
 80752/100000: episode: 8240, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 5.144, mean reward: 0.514 [0.514, 0.514], mean action: 33.200 [15.000, 94.000], mean observation: 3.151 [-1.233, 10.259], loss: 1.329505, mae: 4.984960, mean_q: 5.246115
 80762/100000: episode: 8241, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.752, mean reward: 0.375 [0.311, 0.400], mean action: 54.200 [30.000, 90.000], mean observation: 3.144 [-1.387, 10.243], loss: 1.385285, mae: 4.985160, mean_q: 5.249139
 80772/100000: episode: 8242, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.360, mean reward: 0.436 [0.435, 0.441], mean action: 41.100 [25.000, 89.000], mean observation: 3.138 [-1.870, 10.266], loss: 1.454811, mae: 4.985448, mean_q: 5.247204
 80782/100000: episode: 8243, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.975, mean reward: 0.397 [0.320, 0.518], mean action: 38.500 [15.000, 94.000], mean observation: 3.165 [-1.535, 10.368], loss: 1.295132, mae: 4.984798, mean_q: 5.239510
 80792/100000: episode: 8244, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.243, mean reward: 0.424 [0.368, 0.504], mean action: 41.700 [17.000, 93.000], mean observation: 3.159 [-1.392, 10.277], loss: 1.245515, mae: 4.984581, mean_q: 5.236840
 80802/100000: episode: 8245, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.284, mean reward: 0.428 [0.354, 0.511], mean action: 41.600 [16.000, 77.000], mean observation: 3.146 [-1.295, 10.392], loss: 1.084138, mae: 4.984038, mean_q: 5.236315
 80812/100000: episode: 8246, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.885, mean reward: 0.388 [0.356, 0.457], mean action: 47.400 [10.000, 94.000], mean observation: 3.161 [-1.026, 10.345], loss: 1.246493, mae: 4.984942, mean_q: 5.234538
 80822/100000: episode: 8247, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.657, mean reward: 0.466 [0.466, 0.466], mean action: 40.400 [30.000, 74.000], mean observation: 3.159 [-1.679, 10.369], loss: 1.516984, mae: 4.985917, mean_q: 5.233484
 80832/100000: episode: 8248, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.714, mean reward: 0.371 [0.289, 0.434], mean action: 40.900 [0.000, 97.000], mean observation: 3.153 [-1.740, 10.303], loss: 1.377851, mae: 4.985325, mean_q: 5.224640
 80842/100000: episode: 8249, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.767, mean reward: 0.377 [0.346, 0.427], mean action: 43.200 [30.000, 98.000], mean observation: 3.156 [-1.346, 10.338], loss: 1.002680, mae: 4.983840, mean_q: 5.213356
 80852/100000: episode: 8250, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.123, mean reward: 0.412 [0.384, 0.455], mean action: 30.800 [8.000, 63.000], mean observation: 3.158 [-1.076, 10.330], loss: 1.145022, mae: 4.984445, mean_q: 5.208120
 80862/100000: episode: 8251, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.912, mean reward: 0.391 [0.307, 0.453], mean action: 34.600 [3.000, 95.000], mean observation: 3.153 [-1.157, 10.350], loss: 0.871639, mae: 4.983766, mean_q: 5.204637
 80872/100000: episode: 8252, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.390, mean reward: 0.439 [0.411, 0.524], mean action: 36.400 [0.000, 73.000], mean observation: 3.152 [-1.562, 10.415], loss: 1.106496, mae: 4.985340, mean_q: 5.205276
 80882/100000: episode: 8253, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.537, mean reward: 0.454 [0.376, 0.554], mean action: 47.400 [4.000, 96.000], mean observation: 3.168 [-1.450, 10.279], loss: 1.206535, mae: 4.985910, mean_q: 5.207315
 80892/100000: episode: 8254, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.772, mean reward: 0.377 [0.303, 0.556], mean action: 40.700 [10.000, 78.000], mean observation: 3.160 [-0.908, 10.252], loss: 1.318874, mae: 4.986528, mean_q: 5.209863
 80902/100000: episode: 8255, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.243, mean reward: 0.424 [0.327, 0.577], mean action: 40.000 [8.000, 101.000], mean observation: 3.160 [-1.821, 10.257], loss: 1.511227, mae: 4.987206, mean_q: 5.211856
 80912/100000: episode: 8256, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.119, mean reward: 0.412 [0.340, 0.474], mean action: 43.200 [9.000, 100.000], mean observation: 3.152 [-1.573, 10.402], loss: 1.269312, mae: 4.986318, mean_q: 5.209742
 80922/100000: episode: 8257, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 5.329, mean reward: 0.533 [0.533, 0.533], mean action: 47.500 [5.000, 89.000], mean observation: 3.164 [-1.569, 10.258], loss: 1.078779, mae: 4.985323, mean_q: 5.206460
 80932/100000: episode: 8258, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.230, mean reward: 0.423 [0.361, 0.486], mean action: 43.100 [30.000, 80.000], mean observation: 3.150 [-1.423, 10.184], loss: 1.074069, mae: 4.985660, mean_q: 5.203204
 80942/100000: episode: 8259, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.224, mean reward: 0.422 [0.376, 0.502], mean action: 44.200 [23.000, 88.000], mean observation: 3.153 [-2.123, 10.358], loss: 1.133845, mae: 4.986070, mean_q: 5.201732
 80952/100000: episode: 8260, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.188, mean reward: 0.419 [0.392, 0.440], mean action: 43.300 [30.000, 96.000], mean observation: 3.150 [-1.304, 10.279], loss: 1.202137, mae: 4.986344, mean_q: 5.202680
 80962/100000: episode: 8261, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.897, mean reward: 0.390 [0.322, 0.465], mean action: 30.700 [29.000, 38.000], mean observation: 3.163 [-2.024, 10.264], loss: 1.290161, mae: 4.986924, mean_q: 5.208243
 80972/100000: episode: 8262, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.827, mean reward: 0.383 [0.381, 0.393], mean action: 57.700 [30.000, 101.000], mean observation: 3.144 [-2.224, 10.246], loss: 1.381140, mae: 4.987231, mean_q: 5.205153
 80982/100000: episode: 8263, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.871, mean reward: 0.387 [0.319, 0.486], mean action: 40.400 [0.000, 83.000], mean observation: 3.168 [-1.074, 10.327], loss: 1.278458, mae: 4.986889, mean_q: 5.203841
 80992/100000: episode: 8264, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.694, mean reward: 0.469 [0.342, 0.536], mean action: 49.600 [4.000, 94.000], mean observation: 3.154 [-1.204, 10.453], loss: 1.245304, mae: 4.986860, mean_q: 5.202880
 81002/100000: episode: 8265, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.363, mean reward: 0.436 [0.322, 0.514], mean action: 50.700 [30.000, 96.000], mean observation: 3.152 [-1.735, 10.296], loss: 0.899145, mae: 4.985698, mean_q: 5.201032
 81007/100000: episode: 8266, duration: 0.088s, episode steps: 5, steps per second: 57, episode reward: 11.427, mean reward: 2.285 [0.337, 10.000], mean action: 42.800 [24.000, 87.000], mean observation: 3.149 [-1.385, 10.272], loss: 1.200121, mae: 4.987109, mean_q: 5.201667
 81017/100000: episode: 8267, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.213, mean reward: 0.421 [0.385, 0.525], mean action: 38.600 [5.000, 99.000], mean observation: 3.162 [-1.313, 10.404], loss: 1.459607, mae: 4.988179, mean_q: 5.203420
 81027/100000: episode: 8268, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.503, mean reward: 0.450 [0.450, 0.450], mean action: 36.000 [30.000, 81.000], mean observation: 3.153 [-1.570, 10.410], loss: 1.290414, mae: 4.987500, mean_q: 5.206470
 81037/100000: episode: 8269, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.226, mean reward: 0.423 [0.415, 0.457], mean action: 49.200 [24.000, 93.000], mean observation: 3.149 [-1.973, 10.269], loss: 1.109180, mae: 4.986731, mean_q: 5.208531
 81047/100000: episode: 8270, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.144, mean reward: 0.414 [0.398, 0.499], mean action: 41.500 [30.000, 84.000], mean observation: 3.140 [-1.773, 10.344], loss: 1.336620, mae: 4.987848, mean_q: 5.207511
 81057/100000: episode: 8271, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.415, mean reward: 0.441 [0.385, 0.549], mean action: 36.000 [11.000, 84.000], mean observation: 3.155 [-1.806, 10.446], loss: 1.107302, mae: 4.986871, mean_q: 5.207650
 81067/100000: episode: 8272, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.949, mean reward: 0.395 [0.343, 0.465], mean action: 28.800 [2.000, 66.000], mean observation: 3.154 [-2.736, 10.415], loss: 1.105850, mae: 4.987025, mean_q: 5.208754
 81077/100000: episode: 8273, duration: 0.220s, episode steps: 10, steps per second: 45, episode reward: 4.425, mean reward: 0.442 [0.353, 0.539], mean action: 25.200 [0.000, 52.000], mean observation: 3.160 [-2.089, 10.560], loss: 1.135939, mae: 4.987451, mean_q: 5.210599
 81087/100000: episode: 8274, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.946, mean reward: 0.395 [0.335, 0.487], mean action: 47.900 [2.000, 94.000], mean observation: 3.150 [-1.179, 10.286], loss: 1.353630, mae: 4.988484, mean_q: 5.212866
 81097/100000: episode: 8275, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.967, mean reward: 0.397 [0.337, 0.443], mean action: 37.900 [26.000, 76.000], mean observation: 3.154 [-1.406, 10.422], loss: 0.938614, mae: 4.987432, mean_q: 5.215311
 81107/100000: episode: 8276, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 3.994, mean reward: 0.399 [0.357, 0.493], mean action: 32.200 [0.000, 66.000], mean observation: 3.147 [-1.091, 10.368], loss: 1.209939, mae: 4.988940, mean_q: 5.217568
 81117/100000: episode: 8277, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.154, mean reward: 0.415 [0.379, 0.573], mean action: 32.800 [8.000, 71.000], mean observation: 3.157 [-2.270, 10.371], loss: 1.416869, mae: 4.990083, mean_q: 5.219851
 81127/100000: episode: 8278, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.988, mean reward: 0.399 [0.357, 0.470], mean action: 34.200 [12.000, 62.000], mean observation: 3.155 [-1.284, 10.206], loss: 1.188475, mae: 4.989016, mean_q: 5.223302
 81137/100000: episode: 8279, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.557, mean reward: 0.456 [0.309, 0.556], mean action: 48.300 [24.000, 78.000], mean observation: 3.158 [-2.139, 10.447], loss: 1.346703, mae: 4.989662, mean_q: 5.226502
 81147/100000: episode: 8280, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 13.892, mean reward: 1.389 [0.370, 10.000], mean action: 28.300 [4.000, 85.000], mean observation: 3.162 [-2.076, 10.340], loss: 1.075126, mae: 4.988584, mean_q: 5.227889
 81157/100000: episode: 8281, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.332, mean reward: 0.433 [0.428, 0.481], mean action: 41.100 [15.000, 79.000], mean observation: 3.156 [-0.994, 10.264], loss: 1.224542, mae: 4.989789, mean_q: 5.227592
 81167/100000: episode: 8282, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.010, mean reward: 0.401 [0.275, 0.464], mean action: 32.300 [5.000, 78.000], mean observation: 3.143 [-1.223, 10.310], loss: 0.901858, mae: 4.988464, mean_q: 5.227077
 81177/100000: episode: 8283, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.326, mean reward: 0.433 [0.314, 0.531], mean action: 41.100 [26.000, 85.000], mean observation: 3.157 [-1.383, 10.301], loss: 1.325237, mae: 4.990578, mean_q: 5.228120
 81187/100000: episode: 8284, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.690, mean reward: 0.369 [0.315, 0.435], mean action: 32.700 [9.000, 100.000], mean observation: 3.152 [-1.301, 10.323], loss: 1.071744, mae: 4.990016, mean_q: 5.228903
 81197/100000: episode: 8285, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.037, mean reward: 0.404 [0.338, 0.493], mean action: 29.600 [1.000, 78.000], mean observation: 3.157 [-1.786, 10.498], loss: 1.168003, mae: 4.990576, mean_q: 5.227284
 81207/100000: episode: 8286, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.325, mean reward: 0.432 [0.377, 0.571], mean action: 37.700 [5.000, 92.000], mean observation: 3.166 [-1.365, 10.317], loss: 1.347953, mae: 4.991292, mean_q: 5.225570
 81217/100000: episode: 8287, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.512, mean reward: 0.451 [0.451, 0.453], mean action: 53.700 [26.000, 96.000], mean observation: 3.161 [-1.164, 10.424], loss: 1.564072, mae: 4.992174, mean_q: 5.226494
 81227/100000: episode: 8288, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.218, mean reward: 0.422 [0.373, 0.490], mean action: 39.800 [23.000, 71.000], mean observation: 3.149 [-1.054, 10.246], loss: 1.217723, mae: 4.990534, mean_q: 5.228227
 81237/100000: episode: 8289, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.733, mean reward: 0.373 [0.319, 0.425], mean action: 33.500 [26.000, 69.000], mean observation: 3.145 [-1.532, 10.379], loss: 1.451238, mae: 4.991591, mean_q: 5.229665
 81240/100000: episode: 8290, duration: 0.067s, episode steps: 3, steps per second: 45, episode reward: 10.871, mean reward: 3.624 [0.432, 10.000], mean action: 25.000 [15.000, 30.000], mean observation: 3.159 [-1.313, 10.281], loss: 0.924513, mae: 4.989674, mean_q: 5.230553
 81250/100000: episode: 8291, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.197, mean reward: 0.420 [0.373, 0.526], mean action: 41.100 [25.000, 80.000], mean observation: 3.158 [-1.379, 10.359], loss: 1.389486, mae: 4.991494, mean_q: 5.231163
 81260/100000: episode: 8292, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.873, mean reward: 0.487 [0.361, 0.501], mean action: 38.700 [11.000, 89.000], mean observation: 3.169 [-1.243, 10.365], loss: 1.145836, mae: 4.990537, mean_q: 5.232572
 81270/100000: episode: 8293, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.237, mean reward: 0.424 [0.340, 0.509], mean action: 46.400 [1.000, 100.000], mean observation: 3.151 [-1.528, 10.495], loss: 1.171393, mae: 4.990961, mean_q: 5.234078
 81280/100000: episode: 8294, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.240, mean reward: 0.424 [0.351, 0.466], mean action: 35.100 [4.000, 81.000], mean observation: 3.149 [-1.485, 10.408], loss: 1.082840, mae: 4.990882, mean_q: 5.234327
 81290/100000: episode: 8295, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.410, mean reward: 0.441 [0.429, 0.505], mean action: 40.400 [25.000, 98.000], mean observation: 3.146 [-1.691, 10.420], loss: 1.366372, mae: 4.992131, mean_q: 5.238531
 81300/100000: episode: 8296, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.202, mean reward: 0.420 [0.362, 0.573], mean action: 44.700 [7.000, 86.000], mean observation: 3.157 [-1.383, 10.256], loss: 1.505274, mae: 4.992833, mean_q: 5.240970
 81310/100000: episode: 8297, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.163, mean reward: 0.416 [0.385, 0.511], mean action: 40.200 [12.000, 97.000], mean observation: 3.154 [-0.986, 10.440], loss: 1.125665, mae: 4.991316, mean_q: 5.237086
 81320/100000: episode: 8298, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.571, mean reward: 0.457 [0.442, 0.482], mean action: 40.800 [23.000, 69.000], mean observation: 3.156 [-2.024, 10.318], loss: 1.240425, mae: 4.992049, mean_q: 5.234942
 81330/100000: episode: 8299, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.233, mean reward: 0.423 [0.404, 0.469], mean action: 43.800 [24.000, 85.000], mean observation: 3.163 [-1.149, 10.312], loss: 1.126789, mae: 4.991791, mean_q: 5.231277
 81340/100000: episode: 8300, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.106, mean reward: 0.411 [0.353, 0.498], mean action: 35.300 [24.000, 82.000], mean observation: 3.166 [-1.806, 10.265], loss: 0.853707, mae: 4.991094, mean_q: 5.228052
 81350/100000: episode: 8301, duration: 0.228s, episode steps: 10, steps per second: 44, episode reward: 3.980, mean reward: 0.398 [0.306, 0.489], mean action: 29.700 [10.000, 58.000], mean observation: 3.159 [-1.804, 10.309], loss: 1.274746, mae: 4.993320, mean_q: 5.224947
 81360/100000: episode: 8302, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.208, mean reward: 0.421 [0.382, 0.514], mean action: 37.000 [3.000, 98.000], mean observation: 3.148 [-1.609, 10.354], loss: 0.905385, mae: 4.992246, mean_q: 5.222551
 81370/100000: episode: 8303, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.346, mean reward: 0.435 [0.401, 0.508], mean action: 27.100 [12.000, 30.000], mean observation: 3.158 [-1.211, 10.407], loss: 0.936201, mae: 4.992857, mean_q: 5.223933
 81380/100000: episode: 8304, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.329, mean reward: 0.433 [0.411, 0.492], mean action: 34.500 [14.000, 84.000], mean observation: 3.153 [-1.416, 10.217], loss: 1.045544, mae: 4.993755, mean_q: 5.223072
 81390/100000: episode: 8305, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.980, mean reward: 0.398 [0.363, 0.453], mean action: 39.200 [30.000, 82.000], mean observation: 3.155 [-1.416, 10.364], loss: 1.527613, mae: 4.995740, mean_q: 5.221834
 81400/100000: episode: 8306, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.282, mean reward: 0.428 [0.336, 0.570], mean action: 34.600 [0.000, 93.000], mean observation: 3.162 [-1.327, 10.332], loss: 1.380715, mae: 4.994898, mean_q: 5.223115
 81410/100000: episode: 8307, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.789, mean reward: 0.479 [0.476, 0.509], mean action: 53.700 [30.000, 101.000], mean observation: 3.164 [-1.492, 10.280], loss: 1.368881, mae: 4.994920, mean_q: 5.225009
 81420/100000: episode: 8308, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.754, mean reward: 0.375 [0.336, 0.427], mean action: 45.000 [0.000, 99.000], mean observation: 3.151 [-2.181, 10.418], loss: 1.179537, mae: 4.994147, mean_q: 5.226193
 81430/100000: episode: 8309, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.804, mean reward: 0.380 [0.320, 0.469], mean action: 53.000 [30.000, 96.000], mean observation: 3.148 [-2.127, 10.315], loss: 1.641352, mae: 4.996191, mean_q: 5.222564
 81440/100000: episode: 8310, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.086, mean reward: 0.409 [0.335, 0.494], mean action: 35.100 [10.000, 101.000], mean observation: 3.155 [-0.999, 10.382], loss: 1.234299, mae: 4.994394, mean_q: 5.219911
 81450/100000: episode: 8311, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.778, mean reward: 0.378 [0.355, 0.440], mean action: 31.800 [1.000, 75.000], mean observation: 3.141 [-1.627, 10.281], loss: 1.074456, mae: 4.994094, mean_q: 5.217819
 81460/100000: episode: 8312, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.075, mean reward: 0.407 [0.388, 0.436], mean action: 47.500 [29.000, 91.000], mean observation: 3.146 [-1.690, 10.416], loss: 1.630028, mae: 4.996202, mean_q: 5.216836
 81470/100000: episode: 8313, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.139, mean reward: 0.414 [0.359, 0.502], mean action: 72.000 [27.000, 101.000], mean observation: 3.169 [-1.345, 10.327], loss: 1.145496, mae: 4.994224, mean_q: 5.217793
 81480/100000: episode: 8314, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.719, mean reward: 0.372 [0.285, 0.484], mean action: 51.000 [8.000, 94.000], mean observation: 3.167 [-1.682, 10.374], loss: 1.412000, mae: 4.994910, mean_q: 5.220134
 81490/100000: episode: 8315, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.133, mean reward: 0.413 [0.397, 0.464], mean action: 39.400 [30.000, 87.000], mean observation: 3.149 [-1.374, 10.296], loss: 1.304564, mae: 4.994724, mean_q: 5.222286
 81500/100000: episode: 8316, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.426, mean reward: 0.443 [0.437, 0.489], mean action: 42.200 [30.000, 76.000], mean observation: 3.150 [-1.608, 10.292], loss: 1.020482, mae: 4.993926, mean_q: 5.226126
 81510/100000: episode: 8317, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 3.901, mean reward: 0.390 [0.350, 0.510], mean action: 28.200 [12.000, 30.000], mean observation: 3.154 [-2.070, 10.336], loss: 1.338140, mae: 4.995440, mean_q: 5.228621
 81520/100000: episode: 8318, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.783, mean reward: 0.378 [0.361, 0.434], mean action: 48.100 [27.000, 94.000], mean observation: 3.168 [-1.115, 10.300], loss: 1.212038, mae: 4.995323, mean_q: 5.228311
 81530/100000: episode: 8319, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.681, mean reward: 0.368 [0.286, 0.495], mean action: 44.000 [30.000, 93.000], mean observation: 3.155 [-0.971, 10.331], loss: 1.411958, mae: 4.996286, mean_q: 5.229406
 81540/100000: episode: 8320, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.131, mean reward: 0.413 [0.293, 0.486], mean action: 47.400 [30.000, 99.000], mean observation: 3.169 [-1.040, 10.375], loss: 1.412994, mae: 4.996243, mean_q: 5.226820
 81550/100000: episode: 8321, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.913, mean reward: 0.391 [0.350, 0.481], mean action: 38.500 [7.000, 63.000], mean observation: 3.149 [-1.488, 10.245], loss: 1.337515, mae: 4.995931, mean_q: 5.223474
 81560/100000: episode: 8322, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.216, mean reward: 0.422 [0.376, 0.513], mean action: 36.800 [1.000, 92.000], mean observation: 3.161 [-1.058, 10.452], loss: 1.351551, mae: 4.995684, mean_q: 5.223833
 81570/100000: episode: 8323, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.130, mean reward: 0.413 [0.368, 0.491], mean action: 29.700 [0.000, 45.000], mean observation: 3.163 [-0.863, 10.346], loss: 1.206028, mae: 4.995172, mean_q: 5.225781
 81580/100000: episode: 8324, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 3.817, mean reward: 0.382 [0.326, 0.467], mean action: 33.400 [5.000, 68.000], mean observation: 3.168 [-0.969, 10.365], loss: 1.061791, mae: 4.994823, mean_q: 5.226344
 81590/100000: episode: 8325, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.060, mean reward: 0.406 [0.366, 0.512], mean action: 28.200 [0.000, 91.000], mean observation: 3.155 [-1.407, 10.312], loss: 1.622967, mae: 4.996917, mean_q: 5.225709
 81600/100000: episode: 8326, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.620, mean reward: 0.462 [0.363, 0.512], mean action: 38.800 [9.000, 99.000], mean observation: 3.159 [-1.768, 10.517], loss: 1.270668, mae: 4.995495, mean_q: 5.225051
 81610/100000: episode: 8327, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.516, mean reward: 0.452 [0.427, 0.489], mean action: 43.200 [17.000, 87.000], mean observation: 3.166 [-1.996, 10.378], loss: 1.083256, mae: 4.994792, mean_q: 5.226781
 81620/100000: episode: 8328, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.879, mean reward: 0.388 [0.342, 0.537], mean action: 31.800 [11.000, 78.000], mean observation: 3.162 [-1.782, 10.304], loss: 1.405995, mae: 4.996037, mean_q: 5.229836
 81630/100000: episode: 8329, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 3.690, mean reward: 0.369 [0.309, 0.441], mean action: 36.100 [14.000, 72.000], mean observation: 3.162 [-1.340, 10.397], loss: 1.518672, mae: 4.996652, mean_q: 5.225631
 81640/100000: episode: 8330, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.027, mean reward: 0.403 [0.289, 0.509], mean action: 35.700 [5.000, 81.000], mean observation: 3.164 [-1.139, 10.319], loss: 1.434698, mae: 4.996025, mean_q: 5.221622
 81650/100000: episode: 8331, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.205, mean reward: 0.420 [0.396, 0.495], mean action: 40.600 [18.000, 101.000], mean observation: 3.163 [-1.795, 10.230], loss: 1.455741, mae: 4.995914, mean_q: 5.222211
 81660/100000: episode: 8332, duration: 0.228s, episode steps: 10, steps per second: 44, episode reward: 4.080, mean reward: 0.408 [0.373, 0.550], mean action: 27.000 [8.000, 32.000], mean observation: 3.149 [-1.235, 10.392], loss: 1.254867, mae: 4.994954, mean_q: 5.221405
 81670/100000: episode: 8333, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.825, mean reward: 0.382 [0.348, 0.451], mean action: 37.500 [22.000, 95.000], mean observation: 3.146 [-1.634, 10.171], loss: 1.226727, mae: 4.994819, mean_q: 5.214926
 81680/100000: episode: 8334, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.590, mean reward: 0.459 [0.380, 0.548], mean action: 39.300 [22.000, 76.000], mean observation: 3.148 [-1.537, 10.396], loss: 1.172163, mae: 4.994647, mean_q: 5.212944
 81690/100000: episode: 8335, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.783, mean reward: 0.478 [0.429, 0.599], mean action: 33.400 [17.000, 94.000], mean observation: 3.144 [-1.456, 10.282], loss: 1.251465, mae: 4.995006, mean_q: 5.213593
 81700/100000: episode: 8336, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.118, mean reward: 0.412 [0.338, 0.516], mean action: 41.300 [5.000, 79.000], mean observation: 3.151 [-1.508, 10.305], loss: 1.421673, mae: 4.995586, mean_q: 5.214123
 81710/100000: episode: 8337, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.013, mean reward: 0.401 [0.379, 0.471], mean action: 49.000 [30.000, 90.000], mean observation: 3.149 [-1.814, 10.379], loss: 1.489107, mae: 4.995883, mean_q: 5.215526
 81718/100000: episode: 8338, duration: 0.146s, episode steps: 8, steps per second: 55, episode reward: 12.943, mean reward: 1.618 [0.379, 10.000], mean action: 43.375 [30.000, 93.000], mean observation: 3.152 [-1.165, 10.361], loss: 1.201436, mae: 4.994849, mean_q: 5.213058
 81728/100000: episode: 8339, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.272, mean reward: 0.427 [0.350, 0.549], mean action: 35.600 [6.000, 67.000], mean observation: 3.164 [-1.601, 10.255], loss: 1.392597, mae: 4.995391, mean_q: 5.212632
 81738/100000: episode: 8340, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.142, mean reward: 0.414 [0.344, 0.496], mean action: 37.500 [6.000, 89.000], mean observation: 3.153 [-1.531, 10.397], loss: 1.309828, mae: 4.994950, mean_q: 5.211074
 81748/100000: episode: 8341, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.245, mean reward: 0.424 [0.381, 0.509], mean action: 43.200 [14.000, 76.000], mean observation: 3.157 [-1.856, 10.367], loss: 1.117199, mae: 4.994225, mean_q: 5.212463
 81758/100000: episode: 8342, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.149, mean reward: 0.415 [0.346, 0.522], mean action: 48.300 [27.000, 96.000], mean observation: 3.167 [-1.897, 10.313], loss: 0.870480, mae: 4.993623, mean_q: 5.216280
 81768/100000: episode: 8343, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.515, mean reward: 0.351 [0.310, 0.415], mean action: 47.200 [8.000, 99.000], mean observation: 3.154 [-0.912, 10.355], loss: 1.030973, mae: 4.994783, mean_q: 5.220114
 81778/100000: episode: 8344, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.070, mean reward: 0.407 [0.371, 0.480], mean action: 58.000 [6.000, 100.000], mean observation: 3.166 [-0.876, 10.572], loss: 1.281905, mae: 4.996235, mean_q: 5.222197
 81788/100000: episode: 8345, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.279, mean reward: 0.428 [0.306, 0.536], mean action: 31.600 [6.000, 56.000], mean observation: 3.159 [-1.876, 10.452], loss: 1.109985, mae: 4.995706, mean_q: 5.220136
 81798/100000: episode: 8346, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.172, mean reward: 0.417 [0.369, 0.496], mean action: 41.500 [8.000, 86.000], mean observation: 3.153 [-1.450, 10.385], loss: 0.932172, mae: 4.995189, mean_q: 5.214662
 81808/100000: episode: 8347, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.035, mean reward: 0.404 [0.399, 0.426], mean action: 35.700 [3.000, 71.000], mean observation: 3.160 [-1.325, 10.231], loss: 1.406068, mae: 4.997153, mean_q: 5.210659
 81818/100000: episode: 8348, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 5.756, mean reward: 0.576 [0.576, 0.576], mean action: 44.000 [28.000, 89.000], mean observation: 3.158 [-2.337, 10.309], loss: 1.366054, mae: 4.996835, mean_q: 5.206320
 81828/100000: episode: 8349, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.921, mean reward: 0.392 [0.313, 0.519], mean action: 35.800 [5.000, 98.000], mean observation: 3.151 [-1.737, 10.310], loss: 1.139457, mae: 4.996059, mean_q: 5.205321
 81838/100000: episode: 8350, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.003, mean reward: 0.400 [0.368, 0.469], mean action: 40.800 [4.000, 78.000], mean observation: 3.163 [-1.182, 10.265], loss: 1.458851, mae: 4.997068, mean_q: 5.206939
 81848/100000: episode: 8351, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.380, mean reward: 0.438 [0.303, 0.594], mean action: 44.600 [5.000, 95.000], mean observation: 3.149 [-1.818, 10.193], loss: 0.934916, mae: 4.994938, mean_q: 5.209287
 81858/100000: episode: 8352, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.062, mean reward: 0.406 [0.365, 0.473], mean action: 30.600 [13.000, 41.000], mean observation: 3.165 [-1.306, 10.211], loss: 1.216760, mae: 4.996125, mean_q: 5.210299
 81868/100000: episode: 8353, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.799, mean reward: 0.380 [0.336, 0.420], mean action: 55.700 [30.000, 99.000], mean observation: 3.149 [-1.063, 10.362], loss: 1.538899, mae: 4.997701, mean_q: 5.211486
 81878/100000: episode: 8354, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.432, mean reward: 0.343 [0.336, 0.357], mean action: 59.100 [15.000, 98.000], mean observation: 3.148 [-1.174, 10.318], loss: 1.214500, mae: 4.996290, mean_q: 5.212889
 81888/100000: episode: 8355, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 14.567, mean reward: 1.457 [0.507, 10.000], mean action: 42.100 [5.000, 88.000], mean observation: 3.143 [-1.550, 10.428], loss: 1.008019, mae: 4.995630, mean_q: 5.212174
 81898/100000: episode: 8356, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.847, mean reward: 0.385 [0.292, 0.464], mean action: 44.900 [30.000, 68.000], mean observation: 3.157 [-1.786, 10.449], loss: 1.231137, mae: 4.996815, mean_q: 5.210694
 81908/100000: episode: 8357, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.951, mean reward: 0.395 [0.335, 0.455], mean action: 29.300 [1.000, 53.000], mean observation: 3.153 [-1.977, 10.282], loss: 0.949053, mae: 4.995756, mean_q: 5.212198
 81918/100000: episode: 8358, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.874, mean reward: 0.387 [0.313, 0.502], mean action: 39.500 [30.000, 65.000], mean observation: 3.155 [-1.338, 10.263], loss: 1.464670, mae: 4.998291, mean_q: 5.214114
 81928/100000: episode: 8359, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.938, mean reward: 0.394 [0.317, 0.451], mean action: 39.100 [2.000, 96.000], mean observation: 3.163 [-1.348, 10.313], loss: 1.323488, mae: 4.997681, mean_q: 5.216388
 81938/100000: episode: 8360, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.935, mean reward: 0.393 [0.349, 0.433], mean action: 65.600 [30.000, 96.000], mean observation: 3.149 [-2.846, 10.464], loss: 1.396849, mae: 4.997649, mean_q: 5.218534
 81948/100000: episode: 8361, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.893, mean reward: 0.389 [0.347, 0.464], mean action: 34.200 [7.000, 78.000], mean observation: 3.147 [-1.398, 10.282], loss: 1.063729, mae: 4.996092, mean_q: 5.218724
 81958/100000: episode: 8362, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.029, mean reward: 0.403 [0.397, 0.435], mean action: 50.900 [22.000, 81.000], mean observation: 3.145 [-1.799, 10.255], loss: 1.062922, mae: 4.996175, mean_q: 5.219121
 81968/100000: episode: 8363, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.168, mean reward: 0.417 [0.318, 0.493], mean action: 27.000 [0.000, 89.000], mean observation: 3.159 [-1.246, 10.360], loss: 1.366030, mae: 4.997670, mean_q: 5.221238
 81978/100000: episode: 8364, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.832, mean reward: 0.383 [0.373, 0.444], mean action: 53.900 [30.000, 89.000], mean observation: 3.161 [-0.936, 10.418], loss: 1.459238, mae: 4.998372, mean_q: 5.221720
 81988/100000: episode: 8365, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.221, mean reward: 0.422 [0.329, 0.576], mean action: 27.300 [9.000, 30.000], mean observation: 3.152 [-1.244, 10.331], loss: 1.164013, mae: 4.997109, mean_q: 5.223670
 81998/100000: episode: 8366, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.111, mean reward: 0.411 [0.360, 0.443], mean action: 42.300 [5.000, 85.000], mean observation: 3.159 [-2.167, 10.218], loss: 1.162803, mae: 4.997302, mean_q: 5.222031
 82008/100000: episode: 8367, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.122, mean reward: 0.412 [0.371, 0.539], mean action: 41.700 [1.000, 78.000], mean observation: 3.163 [-1.349, 10.341], loss: 1.340648, mae: 4.998336, mean_q: 5.219356
 82018/100000: episode: 8368, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.275, mean reward: 0.428 [0.407, 0.523], mean action: 43.700 [5.000, 96.000], mean observation: 3.144 [-1.488, 10.321], loss: 1.538649, mae: 4.999066, mean_q: 5.220223
 82028/100000: episode: 8369, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.030, mean reward: 0.403 [0.347, 0.505], mean action: 50.500 [30.000, 94.000], mean observation: 3.158 [-1.754, 10.290], loss: 1.173801, mae: 4.997659, mean_q: 5.217744
 82038/100000: episode: 8370, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.849, mean reward: 0.385 [0.327, 0.454], mean action: 33.300 [7.000, 77.000], mean observation: 3.152 [-1.247, 10.302], loss: 1.058647, mae: 4.997194, mean_q: 5.217238
 82048/100000: episode: 8371, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.225, mean reward: 0.422 [0.330, 0.537], mean action: 39.100 [7.000, 91.000], mean observation: 3.168 [-1.140, 10.364], loss: 1.376390, mae: 4.998343, mean_q: 5.217767
 82058/100000: episode: 8372, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.309, mean reward: 0.431 [0.363, 0.515], mean action: 44.900 [13.000, 85.000], mean observation: 3.159 [-1.512, 10.257], loss: 1.420850, mae: 4.998295, mean_q: 5.216982
 82068/100000: episode: 8373, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.996, mean reward: 0.400 [0.359, 0.441], mean action: 41.500 [4.000, 100.000], mean observation: 3.163 [-1.425, 10.471], loss: 1.012377, mae: 4.996787, mean_q: 5.217108
 82078/100000: episode: 8374, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.332, mean reward: 0.433 [0.325, 0.514], mean action: 41.800 [30.000, 90.000], mean observation: 3.147 [-1.942, 10.179], loss: 1.572716, mae: 4.998989, mean_q: 5.220141
 82088/100000: episode: 8375, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.510, mean reward: 0.451 [0.334, 0.534], mean action: 29.100 [2.000, 49.000], mean observation: 3.155 [-1.140, 10.288], loss: 1.105642, mae: 4.997017, mean_q: 5.223009
 82098/100000: episode: 8376, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.050, mean reward: 0.405 [0.295, 0.575], mean action: 33.700 [13.000, 52.000], mean observation: 3.161 [-1.549, 10.377], loss: 1.441748, mae: 4.998445, mean_q: 5.223514
 82108/100000: episode: 8377, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.756, mean reward: 0.376 [0.298, 0.430], mean action: 40.400 [3.000, 82.000], mean observation: 3.164 [-1.100, 10.298], loss: 1.320459, mae: 4.997854, mean_q: 5.217776
 82118/100000: episode: 8378, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.112, mean reward: 0.411 [0.341, 0.523], mean action: 37.400 [7.000, 88.000], mean observation: 3.159 [-1.389, 10.469], loss: 1.487629, mae: 4.998768, mean_q: 5.213171
 82128/100000: episode: 8379, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.215, mean reward: 0.422 [0.403, 0.466], mean action: 37.900 [13.000, 68.000], mean observation: 3.156 [-1.148, 10.363], loss: 1.452028, mae: 4.998559, mean_q: 5.209857
 82138/100000: episode: 8380, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.927, mean reward: 0.393 [0.328, 0.497], mean action: 43.300 [11.000, 92.000], mean observation: 3.142 [-1.204, 10.332], loss: 1.120410, mae: 4.997043, mean_q: 5.208006
 82148/100000: episode: 8381, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.349, mean reward: 0.435 [0.315, 0.523], mean action: 34.900 [17.000, 80.000], mean observation: 3.160 [-2.106, 10.321], loss: 1.341698, mae: 4.997985, mean_q: 5.203908
 82158/100000: episode: 8382, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.359, mean reward: 0.436 [0.332, 0.481], mean action: 55.900 [30.000, 75.000], mean observation: 3.152 [-0.862, 10.369], loss: 1.031950, mae: 4.996521, mean_q: 5.200254
 82168/100000: episode: 8383, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.887, mean reward: 0.389 [0.373, 0.458], mean action: 45.500 [30.000, 101.000], mean observation: 3.150 [-1.396, 10.314], loss: 1.323041, mae: 4.997679, mean_q: 5.198528
 82178/100000: episode: 8384, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.047, mean reward: 0.405 [0.378, 0.493], mean action: 35.100 [17.000, 76.000], mean observation: 3.139 [-1.304, 10.339], loss: 0.969213, mae: 4.996416, mean_q: 5.198864
 82188/100000: episode: 8385, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.516, mean reward: 0.452 [0.368, 0.518], mean action: 32.200 [6.000, 90.000], mean observation: 3.157 [-1.386, 10.275], loss: 1.227229, mae: 4.997724, mean_q: 5.198576
 82198/100000: episode: 8386, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.980, mean reward: 0.398 [0.317, 0.562], mean action: 42.000 [6.000, 94.000], mean observation: 3.154 [-1.657, 10.321], loss: 0.899395, mae: 4.996860, mean_q: 5.199062
 82208/100000: episode: 8387, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.185, mean reward: 0.418 [0.355, 0.497], mean action: 43.400 [8.000, 73.000], mean observation: 3.152 [-1.129, 10.335], loss: 1.313040, mae: 4.998553, mean_q: 5.198165
 82218/100000: episode: 8388, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.717, mean reward: 0.472 [0.388, 0.525], mean action: 48.400 [0.000, 88.000], mean observation: 3.159 [-1.626, 10.297], loss: 1.161721, mae: 4.998322, mean_q: 5.197562
 82228/100000: episode: 8389, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.872, mean reward: 0.387 [0.323, 0.500], mean action: 58.100 [48.000, 99.000], mean observation: 3.141 [-0.622, 10.256], loss: 1.717107, mae: 5.000365, mean_q: 5.198379
 82238/100000: episode: 8390, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.755, mean reward: 0.476 [0.335, 0.595], mean action: 40.200 [0.000, 98.000], mean observation: 3.160 [-1.305, 10.386], loss: 1.256477, mae: 4.998509, mean_q: 5.200424
 82248/100000: episode: 8391, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.830, mean reward: 0.383 [0.284, 0.448], mean action: 42.900 [17.000, 68.000], mean observation: 3.167 [-1.837, 10.234], loss: 1.014084, mae: 4.997325, mean_q: 5.200890
 82258/100000: episode: 8392, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.151, mean reward: 0.415 [0.306, 0.530], mean action: 54.500 [36.000, 91.000], mean observation: 3.156 [-1.318, 10.207], loss: 1.283876, mae: 4.998509, mean_q: 5.200042
 82268/100000: episode: 8393, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.920, mean reward: 0.392 [0.371, 0.497], mean action: 45.600 [7.000, 83.000], mean observation: 3.163 [-0.919, 10.559], loss: 1.194506, mae: 4.997933, mean_q: 5.202176
 82278/100000: episode: 8394, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.964, mean reward: 0.396 [0.328, 0.492], mean action: 64.000 [42.000, 101.000], mean observation: 3.159 [-1.541, 10.371], loss: 1.666407, mae: 4.999847, mean_q: 5.203057
 82288/100000: episode: 8395, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.758, mean reward: 0.476 [0.465, 0.529], mean action: 42.100 [15.000, 52.000], mean observation: 3.164 [-2.127, 10.430], loss: 1.192120, mae: 4.997944, mean_q: 5.203463
 82298/100000: episode: 8396, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.998, mean reward: 0.400 [0.281, 0.512], mean action: 46.900 [1.000, 77.000], mean observation: 3.161 [-2.126, 10.367], loss: 1.208001, mae: 4.997722, mean_q: 5.203361
 82308/100000: episode: 8397, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 3.840, mean reward: 0.384 [0.372, 0.423], mean action: 73.900 [70.000, 91.000], mean observation: 3.167 [-1.680, 10.202], loss: 1.203319, mae: 4.997922, mean_q: 5.204745
 82318/100000: episode: 8398, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.745, mean reward: 0.374 [0.292, 0.461], mean action: 50.100 [11.000, 71.000], mean observation: 3.171 [-1.181, 10.299], loss: 1.053251, mae: 4.997178, mean_q: 5.205833
 82328/100000: episode: 8399, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.636, mean reward: 0.464 [0.463, 0.470], mean action: 54.900 [13.000, 91.000], mean observation: 3.162 [-0.816, 10.286], loss: 1.046984, mae: 4.997413, mean_q: 5.207047
 82338/100000: episode: 8400, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.941, mean reward: 0.394 [0.385, 0.424], mean action: 71.700 [70.000, 87.000], mean observation: 3.158 [-1.683, 10.262], loss: 0.927784, mae: 4.997245, mean_q: 5.208036
 82348/100000: episode: 8401, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.901, mean reward: 0.390 [0.338, 0.443], mean action: 61.400 [9.000, 70.000], mean observation: 3.160 [-1.085, 10.313], loss: 1.245024, mae: 4.998708, mean_q: 5.208430
 82358/100000: episode: 8402, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 5.204, mean reward: 0.520 [0.378, 0.556], mean action: 54.200 [6.000, 70.000], mean observation: 3.159 [-1.904, 10.297], loss: 1.064946, mae: 4.998250, mean_q: 5.209562
 82368/100000: episode: 8403, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.619, mean reward: 0.462 [0.402, 0.517], mean action: 57.100 [2.000, 89.000], mean observation: 3.157 [-0.915, 10.214], loss: 1.073832, mae: 4.998184, mean_q: 5.210648
 82378/100000: episode: 8404, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.959, mean reward: 0.396 [0.367, 0.519], mean action: 72.000 [48.000, 96.000], mean observation: 3.159 [-1.683, 10.326], loss: 1.302035, mae: 4.999251, mean_q: 5.205324
 82388/100000: episode: 8405, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.669, mean reward: 0.367 [0.290, 0.499], mean action: 62.500 [18.000, 100.000], mean observation: 3.160 [-0.980, 10.206], loss: 1.570918, mae: 5.000292, mean_q: 5.197070
 82398/100000: episode: 8406, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.943, mean reward: 0.394 [0.343, 0.453], mean action: 51.500 [48.000, 70.000], mean observation: 3.165 [-1.288, 10.180], loss: 1.248358, mae: 4.998720, mean_q: 5.198058
 82408/100000: episode: 8407, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.702, mean reward: 0.470 [0.353, 0.496], mean action: 47.500 [5.000, 80.000], mean observation: 3.164 [-1.481, 10.403], loss: 1.334797, mae: 4.999112, mean_q: 5.198026
 82418/100000: episode: 8408, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.608, mean reward: 0.361 [0.303, 0.409], mean action: 40.900 [8.000, 81.000], mean observation: 3.160 [-1.424, 10.205], loss: 1.081824, mae: 4.997982, mean_q: 5.194644
 82428/100000: episode: 8409, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.396, mean reward: 0.440 [0.422, 0.518], mean action: 54.400 [23.000, 92.000], mean observation: 3.161 [-1.385, 10.213], loss: 1.154607, mae: 4.998091, mean_q: 5.193458
 82438/100000: episode: 8410, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 4.949, mean reward: 0.495 [0.495, 0.495], mean action: 55.600 [18.000, 98.000], mean observation: 3.176 [-1.080, 10.312], loss: 1.307397, mae: 4.998615, mean_q: 5.193166
 82448/100000: episode: 8411, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.608, mean reward: 0.361 [0.352, 0.405], mean action: 63.300 [44.000, 90.000], mean observation: 3.151 [-1.908, 10.300], loss: 1.142432, mae: 4.997895, mean_q: 5.194186
 82458/100000: episode: 8412, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.843, mean reward: 0.384 [0.338, 0.465], mean action: 44.500 [26.000, 60.000], mean observation: 3.153 [-1.191, 10.353], loss: 1.658023, mae: 4.999880, mean_q: 5.192841
 82468/100000: episode: 8413, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.610, mean reward: 0.461 [0.433, 0.489], mean action: 65.800 [33.000, 101.000], mean observation: 3.161 [-1.004, 10.329], loss: 1.263701, mae: 4.998107, mean_q: 5.187684
 82478/100000: episode: 8414, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.961, mean reward: 0.396 [0.307, 0.454], mean action: 40.800 [12.000, 82.000], mean observation: 3.160 [-1.291, 10.389], loss: 1.281509, mae: 4.997701, mean_q: 5.185296
 82488/100000: episode: 8415, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.119, mean reward: 0.412 [0.351, 0.486], mean action: 26.300 [12.000, 61.000], mean observation: 3.163 [-1.192, 10.230], loss: 1.206463, mae: 4.997258, mean_q: 5.186053
 82498/100000: episode: 8416, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.275, mean reward: 0.428 [0.338, 0.566], mean action: 18.200 [12.000, 40.000], mean observation: 3.162 [-1.388, 10.447], loss: 1.224911, mae: 4.997751, mean_q: 5.187256
 82508/100000: episode: 8417, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.715, mean reward: 0.371 [0.332, 0.454], mean action: 36.700 [1.000, 90.000], mean observation: 3.167 [-1.437, 10.290], loss: 1.088099, mae: 4.997190, mean_q: 5.187996
 82518/100000: episode: 8418, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.882, mean reward: 0.388 [0.307, 0.545], mean action: 47.300 [8.000, 99.000], mean observation: 3.163 [-1.270, 10.366], loss: 1.457129, mae: 4.998478, mean_q: 5.191279
 82528/100000: episode: 8419, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.935, mean reward: 0.393 [0.309, 0.542], mean action: 56.400 [9.000, 70.000], mean observation: 3.153 [-1.496, 10.400], loss: 1.092393, mae: 4.996950, mean_q: 5.196219
 82538/100000: episode: 8420, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.060, mean reward: 0.406 [0.354, 0.441], mean action: 67.600 [2.000, 100.000], mean observation: 3.151 [-0.793, 10.341], loss: 1.144195, mae: 4.997152, mean_q: 5.199031
 82548/100000: episode: 8421, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.943, mean reward: 0.494 [0.494, 0.494], mean action: 64.700 [42.000, 70.000], mean observation: 3.158 [-1.625, 10.236], loss: 1.336878, mae: 4.997937, mean_q: 5.198958
 82558/100000: episode: 8422, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.071, mean reward: 0.407 [0.352, 0.539], mean action: 58.300 [19.000, 70.000], mean observation: 3.165 [-1.381, 10.298], loss: 1.533719, mae: 4.998653, mean_q: 5.196390
 82568/100000: episode: 8423, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.344, mean reward: 0.434 [0.368, 0.514], mean action: 48.500 [3.000, 70.000], mean observation: 3.156 [-1.853, 10.356], loss: 1.364912, mae: 4.997850, mean_q: 5.193320
 82578/100000: episode: 8424, duration: 0.131s, episode steps: 10, steps per second: 77, episode reward: 3.867, mean reward: 0.387 [0.352, 0.456], mean action: 61.100 [31.000, 70.000], mean observation: 3.143 [-0.699, 10.282], loss: 1.386692, mae: 4.997644, mean_q: 5.190408
 82588/100000: episode: 8425, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.231, mean reward: 0.423 [0.420, 0.450], mean action: 60.700 [28.000, 70.000], mean observation: 3.145 [-1.178, 10.229], loss: 1.456810, mae: 4.997613, mean_q: 5.190603
 82598/100000: episode: 8426, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.295, mean reward: 0.429 [0.418, 0.462], mean action: 41.200 [10.000, 96.000], mean observation: 3.154 [-1.319, 10.429], loss: 1.334040, mae: 4.996785, mean_q: 5.191340
 82608/100000: episode: 8427, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.342, mean reward: 0.434 [0.406, 0.489], mean action: 51.100 [30.000, 97.000], mean observation: 3.152 [-0.969, 10.326], loss: 1.320644, mae: 4.996594, mean_q: 5.193365
 82618/100000: episode: 8428, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.389, mean reward: 0.439 [0.427, 0.503], mean action: 48.100 [17.000, 74.000], mean observation: 3.166 [-1.398, 10.374], loss: 1.259981, mae: 4.996086, mean_q: 5.192718
 82628/100000: episode: 8429, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.181, mean reward: 0.418 [0.303, 0.573], mean action: 47.700 [27.000, 100.000], mean observation: 3.158 [-1.656, 10.454], loss: 1.527458, mae: 4.996951, mean_q: 5.189087
 82638/100000: episode: 8430, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.799, mean reward: 0.380 [0.309, 0.451], mean action: 32.500 [18.000, 67.000], mean observation: 3.158 [-1.557, 10.317], loss: 1.375754, mae: 4.996078, mean_q: 5.189087
 82648/100000: episode: 8431, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.173, mean reward: 0.417 [0.350, 0.470], mean action: 27.800 [4.000, 48.000], mean observation: 3.155 [-1.688, 10.271], loss: 1.291626, mae: 4.995464, mean_q: 5.190964
 82658/100000: episode: 8432, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.973, mean reward: 0.397 [0.345, 0.446], mean action: 44.600 [1.000, 84.000], mean observation: 3.162 [-0.922, 10.193], loss: 1.217695, mae: 4.994929, mean_q: 5.193413
 82668/100000: episode: 8433, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.058, mean reward: 0.406 [0.330, 0.526], mean action: 26.500 [1.000, 59.000], mean observation: 3.154 [-2.152, 10.318], loss: 0.885936, mae: 4.993850, mean_q: 5.195985
 82678/100000: episode: 8434, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.056, mean reward: 0.406 [0.313, 0.451], mean action: 38.600 [7.000, 95.000], mean observation: 3.162 [-1.482, 10.458], loss: 1.228195, mae: 4.995520, mean_q: 5.198653
 82688/100000: episode: 8435, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.111, mean reward: 0.411 [0.371, 0.508], mean action: 45.500 [15.000, 90.000], mean observation: 3.138 [-1.116, 10.285], loss: 1.222494, mae: 4.995717, mean_q: 5.199692
 82698/100000: episode: 8436, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.960, mean reward: 0.396 [0.345, 0.458], mean action: 42.600 [26.000, 87.000], mean observation: 3.154 [-1.447, 10.246], loss: 1.330029, mae: 4.996167, mean_q: 5.201881
 82708/100000: episode: 8437, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.074, mean reward: 0.407 [0.377, 0.441], mean action: 52.800 [20.000, 94.000], mean observation: 3.158 [-0.827, 10.305], loss: 1.366157, mae: 4.996323, mean_q: 5.200816
 82718/100000: episode: 8438, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.165, mean reward: 0.417 [0.327, 0.498], mean action: 40.800 [30.000, 76.000], mean observation: 3.162 [-1.999, 10.322], loss: 1.550739, mae: 4.997013, mean_q: 5.200652
 82728/100000: episode: 8439, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.093, mean reward: 0.409 [0.383, 0.544], mean action: 43.700 [10.000, 88.000], mean observation: 3.157 [-1.018, 10.359], loss: 1.229024, mae: 4.995559, mean_q: 5.202898
 82738/100000: episode: 8440, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.173, mean reward: 0.417 [0.358, 0.516], mean action: 45.400 [16.000, 86.000], mean observation: 3.147 [-1.808, 10.473], loss: 1.465158, mae: 4.996239, mean_q: 5.201915
 82748/100000: episode: 8441, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.211, mean reward: 0.421 [0.355, 0.529], mean action: 52.300 [7.000, 89.000], mean observation: 3.149 [-0.973, 10.406], loss: 1.403158, mae: 4.995784, mean_q: 5.197528
 82758/100000: episode: 8442, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.964, mean reward: 0.396 [0.364, 0.463], mean action: 38.800 [5.000, 86.000], mean observation: 3.159 [-0.913, 10.429], loss: 1.269866, mae: 4.995052, mean_q: 5.194787
 82768/100000: episode: 8443, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.036, mean reward: 0.404 [0.347, 0.499], mean action: 28.500 [9.000, 60.000], mean observation: 3.157 [-1.659, 10.316], loss: 0.927199, mae: 4.993625, mean_q: 5.195529
 82778/100000: episode: 8444, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.124, mean reward: 0.412 [0.352, 0.504], mean action: 34.400 [6.000, 88.000], mean observation: 3.154 [-1.669, 10.265], loss: 1.223399, mae: 4.994924, mean_q: 5.193271
 82788/100000: episode: 8445, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.101, mean reward: 0.410 [0.351, 0.452], mean action: 34.600 [2.000, 74.000], mean observation: 3.163 [-0.867, 10.332], loss: 1.231060, mae: 4.994864, mean_q: 5.191081
 82798/100000: episode: 8446, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.861, mean reward: 0.486 [0.484, 0.502], mean action: 44.400 [23.000, 95.000], mean observation: 3.171 [-1.222, 10.330], loss: 1.257315, mae: 4.994959, mean_q: 5.190563
 82808/100000: episode: 8447, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 3.730, mean reward: 0.373 [0.305, 0.423], mean action: 41.200 [8.000, 74.000], mean observation: 3.159 [-1.441, 10.391], loss: 0.987901, mae: 4.993911, mean_q: 5.192745
 82818/100000: episode: 8448, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.657, mean reward: 0.366 [0.302, 0.423], mean action: 42.700 [23.000, 87.000], mean observation: 3.168 [-1.650, 10.271], loss: 1.260064, mae: 4.995125, mean_q: 5.192667
 82820/100000: episode: 8449, duration: 0.061s, episode steps: 2, steps per second: 33, episode reward: 10.374, mean reward: 5.187 [0.374, 10.000], mean action: 30.000 [30.000, 30.000], mean observation: 3.146 [-1.193, 10.216], loss: 0.954717, mae: 4.994285, mean_q: 5.192164
 82830/100000: episode: 8450, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.834, mean reward: 0.383 [0.347, 0.457], mean action: 38.400 [6.000, 99.000], mean observation: 3.159 [-1.797, 10.345], loss: 1.331629, mae: 4.995325, mean_q: 5.192038
 82840/100000: episode: 8451, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.177, mean reward: 0.418 [0.338, 0.475], mean action: 51.400 [7.000, 101.000], mean observation: 3.150 [-1.419, 10.335], loss: 1.150906, mae: 4.994557, mean_q: 5.190988
 82850/100000: episode: 8452, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 3.878, mean reward: 0.388 [0.305, 0.463], mean action: 26.200 [4.000, 49.000], mean observation: 3.160 [-1.793, 10.294], loss: 1.346796, mae: 4.995347, mean_q: 5.190836
 82860/100000: episode: 8453, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.170, mean reward: 0.417 [0.401, 0.506], mean action: 52.700 [37.000, 86.000], mean observation: 3.170 [-1.303, 10.230], loss: 1.016603, mae: 4.993685, mean_q: 5.191137
 82870/100000: episode: 8454, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.734, mean reward: 0.373 [0.298, 0.454], mean action: 40.800 [0.000, 69.000], mean observation: 3.153 [-1.510, 10.272], loss: 1.294384, mae: 4.994748, mean_q: 5.191710
 82880/100000: episode: 8455, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.888, mean reward: 0.389 [0.343, 0.451], mean action: 49.200 [23.000, 70.000], mean observation: 3.159 [-1.423, 10.202], loss: 1.380075, mae: 4.995005, mean_q: 5.194139
 82890/100000: episode: 8456, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.793, mean reward: 0.379 [0.341, 0.437], mean action: 48.100 [23.000, 88.000], mean observation: 3.157 [-1.668, 10.278], loss: 1.156926, mae: 4.993842, mean_q: 5.197421
 82900/100000: episode: 8457, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.083, mean reward: 0.408 [0.373, 0.513], mean action: 42.800 [4.000, 80.000], mean observation: 3.149 [-1.357, 10.349], loss: 1.186793, mae: 4.993711, mean_q: 5.198335
 82910/100000: episode: 8458, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.222, mean reward: 0.422 [0.364, 0.576], mean action: 40.900 [0.000, 91.000], mean observation: 3.148 [-1.626, 10.319], loss: 1.468790, mae: 4.995001, mean_q: 5.199723
 82920/100000: episode: 8459, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.258, mean reward: 0.426 [0.425, 0.431], mean action: 57.700 [29.000, 101.000], mean observation: 3.146 [-1.935, 10.304], loss: 1.100252, mae: 4.993020, mean_q: 5.201190
 82930/100000: episode: 8460, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.515, mean reward: 0.351 [0.293, 0.442], mean action: 43.700 [3.000, 50.000], mean observation: 3.171 [-1.533, 10.273], loss: 1.137944, mae: 4.993317, mean_q: 5.201415
 82940/100000: episode: 8461, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.809, mean reward: 0.481 [0.481, 0.481], mean action: 62.600 [48.000, 99.000], mean observation: 3.160 [-1.386, 10.289], loss: 0.954082, mae: 4.992604, mean_q: 5.201188
 82950/100000: episode: 8462, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.395, mean reward: 0.440 [0.433, 0.471], mean action: 47.200 [37.000, 51.000], mean observation: 3.161 [-1.002, 10.374], loss: 1.393009, mae: 4.994278, mean_q: 5.201348
 82960/100000: episode: 8463, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.230, mean reward: 0.423 [0.328, 0.461], mean action: 49.300 [1.000, 100.000], mean observation: 3.163 [-1.261, 10.326], loss: 1.236974, mae: 4.993689, mean_q: 5.201226
 82970/100000: episode: 8464, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.183, mean reward: 0.418 [0.361, 0.559], mean action: 60.400 [23.000, 91.000], mean observation: 3.135 [-1.418, 10.434], loss: 1.323488, mae: 4.994143, mean_q: 5.202184
 82980/100000: episode: 8465, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.726, mean reward: 0.473 [0.473, 0.473], mean action: 71.800 [53.000, 95.000], mean observation: 3.174 [-1.121, 10.380], loss: 1.054938, mae: 4.992946, mean_q: 5.203373
 82990/100000: episode: 8466, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.986, mean reward: 0.399 [0.327, 0.535], mean action: 51.100 [17.000, 72.000], mean observation: 3.157 [-1.198, 10.188], loss: 1.297495, mae: 4.993921, mean_q: 5.204075
 82993/100000: episode: 8467, duration: 0.083s, episode steps: 3, steps per second: 36, episode reward: 10.826, mean reward: 3.609 [0.394, 10.000], mean action: 29.000 [20.000, 42.000], mean observation: 3.142 [-1.189, 10.182], loss: 1.752701, mae: 4.995687, mean_q: 5.202624
 83003/100000: episode: 8468, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.879, mean reward: 0.388 [0.311, 0.429], mean action: 54.900 [4.000, 100.000], mean observation: 3.164 [-1.807, 10.232], loss: 1.180693, mae: 4.993239, mean_q: 5.203798
 83013/100000: episode: 8469, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.055, mean reward: 0.405 [0.345, 0.478], mean action: 41.000 [23.000, 48.000], mean observation: 3.144 [-1.779, 10.187], loss: 1.274207, mae: 4.993443, mean_q: 5.204149
 83023/100000: episode: 8470, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.169, mean reward: 0.417 [0.345, 0.441], mean action: 49.400 [6.000, 84.000], mean observation: 3.170 [-1.379, 10.411], loss: 1.163979, mae: 4.992728, mean_q: 5.204951
 83033/100000: episode: 8471, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.751, mean reward: 0.375 [0.330, 0.468], mean action: 49.500 [34.000, 67.000], mean observation: 3.171 [-1.693, 10.264], loss: 1.268948, mae: 4.993053, mean_q: 5.205936
 83043/100000: episode: 8472, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.827, mean reward: 0.383 [0.341, 0.506], mean action: 55.400 [2.000, 90.000], mean observation: 3.165 [-1.779, 10.319], loss: 1.459426, mae: 4.993982, mean_q: 5.203118
 83053/100000: episode: 8473, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.306, mean reward: 0.431 [0.429, 0.445], mean action: 66.400 [21.000, 83.000], mean observation: 3.157 [-1.137, 10.300], loss: 0.968547, mae: 4.991601, mean_q: 5.203823
 83063/100000: episode: 8474, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.095, mean reward: 0.410 [0.392, 0.432], mean action: 70.700 [51.000, 84.000], mean observation: 3.156 [-1.421, 10.284], loss: 0.930901, mae: 4.991579, mean_q: 5.205881
 83073/100000: episode: 8475, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.014, mean reward: 0.401 [0.360, 0.508], mean action: 46.300 [6.000, 98.000], mean observation: 3.155 [-1.367, 10.294], loss: 1.233747, mae: 4.992839, mean_q: 5.206471
 83083/100000: episode: 8476, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.140, mean reward: 0.414 [0.343, 0.449], mean action: 65.300 [11.000, 94.000], mean observation: 3.164 [-1.244, 10.330], loss: 1.167801, mae: 4.992487, mean_q: 5.203480
 83093/100000: episode: 8477, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.236, mean reward: 0.424 [0.379, 0.488], mean action: 60.600 [11.000, 94.000], mean observation: 3.144 [-1.575, 10.298], loss: 1.212400, mae: 4.992843, mean_q: 5.203840
 83103/100000: episode: 8478, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.058, mean reward: 0.406 [0.325, 0.481], mean action: 47.700 [13.000, 70.000], mean observation: 3.145 [-1.512, 10.403], loss: 1.105378, mae: 4.992143, mean_q: 5.204474
 83113/100000: episode: 8479, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 4.760, mean reward: 0.476 [0.476, 0.476], mean action: 68.400 [53.000, 76.000], mean observation: 3.156 [-0.742, 10.198], loss: 0.869230, mae: 4.991349, mean_q: 5.200271
 83123/100000: episode: 8480, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 3.905, mean reward: 0.390 [0.331, 0.499], mean action: 72.200 [45.000, 101.000], mean observation: 3.145 [-1.774, 10.272], loss: 1.180233, mae: 4.992860, mean_q: 5.194861
 83133/100000: episode: 8481, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.077, mean reward: 0.408 [0.327, 0.466], mean action: 46.700 [13.000, 96.000], mean observation: 3.163 [-1.445, 10.400], loss: 1.526311, mae: 4.993794, mean_q: 5.191957
 83143/100000: episode: 8482, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.875, mean reward: 0.387 [0.386, 0.404], mean action: 70.500 [29.000, 91.000], mean observation: 3.154 [-2.171, 10.283], loss: 1.393362, mae: 4.993298, mean_q: 5.187414
 83153/100000: episode: 8483, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.918, mean reward: 0.392 [0.356, 0.426], mean action: 63.200 [13.000, 88.000], mean observation: 3.154 [-1.807, 10.263], loss: 1.295608, mae: 4.992734, mean_q: 5.187152
 83163/100000: episode: 8484, duration: 0.126s, episode steps: 10, steps per second: 80, episode reward: 4.639, mean reward: 0.464 [0.392, 0.472], mean action: 67.500 [14.000, 90.000], mean observation: 3.170 [-1.246, 10.316], loss: 1.265056, mae: 4.992203, mean_q: 5.188189
 83173/100000: episode: 8485, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.768, mean reward: 0.377 [0.353, 0.415], mean action: 66.600 [24.000, 93.000], mean observation: 3.155 [-1.784, 10.293], loss: 1.366157, mae: 4.992414, mean_q: 5.189706
 83183/100000: episode: 8486, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.848, mean reward: 0.385 [0.353, 0.460], mean action: 67.400 [45.000, 94.000], mean observation: 3.157 [-1.118, 10.266], loss: 1.027778, mae: 4.990682, mean_q: 5.188275
 83193/100000: episode: 8487, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.789, mean reward: 0.379 [0.357, 0.419], mean action: 55.100 [4.000, 82.000], mean observation: 3.150 [-1.065, 10.285], loss: 1.399164, mae: 4.992124, mean_q: 5.188722
 83203/100000: episode: 8488, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.803, mean reward: 0.380 [0.345, 0.446], mean action: 56.900 [11.000, 71.000], mean observation: 3.161 [-1.271, 10.252], loss: 1.126220, mae: 4.990973, mean_q: 5.189923
 83213/100000: episode: 8489, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.214, mean reward: 0.421 [0.344, 0.525], mean action: 63.200 [5.000, 86.000], mean observation: 3.142 [-0.813, 10.346], loss: 1.195402, mae: 4.991300, mean_q: 5.192567
 83223/100000: episode: 8490, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.024, mean reward: 0.402 [0.346, 0.445], mean action: 64.300 [1.000, 96.000], mean observation: 3.151 [-1.513, 10.276], loss: 1.025491, mae: 4.990713, mean_q: 5.194304
 83233/100000: episode: 8491, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.989, mean reward: 0.399 [0.374, 0.456], mean action: 59.500 [12.000, 93.000], mean observation: 3.156 [-1.538, 10.384], loss: 1.388414, mae: 4.992230, mean_q: 5.192891
 83243/100000: episode: 8492, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.954, mean reward: 0.395 [0.323, 0.583], mean action: 55.000 [3.000, 101.000], mean observation: 3.164 [-1.646, 10.335], loss: 1.093840, mae: 4.991111, mean_q: 5.187995
 83253/100000: episode: 8493, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.230, mean reward: 0.423 [0.411, 0.484], mean action: 57.500 [15.000, 98.000], mean observation: 3.152 [-1.762, 10.285], loss: 1.286422, mae: 4.992110, mean_q: 5.187059
 83263/100000: episode: 8494, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.846, mean reward: 0.385 [0.311, 0.492], mean action: 48.900 [1.000, 93.000], mean observation: 3.161 [-1.385, 10.209], loss: 1.095354, mae: 4.991429, mean_q: 5.187687
 83273/100000: episode: 8495, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.416, mean reward: 0.442 [0.348, 0.491], mean action: 56.700 [11.000, 70.000], mean observation: 3.153 [-1.435, 10.272], loss: 1.264992, mae: 4.992522, mean_q: 5.189624
 83283/100000: episode: 8496, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.069, mean reward: 0.407 [0.347, 0.461], mean action: 66.800 [34.000, 74.000], mean observation: 3.144 [-1.498, 10.242], loss: 1.301979, mae: 4.992552, mean_q: 5.190941
 83293/100000: episode: 8497, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 4.189, mean reward: 0.419 [0.359, 0.519], mean action: 55.400 [6.000, 86.000], mean observation: 3.164 [-1.959, 10.341], loss: 1.086661, mae: 4.991792, mean_q: 5.188204
 83303/100000: episode: 8498, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.155, mean reward: 0.415 [0.402, 0.456], mean action: 72.400 [19.000, 99.000], mean observation: 3.153 [-1.538, 10.248], loss: 1.120131, mae: 4.991904, mean_q: 5.187769
 83313/100000: episode: 8499, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.914, mean reward: 0.491 [0.491, 0.491], mean action: 59.000 [10.000, 97.000], mean observation: 3.165 [-1.904, 10.319], loss: 1.124573, mae: 4.991965, mean_q: 5.189742
 83323/100000: episode: 8500, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.792, mean reward: 0.379 [0.329, 0.442], mean action: 61.100 [5.000, 76.000], mean observation: 3.154 [-2.157, 10.310], loss: 1.270587, mae: 4.992665, mean_q: 5.191737
 83333/100000: episode: 8501, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.212, mean reward: 0.421 [0.305, 0.479], mean action: 66.900 [25.000, 88.000], mean observation: 3.167 [-1.360, 10.284], loss: 1.380141, mae: 4.993032, mean_q: 5.194219
 83343/100000: episode: 8502, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 3.700, mean reward: 0.370 [0.368, 0.385], mean action: 65.000 [19.000, 77.000], mean observation: 3.151 [-1.143, 10.369], loss: 0.986882, mae: 4.991391, mean_q: 5.196393
 83350/100000: episode: 8503, duration: 0.113s, episode steps: 7, steps per second: 62, episode reward: 12.369, mean reward: 1.767 [0.350, 10.000], mean action: 49.714 [2.000, 70.000], mean observation: 3.171 [-1.480, 10.184], loss: 1.281717, mae: 4.992673, mean_q: 5.197806
 83360/100000: episode: 8504, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.797, mean reward: 0.380 [0.265, 0.446], mean action: 63.000 [31.000, 74.000], mean observation: 3.157 [-1.511, 10.289], loss: 1.267138, mae: 4.992600, mean_q: 5.197184
 83370/100000: episode: 8505, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.842, mean reward: 0.384 [0.325, 0.437], mean action: 60.300 [10.000, 83.000], mean observation: 3.153 [-1.249, 10.223], loss: 1.191700, mae: 4.992209, mean_q: 5.194688
 83380/100000: episode: 8506, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 4.163, mean reward: 0.416 [0.391, 0.429], mean action: 66.700 [33.000, 88.000], mean observation: 3.158 [-1.065, 10.361], loss: 1.323547, mae: 4.992673, mean_q: 5.194367
 83390/100000: episode: 8507, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.709, mean reward: 0.371 [0.313, 0.392], mean action: 68.400 [61.000, 70.000], mean observation: 3.163 [-1.515, 10.236], loss: 1.063285, mae: 4.991554, mean_q: 5.195481
 83400/100000: episode: 8508, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.144, mean reward: 0.414 [0.341, 0.588], mean action: 53.600 [4.000, 78.000], mean observation: 3.155 [-1.531, 10.376], loss: 1.200741, mae: 4.992155, mean_q: 5.197034
 83410/100000: episode: 8509, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.782, mean reward: 0.378 [0.349, 0.484], mean action: 61.200 [34.000, 91.000], mean observation: 3.161 [-1.415, 10.477], loss: 1.160774, mae: 4.992375, mean_q: 5.196627
 83420/100000: episode: 8510, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.011, mean reward: 0.401 [0.365, 0.504], mean action: 58.500 [26.000, 84.000], mean observation: 3.150 [-1.847, 10.248], loss: 1.348411, mae: 4.993108, mean_q: 5.197935
 83430/100000: episode: 8511, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.894, mean reward: 0.389 [0.322, 0.468], mean action: 66.200 [2.000, 93.000], mean observation: 3.163 [-0.745, 10.239], loss: 1.480642, mae: 4.993558, mean_q: 5.199522
 83440/100000: episode: 8512, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.022, mean reward: 0.402 [0.402, 0.402], mean action: 68.200 [27.000, 99.000], mean observation: 3.157 [-1.134, 10.273], loss: 1.166687, mae: 4.991941, mean_q: 5.200963
 83450/100000: episode: 8513, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.097, mean reward: 0.410 [0.302, 0.523], mean action: 60.400 [14.000, 95.000], mean observation: 3.151 [-1.636, 10.377], loss: 0.872358, mae: 4.990859, mean_q: 5.202592
 83460/100000: episode: 8514, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.736, mean reward: 0.374 [0.306, 0.462], mean action: 55.400 [12.000, 70.000], mean observation: 3.162 [-1.614, 10.203], loss: 1.008266, mae: 4.991719, mean_q: 5.206058
 83470/100000: episode: 8515, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.918, mean reward: 0.392 [0.383, 0.422], mean action: 59.100 [15.000, 97.000], mean observation: 3.157 [-2.131, 10.326], loss: 1.087196, mae: 4.992296, mean_q: 5.206977
 83480/100000: episode: 8516, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.920, mean reward: 0.392 [0.370, 0.411], mean action: 47.900 [5.000, 96.000], mean observation: 3.166 [-1.531, 10.391], loss: 1.243275, mae: 4.992832, mean_q: 5.202563
 83490/100000: episode: 8517, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.418, mean reward: 0.442 [0.399, 0.527], mean action: 61.000 [11.000, 99.000], mean observation: 3.159 [-1.884, 10.360], loss: 1.148198, mae: 4.992734, mean_q: 5.195510
 83500/100000: episode: 8518, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.125, mean reward: 0.412 [0.308, 0.524], mean action: 56.600 [0.000, 98.000], mean observation: 3.162 [-1.834, 10.267], loss: 1.292142, mae: 4.993627, mean_q: 5.188951
 83510/100000: episode: 8519, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.524, mean reward: 0.452 [0.313, 0.544], mean action: 50.800 [0.000, 74.000], mean observation: 3.146 [-1.458, 10.316], loss: 1.510925, mae: 4.994464, mean_q: 5.187279
 83520/100000: episode: 8520, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.020, mean reward: 0.402 [0.341, 0.461], mean action: 63.200 [6.000, 88.000], mean observation: 3.155 [-1.769, 10.369], loss: 0.959260, mae: 4.992017, mean_q: 5.184993
 83530/100000: episode: 8521, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.040, mean reward: 0.404 [0.397, 0.434], mean action: 61.600 [17.000, 70.000], mean observation: 3.151 [-1.353, 10.308], loss: 1.351723, mae: 4.993797, mean_q: 5.185105
 83540/100000: episode: 8522, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.281, mean reward: 0.428 [0.380, 0.468], mean action: 49.900 [3.000, 100.000], mean observation: 3.167 [-1.160, 10.386], loss: 1.092929, mae: 4.992761, mean_q: 5.184802
 83550/100000: episode: 8523, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.182, mean reward: 0.418 [0.346, 0.527], mean action: 76.300 [59.000, 101.000], mean observation: 3.151 [-1.269, 10.295], loss: 1.156655, mae: 4.993003, mean_q: 5.184815
 83560/100000: episode: 8524, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.069, mean reward: 0.407 [0.367, 0.447], mean action: 69.900 [33.000, 93.000], mean observation: 3.157 [-0.790, 10.467], loss: 1.342735, mae: 4.993675, mean_q: 5.185344
 83570/100000: episode: 8525, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 3.969, mean reward: 0.397 [0.334, 0.445], mean action: 65.800 [28.000, 96.000], mean observation: 3.168 [-1.098, 10.377], loss: 1.467499, mae: 4.993891, mean_q: 5.185967
 83580/100000: episode: 8526, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.020, mean reward: 0.402 [0.357, 0.456], mean action: 51.600 [8.000, 94.000], mean observation: 3.162 [-2.126, 10.242], loss: 1.155616, mae: 4.992589, mean_q: 5.181639
 83590/100000: episode: 8527, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.987, mean reward: 0.399 [0.386, 0.458], mean action: 63.100 [37.000, 70.000], mean observation: 3.162 [-1.631, 10.203], loss: 1.204140, mae: 4.992681, mean_q: 5.178760
 83600/100000: episode: 8528, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.048, mean reward: 0.405 [0.368, 0.502], mean action: 68.000 [53.000, 70.000], mean observation: 3.175 [-0.941, 10.229], loss: 1.125149, mae: 4.992130, mean_q: 5.176046
 83610/100000: episode: 8529, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 5.273, mean reward: 0.527 [0.527, 0.527], mean action: 65.000 [39.000, 70.000], mean observation: 3.163 [-1.641, 10.210], loss: 1.222232, mae: 4.992591, mean_q: 5.175526
 83620/100000: episode: 8530, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.160, mean reward: 0.416 [0.370, 0.493], mean action: 62.600 [1.000, 98.000], mean observation: 3.156 [-1.123, 10.364], loss: 1.530488, mae: 4.993726, mean_q: 5.176491
 83630/100000: episode: 8531, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.115, mean reward: 0.412 [0.403, 0.457], mean action: 61.100 [17.000, 98.000], mean observation: 3.159 [-1.179, 10.374], loss: 1.194611, mae: 4.991960, mean_q: 5.173899
 83640/100000: episode: 8532, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.658, mean reward: 0.466 [0.446, 0.504], mean action: 50.600 [9.000, 94.000], mean observation: 3.154 [-1.914, 10.303], loss: 1.061377, mae: 4.991252, mean_q: 5.170077
 83650/100000: episode: 8533, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.186, mean reward: 0.419 [0.353, 0.476], mean action: 61.200 [5.000, 93.000], mean observation: 3.162 [-1.621, 10.347], loss: 1.257465, mae: 4.992015, mean_q: 5.169841
 83660/100000: episode: 8534, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.599, mean reward: 0.360 [0.336, 0.429], mean action: 60.700 [18.000, 97.000], mean observation: 3.155 [-0.888, 10.206], loss: 1.394070, mae: 4.992537, mean_q: 5.170467
 83670/100000: episode: 8535, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.863, mean reward: 0.386 [0.355, 0.486], mean action: 68.000 [38.000, 101.000], mean observation: 3.152 [-1.212, 10.232], loss: 0.938059, mae: 4.990646, mean_q: 5.170966
 83680/100000: episode: 8536, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 5.214, mean reward: 0.521 [0.521, 0.522], mean action: 64.300 [35.000, 93.000], mean observation: 3.171 [-1.419, 10.219], loss: 1.244636, mae: 4.991993, mean_q: 5.171486
 83690/100000: episode: 8537, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.201, mean reward: 0.420 [0.350, 0.468], mean action: 53.400 [1.000, 83.000], mean observation: 3.165 [-0.826, 10.318], loss: 1.539700, mae: 4.993191, mean_q: 5.173397
 83700/100000: episode: 8538, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.034, mean reward: 0.403 [0.315, 0.428], mean action: 61.000 [28.000, 91.000], mean observation: 3.165 [-1.830, 10.338], loss: 0.787597, mae: 4.990157, mean_q: 5.174878
 83710/100000: episode: 8539, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 5.601, mean reward: 0.560 [0.560, 0.560], mean action: 61.000 [33.000, 82.000], mean observation: 3.162 [-1.988, 10.364], loss: 1.129649, mae: 4.991919, mean_q: 5.175729
 83720/100000: episode: 8540, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 3.453, mean reward: 0.345 [0.287, 0.435], mean action: 67.800 [35.000, 93.000], mean observation: 3.166 [-1.448, 10.356], loss: 1.129084, mae: 4.991945, mean_q: 5.174450
 83730/100000: episode: 8541, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.842, mean reward: 0.384 [0.313, 0.444], mean action: 45.700 [5.000, 70.000], mean observation: 3.156 [-1.448, 10.284], loss: 1.198002, mae: 4.992290, mean_q: 5.174222
 83740/100000: episode: 8542, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.054, mean reward: 0.405 [0.339, 0.573], mean action: 37.400 [1.000, 87.000], mean observation: 3.160 [-1.631, 10.300], loss: 1.421874, mae: 4.993075, mean_q: 5.173522
 83750/100000: episode: 8543, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 3.850, mean reward: 0.385 [0.310, 0.449], mean action: 31.300 [0.000, 67.000], mean observation: 3.147 [-1.255, 10.318], loss: 1.201820, mae: 4.992206, mean_q: 5.173419
 83760/100000: episode: 8544, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.971, mean reward: 0.397 [0.339, 0.452], mean action: 47.600 [3.000, 93.000], mean observation: 3.154 [-1.280, 10.365], loss: 0.886814, mae: 4.990826, mean_q: 5.172255
 83770/100000: episode: 8545, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.321, mean reward: 0.432 [0.402, 0.506], mean action: 46.300 [5.000, 100.000], mean observation: 3.144 [-1.352, 10.308], loss: 1.228304, mae: 4.992310, mean_q: 5.173775
 83780/100000: episode: 8546, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.061, mean reward: 0.406 [0.306, 0.485], mean action: 37.700 [30.000, 77.000], mean observation: 3.168 [-1.031, 10.225], loss: 1.243832, mae: 4.992271, mean_q: 5.173833
 83790/100000: episode: 8547, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.998, mean reward: 0.400 [0.365, 0.484], mean action: 38.000 [0.000, 83.000], mean observation: 3.154 [-1.277, 10.453], loss: 1.421092, mae: 4.992809, mean_q: 5.172207
 83800/100000: episode: 8548, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.299, mean reward: 0.430 [0.358, 0.475], mean action: 33.900 [8.000, 86.000], mean observation: 3.162 [-1.449, 10.283], loss: 1.018510, mae: 4.991429, mean_q: 5.169294
 83810/100000: episode: 8549, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.549, mean reward: 0.455 [0.440, 0.531], mean action: 42.900 [29.000, 90.000], mean observation: 3.158 [-1.977, 10.429], loss: 1.139711, mae: 4.991738, mean_q: 5.168510
 83820/100000: episode: 8550, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.481, mean reward: 0.448 [0.399, 0.571], mean action: 49.200 [30.000, 96.000], mean observation: 3.149 [-2.096, 10.421], loss: 1.083087, mae: 4.991723, mean_q: 5.167384
 83830/100000: episode: 8551, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.466, mean reward: 0.447 [0.393, 0.554], mean action: 28.000 [0.000, 78.000], mean observation: 3.160 [-1.195, 10.325], loss: 0.947236, mae: 4.991488, mean_q: 5.168374
 83840/100000: episode: 8552, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.967, mean reward: 0.397 [0.332, 0.472], mean action: 39.600 [30.000, 73.000], mean observation: 3.138 [-1.629, 10.295], loss: 1.116418, mae: 4.992407, mean_q: 5.170574
 83849/100000: episode: 8553, duration: 0.164s, episode steps: 9, steps per second: 55, episode reward: 13.228, mean reward: 1.470 [0.316, 10.000], mean action: 39.444 [19.000, 87.000], mean observation: 3.149 [-1.758, 10.265], loss: 0.784735, mae: 4.991319, mean_q: 5.172377
 83859/100000: episode: 8554, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.929, mean reward: 0.393 [0.340, 0.485], mean action: 47.100 [0.000, 89.000], mean observation: 3.157 [-1.740, 10.341], loss: 0.813779, mae: 4.991868, mean_q: 5.174361
 83869/100000: episode: 8555, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.293, mean reward: 0.429 [0.355, 0.545], mean action: 46.600 [1.000, 101.000], mean observation: 3.167 [-0.838, 10.372], loss: 1.300996, mae: 4.994236, mean_q: 5.173561
 83879/100000: episode: 8556, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.383, mean reward: 0.438 [0.347, 0.485], mean action: 53.500 [9.000, 70.000], mean observation: 3.152 [-1.276, 10.316], loss: 1.282575, mae: 4.994296, mean_q: 5.170499
 83889/100000: episode: 8557, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.096, mean reward: 0.410 [0.348, 0.503], mean action: 62.100 [35.000, 70.000], mean observation: 3.149 [-2.216, 10.252], loss: 1.088519, mae: 4.993594, mean_q: 5.171794
 83899/100000: episode: 8558, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.515, mean reward: 0.451 [0.451, 0.451], mean action: 66.900 [36.000, 75.000], mean observation: 3.163 [-1.886, 10.381], loss: 0.967414, mae: 4.993026, mean_q: 5.173310
 83909/100000: episode: 8559, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.171, mean reward: 0.417 [0.319, 0.473], mean action: 56.200 [2.000, 70.000], mean observation: 3.158 [-1.747, 10.242], loss: 1.348044, mae: 4.994786, mean_q: 5.175197
 83919/100000: episode: 8560, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.783, mean reward: 0.378 [0.327, 0.423], mean action: 67.100 [33.000, 95.000], mean observation: 3.163 [-0.967, 10.318], loss: 1.146929, mae: 4.994001, mean_q: 5.176957
 83929/100000: episode: 8561, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.618, mean reward: 0.462 [0.429, 0.483], mean action: 65.000 [8.000, 83.000], mean observation: 3.160 [-1.712, 10.309], loss: 1.642316, mae: 4.995719, mean_q: 5.179939
 83939/100000: episode: 8562, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 5.526, mean reward: 0.553 [0.551, 0.554], mean action: 66.700 [20.000, 90.000], mean observation: 3.171 [-1.181, 10.295], loss: 1.154179, mae: 4.993676, mean_q: 5.182943
 83949/100000: episode: 8563, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.035, mean reward: 0.404 [0.279, 0.474], mean action: 60.800 [3.000, 92.000], mean observation: 3.165 [-1.290, 10.320], loss: 1.512062, mae: 4.995001, mean_q: 5.182300
 83959/100000: episode: 8564, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.712, mean reward: 0.471 [0.463, 0.491], mean action: 58.200 [10.000, 101.000], mean observation: 3.178 [-0.865, 10.388], loss: 1.100278, mae: 4.993107, mean_q: 5.182733
 83969/100000: episode: 8565, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.548, mean reward: 0.455 [0.447, 0.516], mean action: 66.300 [30.000, 96.000], mean observation: 3.171 [-1.362, 10.290], loss: 1.092281, mae: 4.993002, mean_q: 5.180106
 83979/100000: episode: 8566, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.548, mean reward: 0.455 [0.446, 0.524], mean action: 61.700 [19.000, 98.000], mean observation: 3.133 [-1.777, 10.368], loss: 1.398329, mae: 4.994304, mean_q: 5.179801
 83989/100000: episode: 8567, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.427, mean reward: 0.443 [0.339, 0.565], mean action: 44.600 [0.000, 100.000], mean observation: 3.158 [-1.325, 10.306], loss: 1.327694, mae: 4.993841, mean_q: 5.179190
 83999/100000: episode: 8568, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.939, mean reward: 0.494 [0.366, 0.535], mean action: 57.800 [9.000, 70.000], mean observation: 3.162 [-1.610, 10.372], loss: 1.182230, mae: 4.993263, mean_q: 5.177844
 84009/100000: episode: 8569, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.628, mean reward: 0.463 [0.456, 0.528], mean action: 63.000 [6.000, 99.000], mean observation: 3.154 [-1.285, 10.162], loss: 1.053174, mae: 4.992656, mean_q: 5.179593
 84019/100000: episode: 8570, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 3.885, mean reward: 0.388 [0.324, 0.497], mean action: 68.400 [23.000, 99.000], mean observation: 3.157 [-1.203, 10.354], loss: 1.283523, mae: 4.993399, mean_q: 5.181869
 84029/100000: episode: 8571, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.612, mean reward: 0.361 [0.320, 0.421], mean action: 65.100 [35.000, 94.000], mean observation: 3.165 [-0.865, 10.303], loss: 0.911107, mae: 4.992242, mean_q: 5.181932
 84039/100000: episode: 8572, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.857, mean reward: 0.386 [0.351, 0.423], mean action: 55.900 [13.000, 79.000], mean observation: 3.150 [-2.355, 10.211], loss: 1.286856, mae: 4.993857, mean_q: 5.182553
 84049/100000: episode: 8573, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.146, mean reward: 0.415 [0.408, 0.448], mean action: 63.400 [23.000, 70.000], mean observation: 3.159 [-1.080, 10.252], loss: 0.882465, mae: 4.992311, mean_q: 5.184440
 84059/100000: episode: 8574, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.662, mean reward: 0.366 [0.299, 0.438], mean action: 59.700 [22.000, 73.000], mean observation: 3.155 [-1.440, 10.325], loss: 1.050591, mae: 4.993033, mean_q: 5.187260
 84069/100000: episode: 8575, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.990, mean reward: 0.399 [0.376, 0.452], mean action: 64.900 [11.000, 97.000], mean observation: 3.144 [-0.955, 10.214], loss: 0.879833, mae: 4.992436, mean_q: 5.187739
 84079/100000: episode: 8576, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.554, mean reward: 0.455 [0.375, 0.549], mean action: 66.700 [45.000, 81.000], mean observation: 3.150 [-0.961, 10.274], loss: 1.117029, mae: 4.993789, mean_q: 5.188232
 84080/100000: episode: 8577, duration: 0.028s, episode steps: 1, steps per second: 35, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 64.000 [64.000, 64.000], mean observation: 3.166 [-0.720, 10.189], loss: 1.104696, mae: 4.994124, mean_q: 5.189411
 84090/100000: episode: 8578, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 5.185, mean reward: 0.518 [0.517, 0.525], mean action: 64.700 [15.000, 99.000], mean observation: 3.179 [-0.795, 10.263], loss: 1.296162, mae: 4.994525, mean_q: 5.190235
 84100/100000: episode: 8579, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.786, mean reward: 0.479 [0.346, 0.563], mean action: 66.400 [18.000, 94.000], mean observation: 3.160 [-1.804, 10.356], loss: 1.498261, mae: 4.995020, mean_q: 5.187629
 84110/100000: episode: 8580, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.452, mean reward: 0.445 [0.417, 0.489], mean action: 62.300 [38.000, 93.000], mean observation: 3.179 [-2.208, 10.399], loss: 1.422769, mae: 4.994437, mean_q: 5.177372
 84120/100000: episode: 8581, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.079, mean reward: 0.408 [0.312, 0.480], mean action: 43.700 [5.000, 85.000], mean observation: 3.165 [-1.886, 10.350], loss: 1.379283, mae: 4.993951, mean_q: 5.172685
 84130/100000: episode: 8582, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.969, mean reward: 0.397 [0.327, 0.594], mean action: 21.500 [5.000, 94.000], mean observation: 3.150 [-1.658, 10.384], loss: 1.485905, mae: 4.993826, mean_q: 5.173616
 84139/100000: episode: 8583, duration: 0.157s, episode steps: 9, steps per second: 57, episode reward: 13.220, mean reward: 1.469 [0.332, 10.000], mean action: 43.222 [5.000, 85.000], mean observation: 3.151 [-1.538, 10.340], loss: 1.426336, mae: 4.993168, mean_q: 5.174532
 84140/100000: episode: 8584, duration: 0.022s, episode steps: 1, steps per second: 45, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 30.000 [30.000, 30.000], mean observation: 3.151 [-0.542, 10.100], loss: 0.104483, mae: 4.987694, mean_q: 5.173119
 84150/100000: episode: 8585, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.907, mean reward: 0.391 [0.307, 0.523], mean action: 21.300 [5.000, 96.000], mean observation: 3.151 [-1.440, 10.207], loss: 1.050376, mae: 4.991662, mean_q: 5.171087
 84160/100000: episode: 8586, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.422, mean reward: 0.442 [0.375, 0.512], mean action: 26.400 [5.000, 52.000], mean observation: 3.154 [-1.369, 10.416], loss: 1.050212, mae: 4.991573, mean_q: 5.169888
 84170/100000: episode: 8587, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.257, mean reward: 0.426 [0.372, 0.527], mean action: 35.100 [0.000, 83.000], mean observation: 3.160 [-1.501, 10.287], loss: 1.421546, mae: 4.992970, mean_q: 5.170680
 84180/100000: episode: 8588, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.855, mean reward: 0.386 [0.303, 0.459], mean action: 28.900 [4.000, 83.000], mean observation: 3.153 [-1.629, 10.199], loss: 1.080340, mae: 4.991346, mean_q: 5.168223
 84190/100000: episode: 8589, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.693, mean reward: 0.369 [0.326, 0.436], mean action: 43.400 [5.000, 83.000], mean observation: 3.163 [-1.265, 10.281], loss: 1.104325, mae: 4.991323, mean_q: 5.162045
 84200/100000: episode: 8590, duration: 0.222s, episode steps: 10, steps per second: 45, episode reward: 4.328, mean reward: 0.433 [0.410, 0.492], mean action: 11.000 [5.000, 65.000], mean observation: 3.167 [-1.172, 10.292], loss: 1.420798, mae: 4.992537, mean_q: 5.160491
 84210/100000: episode: 8591, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.019, mean reward: 0.402 [0.324, 0.490], mean action: 37.700 [5.000, 85.000], mean observation: 3.160 [-1.289, 10.287], loss: 1.222634, mae: 4.991467, mean_q: 5.160341
 84220/100000: episode: 8592, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 3.911, mean reward: 0.391 [0.306, 0.479], mean action: 18.100 [2.000, 94.000], mean observation: 3.149 [-1.307, 10.189], loss: 1.112846, mae: 4.990945, mean_q: 5.160517
 84230/100000: episode: 8593, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 3.972, mean reward: 0.397 [0.352, 0.474], mean action: 21.400 [5.000, 64.000], mean observation: 3.154 [-1.426, 10.319], loss: 1.092685, mae: 4.990962, mean_q: 5.160765
 84240/100000: episode: 8594, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.221, mean reward: 0.422 [0.371, 0.520], mean action: 34.100 [0.000, 101.000], mean observation: 3.160 [-1.392, 10.446], loss: 1.224453, mae: 4.991561, mean_q: 5.161239
 84250/100000: episode: 8595, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.963, mean reward: 0.396 [0.324, 0.474], mean action: 40.400 [5.000, 87.000], mean observation: 3.153 [-1.871, 10.332], loss: 1.390162, mae: 4.992352, mean_q: 5.162323
 84260/100000: episode: 8596, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.219, mean reward: 0.422 [0.344, 0.551], mean action: 15.900 [5.000, 64.000], mean observation: 3.159 [-1.523, 10.405], loss: 0.730461, mae: 4.989619, mean_q: 5.163690
 84270/100000: episode: 8597, duration: 0.107s, episode steps: 10, steps per second: 93, episode reward: 4.664, mean reward: 0.466 [0.357, 0.484], mean action: 76.300 [26.000, 93.000], mean observation: 3.153 [-0.916, 10.387], loss: 0.798295, mae: 4.990033, mean_q: 5.163901
 84280/100000: episode: 8598, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 5.523, mean reward: 0.552 [0.552, 0.552], mean action: 60.000 [18.000, 89.000], mean observation: 3.131 [-1.515, 10.216], loss: 1.142651, mae: 4.991748, mean_q: 5.165071
 84290/100000: episode: 8599, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.139, mean reward: 0.414 [0.358, 0.514], mean action: 59.600 [0.000, 101.000], mean observation: 3.150 [-1.500, 10.382], loss: 1.085233, mae: 4.991653, mean_q: 5.166404
 84300/100000: episode: 8600, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 4.415, mean reward: 0.442 [0.432, 0.529], mean action: 76.900 [19.000, 99.000], mean observation: 3.153 [-0.825, 10.276], loss: 0.955913, mae: 4.991185, mean_q: 5.167544
 84310/100000: episode: 8601, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.046, mean reward: 0.405 [0.373, 0.425], mean action: 74.600 [20.000, 96.000], mean observation: 3.157 [-1.253, 10.238], loss: 1.243099, mae: 4.992443, mean_q: 5.168417
 84320/100000: episode: 8602, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.095, mean reward: 0.409 [0.391, 0.441], mean action: 79.000 [41.000, 99.000], mean observation: 3.156 [-1.428, 10.467], loss: 1.331021, mae: 4.992786, mean_q: 5.169299
 84330/100000: episode: 8603, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.787, mean reward: 0.379 [0.342, 0.414], mean action: 67.000 [3.000, 87.000], mean observation: 3.156 [-1.601, 10.265], loss: 1.259151, mae: 4.992467, mean_q: 5.169490
 84340/100000: episode: 8604, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 4.188, mean reward: 0.419 [0.383, 0.534], mean action: 76.900 [48.000, 87.000], mean observation: 3.149 [-1.393, 10.308], loss: 1.058491, mae: 4.991763, mean_q: 5.165683
 84350/100000: episode: 8605, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.095, mean reward: 0.409 [0.308, 0.576], mean action: 30.500 [3.000, 91.000], mean observation: 3.160 [-2.086, 10.306], loss: 1.318369, mae: 4.992908, mean_q: 5.165843
 84360/100000: episode: 8606, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.658, mean reward: 0.366 [0.351, 0.403], mean action: 63.400 [10.000, 87.000], mean observation: 3.145 [-1.227, 10.253], loss: 1.253361, mae: 4.992640, mean_q: 5.165127
 84370/100000: episode: 8607, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 4.027, mean reward: 0.403 [0.393, 0.446], mean action: 79.300 [46.000, 87.000], mean observation: 3.172 [-1.361, 10.356], loss: 0.980894, mae: 4.991385, mean_q: 5.165878
 84380/100000: episode: 8608, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.329, mean reward: 0.433 [0.384, 0.484], mean action: 66.400 [5.000, 88.000], mean observation: 3.149 [-1.624, 10.348], loss: 1.121120, mae: 4.992143, mean_q: 5.167614
 84390/100000: episode: 8609, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.927, mean reward: 0.493 [0.376, 0.551], mean action: 69.700 [4.000, 94.000], mean observation: 3.138 [-1.493, 10.253], loss: 1.151732, mae: 4.992691, mean_q: 5.169651
 84400/100000: episode: 8610, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 3.878, mean reward: 0.388 [0.383, 0.421], mean action: 77.800 [26.000, 95.000], mean observation: 3.167 [-0.978, 10.301], loss: 1.236333, mae: 4.993072, mean_q: 5.171229
 84410/100000: episode: 8611, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 5.126, mean reward: 0.513 [0.506, 0.570], mean action: 60.200 [14.000, 100.000], mean observation: 3.173 [-1.474, 10.362], loss: 1.510785, mae: 4.994158, mean_q: 5.172491
 84420/100000: episode: 8612, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 3.935, mean reward: 0.394 [0.352, 0.450], mean action: 64.400 [9.000, 95.000], mean observation: 3.161 [-1.077, 10.472], loss: 1.115969, mae: 4.992600, mean_q: 5.174114
 84430/100000: episode: 8613, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.184, mean reward: 0.418 [0.357, 0.498], mean action: 60.500 [6.000, 87.000], mean observation: 3.151 [-1.004, 10.302], loss: 1.005352, mae: 4.992183, mean_q: 5.175296
 84440/100000: episode: 8614, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.906, mean reward: 0.491 [0.491, 0.491], mean action: 70.900 [32.000, 99.000], mean observation: 3.161 [-0.842, 10.393], loss: 1.357496, mae: 4.993714, mean_q: 5.172246
 84450/100000: episode: 8615, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.805, mean reward: 0.381 [0.314, 0.438], mean action: 60.300 [19.000, 90.000], mean observation: 3.158 [-1.693, 10.274], loss: 1.591343, mae: 4.994401, mean_q: 5.171256
 84460/100000: episode: 8616, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.722, mean reward: 0.372 [0.326, 0.422], mean action: 73.500 [2.000, 88.000], mean observation: 3.163 [-0.756, 10.323], loss: 0.642505, mae: 4.990714, mean_q: 5.171983
 84470/100000: episode: 8617, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.521, mean reward: 0.352 [0.345, 0.405], mean action: 73.600 [24.000, 98.000], mean observation: 3.142 [-1.728, 10.298], loss: 1.178970, mae: 4.992999, mean_q: 5.173079
 84480/100000: episode: 8618, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.202, mean reward: 0.420 [0.301, 0.529], mean action: 57.300 [14.000, 101.000], mean observation: 3.172 [-1.579, 10.574], loss: 1.282511, mae: 4.993286, mean_q: 5.174228
 84490/100000: episode: 8619, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 3.833, mean reward: 0.383 [0.340, 0.542], mean action: 78.100 [15.000, 87.000], mean observation: 3.147 [-1.546, 10.198], loss: 0.916250, mae: 4.991879, mean_q: 5.176013
 84500/100000: episode: 8620, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.355, mean reward: 0.436 [0.397, 0.449], mean action: 67.200 [12.000, 100.000], mean observation: 3.168 [-1.477, 10.340], loss: 1.140916, mae: 4.992888, mean_q: 5.178680
 84510/100000: episode: 8621, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.118, mean reward: 0.412 [0.412, 0.412], mean action: 63.700 [8.000, 87.000], mean observation: 3.160 [-1.828, 10.273], loss: 1.085910, mae: 4.992645, mean_q: 5.182251
 84520/100000: episode: 8622, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.745, mean reward: 0.374 [0.308, 0.439], mean action: 71.800 [33.000, 87.000], mean observation: 3.159 [-1.034, 10.361], loss: 1.343943, mae: 4.993905, mean_q: 5.177368
 84530/100000: episode: 8623, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.557, mean reward: 0.456 [0.316, 0.539], mean action: 79.100 [0.000, 91.000], mean observation: 3.153 [-1.391, 10.325], loss: 1.160230, mae: 4.993064, mean_q: 5.167063
 84540/100000: episode: 8624, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.468, mean reward: 0.447 [0.401, 0.542], mean action: 69.900 [9.000, 95.000], mean observation: 3.154 [-0.904, 10.309], loss: 1.313240, mae: 4.993511, mean_q: 5.164326
 84550/100000: episode: 8625, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.371, mean reward: 0.437 [0.420, 0.499], mean action: 69.600 [23.000, 87.000], mean observation: 3.156 [-1.814, 10.361], loss: 1.323793, mae: 4.993366, mean_q: 5.164303
 84560/100000: episode: 8626, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.793, mean reward: 0.379 [0.294, 0.471], mean action: 77.100 [58.000, 87.000], mean observation: 3.152 [-1.826, 10.305], loss: 1.328902, mae: 4.993314, mean_q: 5.166315
 84570/100000: episode: 8627, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.884, mean reward: 0.388 [0.325, 0.529], mean action: 63.500 [22.000, 87.000], mean observation: 3.154 [-1.806, 10.470], loss: 1.426002, mae: 4.993739, mean_q: 5.168281
 84580/100000: episode: 8628, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.893, mean reward: 0.389 [0.363, 0.496], mean action: 69.600 [13.000, 87.000], mean observation: 3.131 [-0.953, 10.275], loss: 1.184044, mae: 4.992720, mean_q: 5.170612
 84590/100000: episode: 8629, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.882, mean reward: 0.488 [0.387, 0.530], mean action: 64.900 [7.000, 92.000], mean observation: 3.167 [-2.000, 10.396], loss: 1.396590, mae: 4.993110, mean_q: 5.173267
 84600/100000: episode: 8630, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.168, mean reward: 0.417 [0.374, 0.442], mean action: 51.000 [1.000, 97.000], mean observation: 3.166 [-1.828, 10.303], loss: 1.350633, mae: 4.992754, mean_q: 5.175655
 84610/100000: episode: 8631, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.013, mean reward: 0.401 [0.310, 0.437], mean action: 59.900 [11.000, 92.000], mean observation: 3.138 [-1.754, 10.283], loss: 1.281568, mae: 4.992585, mean_q: 5.178361
 84620/100000: episode: 8632, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.098, mean reward: 0.410 [0.340, 0.484], mean action: 73.400 [5.000, 101.000], mean observation: 3.161 [-1.518, 10.389], loss: 1.153444, mae: 4.991893, mean_q: 5.181649
 84630/100000: episode: 8633, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.662, mean reward: 0.466 [0.373, 0.483], mean action: 67.700 [4.000, 87.000], mean observation: 3.175 [-1.290, 10.269], loss: 1.114757, mae: 4.991826, mean_q: 5.185380
 84640/100000: episode: 8634, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 4.093, mean reward: 0.409 [0.389, 0.449], mean action: 77.100 [28.000, 92.000], mean observation: 3.150 [-1.676, 10.233], loss: 1.151753, mae: 4.992308, mean_q: 5.188215
 84650/100000: episode: 8635, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.010, mean reward: 0.401 [0.389, 0.448], mean action: 66.400 [32.000, 87.000], mean observation: 3.163 [-1.437, 10.417], loss: 0.741738, mae: 4.991006, mean_q: 5.190419
 84660/100000: episode: 8636, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.705, mean reward: 0.370 [0.333, 0.495], mean action: 61.400 [4.000, 90.000], mean observation: 3.148 [-1.666, 10.240], loss: 1.027841, mae: 4.992344, mean_q: 5.187022
 84670/100000: episode: 8637, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 4.611, mean reward: 0.461 [0.375, 0.535], mean action: 69.200 [28.000, 87.000], mean observation: 3.159 [-1.169, 10.337], loss: 1.022921, mae: 4.992616, mean_q: 5.180915
 84680/100000: episode: 8638, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.015, mean reward: 0.402 [0.314, 0.535], mean action: 59.300 [23.000, 87.000], mean observation: 3.163 [-0.978, 10.310], loss: 1.214800, mae: 4.993597, mean_q: 5.180551
 84681/100000: episode: 8639, duration: 0.042s, episode steps: 1, steps per second: 24, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 57.000 [57.000, 57.000], mean observation: 3.146 [-1.154, 10.126], loss: 0.783357, mae: 4.991674, mean_q: 5.181279
 84691/100000: episode: 8640, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 3.785, mean reward: 0.379 [0.379, 0.379], mean action: 87.700 [80.000, 98.000], mean observation: 3.137 [-1.197, 10.338], loss: 1.076215, mae: 4.992985, mean_q: 5.179546
 84701/100000: episode: 8641, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.859, mean reward: 0.386 [0.358, 0.443], mean action: 65.500 [1.000, 98.000], mean observation: 3.151 [-2.221, 10.415], loss: 1.027364, mae: 4.993111, mean_q: 5.177738
 84711/100000: episode: 8642, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 5.489, mean reward: 0.549 [0.313, 0.631], mean action: 66.100 [7.000, 99.000], mean observation: 3.153 [-2.233, 10.283], loss: 1.424833, mae: 4.994984, mean_q: 5.174987
 84721/100000: episode: 8643, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 3.686, mean reward: 0.369 [0.340, 0.436], mean action: 66.700 [7.000, 97.000], mean observation: 3.140 [-1.842, 10.213], loss: 1.085288, mae: 4.993578, mean_q: 5.173110
 84731/100000: episode: 8644, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.136, mean reward: 0.414 [0.334, 0.545], mean action: 25.100 [5.000, 70.000], mean observation: 3.158 [-1.326, 10.302], loss: 1.060382, mae: 4.993509, mean_q: 5.174523
 84741/100000: episode: 8645, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.177, mean reward: 0.418 [0.312, 0.507], mean action: 24.400 [5.000, 85.000], mean observation: 3.154 [-1.829, 10.275], loss: 1.188434, mae: 4.994196, mean_q: 5.176115
 84751/100000: episode: 8646, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.848, mean reward: 0.385 [0.337, 0.458], mean action: 64.100 [21.000, 98.000], mean observation: 3.170 [-1.056, 10.322], loss: 0.918684, mae: 4.993342, mean_q: 5.176764
 84761/100000: episode: 8647, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 4.313, mean reward: 0.431 [0.427, 0.460], mean action: 59.200 [9.000, 87.000], mean observation: 3.148 [-1.416, 10.432], loss: 1.215037, mae: 4.994594, mean_q: 5.178411
 84771/100000: episode: 8648, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 3.983, mean reward: 0.398 [0.368, 0.410], mean action: 82.700 [48.000, 99.000], mean observation: 3.163 [-0.860, 10.460], loss: 1.322549, mae: 4.995141, mean_q: 5.176366
 84781/100000: episode: 8649, duration: 0.127s, episode steps: 10, steps per second: 78, episode reward: 4.199, mean reward: 0.420 [0.370, 0.483], mean action: 68.300 [9.000, 87.000], mean observation: 3.158 [-1.615, 10.474], loss: 0.892833, mae: 4.993455, mean_q: 5.175337
 84789/100000: episode: 8650, duration: 0.122s, episode steps: 8, steps per second: 66, episode reward: 12.824, mean reward: 1.603 [0.394, 10.000], mean action: 56.000 [5.000, 87.000], mean observation: 3.157 [-1.185, 10.307], loss: 0.904802, mae: 4.993698, mean_q: 5.172080
 84799/100000: episode: 8651, duration: 0.232s, episode steps: 10, steps per second: 43, episode reward: 4.006, mean reward: 0.401 [0.288, 0.523], mean action: 22.600 [5.000, 84.000], mean observation: 3.163 [-1.058, 10.432], loss: 1.623721, mae: 4.996327, mean_q: 5.171014
 84808/100000: episode: 8652, duration: 0.171s, episode steps: 9, steps per second: 53, episode reward: 13.237, mean reward: 1.471 [0.333, 10.000], mean action: 32.333 [5.000, 87.000], mean observation: 3.158 [-1.354, 10.430], loss: 1.256142, mae: 4.994676, mean_q: 5.178064
 84818/100000: episode: 8653, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.180, mean reward: 0.418 [0.352, 0.531], mean action: 26.800 [5.000, 92.000], mean observation: 3.154 [-1.578, 10.259], loss: 1.297326, mae: 4.994762, mean_q: 5.181922
 84828/100000: episode: 8654, duration: 0.233s, episode steps: 10, steps per second: 43, episode reward: 4.224, mean reward: 0.422 [0.324, 0.565], mean action: 18.900 [5.000, 50.000], mean observation: 3.160 [-1.664, 10.481], loss: 1.530563, mae: 4.995477, mean_q: 5.183956
 84838/100000: episode: 8655, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 3.913, mean reward: 0.391 [0.339, 0.493], mean action: 14.600 [5.000, 55.000], mean observation: 3.159 [-1.392, 10.298], loss: 1.164577, mae: 4.993875, mean_q: 5.185555
 84848/100000: episode: 8656, duration: 0.190s, episode steps: 10, steps per second: 52, episode reward: 3.891, mean reward: 0.389 [0.320, 0.474], mean action: 36.500 [5.000, 70.000], mean observation: 3.157 [-2.084, 10.325], loss: 1.316322, mae: 4.994136, mean_q: 5.187525
 84858/100000: episode: 8657, duration: 0.230s, episode steps: 10, steps per second: 44, episode reward: 4.116, mean reward: 0.412 [0.338, 0.456], mean action: 13.500 [5.000, 90.000], mean observation: 3.162 [-2.551, 10.339], loss: 1.158667, mae: 4.993280, mean_q: 5.189407
 84868/100000: episode: 8658, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.846, mean reward: 0.485 [0.352, 0.593], mean action: 37.600 [5.000, 86.000], mean observation: 3.148 [-1.621, 10.275], loss: 1.098072, mae: 4.993002, mean_q: 5.190426
 84878/100000: episode: 8659, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.145, mean reward: 0.414 [0.366, 0.444], mean action: 14.900 [5.000, 72.000], mean observation: 3.147 [-1.155, 10.230], loss: 1.165662, mae: 4.993210, mean_q: 5.189794
 84888/100000: episode: 8660, duration: 0.225s, episode steps: 10, steps per second: 45, episode reward: 4.232, mean reward: 0.423 [0.310, 0.510], mean action: 22.200 [0.000, 51.000], mean observation: 3.152 [-1.823, 10.368], loss: 1.287598, mae: 4.993962, mean_q: 5.187388
 84898/100000: episode: 8661, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.802, mean reward: 0.380 [0.311, 0.512], mean action: 51.400 [5.000, 98.000], mean observation: 3.150 [-1.252, 10.444], loss: 1.392057, mae: 4.994211, mean_q: 5.187140
 84908/100000: episode: 8662, duration: 0.215s, episode steps: 10, steps per second: 46, episode reward: 3.899, mean reward: 0.390 [0.331, 0.514], mean action: 12.100 [5.000, 46.000], mean observation: 3.159 [-1.042, 10.337], loss: 1.201106, mae: 4.993186, mean_q: 5.183868
 84918/100000: episode: 8663, duration: 0.215s, episode steps: 10, steps per second: 47, episode reward: 4.429, mean reward: 0.443 [0.352, 0.589], mean action: 10.500 [1.000, 45.000], mean observation: 3.157 [-1.204, 10.281], loss: 1.189923, mae: 4.993126, mean_q: 5.182767
 84928/100000: episode: 8664, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.813, mean reward: 0.381 [0.292, 0.465], mean action: 37.100 [5.000, 88.000], mean observation: 3.154 [-1.603, 10.408], loss: 1.254346, mae: 4.993421, mean_q: 5.183457
 84938/100000: episode: 8665, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.985, mean reward: 0.398 [0.318, 0.522], mean action: 22.400 [5.000, 98.000], mean observation: 3.153 [-1.596, 10.242], loss: 1.288197, mae: 4.993087, mean_q: 5.184599
 84948/100000: episode: 8666, duration: 0.227s, episode steps: 10, steps per second: 44, episode reward: 4.006, mean reward: 0.401 [0.289, 0.523], mean action: 16.600 [0.000, 75.000], mean observation: 3.149 [-1.414, 10.311], loss: 1.025515, mae: 4.992068, mean_q: 5.186225
 84958/100000: episode: 8667, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 3.718, mean reward: 0.372 [0.288, 0.446], mean action: 29.500 [5.000, 92.000], mean observation: 3.164 [-1.751, 10.268], loss: 1.090056, mae: 4.992392, mean_q: 5.187534
 84968/100000: episode: 8668, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.345, mean reward: 0.435 [0.361, 0.478], mean action: 25.700 [5.000, 83.000], mean observation: 3.160 [-1.455, 10.371], loss: 1.094712, mae: 4.992474, mean_q: 5.188806
 84978/100000: episode: 8669, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.192, mean reward: 0.419 [0.355, 0.488], mean action: 37.300 [1.000, 91.000], mean observation: 3.160 [-1.962, 10.442], loss: 1.292189, mae: 4.993329, mean_q: 5.191081
 84988/100000: episode: 8670, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.711, mean reward: 0.471 [0.355, 0.565], mean action: 35.100 [5.000, 96.000], mean observation: 3.153 [-0.943, 10.412], loss: 1.258645, mae: 4.993258, mean_q: 5.200238
 84998/100000: episode: 8671, duration: 0.220s, episode steps: 10, steps per second: 45, episode reward: 3.838, mean reward: 0.384 [0.321, 0.491], mean action: 27.200 [4.000, 89.000], mean observation: 3.162 [-1.863, 10.400], loss: 1.234469, mae: 4.993054, mean_q: 5.205365
 85008/100000: episode: 8672, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.042, mean reward: 0.404 [0.323, 0.543], mean action: 35.300 [5.000, 91.000], mean observation: 3.154 [-1.341, 10.319], loss: 1.131609, mae: 4.992575, mean_q: 5.206574
 85018/100000: episode: 8673, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.091, mean reward: 0.409 [0.263, 0.483], mean action: 30.800 [5.000, 61.000], mean observation: 3.150 [-1.134, 10.183], loss: 1.113706, mae: 4.992616, mean_q: 5.197170
 85028/100000: episode: 8674, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.629, mean reward: 0.463 [0.371, 0.510], mean action: 31.200 [5.000, 76.000], mean observation: 3.152 [-0.919, 10.268], loss: 0.959221, mae: 4.992083, mean_q: 5.193103
 85038/100000: episode: 8675, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.001, mean reward: 0.400 [0.342, 0.485], mean action: 32.800 [5.000, 93.000], mean observation: 3.152 [-1.405, 10.230], loss: 1.163114, mae: 4.992951, mean_q: 5.192541
 85048/100000: episode: 8676, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.285, mean reward: 0.429 [0.330, 0.582], mean action: 41.300 [5.000, 100.000], mean observation: 3.170 [-1.071, 10.316], loss: 1.223259, mae: 4.993385, mean_q: 5.190142
 85058/100000: episode: 8677, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.418, mean reward: 0.442 [0.383, 0.482], mean action: 33.500 [5.000, 101.000], mean observation: 3.156 [-1.194, 10.301], loss: 1.349508, mae: 4.993824, mean_q: 5.189076
 85068/100000: episode: 8678, duration: 0.215s, episode steps: 10, steps per second: 46, episode reward: 4.065, mean reward: 0.406 [0.335, 0.570], mean action: 18.300 [5.000, 54.000], mean observation: 3.156 [-1.526, 10.302], loss: 1.469996, mae: 4.994075, mean_q: 5.189963
 85078/100000: episode: 8679, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.351, mean reward: 0.435 [0.299, 0.551], mean action: 30.400 [5.000, 84.000], mean observation: 3.163 [-1.095, 10.429], loss: 1.155837, mae: 4.992744, mean_q: 5.188686
 85088/100000: episode: 8680, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.432, mean reward: 0.443 [0.417, 0.592], mean action: 22.800 [4.000, 66.000], mean observation: 3.169 [-1.156, 10.388], loss: 1.223278, mae: 4.993145, mean_q: 5.187336
 85098/100000: episode: 8681, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.542, mean reward: 0.454 [0.339, 0.543], mean action: 35.600 [5.000, 101.000], mean observation: 3.160 [-1.063, 10.285], loss: 1.534710, mae: 4.994106, mean_q: 5.187786
 85108/100000: episode: 8682, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.771, mean reward: 0.477 [0.347, 0.571], mean action: 24.100 [2.000, 81.000], mean observation: 3.141 [-1.570, 10.261], loss: 1.330272, mae: 4.993033, mean_q: 5.185776
 85118/100000: episode: 8683, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 3.773, mean reward: 0.377 [0.319, 0.474], mean action: 30.100 [1.000, 96.000], mean observation: 3.159 [-1.283, 10.284], loss: 1.449078, mae: 4.993353, mean_q: 5.185659
 85128/100000: episode: 8684, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.810, mean reward: 0.381 [0.326, 0.456], mean action: 36.300 [4.000, 79.000], mean observation: 3.149 [-1.312, 10.308], loss: 1.260458, mae: 4.992537, mean_q: 5.188145
 85137/100000: episode: 8685, duration: 0.191s, episode steps: 9, steps per second: 47, episode reward: 13.232, mean reward: 1.470 [0.340, 10.000], mean action: 26.222 [5.000, 84.000], mean observation: 3.162 [-1.726, 10.683], loss: 1.229487, mae: 4.992144, mean_q: 5.190397
 85147/100000: episode: 8686, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.094, mean reward: 0.409 [0.359, 0.462], mean action: 25.200 [5.000, 97.000], mean observation: 3.152 [-1.344, 10.361], loss: 1.129854, mae: 4.991883, mean_q: 5.192654
 85157/100000: episode: 8687, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 4.011, mean reward: 0.401 [0.317, 0.506], mean action: 29.600 [5.000, 100.000], mean observation: 3.167 [-1.469, 10.229], loss: 1.257827, mae: 4.992429, mean_q: 5.191705
 85167/100000: episode: 8688, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.116, mean reward: 0.412 [0.301, 0.571], mean action: 30.600 [5.000, 92.000], mean observation: 3.148 [-0.992, 10.389], loss: 1.025597, mae: 4.991408, mean_q: 5.190974
 85177/100000: episode: 8689, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.108, mean reward: 0.411 [0.332, 0.485], mean action: 38.000 [5.000, 96.000], mean observation: 3.174 [-1.218, 10.451], loss: 1.345835, mae: 4.992887, mean_q: 5.192838
 85178/100000: episode: 8690, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 49.000 [49.000, 49.000], mean observation: 3.152 [-0.920, 10.468], loss: 1.086514, mae: 4.991927, mean_q: 5.193675
 85188/100000: episode: 8691, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.126, mean reward: 0.413 [0.287, 0.523], mean action: 30.300 [2.000, 65.000], mean observation: 3.157 [-1.474, 10.250], loss: 1.289797, mae: 4.992372, mean_q: 5.193017
 85198/100000: episode: 8692, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.475, mean reward: 0.448 [0.362, 0.568], mean action: 17.600 [5.000, 83.000], mean observation: 3.155 [-1.354, 10.280], loss: 1.445746, mae: 4.992736, mean_q: 5.192055
 85208/100000: episode: 8693, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.197, mean reward: 0.420 [0.303, 0.507], mean action: 44.300 [5.000, 101.000], mean observation: 3.152 [-1.492, 10.237], loss: 1.216668, mae: 4.991766, mean_q: 5.192477
 85218/100000: episode: 8694, duration: 0.217s, episode steps: 10, steps per second: 46, episode reward: 4.200, mean reward: 0.420 [0.319, 0.594], mean action: 11.200 [5.000, 22.000], mean observation: 3.158 [-1.223, 10.226], loss: 1.245462, mae: 4.991859, mean_q: 5.192737
 85225/100000: episode: 8695, duration: 0.144s, episode steps: 7, steps per second: 49, episode reward: 13.020, mean reward: 1.860 [0.343, 10.000], mean action: 28.857 [5.000, 60.000], mean observation: 3.154 [-1.296, 10.249], loss: 1.222381, mae: 4.991362, mean_q: 5.190192
 85235/100000: episode: 8696, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.303, mean reward: 0.430 [0.347, 0.520], mean action: 22.400 [5.000, 60.000], mean observation: 3.149 [-1.762, 10.280], loss: 0.982382, mae: 4.990154, mean_q: 5.186118
 85245/100000: episode: 8697, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.324, mean reward: 0.432 [0.344, 0.532], mean action: 20.300 [5.000, 82.000], mean observation: 3.160 [-1.439, 10.345], loss: 1.192738, mae: 4.991057, mean_q: 5.183688
 85255/100000: episode: 8698, duration: 0.217s, episode steps: 10, steps per second: 46, episode reward: 4.277, mean reward: 0.428 [0.352, 0.592], mean action: 21.500 [5.000, 84.000], mean observation: 3.163 [-1.280, 10.319], loss: 1.129011, mae: 4.990946, mean_q: 5.181200
 85265/100000: episode: 8699, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.189, mean reward: 0.419 [0.378, 0.478], mean action: 41.400 [3.000, 100.000], mean observation: 3.146 [-1.736, 10.326], loss: 1.045565, mae: 4.990952, mean_q: 5.181691
 85275/100000: episode: 8700, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.097, mean reward: 0.410 [0.317, 0.475], mean action: 9.000 [5.000, 45.000], mean observation: 3.164 [-1.189, 10.342], loss: 1.200116, mae: 4.991634, mean_q: 5.183465
 85285/100000: episode: 8701, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.541, mean reward: 0.454 [0.407, 0.512], mean action: 35.300 [5.000, 84.000], mean observation: 3.163 [-1.083, 10.389], loss: 1.082756, mae: 4.991012, mean_q: 5.178443
 85295/100000: episode: 8702, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.820, mean reward: 0.382 [0.296, 0.443], mean action: 37.600 [5.000, 100.000], mean observation: 3.158 [-1.732, 10.400], loss: 0.986300, mae: 4.991288, mean_q: 5.174548
 85305/100000: episode: 8703, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.033, mean reward: 0.403 [0.361, 0.441], mean action: 35.700 [5.000, 98.000], mean observation: 3.148 [-1.055, 10.422], loss: 1.282392, mae: 4.992455, mean_q: 5.172417
 85315/100000: episode: 8704, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.107, mean reward: 0.411 [0.347, 0.510], mean action: 32.200 [5.000, 101.000], mean observation: 3.161 [-1.647, 10.389], loss: 1.084046, mae: 4.992029, mean_q: 5.173216
 85325/100000: episode: 8705, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.131, mean reward: 0.413 [0.324, 0.453], mean action: 19.400 [5.000, 62.000], mean observation: 3.158 [-1.165, 10.474], loss: 1.447955, mae: 4.993311, mean_q: 5.178781
 85335/100000: episode: 8706, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.957, mean reward: 0.396 [0.284, 0.598], mean action: 27.700 [5.000, 72.000], mean observation: 3.160 [-1.088, 10.405], loss: 1.467834, mae: 4.993320, mean_q: 5.182968
 85345/100000: episode: 8707, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.505, mean reward: 0.450 [0.413, 0.474], mean action: 50.300 [5.000, 100.000], mean observation: 3.157 [-1.341, 10.279], loss: 1.179034, mae: 4.992358, mean_q: 5.181172
 85355/100000: episode: 8708, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.254, mean reward: 0.425 [0.366, 0.514], mean action: 38.700 [5.000, 71.000], mean observation: 3.146 [-1.754, 10.235], loss: 1.086269, mae: 4.992083, mean_q: 5.177695
 85365/100000: episode: 8709, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.966, mean reward: 0.397 [0.339, 0.533], mean action: 24.100 [5.000, 97.000], mean observation: 3.164 [-1.562, 10.385], loss: 1.413238, mae: 4.993451, mean_q: 5.177216
 85375/100000: episode: 8710, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.200, mean reward: 0.420 [0.384, 0.490], mean action: 30.300 [5.000, 85.000], mean observation: 3.148 [-1.392, 10.413], loss: 1.144033, mae: 4.992152, mean_q: 5.177768
 85385/100000: episode: 8711, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.410, mean reward: 0.441 [0.345, 0.546], mean action: 26.200 [5.000, 77.000], mean observation: 3.156 [-1.581, 10.291], loss: 1.620387, mae: 4.993727, mean_q: 5.177499
 85395/100000: episode: 8712, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.317, mean reward: 0.432 [0.331, 0.501], mean action: 16.200 [2.000, 44.000], mean observation: 3.159 [-1.208, 10.438], loss: 1.161543, mae: 4.991524, mean_q: 5.176426
 85405/100000: episode: 8713, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.237, mean reward: 0.424 [0.377, 0.470], mean action: 26.000 [5.000, 96.000], mean observation: 3.156 [-1.620, 10.498], loss: 0.996768, mae: 4.990966, mean_q: 5.177547
 85415/100000: episode: 8714, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.330, mean reward: 0.433 [0.362, 0.572], mean action: 19.500 [5.000, 87.000], mean observation: 3.163 [-1.684, 10.422], loss: 1.417736, mae: 4.992672, mean_q: 5.179332
 85425/100000: episode: 8715, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.552, mean reward: 0.455 [0.392, 0.525], mean action: 43.600 [5.000, 98.000], mean observation: 3.152 [-1.254, 10.382], loss: 0.935360, mae: 4.990620, mean_q: 5.181617
 85435/100000: episode: 8716, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 4.350, mean reward: 0.435 [0.312, 0.497], mean action: 12.300 [5.000, 63.000], mean observation: 3.159 [-0.974, 10.497], loss: 1.002434, mae: 4.991130, mean_q: 5.183043
 85445/100000: episode: 8717, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.605, mean reward: 0.461 [0.391, 0.495], mean action: 38.400 [5.000, 96.000], mean observation: 3.167 [-1.597, 10.310], loss: 1.423452, mae: 4.993004, mean_q: 5.179820
 85455/100000: episode: 8718, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.997, mean reward: 0.400 [0.358, 0.452], mean action: 30.000 [2.000, 100.000], mean observation: 3.153 [-1.064, 10.468], loss: 1.085690, mae: 4.991624, mean_q: 5.178524
 85465/100000: episode: 8719, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.498, mean reward: 0.450 [0.384, 0.552], mean action: 18.900 [5.000, 87.000], mean observation: 3.159 [-1.118, 10.533], loss: 1.525263, mae: 4.993509, mean_q: 5.176201
 85475/100000: episode: 8720, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.832, mean reward: 0.383 [0.339, 0.431], mean action: 33.100 [5.000, 75.000], mean observation: 3.150 [-1.259, 10.279], loss: 1.490772, mae: 4.992836, mean_q: 5.173065
 85485/100000: episode: 8721, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.968, mean reward: 0.397 [0.317, 0.569], mean action: 14.700 [5.000, 66.000], mean observation: 3.159 [-1.168, 10.404], loss: 1.309811, mae: 4.992068, mean_q: 5.173427
 85495/100000: episode: 8722, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 3.543, mean reward: 0.354 [0.306, 0.481], mean action: 24.300 [5.000, 72.000], mean observation: 3.151 [-1.185, 10.295], loss: 1.215993, mae: 4.991586, mean_q: 5.175736
 85505/100000: episode: 8723, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.632, mean reward: 0.463 [0.389, 0.589], mean action: 28.200 [3.000, 99.000], mean observation: 3.159 [-1.417, 10.316], loss: 1.344476, mae: 4.991909, mean_q: 5.177668
 85515/100000: episode: 8724, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.041, mean reward: 0.404 [0.338, 0.473], mean action: 34.700 [5.000, 86.000], mean observation: 3.157 [-1.560, 10.460], loss: 1.262803, mae: 4.991354, mean_q: 5.179336
 85525/100000: episode: 8725, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.285, mean reward: 0.428 [0.345, 0.514], mean action: 25.300 [5.000, 90.000], mean observation: 3.156 [-1.864, 10.435], loss: 1.159522, mae: 4.990790, mean_q: 5.181188
 85535/100000: episode: 8726, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.080, mean reward: 0.408 [0.358, 0.509], mean action: 30.800 [5.000, 85.000], mean observation: 3.161 [-1.325, 10.499], loss: 1.088088, mae: 4.990736, mean_q: 5.186892
 85545/100000: episode: 8727, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.869, mean reward: 0.387 [0.288, 0.456], mean action: 14.000 [5.000, 85.000], mean observation: 3.171 [-1.491, 10.499], loss: 1.389388, mae: 4.992023, mean_q: 5.193482
 85555/100000: episode: 8728, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.826, mean reward: 0.483 [0.352, 0.674], mean action: 36.300 [4.000, 98.000], mean observation: 3.139 [-1.184, 10.421], loss: 1.346781, mae: 4.991655, mean_q: 5.196769
 85565/100000: episode: 8729, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.123, mean reward: 0.412 [0.364, 0.522], mean action: 33.700 [5.000, 71.000], mean observation: 3.157 [-1.390, 10.458], loss: 1.397065, mae: 4.991670, mean_q: 5.197578
 85575/100000: episode: 8730, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.337, mean reward: 0.434 [0.339, 0.503], mean action: 44.100 [5.000, 94.000], mean observation: 3.145 [-1.307, 10.438], loss: 1.325039, mae: 4.991602, mean_q: 5.194774
 85585/100000: episode: 8731, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.440, mean reward: 0.444 [0.327, 0.579], mean action: 34.400 [4.000, 94.000], mean observation: 3.152 [-1.053, 10.500], loss: 0.991956, mae: 4.990158, mean_q: 5.189036
 85595/100000: episode: 8732, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.217, mean reward: 0.422 [0.349, 0.458], mean action: 40.000 [5.000, 92.000], mean observation: 3.151 [-1.514, 10.392], loss: 1.380661, mae: 4.991684, mean_q: 5.182921
 85603/100000: episode: 8733, duration: 0.163s, episode steps: 8, steps per second: 49, episode reward: 13.098, mean reward: 1.637 [0.354, 10.000], mean action: 22.625 [2.000, 83.000], mean observation: 3.167 [-1.574, 10.524], loss: 0.918653, mae: 4.989803, mean_q: 5.181159
 85613/100000: episode: 8734, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.082, mean reward: 0.408 [0.318, 0.445], mean action: 48.600 [5.000, 101.000], mean observation: 3.153 [-1.448, 10.320], loss: 1.317953, mae: 4.991401, mean_q: 5.178628
 85623/100000: episode: 8735, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.056, mean reward: 0.406 [0.326, 0.460], mean action: 39.000 [5.000, 100.000], mean observation: 3.160 [-2.239, 10.252], loss: 1.169296, mae: 4.990638, mean_q: 5.178533
 85631/100000: episode: 8736, duration: 0.183s, episode steps: 8, steps per second: 44, episode reward: 13.161, mean reward: 1.645 [0.363, 10.000], mean action: 9.125 [3.000, 31.000], mean observation: 3.158 [-1.083, 10.129], loss: 1.380145, mae: 4.991545, mean_q: 5.179900
 85641/100000: episode: 8737, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.336, mean reward: 0.434 [0.371, 0.542], mean action: 33.100 [5.000, 82.000], mean observation: 3.156 [-1.537, 10.383], loss: 1.208365, mae: 4.990849, mean_q: 5.180883
 85651/100000: episode: 8738, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.164, mean reward: 0.416 [0.368, 0.489], mean action: 21.400 [5.000, 78.000], mean observation: 3.159 [-2.504, 10.299], loss: 1.307169, mae: 4.991123, mean_q: 5.183606
 85661/100000: episode: 8739, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.914, mean reward: 0.391 [0.323, 0.504], mean action: 21.100 [5.000, 51.000], mean observation: 3.155 [-1.478, 10.390], loss: 1.050543, mae: 4.990195, mean_q: 5.187825
 85671/100000: episode: 8740, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.888, mean reward: 0.389 [0.304, 0.446], mean action: 28.900 [1.000, 101.000], mean observation: 3.162 [-1.258, 10.312], loss: 1.306251, mae: 4.991288, mean_q: 5.188890
 85681/100000: episode: 8741, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.754, mean reward: 0.375 [0.278, 0.490], mean action: 45.900 [5.000, 98.000], mean observation: 3.157 [-1.175, 10.370], loss: 1.019422, mae: 4.990396, mean_q: 5.188376
 85691/100000: episode: 8742, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.092, mean reward: 0.409 [0.354, 0.465], mean action: 29.800 [5.000, 78.000], mean observation: 3.158 [-1.270, 10.341], loss: 1.691579, mae: 4.993264, mean_q: 5.189701
 85698/100000: episode: 8743, duration: 0.122s, episode steps: 7, steps per second: 57, episode reward: 12.321, mean reward: 1.760 [0.342, 10.000], mean action: 47.286 [5.000, 90.000], mean observation: 3.159 [-0.942, 10.274], loss: 1.035155, mae: 4.990528, mean_q: 5.188211
 85708/100000: episode: 8744, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.042, mean reward: 0.404 [0.332, 0.478], mean action: 34.400 [10.000, 90.000], mean observation: 3.152 [-1.474, 10.282], loss: 0.988290, mae: 4.990458, mean_q: 5.186092
 85718/100000: episode: 8745, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.073, mean reward: 0.407 [0.323, 0.459], mean action: 41.100 [10.000, 96.000], mean observation: 3.167 [-1.582, 10.441], loss: 1.187906, mae: 4.991492, mean_q: 5.187551
 85728/100000: episode: 8746, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.962, mean reward: 0.396 [0.348, 0.449], mean action: 47.800 [10.000, 91.000], mean observation: 3.151 [-1.690, 10.260], loss: 0.929403, mae: 4.990975, mean_q: 5.187652
 85738/100000: episode: 8747, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.960, mean reward: 0.396 [0.351, 0.483], mean action: 24.400 [10.000, 92.000], mean observation: 3.156 [-1.529, 10.296], loss: 1.077464, mae: 4.991925, mean_q: 5.183224
 85748/100000: episode: 8748, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.375, mean reward: 0.438 [0.298, 0.506], mean action: 35.600 [10.000, 99.000], mean observation: 3.166 [-1.567, 10.467], loss: 1.174641, mae: 4.992715, mean_q: 5.181906
 85751/100000: episode: 8749, duration: 0.067s, episode steps: 3, steps per second: 45, episode reward: 10.857, mean reward: 3.619 [0.403, 10.000], mean action: 36.667 [10.000, 90.000], mean observation: 3.171 [-2.000, 10.372], loss: 0.796975, mae: 4.991205, mean_q: 5.182106
 85761/100000: episode: 8750, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.249, mean reward: 0.425 [0.333, 0.507], mean action: 30.600 [0.000, 93.000], mean observation: 3.152 [-1.438, 10.450], loss: 1.304256, mae: 4.993304, mean_q: 5.183208
 85771/100000: episode: 8751, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.348, mean reward: 0.435 [0.316, 0.512], mean action: 21.200 [1.000, 91.000], mean observation: 3.152 [-1.423, 10.449], loss: 1.222185, mae: 4.992617, mean_q: 5.184373
 85781/100000: episode: 8752, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.321, mean reward: 0.432 [0.427, 0.476], mean action: 26.100 [7.000, 88.000], mean observation: 3.146 [-1.535, 10.351], loss: 1.424577, mae: 4.993208, mean_q: 5.184744
 85791/100000: episode: 8753, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.097, mean reward: 0.410 [0.304, 0.457], mean action: 37.300 [1.000, 99.000], mean observation: 3.149 [-2.207, 10.247], loss: 1.261107, mae: 4.992538, mean_q: 5.184363
 85801/100000: episode: 8754, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.319, mean reward: 0.432 [0.335, 0.605], mean action: 22.200 [4.000, 61.000], mean observation: 3.151 [-1.233, 10.168], loss: 1.174476, mae: 4.992072, mean_q: 5.179950
 85811/100000: episode: 8755, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 5.002, mean reward: 0.500 [0.500, 0.500], mean action: 25.000 [4.000, 62.000], mean observation: 3.167 [-2.242, 10.275], loss: 1.282821, mae: 4.992325, mean_q: 5.180315
 85821/100000: episode: 8756, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.613, mean reward: 0.461 [0.459, 0.478], mean action: 19.600 [4.000, 78.000], mean observation: 3.152 [-2.009, 10.386], loss: 1.314995, mae: 4.992518, mean_q: 5.181026
 85831/100000: episode: 8757, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.105, mean reward: 0.410 [0.277, 0.501], mean action: 21.400 [4.000, 99.000], mean observation: 3.157 [-1.453, 10.352], loss: 1.148685, mae: 4.991633, mean_q: 5.182293
 85841/100000: episode: 8758, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.184, mean reward: 0.418 [0.387, 0.470], mean action: 48.600 [4.000, 98.000], mean observation: 3.160 [-1.269, 10.374], loss: 1.424511, mae: 4.992545, mean_q: 5.180839
 85851/100000: episode: 8759, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 3.898, mean reward: 0.390 [0.358, 0.509], mean action: 16.500 [4.000, 41.000], mean observation: 3.162 [-1.137, 10.344], loss: 1.492660, mae: 4.992302, mean_q: 5.179901
 85861/100000: episode: 8760, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 3.958, mean reward: 0.396 [0.349, 0.495], mean action: 16.600 [10.000, 74.000], mean observation: 3.159 [-1.218, 10.387], loss: 1.188067, mae: 4.990773, mean_q: 5.181948
 85871/100000: episode: 8761, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.806, mean reward: 0.481 [0.366, 0.542], mean action: 40.700 [10.000, 97.000], mean observation: 3.133 [-1.736, 10.371], loss: 1.090855, mae: 4.990192, mean_q: 5.184785
 85881/100000: episode: 8762, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.230, mean reward: 0.423 [0.342, 0.463], mean action: 50.500 [10.000, 99.000], mean observation: 3.153 [-1.256, 10.139], loss: 1.222505, mae: 4.990634, mean_q: 5.188056
 85885/100000: episode: 8763, duration: 0.090s, episode steps: 4, steps per second: 44, episode reward: 11.341, mean reward: 2.835 [0.396, 10.000], mean action: 11.500 [10.000, 16.000], mean observation: 3.176 [-1.612, 10.255], loss: 1.616795, mae: 4.992236, mean_q: 5.189566
 85895/100000: episode: 8764, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.513, mean reward: 0.451 [0.391, 0.489], mean action: 30.700 [10.000, 70.000], mean observation: 3.156 [-1.297, 10.495], loss: 0.952793, mae: 4.989631, mean_q: 5.190969
 85905/100000: episode: 8765, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.329, mean reward: 0.433 [0.338, 0.531], mean action: 47.500 [10.000, 92.000], mean observation: 3.155 [-1.460, 10.339], loss: 1.044967, mae: 4.990108, mean_q: 5.193442
 85915/100000: episode: 8766, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.532, mean reward: 0.453 [0.351, 0.513], mean action: 35.800 [10.000, 90.000], mean observation: 3.159 [-1.433, 10.278], loss: 1.265880, mae: 4.990945, mean_q: 5.195770
 85925/100000: episode: 8767, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.925, mean reward: 0.393 [0.292, 0.474], mean action: 25.800 [10.000, 68.000], mean observation: 3.149 [-1.455, 10.396], loss: 1.031654, mae: 4.990101, mean_q: 5.197335
 85935/100000: episode: 8768, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.987, mean reward: 0.399 [0.315, 0.458], mean action: 27.000 [10.000, 100.000], mean observation: 3.157 [-1.197, 10.390], loss: 1.399930, mae: 4.991898, mean_q: 5.193631
 85945/100000: episode: 8769, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 3.645, mean reward: 0.365 [0.313, 0.464], mean action: 33.000 [10.000, 92.000], mean observation: 3.157 [-1.107, 10.336], loss: 1.487231, mae: 4.992306, mean_q: 5.193247
 85955/100000: episode: 8770, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.042, mean reward: 0.404 [0.347, 0.454], mean action: 28.000 [0.000, 96.000], mean observation: 3.154 [-1.813, 10.187], loss: 1.217403, mae: 4.991423, mean_q: 5.193844
 85965/100000: episode: 8771, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.211, mean reward: 0.421 [0.300, 0.567], mean action: 20.900 [10.000, 80.000], mean observation: 3.151 [-1.221, 10.263], loss: 0.791965, mae: 4.989771, mean_q: 5.195563
 85975/100000: episode: 8772, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.011, mean reward: 0.401 [0.308, 0.509], mean action: 28.500 [10.000, 77.000], mean observation: 3.159 [-1.600, 10.381], loss: 1.081532, mae: 4.991336, mean_q: 5.197125
 85985/100000: episode: 8773, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.083, mean reward: 0.408 [0.352, 0.556], mean action: 32.000 [10.000, 76.000], mean observation: 3.156 [-1.767, 10.289], loss: 1.094350, mae: 4.991645, mean_q: 5.198636
 85995/100000: episode: 8774, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.143, mean reward: 0.414 [0.348, 0.508], mean action: 31.900 [10.000, 79.000], mean observation: 3.152 [-1.526, 10.231], loss: 1.413203, mae: 4.993093, mean_q: 5.199978
 86005/100000: episode: 8775, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.375, mean reward: 0.437 [0.348, 0.512], mean action: 26.800 [10.000, 88.000], mean observation: 3.162 [-1.293, 10.286], loss: 1.257249, mae: 4.992337, mean_q: 5.201332
 86015/100000: episode: 8776, duration: 0.231s, episode steps: 10, steps per second: 43, episode reward: 3.991, mean reward: 0.399 [0.339, 0.491], mean action: 10.500 [10.000, 15.000], mean observation: 3.159 [-1.689, 10.308], loss: 0.958480, mae: 4.990877, mean_q: 5.201118
 86025/100000: episode: 8777, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.008, mean reward: 0.401 [0.305, 0.453], mean action: 42.900 [10.000, 101.000], mean observation: 3.151 [-1.331, 10.277], loss: 1.251156, mae: 4.992011, mean_q: 5.196825
 86035/100000: episode: 8778, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.923, mean reward: 0.392 [0.331, 0.453], mean action: 36.100 [10.000, 100.000], mean observation: 3.146 [-1.915, 10.245], loss: 1.381657, mae: 4.992642, mean_q: 5.196836
 86045/100000: episode: 8779, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.928, mean reward: 0.393 [0.328, 0.553], mean action: 37.200 [10.000, 99.000], mean observation: 3.151 [-2.172, 10.254], loss: 1.030100, mae: 4.991416, mean_q: 5.198090
 86055/100000: episode: 8780, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.744, mean reward: 0.374 [0.305, 0.438], mean action: 43.600 [10.000, 98.000], mean observation: 3.157 [-1.447, 10.394], loss: 0.998737, mae: 4.991153, mean_q: 5.199462
 86065/100000: episode: 8781, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 3.998, mean reward: 0.400 [0.318, 0.537], mean action: 18.100 [10.000, 91.000], mean observation: 3.164 [-0.960, 10.346], loss: 1.038965, mae: 4.991736, mean_q: 5.200108
 86075/100000: episode: 8782, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.963, mean reward: 0.396 [0.370, 0.558], mean action: 36.600 [10.000, 100.000], mean observation: 3.160 [-1.399, 10.319], loss: 1.009823, mae: 4.991969, mean_q: 5.197168
 86085/100000: episode: 8783, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.312, mean reward: 0.431 [0.333, 0.511], mean action: 41.500 [0.000, 99.000], mean observation: 3.154 [-1.109, 10.271], loss: 1.155972, mae: 4.992914, mean_q: 5.196757
 86095/100000: episode: 8784, duration: 0.224s, episode steps: 10, steps per second: 45, episode reward: 4.201, mean reward: 0.420 [0.367, 0.519], mean action: 19.500 [10.000, 79.000], mean observation: 3.164 [-1.080, 10.389], loss: 1.298334, mae: 4.993404, mean_q: 5.203036
 86105/100000: episode: 8785, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.191, mean reward: 0.419 [0.369, 0.517], mean action: 40.400 [10.000, 100.000], mean observation: 3.149 [-1.247, 10.249], loss: 1.495975, mae: 4.994531, mean_q: 5.208228
 86115/100000: episode: 8786, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 3.743, mean reward: 0.374 [0.347, 0.424], mean action: 26.700 [10.000, 96.000], mean observation: 3.156 [-1.566, 10.280], loss: 1.126526, mae: 4.992850, mean_q: 5.210540
 86125/100000: episode: 8787, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.042, mean reward: 0.404 [0.321, 0.488], mean action: 17.100 [10.000, 51.000], mean observation: 3.159 [-2.083, 10.310], loss: 1.229925, mae: 4.993105, mean_q: 5.208153
 86135/100000: episode: 8788, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.087, mean reward: 0.409 [0.392, 0.448], mean action: 26.500 [10.000, 95.000], mean observation: 3.158 [-1.770, 10.285], loss: 1.227820, mae: 4.993367, mean_q: 5.206974
 86145/100000: episode: 8789, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 3.953, mean reward: 0.395 [0.315, 0.508], mean action: 31.100 [10.000, 82.000], mean observation: 3.164 [-1.197, 10.462], loss: 1.418481, mae: 4.994000, mean_q: 5.208564
 86155/100000: episode: 8790, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 4.164, mean reward: 0.416 [0.296, 0.521], mean action: 24.000 [10.000, 65.000], mean observation: 3.161 [-1.603, 10.272], loss: 1.225717, mae: 4.993429, mean_q: 5.209970
 86165/100000: episode: 8791, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.293, mean reward: 0.429 [0.348, 0.464], mean action: 25.400 [10.000, 89.000], mean observation: 3.148 [-1.596, 10.329], loss: 1.481622, mae: 4.994592, mean_q: 5.210952
 86175/100000: episode: 8792, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 3.698, mean reward: 0.370 [0.319, 0.438], mean action: 17.200 [10.000, 56.000], mean observation: 3.160 [-1.463, 10.288], loss: 0.992701, mae: 4.992648, mean_q: 5.208885
 86185/100000: episode: 8793, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.800, mean reward: 0.380 [0.352, 0.480], mean action: 40.700 [10.000, 81.000], mean observation: 3.166 [-1.733, 10.246], loss: 1.332778, mae: 4.993872, mean_q: 5.205237
 86195/100000: episode: 8794, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.079, mean reward: 0.408 [0.346, 0.586], mean action: 32.300 [3.000, 98.000], mean observation: 3.158 [-1.343, 10.299], loss: 1.419099, mae: 4.993856, mean_q: 5.197982
 86205/100000: episode: 8795, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.439, mean reward: 0.444 [0.367, 0.508], mean action: 43.400 [10.000, 93.000], mean observation: 3.154 [-1.437, 10.346], loss: 1.015553, mae: 4.992240, mean_q: 5.197098
 86215/100000: episode: 8796, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.023, mean reward: 0.402 [0.314, 0.542], mean action: 17.300 [10.000, 61.000], mean observation: 3.160 [-1.708, 10.349], loss: 1.202659, mae: 4.993022, mean_q: 5.197750
 86225/100000: episode: 8797, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.349, mean reward: 0.435 [0.315, 0.531], mean action: 26.000 [10.000, 60.000], mean observation: 3.157 [-1.553, 10.322], loss: 1.382512, mae: 4.993913, mean_q: 5.198730
 86235/100000: episode: 8798, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.316, mean reward: 0.432 [0.353, 0.464], mean action: 38.400 [10.000, 88.000], mean observation: 3.152 [-1.273, 10.391], loss: 1.525702, mae: 4.994422, mean_q: 5.198424
 86245/100000: episode: 8799, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.116, mean reward: 0.412 [0.371, 0.476], mean action: 40.100 [10.000, 83.000], mean observation: 3.158 [-1.366, 10.453], loss: 0.930320, mae: 4.992085, mean_q: 5.194758
 86255/100000: episode: 8800, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.260, mean reward: 0.426 [0.324, 0.474], mean action: 49.700 [10.000, 98.000], mean observation: 3.140 [-0.936, 10.249], loss: 1.100815, mae: 4.992965, mean_q: 5.193768
 86265/100000: episode: 8801, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.119, mean reward: 0.412 [0.352, 0.556], mean action: 26.600 [10.000, 97.000], mean observation: 3.162 [-1.904, 10.351], loss: 1.351556, mae: 4.994009, mean_q: 5.195777
 86275/100000: episode: 8802, duration: 0.214s, episode steps: 10, steps per second: 47, episode reward: 4.402, mean reward: 0.440 [0.377, 0.501], mean action: 38.000 [10.000, 100.000], mean observation: 3.167 [-1.710, 10.375], loss: 1.419693, mae: 4.994437, mean_q: 5.202441
 86285/100000: episode: 8803, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.241, mean reward: 0.424 [0.363, 0.567], mean action: 29.000 [10.000, 79.000], mean observation: 3.158 [-1.283, 10.281], loss: 1.236238, mae: 4.993518, mean_q: 5.202571
 86295/100000: episode: 8804, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.210, mean reward: 0.421 [0.349, 0.554], mean action: 37.200 [3.000, 100.000], mean observation: 3.165 [-1.308, 10.463], loss: 1.540665, mae: 4.994516, mean_q: 5.200767
 86305/100000: episode: 8805, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.728, mean reward: 0.373 [0.331, 0.454], mean action: 28.100 [7.000, 99.000], mean observation: 3.156 [-1.168, 10.425], loss: 1.058283, mae: 4.992239, mean_q: 5.197893
 86315/100000: episode: 8806, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 3.965, mean reward: 0.396 [0.352, 0.458], mean action: 29.100 [7.000, 86.000], mean observation: 3.158 [-1.362, 10.364], loss: 1.238048, mae: 4.993099, mean_q: 5.195712
 86325/100000: episode: 8807, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.236, mean reward: 0.424 [0.345, 0.575], mean action: 23.100 [10.000, 55.000], mean observation: 3.160 [-1.119, 10.389], loss: 1.575467, mae: 4.994465, mean_q: 5.193040
 86335/100000: episode: 8808, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.042, mean reward: 0.404 [0.311, 0.501], mean action: 27.200 [3.000, 71.000], mean observation: 3.163 [-1.699, 10.337], loss: 1.205841, mae: 4.992501, mean_q: 5.189421
 86345/100000: episode: 8809, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.088, mean reward: 0.409 [0.388, 0.483], mean action: 43.600 [10.000, 100.000], mean observation: 3.159 [-1.951, 10.320], loss: 0.768249, mae: 4.990950, mean_q: 5.187818
 86355/100000: episode: 8810, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.960, mean reward: 0.396 [0.336, 0.443], mean action: 39.200 [10.000, 82.000], mean observation: 3.155 [-1.010, 10.202], loss: 1.598228, mae: 4.994410, mean_q: 5.188517
 86365/100000: episode: 8811, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.265, mean reward: 0.426 [0.308, 0.511], mean action: 27.400 [4.000, 96.000], mean observation: 3.158 [-1.079, 10.448], loss: 0.883320, mae: 4.991648, mean_q: 5.189042
 86371/100000: episode: 8812, duration: 0.119s, episode steps: 6, steps per second: 51, episode reward: 12.069, mean reward: 2.011 [0.378, 10.000], mean action: 49.167 [10.000, 96.000], mean observation: 3.157 [-0.826, 10.247], loss: 1.442599, mae: 4.994137, mean_q: 5.191553
 86381/100000: episode: 8813, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.534, mean reward: 0.453 [0.443, 0.493], mean action: 22.800 [10.000, 76.000], mean observation: 3.165 [-2.155, 10.391], loss: 1.473105, mae: 4.994208, mean_q: 5.192407
 86391/100000: episode: 8814, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.266, mean reward: 0.427 [0.337, 0.491], mean action: 40.900 [10.000, 90.000], mean observation: 3.160 [-1.093, 10.341], loss: 1.035462, mae: 4.992355, mean_q: 5.191663
 86401/100000: episode: 8815, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.023, mean reward: 0.402 [0.308, 0.469], mean action: 25.100 [6.000, 90.000], mean observation: 3.160 [-1.409, 10.276], loss: 1.115448, mae: 4.992850, mean_q: 5.193137
 86411/100000: episode: 8816, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.735, mean reward: 0.474 [0.406, 0.540], mean action: 44.300 [1.000, 101.000], mean observation: 3.162 [-1.066, 10.308], loss: 1.366863, mae: 4.993930, mean_q: 5.194678
 86421/100000: episode: 8817, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.133, mean reward: 0.413 [0.368, 0.452], mean action: 27.700 [6.000, 99.000], mean observation: 3.166 [-1.468, 10.320], loss: 1.393696, mae: 4.994049, mean_q: 5.197440
 86431/100000: episode: 8818, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.232, mean reward: 0.423 [0.328, 0.584], mean action: 31.700 [1.000, 99.000], mean observation: 3.145 [-1.070, 10.380], loss: 1.371353, mae: 4.993829, mean_q: 5.200142
 86441/100000: episode: 8819, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.779, mean reward: 0.378 [0.322, 0.471], mean action: 25.600 [10.000, 95.000], mean observation: 3.158 [-2.146, 10.454], loss: 1.311531, mae: 4.993543, mean_q: 5.199972
 86451/100000: episode: 8820, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.075, mean reward: 0.407 [0.365, 0.468], mean action: 21.900 [10.000, 67.000], mean observation: 3.154 [-1.667, 10.386], loss: 1.071507, mae: 4.992611, mean_q: 5.197680
 86461/100000: episode: 8821, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.052, mean reward: 0.405 [0.385, 0.450], mean action: 27.200 [10.000, 96.000], mean observation: 3.149 [-1.720, 10.277], loss: 1.262108, mae: 4.993363, mean_q: 5.191024
 86471/100000: episode: 8822, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.675, mean reward: 0.368 [0.333, 0.433], mean action: 33.600 [10.000, 90.000], mean observation: 3.166 [-1.741, 10.371], loss: 1.091559, mae: 4.992709, mean_q: 5.188742
 86481/100000: episode: 8823, duration: 0.225s, episode steps: 10, steps per second: 44, episode reward: 4.011, mean reward: 0.401 [0.341, 0.491], mean action: 19.600 [10.000, 60.000], mean observation: 3.158 [-1.359, 10.435], loss: 1.086578, mae: 4.992766, mean_q: 5.186455
 86491/100000: episode: 8824, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.037, mean reward: 0.404 [0.268, 0.510], mean action: 23.800 [15.000, 69.000], mean observation: 3.166 [-1.216, 10.152], loss: 1.747139, mae: 4.994997, mean_q: 5.182639
 86501/100000: episode: 8825, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.865, mean reward: 0.387 [0.332, 0.427], mean action: 42.600 [16.000, 98.000], mean observation: 3.157 [-1.084, 10.290], loss: 1.300607, mae: 4.992967, mean_q: 5.179316
 86511/100000: episode: 8826, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.432, mean reward: 0.443 [0.382, 0.480], mean action: 28.700 [0.000, 81.000], mean observation: 3.146 [-1.313, 10.333], loss: 1.224704, mae: 4.992322, mean_q: 5.177537
 86520/100000: episode: 8827, duration: 0.180s, episode steps: 9, steps per second: 50, episode reward: 13.781, mean reward: 1.531 [0.416, 10.000], mean action: 31.889 [1.000, 64.000], mean observation: 3.152 [-1.196, 10.193], loss: 0.986177, mae: 4.990996, mean_q: 5.178579
 86529/100000: episode: 8828, duration: 0.189s, episode steps: 9, steps per second: 48, episode reward: 13.021, mean reward: 1.447 [0.315, 10.000], mean action: 34.000 [13.000, 87.000], mean observation: 3.160 [-1.278, 10.361], loss: 1.201731, mae: 4.992076, mean_q: 5.180984
 86539/100000: episode: 8829, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.210, mean reward: 0.421 [0.346, 0.539], mean action: 28.100 [16.000, 72.000], mean observation: 3.154 [-1.172, 10.275], loss: 1.220274, mae: 4.992387, mean_q: 5.181897
 86549/100000: episode: 8830, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.443, mean reward: 0.444 [0.339, 0.517], mean action: 29.700 [8.000, 93.000], mean observation: 3.159 [-1.315, 10.314], loss: 1.250201, mae: 4.992459, mean_q: 5.181533
 86559/100000: episode: 8831, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.955, mean reward: 0.395 [0.334, 0.528], mean action: 40.400 [12.000, 100.000], mean observation: 3.144 [-1.281, 10.437], loss: 0.929438, mae: 4.990991, mean_q: 5.180223
 86569/100000: episode: 8832, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.189, mean reward: 0.419 [0.299, 0.591], mean action: 36.700 [16.000, 95.000], mean observation: 3.154 [-1.197, 10.405], loss: 1.466642, mae: 4.993175, mean_q: 5.179904
 86579/100000: episode: 8833, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 3.968, mean reward: 0.397 [0.259, 0.463], mean action: 31.100 [16.000, 79.000], mean observation: 3.161 [-1.321, 10.409], loss: 1.144284, mae: 4.991621, mean_q: 5.178696
 86589/100000: episode: 8834, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.901, mean reward: 0.390 [0.345, 0.535], mean action: 22.100 [10.000, 70.000], mean observation: 3.156 [-1.322, 10.310], loss: 1.433931, mae: 4.992558, mean_q: 5.177513
 86599/100000: episode: 8835, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.498, mean reward: 0.450 [0.352, 0.522], mean action: 30.300 [5.000, 95.000], mean observation: 3.159 [-1.783, 10.401], loss: 1.544272, mae: 4.992753, mean_q: 5.178170
 86609/100000: episode: 8836, duration: 0.220s, episode steps: 10, steps per second: 45, episode reward: 4.021, mean reward: 0.402 [0.336, 0.500], mean action: 18.100 [1.000, 50.000], mean observation: 3.146 [-1.270, 10.285], loss: 1.237172, mae: 4.991081, mean_q: 5.179584
 86613/100000: episode: 8837, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 11.178, mean reward: 2.794 [0.337, 10.000], mean action: 62.750 [10.000, 96.000], mean observation: 3.153 [-0.993, 10.163], loss: 1.192487, mae: 4.990790, mean_q: 5.176099
 86623/100000: episode: 8838, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.113, mean reward: 0.411 [0.367, 0.501], mean action: 23.400 [11.000, 80.000], mean observation: 3.168 [-1.184, 10.322], loss: 1.318860, mae: 4.991337, mean_q: 5.175884
 86633/100000: episode: 8839, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.829, mean reward: 0.383 [0.363, 0.425], mean action: 23.600 [15.000, 80.000], mean observation: 3.165 [-1.280, 10.263], loss: 1.382662, mae: 4.991403, mean_q: 5.175703
 86643/100000: episode: 8840, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.148, mean reward: 0.415 [0.315, 0.507], mean action: 22.900 [8.000, 51.000], mean observation: 3.154 [-1.587, 10.315], loss: 1.396120, mae: 4.990959, mean_q: 5.176953
 86653/100000: episode: 8841, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.315, mean reward: 0.432 [0.354, 0.527], mean action: 33.400 [7.000, 100.000], mean observation: 3.153 [-1.384, 10.408], loss: 1.437553, mae: 4.990881, mean_q: 5.177074
 86663/100000: episode: 8842, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 3.897, mean reward: 0.390 [0.344, 0.470], mean action: 30.800 [0.000, 85.000], mean observation: 3.149 [-1.364, 10.439], loss: 1.050908, mae: 4.988951, mean_q: 5.177214
 86673/100000: episode: 8843, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.437, mean reward: 0.444 [0.355, 0.517], mean action: 39.900 [1.000, 92.000], mean observation: 3.160 [-0.903, 10.280], loss: 1.193002, mae: 4.989627, mean_q: 5.179324
 86683/100000: episode: 8844, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.532, mean reward: 0.453 [0.335, 0.530], mean action: 25.300 [10.000, 79.000], mean observation: 3.156 [-1.827, 10.459], loss: 1.291543, mae: 4.989997, mean_q: 5.181687
 86693/100000: episode: 8845, duration: 0.219s, episode steps: 10, steps per second: 46, episode reward: 4.071, mean reward: 0.407 [0.319, 0.506], mean action: 18.400 [4.000, 88.000], mean observation: 3.156 [-1.163, 10.405], loss: 1.334753, mae: 4.990180, mean_q: 5.183188
 86703/100000: episode: 8846, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.990, mean reward: 0.399 [0.324, 0.584], mean action: 40.700 [10.000, 85.000], mean observation: 3.155 [-1.663, 10.435], loss: 1.576517, mae: 4.991025, mean_q: 5.184101
 86713/100000: episode: 8847, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.449, mean reward: 0.445 [0.376, 0.534], mean action: 21.500 [10.000, 83.000], mean observation: 3.158 [-1.223, 10.397], loss: 1.127043, mae: 4.989146, mean_q: 5.181574
 86723/100000: episode: 8848, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.829, mean reward: 0.383 [0.272, 0.490], mean action: 42.200 [10.000, 73.000], mean observation: 3.144 [-1.100, 10.215], loss: 1.053677, mae: 4.989048, mean_q: 5.177209
 86733/100000: episode: 8849, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 4.135, mean reward: 0.413 [0.323, 0.543], mean action: 70.000 [26.000, 97.000], mean observation: 3.164 [-1.937, 10.318], loss: 1.384641, mae: 4.990552, mean_q: 5.177140
 86743/100000: episode: 8850, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.231, mean reward: 0.423 [0.357, 0.510], mean action: 62.900 [26.000, 67.000], mean observation: 3.145 [-1.156, 10.353], loss: 1.190168, mae: 4.989704, mean_q: 5.177848
 86753/100000: episode: 8851, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.082, mean reward: 0.408 [0.325, 0.551], mean action: 43.300 [6.000, 67.000], mean observation: 3.167 [-1.355, 10.537], loss: 1.392727, mae: 4.990376, mean_q: 5.179225
 86763/100000: episode: 8852, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.146, mean reward: 0.415 [0.360, 0.445], mean action: 56.700 [28.000, 84.000], mean observation: 3.170 [-1.666, 10.331], loss: 1.011853, mae: 4.988853, mean_q: 5.180160
 86773/100000: episode: 8853, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.516, mean reward: 0.352 [0.310, 0.382], mean action: 62.400 [16.000, 78.000], mean observation: 3.157 [-0.839, 10.270], loss: 1.011439, mae: 4.988873, mean_q: 5.177283
 86783/100000: episode: 8854, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.495, mean reward: 0.450 [0.362, 0.484], mean action: 59.200 [16.000, 97.000], mean observation: 3.160 [-2.149, 10.245], loss: 0.851215, mae: 4.988396, mean_q: 5.172693
 86793/100000: episode: 8855, duration: 0.231s, episode steps: 10, steps per second: 43, episode reward: 4.243, mean reward: 0.424 [0.330, 0.583], mean action: 16.500 [6.000, 31.000], mean observation: 3.152 [-1.737, 10.280], loss: 0.960099, mae: 4.989114, mean_q: 5.173991
 86795/100000: episode: 8856, duration: 0.060s, episode steps: 2, steps per second: 33, episode reward: 10.429, mean reward: 5.215 [0.429, 10.000], mean action: 24.500 [16.000, 33.000], mean observation: 3.161 [-1.220, 10.100], loss: 0.625867, mae: 4.988186, mean_q: 5.175143
 86805/100000: episode: 8857, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.204, mean reward: 0.420 [0.338, 0.480], mean action: 49.800 [16.000, 87.000], mean observation: 3.159 [-1.594, 10.334], loss: 1.358220, mae: 4.991154, mean_q: 5.174579
 86815/100000: episode: 8858, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 5.424, mean reward: 0.542 [0.523, 0.580], mean action: 20.200 [1.000, 59.000], mean observation: 3.157 [-1.488, 10.374], loss: 1.345532, mae: 4.990912, mean_q: 5.173993
 86825/100000: episode: 8859, duration: 0.228s, episode steps: 10, steps per second: 44, episode reward: 4.041, mean reward: 0.404 [0.343, 0.499], mean action: 17.000 [0.000, 40.000], mean observation: 3.161 [-1.411, 10.379], loss: 1.085688, mae: 4.989722, mean_q: 5.175709
 86835/100000: episode: 8860, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.112, mean reward: 0.411 [0.401, 0.479], mean action: 25.200 [16.000, 73.000], mean observation: 3.157 [-1.413, 10.226], loss: 0.916449, mae: 4.989185, mean_q: 5.177393
 86845/100000: episode: 8861, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.028, mean reward: 0.403 [0.319, 0.496], mean action: 41.000 [16.000, 100.000], mean observation: 3.164 [-1.424, 10.427], loss: 1.271959, mae: 4.990298, mean_q: 5.178602
 86855/100000: episode: 8862, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.933, mean reward: 0.393 [0.377, 0.449], mean action: 29.900 [16.000, 77.000], mean observation: 3.158 [-1.737, 10.273], loss: 1.309245, mae: 4.990674, mean_q: 5.181296
 86865/100000: episode: 8863, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.022, mean reward: 0.402 [0.306, 0.495], mean action: 27.100 [5.000, 95.000], mean observation: 3.159 [-1.731, 10.438], loss: 0.994109, mae: 4.989227, mean_q: 5.183416
 86875/100000: episode: 8864, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.034, mean reward: 0.403 [0.339, 0.542], mean action: 47.100 [11.000, 99.000], mean observation: 3.165 [-1.810, 10.245], loss: 1.254417, mae: 4.990296, mean_q: 5.179469
 86885/100000: episode: 8865, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.832, mean reward: 0.383 [0.347, 0.445], mean action: 41.900 [11.000, 98.000], mean observation: 3.149 [-1.829, 10.248], loss: 1.367423, mae: 4.990528, mean_q: 5.177518
 86895/100000: episode: 8866, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.998, mean reward: 0.400 [0.335, 0.486], mean action: 35.400 [7.000, 90.000], mean observation: 3.151 [-1.775, 10.348], loss: 1.222872, mae: 4.989748, mean_q: 5.177791
 86905/100000: episode: 8867, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.286, mean reward: 0.429 [0.366, 0.449], mean action: 45.600 [16.000, 92.000], mean observation: 3.161 [-1.539, 10.246], loss: 1.087791, mae: 4.989213, mean_q: 5.176112
 86915/100000: episode: 8868, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.436, mean reward: 0.444 [0.374, 0.510], mean action: 21.600 [4.000, 48.000], mean observation: 3.168 [-1.727, 10.356], loss: 1.373263, mae: 4.990323, mean_q: 5.175488
 86925/100000: episode: 8869, duration: 0.212s, episode steps: 10, steps per second: 47, episode reward: 4.228, mean reward: 0.423 [0.333, 0.572], mean action: 17.900 [3.000, 34.000], mean observation: 3.161 [-1.559, 10.331], loss: 1.043386, mae: 4.989079, mean_q: 5.173287
 86935/100000: episode: 8870, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.061, mean reward: 0.406 [0.333, 0.463], mean action: 42.900 [3.000, 91.000], mean observation: 3.157 [-1.384, 10.423], loss: 1.393288, mae: 4.990519, mean_q: 5.172447
 86945/100000: episode: 8871, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.657, mean reward: 0.366 [0.336, 0.446], mean action: 42.200 [12.000, 97.000], mean observation: 3.150 [-1.274, 10.248], loss: 1.535499, mae: 4.990936, mean_q: 5.168284
 86955/100000: episode: 8872, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.660, mean reward: 0.366 [0.328, 0.414], mean action: 71.400 [9.000, 93.000], mean observation: 3.149 [-0.776, 10.390], loss: 1.303261, mae: 4.989584, mean_q: 5.168670
 86965/100000: episode: 8873, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.932, mean reward: 0.393 [0.383, 0.438], mean action: 76.000 [6.000, 93.000], mean observation: 3.147 [-1.421, 10.361], loss: 1.251250, mae: 4.989347, mean_q: 5.169706
 86975/100000: episode: 8874, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.324, mean reward: 0.432 [0.411, 0.459], mean action: 67.600 [1.000, 93.000], mean observation: 3.147 [-1.274, 10.402], loss: 1.215907, mae: 4.988886, mean_q: 5.171266
 86985/100000: episode: 8875, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.680, mean reward: 0.468 [0.385, 0.477], mean action: 71.000 [2.000, 93.000], mean observation: 3.151 [-1.234, 10.253], loss: 1.182871, mae: 4.988803, mean_q: 5.173144
 86995/100000: episode: 8876, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 3.875, mean reward: 0.387 [0.357, 0.418], mean action: 65.600 [1.000, 94.000], mean observation: 3.162 [-1.580, 10.279], loss: 0.985111, mae: 4.988091, mean_q: 5.174972
 87005/100000: episode: 8877, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.573, mean reward: 0.357 [0.313, 0.525], mean action: 81.700 [11.000, 93.000], mean observation: 3.171 [-0.747, 10.318], loss: 1.138534, mae: 4.989382, mean_q: 5.176962
 87015/100000: episode: 8878, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.862, mean reward: 0.386 [0.386, 0.386], mean action: 72.100 [23.000, 93.000], mean observation: 3.156 [-1.511, 10.380], loss: 1.263777, mae: 4.989688, mean_q: 5.175760
 87025/100000: episode: 8879, duration: 0.109s, episode steps: 10, steps per second: 91, episode reward: 3.708, mean reward: 0.371 [0.351, 0.435], mean action: 73.900 [16.000, 95.000], mean observation: 3.145 [-0.874, 10.289], loss: 1.375346, mae: 4.989911, mean_q: 5.171980
 87035/100000: episode: 8880, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.095, mean reward: 0.409 [0.333, 0.509], mean action: 36.400 [2.000, 93.000], mean observation: 3.153 [-1.356, 10.419], loss: 1.069113, mae: 4.988344, mean_q: 5.173097
 87045/100000: episode: 8881, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 4.107, mean reward: 0.411 [0.355, 0.442], mean action: 22.100 [2.000, 74.000], mean observation: 3.163 [-1.607, 10.373], loss: 1.276170, mae: 4.989194, mean_q: 5.177598
 87055/100000: episode: 8882, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.214, mean reward: 0.421 [0.368, 0.475], mean action: 28.400 [2.000, 94.000], mean observation: 3.156 [-1.257, 10.296], loss: 0.971886, mae: 4.987800, mean_q: 5.181008
 87062/100000: episode: 8883, duration: 0.134s, episode steps: 7, steps per second: 52, episode reward: 12.628, mean reward: 1.804 [0.413, 10.000], mean action: 40.000 [2.000, 99.000], mean observation: 3.167 [-0.832, 10.463], loss: 1.061046, mae: 4.988302, mean_q: 5.183590
 87072/100000: episode: 8884, duration: 0.210s, episode steps: 10, steps per second: 48, episode reward: 4.113, mean reward: 0.411 [0.314, 0.517], mean action: 17.100 [2.000, 99.000], mean observation: 3.149 [-1.153, 10.290], loss: 1.214398, mae: 4.989043, mean_q: 5.186832
 87082/100000: episode: 8885, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.020, mean reward: 0.402 [0.332, 0.474], mean action: 45.200 [2.000, 95.000], mean observation: 3.165 [-1.887, 10.206], loss: 1.121400, mae: 4.988836, mean_q: 5.188425
 87092/100000: episode: 8886, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.301, mean reward: 0.430 [0.380, 0.518], mean action: 28.800 [2.000, 80.000], mean observation: 3.152 [-1.581, 10.317], loss: 1.210642, mae: 4.989380, mean_q: 5.189415
 87102/100000: episode: 8887, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.225, mean reward: 0.423 [0.340, 0.566], mean action: 17.600 [2.000, 91.000], mean observation: 3.153 [-1.223, 10.474], loss: 1.294857, mae: 4.989765, mean_q: 5.186516
 87112/100000: episode: 8888, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.317, mean reward: 0.432 [0.351, 0.560], mean action: 30.900 [2.000, 91.000], mean observation: 3.168 [-1.387, 10.340], loss: 1.223330, mae: 4.989451, mean_q: 5.184974
 87122/100000: episode: 8889, duration: 0.239s, episode steps: 10, steps per second: 42, episode reward: 4.310, mean reward: 0.431 [0.276, 0.571], mean action: 7.800 [2.000, 60.000], mean observation: 3.157 [-1.247, 10.548], loss: 1.247532, mae: 4.989465, mean_q: 5.183757
 87125/100000: episode: 8890, duration: 0.094s, episode steps: 3, steps per second: 32, episode reward: 10.961, mean reward: 3.654 [0.420, 10.000], mean action: 9.000 [2.000, 23.000], mean observation: 3.149 [-0.810, 10.100], loss: 0.800519, mae: 4.987989, mean_q: 5.183984
 87135/100000: episode: 8891, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.287, mean reward: 0.429 [0.358, 0.502], mean action: 34.700 [2.000, 93.000], mean observation: 3.153 [-2.158, 10.399], loss: 1.182472, mae: 4.989221, mean_q: 5.181184
 87145/100000: episode: 8892, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.204, mean reward: 0.420 [0.420, 0.420], mean action: 79.300 [39.000, 93.000], mean observation: 3.144 [-1.617, 10.257], loss: 1.157569, mae: 4.988875, mean_q: 5.181059
 87155/100000: episode: 8893, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.337, mean reward: 0.434 [0.351, 0.537], mean action: 70.100 [36.000, 93.000], mean observation: 3.150 [-1.444, 10.312], loss: 1.240966, mae: 4.989012, mean_q: 5.183486
 87165/100000: episode: 8894, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 3.589, mean reward: 0.359 [0.322, 0.538], mean action: 73.700 [18.000, 93.000], mean observation: 3.156 [-1.643, 10.328], loss: 0.983224, mae: 4.987716, mean_q: 5.184855
 87175/100000: episode: 8895, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.844, mean reward: 0.384 [0.379, 0.401], mean action: 68.300 [20.000, 93.000], mean observation: 3.171 [-1.307, 10.283], loss: 1.067073, mae: 4.988064, mean_q: 5.186557
 87185/100000: episode: 8896, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.152, mean reward: 0.415 [0.353, 0.493], mean action: 66.400 [14.000, 93.000], mean observation: 3.154 [-2.308, 10.323], loss: 1.332497, mae: 4.989144, mean_q: 5.187827
 87195/100000: episode: 8897, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.447, mean reward: 0.445 [0.374, 0.536], mean action: 68.200 [3.000, 93.000], mean observation: 3.155 [-1.068, 10.228], loss: 0.983035, mae: 4.988147, mean_q: 5.183827
 87205/100000: episode: 8898, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.010, mean reward: 0.401 [0.361, 0.418], mean action: 79.600 [21.000, 93.000], mean observation: 3.150 [-1.431, 10.273], loss: 1.018949, mae: 4.988338, mean_q: 5.182887
 87215/100000: episode: 8899, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.496, mean reward: 0.350 [0.306, 0.444], mean action: 81.300 [34.000, 93.000], mean observation: 3.160 [-0.751, 10.208], loss: 1.086262, mae: 4.988789, mean_q: 5.183802
 87225/100000: episode: 8900, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.381, mean reward: 0.438 [0.341, 0.532], mean action: 68.800 [21.000, 93.000], mean observation: 3.157 [-1.609, 10.321], loss: 1.461885, mae: 4.990125, mean_q: 5.184447
 87235/100000: episode: 8901, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.066, mean reward: 0.407 [0.331, 0.545], mean action: 65.200 [12.000, 93.000], mean observation: 3.158 [-1.098, 10.341], loss: 1.219031, mae: 4.989235, mean_q: 5.184819
 87236/100000: episode: 8902, duration: 0.023s, episode steps: 1, steps per second: 44, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 48.000 [48.000, 48.000], mean observation: 3.160 [-1.365, 10.665], loss: 0.814168, mae: 4.987641, mean_q: 5.185506
 87246/100000: episode: 8903, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.874, mean reward: 0.387 [0.327, 0.451], mean action: 68.700 [5.000, 93.000], mean observation: 3.168 [-1.140, 10.285], loss: 1.153590, mae: 4.988938, mean_q: 5.186350
 87256/100000: episode: 8904, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.587, mean reward: 0.459 [0.426, 0.481], mean action: 70.100 [13.000, 93.000], mean observation: 3.158 [-1.316, 10.386], loss: 1.215808, mae: 4.989510, mean_q: 5.187315
 87266/100000: episode: 8905, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 5.001, mean reward: 0.500 [0.424, 0.569], mean action: 67.300 [18.000, 93.000], mean observation: 3.154 [-1.073, 10.260], loss: 1.188370, mae: 4.989389, mean_q: 5.189536
 87276/100000: episode: 8906, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.261, mean reward: 0.426 [0.367, 0.467], mean action: 76.900 [29.000, 101.000], mean observation: 3.151 [-1.114, 10.450], loss: 1.469681, mae: 4.990491, mean_q: 5.192979
 87286/100000: episode: 8907, duration: 0.091s, episode steps: 10, steps per second: 110, episode reward: 4.853, mean reward: 0.485 [0.417, 0.497], mean action: 86.400 [29.000, 93.000], mean observation: 3.157 [-1.402, 10.357], loss: 1.306200, mae: 4.989774, mean_q: 5.195237
 87296/100000: episode: 8908, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 4.518, mean reward: 0.452 [0.365, 0.535], mean action: 70.700 [9.000, 93.000], mean observation: 3.161 [-1.210, 10.405], loss: 1.087901, mae: 4.989066, mean_q: 5.196571
 87306/100000: episode: 8909, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.973, mean reward: 0.497 [0.489, 0.501], mean action: 72.400 [21.000, 100.000], mean observation: 3.164 [-1.161, 10.322], loss: 1.130399, mae: 4.989357, mean_q: 5.198978
 87316/100000: episode: 8910, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 4.415, mean reward: 0.441 [0.439, 0.452], mean action: 84.200 [47.000, 93.000], mean observation: 3.146 [-1.021, 10.305], loss: 1.064052, mae: 4.989203, mean_q: 5.201153
 87326/100000: episode: 8911, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.473, mean reward: 0.347 [0.287, 0.487], mean action: 66.500 [12.000, 99.000], mean observation: 3.150 [-1.602, 10.277], loss: 1.056962, mae: 4.989536, mean_q: 5.202470
 87336/100000: episode: 8912, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 4.110, mean reward: 0.411 [0.395, 0.422], mean action: 74.600 [20.000, 93.000], mean observation: 3.159 [-2.190, 10.398], loss: 1.148746, mae: 4.990244, mean_q: 5.203955
 87346/100000: episode: 8913, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.852, mean reward: 0.385 [0.377, 0.415], mean action: 77.800 [49.000, 93.000], mean observation: 3.151 [-1.424, 10.307], loss: 1.119286, mae: 4.990345, mean_q: 5.205661
 87356/100000: episode: 8914, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.649, mean reward: 0.365 [0.334, 0.433], mean action: 70.400 [19.000, 95.000], mean observation: 3.148 [-1.096, 10.319], loss: 1.143710, mae: 4.990846, mean_q: 5.205438
 87366/100000: episode: 8915, duration: 0.109s, episode steps: 10, steps per second: 91, episode reward: 4.367, mean reward: 0.437 [0.333, 0.448], mean action: 78.400 [5.000, 99.000], mean observation: 3.148 [-1.157, 10.323], loss: 1.202207, mae: 4.991254, mean_q: 5.202413
 87376/100000: episode: 8916, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.222, mean reward: 0.422 [0.373, 0.526], mean action: 82.000 [27.000, 93.000], mean observation: 3.154 [-1.601, 10.243], loss: 1.265029, mae: 4.991704, mean_q: 5.202549
 87386/100000: episode: 8917, duration: 0.120s, episode steps: 10, steps per second: 84, episode reward: 4.179, mean reward: 0.418 [0.360, 0.518], mean action: 75.900 [32.000, 93.000], mean observation: 3.157 [-1.224, 10.313], loss: 0.934400, mae: 4.990548, mean_q: 5.203333
 87387/100000: episode: 8918, duration: 0.028s, episode steps: 1, steps per second: 35, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 82.000 [82.000, 82.000], mean observation: 3.155 [-0.427, 10.100], loss: 2.805391, mae: 4.998050, mean_q: 5.204241
 87397/100000: episode: 8919, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 3.797, mean reward: 0.380 [0.348, 0.419], mean action: 85.700 [43.000, 93.000], mean observation: 3.162 [-1.379, 10.404], loss: 1.356042, mae: 4.992325, mean_q: 5.205497
 87407/100000: episode: 8920, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 3.682, mean reward: 0.368 [0.327, 0.421], mean action: 79.700 [13.000, 101.000], mean observation: 3.146 [-1.207, 10.299], loss: 1.163157, mae: 4.991900, mean_q: 5.207812
 87417/100000: episode: 8921, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 3.162, mean reward: 0.316 [0.288, 0.399], mean action: 88.500 [57.000, 101.000], mean observation: 3.145 [-1.214, 10.307], loss: 1.375268, mae: 4.992864, mean_q: 5.210546
 87427/100000: episode: 8922, duration: 0.107s, episode steps: 10, steps per second: 94, episode reward: 3.974, mean reward: 0.397 [0.397, 0.397], mean action: 87.200 [52.000, 95.000], mean observation: 3.160 [-1.185, 10.300], loss: 1.234739, mae: 4.992454, mean_q: 5.212132
 87437/100000: episode: 8923, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.172, mean reward: 0.317 [0.287, 0.360], mean action: 77.800 [25.000, 93.000], mean observation: 3.161 [-1.461, 10.278], loss: 1.035000, mae: 4.991826, mean_q: 5.214143
 87447/100000: episode: 8924, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 3.824, mean reward: 0.382 [0.325, 0.451], mean action: 77.300 [1.000, 93.000], mean observation: 3.167 [-0.934, 10.360], loss: 1.099654, mae: 4.992342, mean_q: 5.216416
 87457/100000: episode: 8925, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 4.614, mean reward: 0.461 [0.449, 0.538], mean action: 87.100 [60.000, 93.000], mean observation: 3.153 [-1.494, 10.330], loss: 1.381902, mae: 4.993786, mean_q: 5.218699
 87467/100000: episode: 8926, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.501, mean reward: 0.450 [0.450, 0.450], mean action: 59.200 [4.000, 93.000], mean observation: 3.162 [-1.769, 10.367], loss: 1.046613, mae: 4.992634, mean_q: 5.220168
 87477/100000: episode: 8927, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.445, mean reward: 0.345 [0.332, 0.371], mean action: 84.400 [27.000, 96.000], mean observation: 3.157 [-0.888, 10.365], loss: 1.066457, mae: 4.993169, mean_q: 5.219884
 87487/100000: episode: 8928, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.239, mean reward: 0.424 [0.375, 0.474], mean action: 64.200 [4.000, 98.000], mean observation: 3.147 [-1.083, 10.342], loss: 1.469343, mae: 4.995121, mean_q: 5.215880
 87497/100000: episode: 8929, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.673, mean reward: 0.367 [0.308, 0.507], mean action: 76.900 [22.000, 93.000], mean observation: 3.169 [-0.618, 10.356], loss: 1.080304, mae: 4.993504, mean_q: 5.211107
 87507/100000: episode: 8930, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.008, mean reward: 0.401 [0.328, 0.503], mean action: 81.200 [45.000, 96.000], mean observation: 3.138 [-1.267, 10.315], loss: 1.271124, mae: 4.994334, mean_q: 5.209335
 87517/100000: episode: 8931, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.294, mean reward: 0.429 [0.347, 0.549], mean action: 80.200 [53.000, 93.000], mean observation: 3.161 [-0.868, 10.358], loss: 1.357696, mae: 4.994751, mean_q: 5.210237
 87527/100000: episode: 8932, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.655, mean reward: 0.465 [0.344, 0.487], mean action: 73.800 [12.000, 93.000], mean observation: 3.154 [-2.078, 10.343], loss: 1.508623, mae: 4.995156, mean_q: 5.209229
 87537/100000: episode: 8933, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 4.466, mean reward: 0.447 [0.433, 0.468], mean action: 79.200 [35.000, 93.000], mean observation: 3.168 [-0.723, 10.311], loss: 1.227215, mae: 4.993844, mean_q: 5.206895
 87547/100000: episode: 8934, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.107, mean reward: 0.411 [0.342, 0.490], mean action: 65.200 [9.000, 93.000], mean observation: 3.153 [-1.299, 10.425], loss: 1.540201, mae: 4.994991, mean_q: 5.208387
 87557/100000: episode: 8935, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 4.148, mean reward: 0.415 [0.330, 0.533], mean action: 76.700 [39.000, 93.000], mean observation: 3.153 [-1.295, 10.391], loss: 0.975213, mae: 4.992586, mean_q: 5.210997
 87567/100000: episode: 8936, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.390, mean reward: 0.439 [0.343, 0.489], mean action: 72.100 [8.000, 93.000], mean observation: 3.150 [-1.413, 10.438], loss: 1.026000, mae: 4.992879, mean_q: 5.214329
 87568/100000: episode: 8937, duration: 0.023s, episode steps: 1, steps per second: 43, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 93.000 [93.000, 93.000], mean observation: 3.181 [-0.801, 10.628], loss: 0.803038, mae: 4.991960, mean_q: 5.216124
 87578/100000: episode: 8938, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.657, mean reward: 0.366 [0.362, 0.391], mean action: 70.700 [23.000, 99.000], mean observation: 3.163 [-1.144, 10.318], loss: 1.193580, mae: 4.993979, mean_q: 5.217497
 87588/100000: episode: 8939, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 5.223, mean reward: 0.522 [0.522, 0.522], mean action: 60.000 [10.000, 93.000], mean observation: 3.163 [-1.636, 10.416], loss: 1.340233, mae: 4.994418, mean_q: 5.220239
 87598/100000: episode: 8940, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 3.891, mean reward: 0.389 [0.362, 0.427], mean action: 79.100 [35.000, 93.000], mean observation: 3.146 [-1.793, 10.299], loss: 1.418978, mae: 4.994934, mean_q: 5.222767
 87608/100000: episode: 8941, duration: 0.126s, episode steps: 10, steps per second: 80, episode reward: 4.004, mean reward: 0.400 [0.385, 0.431], mean action: 75.900 [35.000, 93.000], mean observation: 3.162 [-1.577, 10.260], loss: 1.370264, mae: 4.994870, mean_q: 5.220792
 87618/100000: episode: 8942, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 3.647, mean reward: 0.365 [0.306, 0.418], mean action: 67.000 [3.000, 93.000], mean observation: 3.165 [-1.286, 10.200], loss: 0.905744, mae: 4.992870, mean_q: 5.219368
 87628/100000: episode: 8943, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.583, mean reward: 0.458 [0.391, 0.505], mean action: 55.700 [7.000, 101.000], mean observation: 3.157 [-1.074, 10.380], loss: 1.324811, mae: 4.994803, mean_q: 5.220679
 87638/100000: episode: 8944, duration: 0.127s, episode steps: 10, steps per second: 78, episode reward: 4.163, mean reward: 0.416 [0.357, 0.526], mean action: 69.400 [15.000, 100.000], mean observation: 3.153 [-1.116, 10.251], loss: 1.140090, mae: 4.994498, mean_q: 5.223628
 87648/100000: episode: 8945, duration: 0.109s, episode steps: 10, steps per second: 91, episode reward: 3.730, mean reward: 0.373 [0.369, 0.406], mean action: 87.300 [50.000, 93.000], mean observation: 3.158 [-1.758, 10.291], loss: 1.094861, mae: 4.994525, mean_q: 5.227094
 87658/100000: episode: 8946, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.050, mean reward: 0.405 [0.348, 0.542], mean action: 62.700 [0.000, 93.000], mean observation: 3.149 [-1.961, 10.239], loss: 1.668507, mae: 4.996867, mean_q: 5.226708
 87668/100000: episode: 8947, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.422, mean reward: 0.442 [0.294, 0.523], mean action: 64.100 [10.000, 94.000], mean observation: 3.145 [-1.390, 10.231], loss: 1.234751, mae: 4.995139, mean_q: 5.219249
 87678/100000: episode: 8948, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.320, mean reward: 0.432 [0.424, 0.465], mean action: 72.600 [11.000, 93.000], mean observation: 3.156 [-1.152, 10.304], loss: 1.009799, mae: 4.994517, mean_q: 5.213040
 87688/100000: episode: 8949, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 4.215, mean reward: 0.421 [0.339, 0.490], mean action: 77.800 [17.000, 93.000], mean observation: 3.163 [-1.411, 10.363], loss: 1.328947, mae: 4.996057, mean_q: 5.209066
 87698/100000: episode: 8950, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.140, mean reward: 0.414 [0.383, 0.473], mean action: 74.100 [4.000, 93.000], mean observation: 3.158 [-1.164, 10.350], loss: 1.101783, mae: 4.995304, mean_q: 5.200743
 87708/100000: episode: 8951, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.625, mean reward: 0.463 [0.450, 0.502], mean action: 66.500 [5.000, 93.000], mean observation: 3.166 [-1.505, 10.299], loss: 1.112737, mae: 4.995409, mean_q: 5.198848
 87718/100000: episode: 8952, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 4.473, mean reward: 0.447 [0.345, 0.473], mean action: 78.600 [6.000, 93.000], mean observation: 3.171 [-0.892, 10.423], loss: 1.058204, mae: 4.995413, mean_q: 5.199771
 87728/100000: episode: 8953, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.384, mean reward: 0.438 [0.395, 0.468], mean action: 58.800 [2.000, 93.000], mean observation: 3.161 [-1.030, 10.368], loss: 1.344043, mae: 4.996401, mean_q: 5.200983
 87738/100000: episode: 8954, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.627, mean reward: 0.363 [0.325, 0.387], mean action: 62.700 [11.000, 93.000], mean observation: 3.160 [-1.114, 10.413], loss: 1.151081, mae: 4.996032, mean_q: 5.198702
 87748/100000: episode: 8955, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.196, mean reward: 0.420 [0.371, 0.497], mean action: 61.800 [21.000, 99.000], mean observation: 3.159 [-1.484, 10.314], loss: 1.118400, mae: 4.995718, mean_q: 5.195735
 87758/100000: episode: 8956, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.356, mean reward: 0.436 [0.323, 0.505], mean action: 74.800 [23.000, 93.000], mean observation: 3.160 [-1.880, 10.402], loss: 1.097500, mae: 4.995978, mean_q: 5.194228
 87768/100000: episode: 8957, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.934, mean reward: 0.393 [0.343, 0.518], mean action: 60.500 [11.000, 93.000], mean observation: 3.150 [-1.270, 10.159], loss: 1.268296, mae: 4.997159, mean_q: 5.192017
 87778/100000: episode: 8958, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.409, mean reward: 0.441 [0.423, 0.494], mean action: 63.000 [1.000, 93.000], mean observation: 3.153 [-1.764, 10.249], loss: 1.432181, mae: 4.997775, mean_q: 5.190761
 87781/100000: episode: 8959, duration: 0.080s, episode steps: 3, steps per second: 38, episode reward: 10.748, mean reward: 3.583 [0.342, 10.000], mean action: 8.333 [4.000, 17.000], mean observation: 3.156 [-1.334, 10.177], loss: 1.389532, mae: 4.997488, mean_q: 5.190618
 87791/100000: episode: 8960, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.375, mean reward: 0.437 [0.403, 0.508], mean action: 38.000 [4.000, 87.000], mean observation: 3.167 [-1.223, 10.475], loss: 1.092418, mae: 4.996048, mean_q: 5.191641
 87801/100000: episode: 8961, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.800, mean reward: 0.380 [0.297, 0.483], mean action: 29.400 [4.000, 100.000], mean observation: 3.150 [-1.916, 10.229], loss: 1.287504, mae: 4.996900, mean_q: 5.193315
 87811/100000: episode: 8962, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.944, mean reward: 0.394 [0.335, 0.473], mean action: 26.600 [4.000, 96.000], mean observation: 3.154 [-1.171, 10.468], loss: 1.295496, mae: 4.996919, mean_q: 5.195525
 87821/100000: episode: 8963, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.186, mean reward: 0.419 [0.356, 0.564], mean action: 47.300 [4.000, 98.000], mean observation: 3.162 [-1.554, 10.305], loss: 1.272259, mae: 4.996932, mean_q: 5.196970
 87831/100000: episode: 8964, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.437, mean reward: 0.444 [0.324, 0.539], mean action: 11.500 [0.000, 66.000], mean observation: 3.160 [-1.317, 10.426], loss: 1.252554, mae: 4.996555, mean_q: 5.197742
 87841/100000: episode: 8965, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.005, mean reward: 0.401 [0.311, 0.468], mean action: 30.100 [4.000, 85.000], mean observation: 3.156 [-1.201, 10.351], loss: 1.164095, mae: 4.996314, mean_q: 5.198913
 87851/100000: episode: 8966, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 3.750, mean reward: 0.375 [0.299, 0.443], mean action: 18.600 [2.000, 87.000], mean observation: 3.158 [-1.794, 10.275], loss: 1.144686, mae: 4.996367, mean_q: 5.200184
 87861/100000: episode: 8967, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.460, mean reward: 0.446 [0.361, 0.531], mean action: 23.300 [4.000, 62.000], mean observation: 3.147 [-2.345, 10.259], loss: 1.131491, mae: 4.996462, mean_q: 5.202577
 87871/100000: episode: 8968, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.018, mean reward: 0.402 [0.314, 0.484], mean action: 23.100 [1.000, 74.000], mean observation: 3.152 [-1.440, 10.233], loss: 1.507720, mae: 4.998192, mean_q: 5.203965
 87881/100000: episode: 8969, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.430, mean reward: 0.343 [0.294, 0.383], mean action: 41.800 [0.000, 93.000], mean observation: 3.150 [-1.262, 10.282], loss: 1.073898, mae: 4.996289, mean_q: 5.205569
 87891/100000: episode: 8970, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 3.983, mean reward: 0.398 [0.313, 0.445], mean action: 73.600 [1.000, 93.000], mean observation: 3.155 [-1.076, 10.307], loss: 1.383674, mae: 4.997546, mean_q: 5.209060
 87901/100000: episode: 8971, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.064, mean reward: 0.406 [0.308, 0.481], mean action: 64.700 [1.000, 98.000], mean observation: 3.152 [-1.630, 10.348], loss: 1.258301, mae: 4.997397, mean_q: 5.212490
 87911/100000: episode: 8972, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.078, mean reward: 0.408 [0.353, 0.496], mean action: 52.900 [18.000, 93.000], mean observation: 3.156 [-1.573, 10.348], loss: 1.130956, mae: 4.996947, mean_q: 5.215183
 87921/100000: episode: 8973, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 5.020, mean reward: 0.502 [0.502, 0.502], mean action: 75.900 [22.000, 95.000], mean observation: 3.147 [-1.040, 10.351], loss: 1.127764, mae: 4.996908, mean_q: 5.218682
 87931/100000: episode: 8974, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.143, mean reward: 0.414 [0.391, 0.517], mean action: 77.900 [3.000, 98.000], mean observation: 3.162 [-1.296, 10.509], loss: 1.096495, mae: 4.996882, mean_q: 5.222045
 87941/100000: episode: 8975, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.115, mean reward: 0.412 [0.381, 0.456], mean action: 61.000 [3.000, 93.000], mean observation: 3.165 [-1.169, 10.381], loss: 1.134860, mae: 4.997256, mean_q: 5.224801
 87951/100000: episode: 8976, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.352, mean reward: 0.435 [0.362, 0.494], mean action: 66.300 [5.000, 93.000], mean observation: 3.155 [-1.575, 10.318], loss: 1.265384, mae: 4.997899, mean_q: 5.222868
 87961/100000: episode: 8977, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.120, mean reward: 0.412 [0.331, 0.450], mean action: 62.100 [2.000, 93.000], mean observation: 3.155 [-1.074, 10.371], loss: 1.166321, mae: 4.997836, mean_q: 5.223293
 87971/100000: episode: 8978, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.278, mean reward: 0.428 [0.324, 0.539], mean action: 57.400 [6.000, 94.000], mean observation: 3.156 [-1.165, 10.228], loss: 1.202595, mae: 4.998130, mean_q: 5.225049
 87981/100000: episode: 8979, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.667, mean reward: 0.467 [0.408, 0.553], mean action: 79.700 [26.000, 96.000], mean observation: 3.163 [-1.175, 10.392], loss: 1.313883, mae: 4.998485, mean_q: 5.226762
 87991/100000: episode: 8980, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.378, mean reward: 0.438 [0.352, 0.510], mean action: 71.800 [20.000, 93.000], mean observation: 3.154 [-0.988, 10.357], loss: 1.314123, mae: 4.998404, mean_q: 5.229064
 88001/100000: episode: 8981, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 4.156, mean reward: 0.416 [0.412, 0.451], mean action: 66.500 [23.000, 93.000], mean observation: 3.151 [-1.395, 10.327], loss: 1.071870, mae: 4.997705, mean_q: 5.226399
 88011/100000: episode: 8982, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 3.886, mean reward: 0.389 [0.359, 0.452], mean action: 75.500 [11.000, 93.000], mean observation: 3.168 [-0.754, 10.519], loss: 1.420000, mae: 4.999207, mean_q: 5.220798
 88021/100000: episode: 8983, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 3.425, mean reward: 0.342 [0.314, 0.373], mean action: 80.400 [20.000, 98.000], mean observation: 3.154 [-1.703, 10.423], loss: 1.489925, mae: 4.999361, mean_q: 5.218198
 88031/100000: episode: 8984, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.800, mean reward: 0.380 [0.330, 0.466], mean action: 72.400 [26.000, 93.000], mean observation: 3.166 [-0.790, 10.351], loss: 1.436938, mae: 4.999488, mean_q: 5.216641
 88041/100000: episode: 8985, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.307, mean reward: 0.331 [0.260, 0.388], mean action: 78.200 [45.000, 93.000], mean observation: 3.159 [-0.675, 10.350], loss: 1.254817, mae: 4.998828, mean_q: 5.217128
 88051/100000: episode: 8986, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 3.670, mean reward: 0.367 [0.338, 0.442], mean action: 79.300 [31.000, 96.000], mean observation: 3.143 [-1.487, 10.330], loss: 1.495207, mae: 4.999864, mean_q: 5.214913
 88061/100000: episode: 8987, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.745, mean reward: 0.375 [0.318, 0.434], mean action: 67.200 [2.000, 93.000], mean observation: 3.160 [-1.525, 10.357], loss: 1.192357, mae: 4.998876, mean_q: 5.213373
 88071/100000: episode: 8988, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 5.591, mean reward: 0.559 [0.425, 0.574], mean action: 84.300 [36.000, 95.000], mean observation: 3.167 [-0.762, 10.465], loss: 1.130607, mae: 4.998590, mean_q: 5.212659
 88081/100000: episode: 8989, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 4.105, mean reward: 0.411 [0.337, 0.442], mean action: 75.800 [7.000, 93.000], mean observation: 3.153 [-0.989, 10.244], loss: 1.427114, mae: 4.999964, mean_q: 5.213308
 88091/100000: episode: 8990, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 3.894, mean reward: 0.389 [0.337, 0.446], mean action: 79.200 [44.000, 93.000], mean observation: 3.169 [-1.011, 10.253], loss: 0.940433, mae: 4.998084, mean_q: 5.215022
 88101/100000: episode: 8991, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.486, mean reward: 0.449 [0.385, 0.538], mean action: 69.100 [5.000, 94.000], mean observation: 3.149 [-1.826, 10.235], loss: 1.205020, mae: 4.999097, mean_q: 5.217909
 88111/100000: episode: 8992, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 4.606, mean reward: 0.461 [0.461, 0.461], mean action: 82.200 [40.000, 93.000], mean observation: 3.138 [-1.199, 10.292], loss: 1.004310, mae: 4.998815, mean_q: 5.220669
 88121/100000: episode: 8993, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.452, mean reward: 0.445 [0.379, 0.522], mean action: 50.000 [0.000, 93.000], mean observation: 3.160 [-1.182, 10.448], loss: 1.310827, mae: 5.000222, mean_q: 5.222422
 88131/100000: episode: 8994, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 3.891, mean reward: 0.389 [0.380, 0.424], mean action: 80.000 [20.000, 97.000], mean observation: 3.155 [-2.073, 10.318], loss: 0.996840, mae: 4.999263, mean_q: 5.224495
 88141/100000: episode: 8995, duration: 0.120s, episode steps: 10, steps per second: 84, episode reward: 4.419, mean reward: 0.442 [0.442, 0.442], mean action: 69.500 [24.000, 93.000], mean observation: 3.168 [-1.118, 10.380], loss: 1.076861, mae: 4.999441, mean_q: 5.227059
 88151/100000: episode: 8996, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.046, mean reward: 0.405 [0.360, 0.452], mean action: 55.600 [7.000, 93.000], mean observation: 3.157 [-1.144, 10.362], loss: 1.187315, mae: 5.000121, mean_q: 5.229601
 88161/100000: episode: 8997, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.000, mean reward: 0.400 [0.327, 0.464], mean action: 57.200 [8.000, 93.000], mean observation: 3.143 [-1.379, 10.207], loss: 1.305267, mae: 5.001151, mean_q: 5.232148
 88171/100000: episode: 8998, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.259, mean reward: 0.426 [0.408, 0.438], mean action: 64.600 [1.000, 93.000], mean observation: 3.142 [-1.216, 10.276], loss: 1.337882, mae: 5.001695, mean_q: 5.234700
 88181/100000: episode: 8999, duration: 0.093s, episode steps: 10, steps per second: 108, episode reward: 4.471, mean reward: 0.447 [0.417, 0.486], mean action: 87.500 [38.000, 93.000], mean observation: 3.173 [-1.115, 10.463], loss: 1.106407, mae: 5.001060, mean_q: 5.233818
 88191/100000: episode: 9000, duration: 0.114s, episode steps: 10, steps per second: 87, episode reward: 3.622, mean reward: 0.362 [0.339, 0.474], mean action: 78.300 [4.000, 96.000], mean observation: 3.159 [-0.940, 10.329], loss: 0.998218, mae: 5.000988, mean_q: 5.234532
 88201/100000: episode: 9001, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.538, mean reward: 0.354 [0.319, 0.397], mean action: 70.700 [5.000, 95.000], mean observation: 3.165 [-1.434, 10.369], loss: 1.547261, mae: 5.003368, mean_q: 5.236198
 88211/100000: episode: 9002, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 4.524, mean reward: 0.452 [0.452, 0.454], mean action: 68.900 [9.000, 93.000], mean observation: 3.150 [-1.059, 10.321], loss: 1.079647, mae: 5.001477, mean_q: 5.236742
 88221/100000: episode: 9003, duration: 0.105s, episode steps: 10, steps per second: 96, episode reward: 3.755, mean reward: 0.375 [0.371, 0.411], mean action: 79.300 [42.000, 93.000], mean observation: 3.166 [-0.617, 10.212], loss: 1.350583, mae: 5.002987, mean_q: 5.231624
 88231/100000: episode: 9004, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.234, mean reward: 0.423 [0.335, 0.528], mean action: 69.600 [4.000, 93.000], mean observation: 3.155 [-1.183, 10.483], loss: 1.358242, mae: 5.003037, mean_q: 5.231210
 88241/100000: episode: 9005, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.228, mean reward: 0.423 [0.381, 0.466], mean action: 79.400 [41.000, 96.000], mean observation: 3.142 [-2.319, 10.258], loss: 1.114015, mae: 5.002216, mean_q: 5.228711
 88251/100000: episode: 9006, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.565, mean reward: 0.457 [0.453, 0.477], mean action: 77.200 [29.000, 93.000], mean observation: 3.148 [-2.208, 10.372], loss: 1.185850, mae: 5.002791, mean_q: 5.226697
 88261/100000: episode: 9007, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.875, mean reward: 0.388 [0.360, 0.435], mean action: 50.600 [5.000, 93.000], mean observation: 3.159 [-1.511, 10.314], loss: 1.074315, mae: 5.002462, mean_q: 5.228642
 88271/100000: episode: 9008, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 3.734, mean reward: 0.373 [0.337, 0.501], mean action: 74.000 [0.000, 93.000], mean observation: 3.151 [-1.340, 10.320], loss: 0.954394, mae: 5.002202, mean_q: 5.229491
 88281/100000: episode: 9009, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.196, mean reward: 0.420 [0.338, 0.456], mean action: 76.700 [4.000, 93.000], mean observation: 3.151 [-1.307, 10.359], loss: 1.282798, mae: 5.003873, mean_q: 5.227021
 88291/100000: episode: 9010, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.245, mean reward: 0.424 [0.367, 0.463], mean action: 66.900 [17.000, 93.000], mean observation: 3.177 [-1.046, 10.313], loss: 1.471106, mae: 5.004669, mean_q: 5.220927
 88301/100000: episode: 9011, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.210, mean reward: 0.421 [0.386, 0.544], mean action: 65.200 [2.000, 94.000], mean observation: 3.142 [-1.156, 10.322], loss: 1.308109, mae: 5.004003, mean_q: 5.216213
 88311/100000: episode: 9012, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.534, mean reward: 0.453 [0.401, 0.491], mean action: 52.500 [2.000, 100.000], mean observation: 3.145 [-1.254, 10.208], loss: 0.844175, mae: 5.002088, mean_q: 5.213974
 88321/100000: episode: 9013, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.910, mean reward: 0.491 [0.465, 0.551], mean action: 64.400 [15.000, 93.000], mean observation: 3.141 [-1.590, 10.321], loss: 1.468488, mae: 5.004628, mean_q: 5.212724
 88331/100000: episode: 9014, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.607, mean reward: 0.361 [0.292, 0.429], mean action: 65.500 [8.000, 96.000], mean observation: 3.151 [-1.623, 10.316], loss: 1.268083, mae: 5.003870, mean_q: 5.209579
 88341/100000: episode: 9015, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.396, mean reward: 0.440 [0.394, 0.525], mean action: 39.500 [3.000, 93.000], mean observation: 3.164 [-1.612, 10.236], loss: 1.201723, mae: 5.003181, mean_q: 5.203394
 88351/100000: episode: 9016, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.278, mean reward: 0.428 [0.332, 0.500], mean action: 35.800 [12.000, 66.000], mean observation: 3.159 [-1.507, 10.456], loss: 1.269331, mae: 5.003319, mean_q: 5.203172
 88361/100000: episode: 9017, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.167, mean reward: 0.417 [0.346, 0.526], mean action: 47.500 [31.000, 98.000], mean observation: 3.149 [-1.168, 10.356], loss: 1.098266, mae: 5.002346, mean_q: 5.203310
 88371/100000: episode: 9018, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.250, mean reward: 0.425 [0.376, 0.496], mean action: 41.300 [8.000, 83.000], mean observation: 3.158 [-1.144, 10.433], loss: 1.255586, mae: 5.003019, mean_q: 5.204182
 88381/100000: episode: 9019, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.939, mean reward: 0.394 [0.319, 0.479], mean action: 35.500 [18.000, 58.000], mean observation: 3.157 [-1.373, 10.465], loss: 1.163971, mae: 5.002680, mean_q: 5.205139
 88391/100000: episode: 9020, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.130, mean reward: 0.413 [0.389, 0.462], mean action: 37.000 [12.000, 89.000], mean observation: 3.148 [-1.075, 10.392], loss: 1.264048, mae: 5.002904, mean_q: 5.206205
 88401/100000: episode: 9021, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.163, mean reward: 0.416 [0.397, 0.456], mean action: 77.200 [2.000, 96.000], mean observation: 3.155 [-1.912, 10.332], loss: 1.148868, mae: 5.002762, mean_q: 5.208129
 88411/100000: episode: 9022, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.347, mean reward: 0.335 [0.287, 0.473], mean action: 69.100 [10.000, 99.000], mean observation: 3.153 [-1.077, 10.301], loss: 1.176860, mae: 5.002850, mean_q: 5.210481
 88421/100000: episode: 9023, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.845, mean reward: 0.384 [0.324, 0.431], mean action: 39.200 [0.000, 89.000], mean observation: 3.156 [-0.996, 10.260], loss: 1.356784, mae: 5.003684, mean_q: 5.213285
 88431/100000: episode: 9024, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.257, mean reward: 0.426 [0.323, 0.487], mean action: 54.200 [0.000, 87.000], mean observation: 3.153 [-1.054, 10.339], loss: 1.201759, mae: 5.003027, mean_q: 5.216428
 88441/100000: episode: 9025, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.781, mean reward: 0.378 [0.302, 0.507], mean action: 68.300 [16.000, 87.000], mean observation: 3.151 [-1.482, 10.307], loss: 1.316396, mae: 5.003603, mean_q: 5.219591
 88451/100000: episode: 9026, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 4.209, mean reward: 0.421 [0.324, 0.466], mean action: 79.700 [14.000, 87.000], mean observation: 3.160 [-1.017, 10.288], loss: 1.608603, mae: 5.004291, mean_q: 5.222518
 88452/100000: episode: 9027, duration: 0.032s, episode steps: 1, steps per second: 31, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 40.000 [40.000, 40.000], mean observation: 3.173 [-0.899, 10.139], loss: 1.116082, mae: 5.002427, mean_q: 5.223861
 88462/100000: episode: 9028, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.669, mean reward: 0.367 [0.367, 0.367], mean action: 78.900 [0.000, 100.000], mean observation: 3.144 [-1.407, 10.407], loss: 1.197882, mae: 5.002301, mean_q: 5.223081
 88472/100000: episode: 9029, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 3.810, mean reward: 0.381 [0.323, 0.539], mean action: 75.200 [22.000, 93.000], mean observation: 3.156 [-1.653, 10.569], loss: 1.256225, mae: 5.002398, mean_q: 5.222100
 88482/100000: episode: 9030, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.558, mean reward: 0.456 [0.431, 0.473], mean action: 83.200 [48.000, 100.000], mean observation: 3.166 [-1.343, 10.243], loss: 1.259116, mae: 5.002076, mean_q: 5.224229
 88492/100000: episode: 9031, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.026, mean reward: 0.403 [0.387, 0.502], mean action: 65.300 [27.000, 87.000], mean observation: 3.157 [-1.342, 10.278], loss: 1.256789, mae: 5.002053, mean_q: 5.226609
 88502/100000: episode: 9032, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.803, mean reward: 0.380 [0.345, 0.512], mean action: 67.400 [11.000, 87.000], mean observation: 3.157 [-1.515, 10.405], loss: 1.332552, mae: 5.002145, mean_q: 5.228381
 88512/100000: episode: 9033, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.380, mean reward: 0.438 [0.393, 0.488], mean action: 58.500 [20.000, 87.000], mean observation: 3.162 [-2.026, 10.409], loss: 1.018377, mae: 5.000932, mean_q: 5.230259
 88522/100000: episode: 9034, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.966, mean reward: 0.397 [0.326, 0.531], mean action: 55.200 [1.000, 87.000], mean observation: 3.153 [-1.240, 10.279], loss: 1.209066, mae: 5.001860, mean_q: 5.231247
 88532/100000: episode: 9035, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.680, mean reward: 0.368 [0.350, 0.406], mean action: 57.300 [13.000, 87.000], mean observation: 3.144 [-1.362, 10.346], loss: 1.072011, mae: 5.001690, mean_q: 5.229468
 88542/100000: episode: 9036, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 5.041, mean reward: 0.504 [0.392, 0.534], mean action: 57.000 [4.000, 90.000], mean observation: 3.157 [-0.831, 10.361], loss: 1.146613, mae: 5.002000, mean_q: 5.229829
 88552/100000: episode: 9037, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 4.378, mean reward: 0.438 [0.360, 0.520], mean action: 80.800 [50.000, 93.000], mean observation: 3.166 [-1.617, 10.282], loss: 1.020667, mae: 5.001822, mean_q: 5.231312
 88562/100000: episode: 9038, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.691, mean reward: 0.369 [0.328, 0.504], mean action: 79.400 [51.000, 87.000], mean observation: 3.135 [-1.819, 10.274], loss: 1.165262, mae: 5.002490, mean_q: 5.230910
 88572/100000: episode: 9039, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 5.296, mean reward: 0.530 [0.530, 0.530], mean action: 65.200 [10.000, 87.000], mean observation: 3.179 [-0.959, 10.482], loss: 1.111068, mae: 5.002557, mean_q: 5.229396
 88582/100000: episode: 9040, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 5.125, mean reward: 0.513 [0.498, 0.571], mean action: 71.500 [22.000, 94.000], mean observation: 3.165 [-1.269, 10.355], loss: 1.087383, mae: 5.002446, mean_q: 5.228987
 88592/100000: episode: 9041, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.244, mean reward: 0.424 [0.397, 0.466], mean action: 78.800 [17.000, 100.000], mean observation: 3.150 [-1.802, 10.322], loss: 1.395226, mae: 5.004079, mean_q: 5.226697
 88602/100000: episode: 9042, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.829, mean reward: 0.383 [0.325, 0.445], mean action: 63.700 [4.000, 87.000], mean observation: 3.165 [-1.477, 10.375], loss: 1.037542, mae: 5.003028, mean_q: 5.226566
 88612/100000: episode: 9043, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.297, mean reward: 0.430 [0.382, 0.538], mean action: 59.400 [9.000, 90.000], mean observation: 3.171 [-1.041, 10.342], loss: 1.067883, mae: 5.003576, mean_q: 5.224606
 88622/100000: episode: 9044, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.193, mean reward: 0.419 [0.365, 0.480], mean action: 43.800 [0.000, 87.000], mean observation: 3.157 [-1.043, 10.336], loss: 1.308583, mae: 5.004499, mean_q: 5.225015
 88632/100000: episode: 9045, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.954, mean reward: 0.495 [0.486, 0.510], mean action: 69.500 [13.000, 93.000], mean observation: 3.140 [-0.796, 10.419], loss: 1.240814, mae: 5.004166, mean_q: 5.227031
 88642/100000: episode: 9046, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.181, mean reward: 0.418 [0.359, 0.489], mean action: 72.000 [6.000, 92.000], mean observation: 3.156 [-1.414, 10.452], loss: 0.923804, mae: 5.002994, mean_q: 5.229543
 88652/100000: episode: 9047, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 3.908, mean reward: 0.391 [0.313, 0.479], mean action: 70.400 [12.000, 87.000], mean observation: 3.147 [-1.359, 10.200], loss: 0.997426, mae: 5.003654, mean_q: 5.232326
 88662/100000: episode: 9048, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.252, mean reward: 0.425 [0.365, 0.520], mean action: 62.100 [7.000, 99.000], mean observation: 3.157 [-1.124, 10.449], loss: 1.528475, mae: 5.005892, mean_q: 5.234628
 88672/100000: episode: 9049, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.724, mean reward: 0.372 [0.336, 0.417], mean action: 71.700 [22.000, 94.000], mean observation: 3.167 [-0.986, 10.357], loss: 1.231589, mae: 5.004807, mean_q: 5.233555
 88682/100000: episode: 9050, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 3.600, mean reward: 0.360 [0.309, 0.475], mean action: 65.800 [20.000, 89.000], mean observation: 3.154 [-1.144, 10.323], loss: 1.313290, mae: 5.005247, mean_q: 5.230446
 88692/100000: episode: 9051, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 4.024, mean reward: 0.402 [0.390, 0.511], mean action: 83.100 [45.000, 90.000], mean observation: 3.158 [-0.994, 10.387], loss: 1.100488, mae: 5.004566, mean_q: 5.224025
 88702/100000: episode: 9052, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.806, mean reward: 0.381 [0.369, 0.402], mean action: 78.700 [13.000, 93.000], mean observation: 3.147 [-1.262, 10.407], loss: 0.862859, mae: 5.003845, mean_q: 5.215222
 88712/100000: episode: 9053, duration: 0.102s, episode steps: 10, steps per second: 98, episode reward: 3.475, mean reward: 0.348 [0.346, 0.359], mean action: 90.400 [75.000, 93.000], mean observation: 3.139 [-0.843, 10.290], loss: 1.266576, mae: 5.005723, mean_q: 5.214131
 88722/100000: episode: 9054, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.821, mean reward: 0.482 [0.482, 0.482], mean action: 73.600 [47.000, 93.000], mean observation: 3.148 [-1.860, 10.263], loss: 1.571173, mae: 5.006953, mean_q: 5.213688
 88732/100000: episode: 9055, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.819, mean reward: 0.382 [0.345, 0.454], mean action: 66.100 [0.000, 93.000], mean observation: 3.160 [-0.795, 10.246], loss: 0.966856, mae: 5.004708, mean_q: 5.214364
 88742/100000: episode: 9056, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.943, mean reward: 0.394 [0.376, 0.448], mean action: 75.400 [19.000, 93.000], mean observation: 3.145 [-1.017, 10.325], loss: 1.363725, mae: 5.006118, mean_q: 5.215821
 88752/100000: episode: 9057, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.221, mean reward: 0.422 [0.320, 0.546], mean action: 60.700 [0.000, 98.000], mean observation: 3.158 [-1.772, 10.331], loss: 1.411239, mae: 5.006338, mean_q: 5.218345
 88762/100000: episode: 9058, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.587, mean reward: 0.359 [0.325, 0.417], mean action: 61.100 [13.000, 93.000], mean observation: 3.151 [-1.467, 10.317], loss: 0.763416, mae: 5.003871, mean_q: 5.220910
 88772/100000: episode: 9059, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.013, mean reward: 0.401 [0.345, 0.504], mean action: 64.700 [0.000, 97.000], mean observation: 3.157 [-2.344, 10.435], loss: 1.034758, mae: 5.005176, mean_q: 5.222456
 88782/100000: episode: 9060, duration: 0.084s, episode steps: 10, steps per second: 119, episode reward: 4.343, mean reward: 0.434 [0.432, 0.456], mean action: 93.000 [93.000, 93.000], mean observation: 3.128 [-1.333, 10.319], loss: 1.394038, mae: 5.007079, mean_q: 5.222785
 88792/100000: episode: 9061, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 3.489, mean reward: 0.349 [0.346, 0.371], mean action: 83.800 [15.000, 98.000], mean observation: 3.147 [-1.019, 10.443], loss: 1.031891, mae: 5.005551, mean_q: 5.223320
 88802/100000: episode: 9062, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.211, mean reward: 0.421 [0.332, 0.487], mean action: 77.900 [48.000, 101.000], mean observation: 3.149 [-1.420, 10.303], loss: 1.185627, mae: 5.006350, mean_q: 5.218500
 88812/100000: episode: 9063, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.784, mean reward: 0.478 [0.449, 0.508], mean action: 67.500 [23.000, 87.000], mean observation: 3.157 [-0.931, 10.287], loss: 1.170281, mae: 5.006259, mean_q: 5.219790
 88822/100000: episode: 9064, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 3.919, mean reward: 0.392 [0.321, 0.415], mean action: 66.200 [11.000, 88.000], mean observation: 3.148 [-1.797, 10.300], loss: 1.042542, mae: 5.005705, mean_q: 5.221037
 88832/100000: episode: 9065, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.270, mean reward: 0.427 [0.414, 0.518], mean action: 63.000 [2.000, 87.000], mean observation: 3.173 [-1.237, 10.287], loss: 1.224437, mae: 5.006572, mean_q: 5.222641
 88842/100000: episode: 9066, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.921, mean reward: 0.392 [0.328, 0.496], mean action: 66.400 [17.000, 87.000], mean observation: 3.166 [-1.498, 10.367], loss: 1.201999, mae: 5.006332, mean_q: 5.224097
 88852/100000: episode: 9067, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 4.124, mean reward: 0.412 [0.326, 0.458], mean action: 56.400 [8.000, 101.000], mean observation: 3.162 [-1.633, 10.243], loss: 1.046631, mae: 5.005929, mean_q: 5.225709
 88853/100000: episode: 9068, duration: 0.025s, episode steps: 1, steps per second: 40, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 87.000 [87.000, 87.000], mean observation: 3.159 [-1.534, 10.100], loss: 1.463309, mae: 5.007668, mean_q: 5.226558
 88863/100000: episode: 9069, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.700, mean reward: 0.470 [0.385, 0.513], mean action: 55.900 [6.000, 87.000], mean observation: 3.152 [-1.256, 10.231], loss: 1.178115, mae: 5.006587, mean_q: 5.227353
 88873/100000: episode: 9070, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 3.632, mean reward: 0.363 [0.309, 0.379], mean action: 75.500 [24.000, 90.000], mean observation: 3.153 [-1.024, 10.157], loss: 1.218751, mae: 5.006804, mean_q: 5.228764
 88883/100000: episode: 9071, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.894, mean reward: 0.389 [0.361, 0.481], mean action: 77.600 [24.000, 95.000], mean observation: 3.149 [-1.185, 10.303], loss: 1.201916, mae: 5.006809, mean_q: 5.230979
 88893/100000: episode: 9072, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.611, mean reward: 0.361 [0.299, 0.535], mean action: 65.900 [10.000, 87.000], mean observation: 3.158 [-1.230, 10.561], loss: 1.046039, mae: 5.006206, mean_q: 5.230529
 88903/100000: episode: 9073, duration: 0.136s, episode steps: 10, steps per second: 73, episode reward: 4.298, mean reward: 0.430 [0.391, 0.540], mean action: 62.400 [5.000, 87.000], mean observation: 3.151 [-0.776, 10.324], loss: 1.181180, mae: 5.006970, mean_q: 5.227265
 88913/100000: episode: 9074, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.221, mean reward: 0.422 [0.362, 0.481], mean action: 64.200 [4.000, 97.000], mean observation: 3.157 [-0.620, 10.426], loss: 1.057934, mae: 5.006518, mean_q: 5.225837
 88923/100000: episode: 9075, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.078, mean reward: 0.408 [0.380, 0.556], mean action: 45.200 [24.000, 100.000], mean observation: 3.148 [-1.277, 10.231], loss: 1.287595, mae: 5.007893, mean_q: 5.228383
 88933/100000: episode: 9076, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.992, mean reward: 0.399 [0.345, 0.456], mean action: 41.200 [1.000, 95.000], mean observation: 3.154 [-1.090, 10.430], loss: 1.405620, mae: 5.008059, mean_q: 5.230617
 88943/100000: episode: 9077, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.700, mean reward: 0.370 [0.360, 0.435], mean action: 38.900 [6.000, 73.000], mean observation: 3.158 [-1.080, 10.280], loss: 1.278413, mae: 5.007566, mean_q: 5.231674
 88953/100000: episode: 9078, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 3.938, mean reward: 0.394 [0.352, 0.466], mean action: 36.500 [9.000, 73.000], mean observation: 3.160 [-1.729, 10.335], loss: 1.198246, mae: 5.007390, mean_q: 5.232319
 88963/100000: episode: 9079, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.802, mean reward: 0.380 [0.328, 0.447], mean action: 45.800 [23.000, 96.000], mean observation: 3.161 [-1.447, 10.250], loss: 1.212355, mae: 5.007714, mean_q: 5.232633
 88969/100000: episode: 9080, duration: 0.095s, episode steps: 6, steps per second: 63, episode reward: 11.975, mean reward: 1.996 [0.373, 10.000], mean action: 49.500 [35.000, 93.000], mean observation: 3.160 [-1.311, 10.277], loss: 0.878891, mae: 5.006848, mean_q: 5.233303
 88979/100000: episode: 9081, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.866, mean reward: 0.387 [0.333, 0.467], mean action: 38.100 [6.000, 77.000], mean observation: 3.158 [-1.121, 10.272], loss: 1.191717, mae: 5.008033, mean_q: 5.234575
 88989/100000: episode: 9082, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.254, mean reward: 0.425 [0.398, 0.566], mean action: 41.300 [8.000, 100.000], mean observation: 3.171 [-1.551, 10.346], loss: 0.934525, mae: 5.007516, mean_q: 5.236265
 88999/100000: episode: 9083, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.952, mean reward: 0.395 [0.377, 0.425], mean action: 50.900 [27.000, 100.000], mean observation: 3.165 [-1.201, 10.349], loss: 1.269384, mae: 5.009086, mean_q: 5.238062
 89009/100000: episode: 9084, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.253, mean reward: 0.425 [0.393, 0.485], mean action: 46.600 [17.000, 74.000], mean observation: 3.160 [-1.336, 10.411], loss: 1.285545, mae: 5.009208, mean_q: 5.239718
 89019/100000: episode: 9085, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.099, mean reward: 0.410 [0.355, 0.445], mean action: 47.000 [3.000, 97.000], mean observation: 3.155 [-1.099, 10.455], loss: 1.168790, mae: 5.008861, mean_q: 5.238898
 89029/100000: episode: 9086, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.014, mean reward: 0.401 [0.364, 0.466], mean action: 45.900 [16.000, 93.000], mean observation: 3.161 [-1.236, 10.379], loss: 1.042388, mae: 5.008224, mean_q: 5.234917
 89039/100000: episode: 9087, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 3.731, mean reward: 0.373 [0.349, 0.413], mean action: 74.500 [0.000, 93.000], mean observation: 3.143 [-1.725, 10.185], loss: 1.408968, mae: 5.009821, mean_q: 5.236926
 89049/100000: episode: 9088, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.593, mean reward: 0.459 [0.459, 0.459], mean action: 75.400 [29.000, 96.000], mean observation: 3.141 [-1.239, 10.331], loss: 1.257290, mae: 5.009079, mean_q: 5.239537
 89059/100000: episode: 9089, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 3.877, mean reward: 0.388 [0.360, 0.452], mean action: 71.900 [13.000, 93.000], mean observation: 3.171 [-0.723, 10.423], loss: 1.179016, mae: 5.008600, mean_q: 5.242810
 89069/100000: episode: 9090, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.504, mean reward: 0.350 [0.344, 0.405], mean action: 78.700 [26.000, 99.000], mean observation: 3.162 [-1.430, 10.248], loss: 1.524594, mae: 5.010070, mean_q: 5.243508
 89079/100000: episode: 9091, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.890, mean reward: 0.389 [0.311, 0.482], mean action: 63.100 [22.000, 93.000], mean observation: 3.158 [-1.126, 10.372], loss: 1.064185, mae: 5.008357, mean_q: 5.241296
 89089/100000: episode: 9092, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.364, mean reward: 0.436 [0.431, 0.455], mean action: 84.200 [46.000, 100.000], mean observation: 3.162 [-0.979, 10.392], loss: 1.302913, mae: 5.009130, mean_q: 5.242479
 89099/100000: episode: 9093, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.762, mean reward: 0.376 [0.327, 0.408], mean action: 66.700 [24.000, 93.000], mean observation: 3.147 [-1.490, 10.353], loss: 1.276253, mae: 5.008888, mean_q: 5.242429
 89109/100000: episode: 9094, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 3.716, mean reward: 0.372 [0.331, 0.443], mean action: 32.200 [2.000, 46.000], mean observation: 3.157 [-1.909, 10.292], loss: 1.251426, mae: 5.009026, mean_q: 5.243779
 89119/100000: episode: 9095, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 3.877, mean reward: 0.388 [0.300, 0.463], mean action: 39.900 [16.000, 99.000], mean observation: 3.144 [-1.317, 10.311], loss: 1.244217, mae: 5.008676, mean_q: 5.244749
 89129/100000: episode: 9096, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.794, mean reward: 0.379 [0.317, 0.429], mean action: 36.200 [6.000, 70.000], mean observation: 3.167 [-1.973, 10.263], loss: 1.187547, mae: 5.008410, mean_q: 5.243065
 89139/100000: episode: 9097, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.047, mean reward: 0.405 [0.351, 0.555], mean action: 33.400 [17.000, 55.000], mean observation: 3.154 [-1.149, 10.291], loss: 1.086807, mae: 5.008294, mean_q: 5.240244
 89149/100000: episode: 9098, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.209, mean reward: 0.421 [0.351, 0.561], mean action: 47.600 [1.000, 70.000], mean observation: 3.157 [-1.485, 10.425], loss: 1.175735, mae: 5.009232, mean_q: 5.240376
 89159/100000: episode: 9099, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.564, mean reward: 0.356 [0.321, 0.428], mean action: 45.100 [21.000, 99.000], mean observation: 3.161 [-1.012, 10.332], loss: 1.242774, mae: 5.009624, mean_q: 5.241584
 89169/100000: episode: 9100, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.186, mean reward: 0.419 [0.320, 0.500], mean action: 43.000 [14.000, 82.000], mean observation: 3.152 [-1.302, 10.423], loss: 1.201794, mae: 5.009662, mean_q: 5.242478
 89179/100000: episode: 9101, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.073, mean reward: 0.407 [0.377, 0.486], mean action: 49.900 [16.000, 96.000], mean observation: 3.151 [-1.660, 10.401], loss: 1.212404, mae: 5.009801, mean_q: 5.243145
 89189/100000: episode: 9102, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.446, mean reward: 0.445 [0.327, 0.562], mean action: 66.700 [9.000, 99.000], mean observation: 3.161 [-1.329, 10.322], loss: 1.145110, mae: 5.009626, mean_q: 5.242564
 89199/100000: episode: 9103, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.771, mean reward: 0.377 [0.356, 0.426], mean action: 71.000 [4.000, 93.000], mean observation: 3.152 [-1.391, 10.349], loss: 0.949081, mae: 5.009050, mean_q: 5.245183
 89209/100000: episode: 9104, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.846, mean reward: 0.385 [0.315, 0.453], mean action: 76.400 [4.000, 93.000], mean observation: 3.171 [-1.217, 10.326], loss: 1.308628, mae: 5.010856, mean_q: 5.245230
 89219/100000: episode: 9105, duration: 0.123s, episode steps: 10, steps per second: 82, episode reward: 3.631, mean reward: 0.363 [0.293, 0.464], mean action: 69.400 [21.000, 93.000], mean observation: 3.154 [-0.815, 10.352], loss: 1.367661, mae: 5.011352, mean_q: 5.243590
 89229/100000: episode: 9106, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.226, mean reward: 0.423 [0.357, 0.538], mean action: 38.700 [5.000, 93.000], mean observation: 3.153 [-1.391, 10.420], loss: 1.065261, mae: 5.010206, mean_q: 5.240410
 89239/100000: episode: 9107, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.929, mean reward: 0.393 [0.299, 0.514], mean action: 39.800 [0.000, 99.000], mean observation: 3.164 [-1.203, 10.390], loss: 1.419483, mae: 5.011580, mean_q: 5.241732
 89249/100000: episode: 9108, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.154, mean reward: 0.415 [0.351, 0.470], mean action: 50.900 [35.000, 101.000], mean observation: 3.150 [-1.832, 10.263], loss: 1.711492, mae: 5.012356, mean_q: 5.243310
 89259/100000: episode: 9109, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.124, mean reward: 0.412 [0.323, 0.589], mean action: 51.200 [35.000, 100.000], mean observation: 3.151 [-1.177, 10.261], loss: 1.005859, mae: 5.009589, mean_q: 5.245604
 89269/100000: episode: 9110, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.753, mean reward: 0.375 [0.306, 0.468], mean action: 28.700 [4.000, 54.000], mean observation: 3.158 [-1.068, 10.417], loss: 1.453416, mae: 5.011319, mean_q: 5.243747
 89279/100000: episode: 9111, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.050, mean reward: 0.405 [0.371, 0.479], mean action: 47.500 [7.000, 97.000], mean observation: 3.156 [-1.467, 10.548], loss: 1.384983, mae: 5.010925, mean_q: 5.240325
 89289/100000: episode: 9112, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.750, mean reward: 0.375 [0.244, 0.466], mean action: 37.500 [35.000, 48.000], mean observation: 3.161 [-2.242, 10.231], loss: 1.140663, mae: 5.010048, mean_q: 5.239275
 89299/100000: episode: 9113, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.113, mean reward: 0.411 [0.331, 0.499], mean action: 47.700 [8.000, 97.000], mean observation: 3.159 [-1.378, 10.364], loss: 1.475960, mae: 5.011456, mean_q: 5.236645
 89309/100000: episode: 9114, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 13.786, mean reward: 1.379 [0.340, 10.000], mean action: 34.800 [2.000, 88.000], mean observation: 3.154 [-0.977, 10.617], loss: 1.244250, mae: 5.010526, mean_q: 5.232627
 89319/100000: episode: 9115, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.663, mean reward: 0.366 [0.326, 0.419], mean action: 37.900 [0.000, 85.000], mean observation: 3.139 [-1.110, 10.407], loss: 1.259525, mae: 5.010664, mean_q: 5.233050
 89329/100000: episode: 9116, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.215, mean reward: 0.422 [0.397, 0.508], mean action: 35.300 [3.000, 101.000], mean observation: 3.156 [-1.098, 10.252], loss: 1.252230, mae: 5.010862, mean_q: 5.229301
 89339/100000: episode: 9117, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.276, mean reward: 0.428 [0.352, 0.561], mean action: 46.600 [35.000, 83.000], mean observation: 3.166 [-1.453, 10.313], loss: 1.230236, mae: 5.010831, mean_q: 5.225036
 89349/100000: episode: 9118, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.500, mean reward: 0.450 [0.450, 0.450], mean action: 51.600 [35.000, 94.000], mean observation: 3.154 [-1.617, 10.341], loss: 1.022402, mae: 5.010343, mean_q: 5.224298
 89359/100000: episode: 9119, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.300, mean reward: 0.430 [0.342, 0.566], mean action: 32.600 [11.000, 35.000], mean observation: 3.133 [-1.231, 10.390], loss: 1.181009, mae: 5.011187, mean_q: 5.225911
 89369/100000: episode: 9120, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.231, mean reward: 0.423 [0.321, 0.561], mean action: 43.500 [0.000, 99.000], mean observation: 3.159 [-1.076, 10.508], loss: 1.328649, mae: 5.011937, mean_q: 5.223344
 89379/100000: episode: 9121, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.273, mean reward: 0.427 [0.397, 0.525], mean action: 39.300 [20.000, 78.000], mean observation: 3.156 [-1.261, 10.267], loss: 1.345363, mae: 5.012282, mean_q: 5.223784
 89389/100000: episode: 9122, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.824, mean reward: 0.382 [0.292, 0.463], mean action: 32.500 [7.000, 56.000], mean observation: 3.156 [-1.447, 10.395], loss: 1.272691, mae: 5.012201, mean_q: 5.225106
 89399/100000: episode: 9123, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.374, mean reward: 0.437 [0.404, 0.555], mean action: 30.900 [10.000, 58.000], mean observation: 3.150 [-0.899, 10.259], loss: 0.975694, mae: 5.011086, mean_q: 5.227131
 89409/100000: episode: 9124, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.165, mean reward: 0.416 [0.373, 0.535], mean action: 31.100 [4.000, 96.000], mean observation: 3.162 [-1.535, 10.345], loss: 1.335623, mae: 5.012671, mean_q: 5.229687
 89419/100000: episode: 9125, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.996, mean reward: 0.400 [0.319, 0.499], mean action: 54.300 [35.000, 94.000], mean observation: 3.155 [-1.528, 10.356], loss: 0.966458, mae: 5.011331, mean_q: 5.231732
 89429/100000: episode: 9126, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.275, mean reward: 0.428 [0.329, 0.587], mean action: 44.000 [35.000, 81.000], mean observation: 3.158 [-1.107, 10.242], loss: 1.385596, mae: 5.012926, mean_q: 5.234149
 89439/100000: episode: 9127, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.285, mean reward: 0.428 [0.397, 0.491], mean action: 43.200 [9.000, 95.000], mean observation: 3.161 [-1.134, 10.251], loss: 1.211044, mae: 5.012346, mean_q: 5.236794
 89449/100000: episode: 9128, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.762, mean reward: 0.476 [0.357, 0.531], mean action: 42.500 [16.000, 98.000], mean observation: 3.161 [-1.329, 10.280], loss: 1.344081, mae: 5.013013, mean_q: 5.237302
 89459/100000: episode: 9129, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.662, mean reward: 0.366 [0.293, 0.500], mean action: 61.900 [35.000, 99.000], mean observation: 3.147 [-1.141, 10.243], loss: 1.274296, mae: 5.012725, mean_q: 5.233922
 89469/100000: episode: 9130, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.193, mean reward: 0.419 [0.334, 0.573], mean action: 48.600 [18.000, 100.000], mean observation: 3.154 [-1.223, 10.386], loss: 1.375900, mae: 5.012995, mean_q: 5.233670
 89479/100000: episode: 9131, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.038, mean reward: 0.404 [0.355, 0.465], mean action: 49.100 [13.000, 101.000], mean observation: 3.151 [-1.371, 10.289], loss: 1.152201, mae: 5.012091, mean_q: 5.231945
 89489/100000: episode: 9132, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.112, mean reward: 0.411 [0.350, 0.510], mean action: 39.900 [5.000, 89.000], mean observation: 3.159 [-1.536, 10.345], loss: 1.247339, mae: 5.012330, mean_q: 5.228769
 89499/100000: episode: 9133, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.213, mean reward: 0.421 [0.343, 0.529], mean action: 33.900 [15.000, 73.000], mean observation: 3.151 [-1.459, 10.416], loss: 1.357044, mae: 5.012728, mean_q: 5.230364
 89509/100000: episode: 9134, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.288, mean reward: 0.429 [0.342, 0.533], mean action: 46.300 [35.000, 98.000], mean observation: 3.159 [-1.229, 10.351], loss: 1.135740, mae: 5.011817, mean_q: 5.228947
 89519/100000: episode: 9135, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.031, mean reward: 0.403 [0.349, 0.543], mean action: 34.700 [4.000, 86.000], mean observation: 3.160 [-1.221, 10.404], loss: 1.040388, mae: 5.011779, mean_q: 5.224716
 89529/100000: episode: 9136, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.151, mean reward: 0.415 [0.360, 0.447], mean action: 45.500 [11.000, 87.000], mean observation: 3.142 [-1.015, 10.362], loss: 1.399957, mae: 5.013120, mean_q: 5.224246
 89538/100000: episode: 9137, duration: 0.154s, episode steps: 9, steps per second: 59, episode reward: 13.319, mean reward: 1.480 [0.355, 10.000], mean action: 45.000 [35.000, 101.000], mean observation: 3.148 [-1.135, 10.251], loss: 0.945884, mae: 5.011600, mean_q: 5.225712
 89548/100000: episode: 9138, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.258, mean reward: 0.426 [0.333, 0.534], mean action: 42.300 [0.000, 76.000], mean observation: 3.153 [-1.488, 10.343], loss: 1.140398, mae: 5.012589, mean_q: 5.227869
 89558/100000: episode: 9139, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.234, mean reward: 0.423 [0.377, 0.516], mean action: 48.100 [5.000, 96.000], mean observation: 3.152 [-2.055, 10.275], loss: 1.359957, mae: 5.013763, mean_q: 5.229622
 89568/100000: episode: 9140, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.484, mean reward: 0.448 [0.439, 0.481], mean action: 49.900 [35.000, 93.000], mean observation: 3.168 [-1.210, 10.432], loss: 1.180026, mae: 5.012955, mean_q: 5.227927
 89578/100000: episode: 9141, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.860, mean reward: 0.386 [0.355, 0.551], mean action: 56.500 [11.000, 100.000], mean observation: 3.148 [-1.411, 10.319], loss: 1.248737, mae: 5.013527, mean_q: 5.227933
 89588/100000: episode: 9142, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.910, mean reward: 0.391 [0.349, 0.442], mean action: 57.600 [20.000, 96.000], mean observation: 3.159 [-1.471, 10.279], loss: 1.387992, mae: 5.014153, mean_q: 5.229508
 89598/100000: episode: 9143, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.233, mean reward: 0.423 [0.325, 0.533], mean action: 45.200 [0.000, 99.000], mean observation: 3.154 [-1.341, 10.460], loss: 1.390869, mae: 5.013854, mean_q: 5.231038
 89608/100000: episode: 9144, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.470, mean reward: 0.447 [0.371, 0.466], mean action: 41.700 [6.000, 85.000], mean observation: 3.169 [-1.260, 10.290], loss: 1.434422, mae: 5.013794, mean_q: 5.233178
 89618/100000: episode: 9145, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.176, mean reward: 0.418 [0.365, 0.570], mean action: 45.900 [4.000, 91.000], mean observation: 3.160 [-1.378, 10.188], loss: 1.510626, mae: 5.013291, mean_q: 5.232215
 89628/100000: episode: 9146, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.891, mean reward: 0.389 [0.333, 0.479], mean action: 37.000 [10.000, 78.000], mean observation: 3.155 [-1.181, 10.380], loss: 1.093820, mae: 5.011430, mean_q: 5.229318
 89638/100000: episode: 9147, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 4.316, mean reward: 0.432 [0.382, 0.560], mean action: 51.300 [4.000, 94.000], mean observation: 3.153 [-1.112, 10.460], loss: 1.363785, mae: 5.012501, mean_q: 5.226367
 89648/100000: episode: 9148, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.491, mean reward: 0.449 [0.442, 0.517], mean action: 37.900 [7.000, 67.000], mean observation: 3.164 [-1.947, 10.380], loss: 1.189082, mae: 5.011729, mean_q: 5.226442
 89658/100000: episode: 9149, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.608, mean reward: 0.461 [0.357, 0.539], mean action: 44.700 [16.000, 92.000], mean observation: 3.151 [-2.589, 10.488], loss: 1.306623, mae: 5.012394, mean_q: 5.228502
 89668/100000: episode: 9150, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.902, mean reward: 0.490 [0.490, 0.490], mean action: 47.600 [35.000, 100.000], mean observation: 3.174 [-1.313, 10.390], loss: 1.145416, mae: 5.011694, mean_q: 5.230531
 89678/100000: episode: 9151, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.748, mean reward: 0.475 [0.464, 0.496], mean action: 47.800 [35.000, 100.000], mean observation: 3.159 [-0.981, 10.374], loss: 1.003432, mae: 5.011070, mean_q: 5.233675
 89688/100000: episode: 9152, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.203, mean reward: 0.420 [0.397, 0.515], mean action: 42.100 [17.000, 87.000], mean observation: 3.154 [-1.322, 10.249], loss: 1.300064, mae: 5.012458, mean_q: 5.237058
 89696/100000: episode: 9153, duration: 0.127s, episode steps: 8, steps per second: 63, episode reward: 12.807, mean reward: 1.601 [0.346, 10.000], mean action: 47.375 [24.000, 86.000], mean observation: 3.174 [-1.463, 10.384], loss: 1.114852, mae: 5.011856, mean_q: 5.240429
 89705/100000: episode: 9154, duration: 0.157s, episode steps: 9, steps per second: 57, episode reward: 13.347, mean reward: 1.483 [0.359, 10.000], mean action: 39.000 [12.000, 75.000], mean observation: 3.150 [-1.356, 10.267], loss: 1.701250, mae: 5.014048, mean_q: 5.242837
 89715/100000: episode: 9155, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.770, mean reward: 0.377 [0.340, 0.436], mean action: 53.700 [35.000, 97.000], mean observation: 3.157 [-1.451, 10.262], loss: 1.492741, mae: 5.012713, mean_q: 5.244714
 89725/100000: episode: 9156, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 5.088, mean reward: 0.509 [0.509, 0.509], mean action: 51.000 [15.000, 99.000], mean observation: 3.159 [-0.835, 10.409], loss: 1.240809, mae: 5.011700, mean_q: 5.245236
 89735/100000: episode: 9157, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.589, mean reward: 0.459 [0.452, 0.521], mean action: 42.100 [12.000, 93.000], mean observation: 3.155 [-1.172, 10.539], loss: 1.608197, mae: 5.012889, mean_q: 5.246767
 89745/100000: episode: 9158, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.568, mean reward: 0.457 [0.449, 0.520], mean action: 48.200 [35.000, 98.000], mean observation: 3.160 [-1.329, 10.259], loss: 1.429279, mae: 5.011915, mean_q: 5.248532
 89755/100000: episode: 9159, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.746, mean reward: 0.375 [0.319, 0.424], mean action: 45.700 [21.000, 96.000], mean observation: 3.160 [-1.774, 10.376], loss: 1.534530, mae: 5.012207, mean_q: 5.249803
 89765/100000: episode: 9160, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.035, mean reward: 0.403 [0.341, 0.474], mean action: 48.400 [21.000, 98.000], mean observation: 3.157 [-1.141, 10.444], loss: 1.350545, mae: 5.011436, mean_q: 5.251240
 89775/100000: episode: 9161, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.050, mean reward: 0.405 [0.311, 0.446], mean action: 45.800 [30.000, 88.000], mean observation: 3.142 [-2.112, 10.362], loss: 1.148169, mae: 5.010760, mean_q: 5.252643
 89785/100000: episode: 9162, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.893, mean reward: 0.389 [0.360, 0.476], mean action: 36.800 [4.000, 79.000], mean observation: 3.154 [-1.332, 10.369], loss: 1.346207, mae: 5.011505, mean_q: 5.254545
 89795/100000: episode: 9163, duration: 0.129s, episode steps: 10, steps per second: 77, episode reward: 3.847, mean reward: 0.385 [0.379, 0.404], mean action: 63.800 [35.000, 98.000], mean observation: 3.164 [-0.759, 10.354], loss: 1.264606, mae: 5.011068, mean_q: 5.256699
 89805/100000: episode: 9164, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.774, mean reward: 0.377 [0.353, 0.442], mean action: 51.400 [2.000, 94.000], mean observation: 3.147 [-1.488, 10.364], loss: 1.222109, mae: 5.011185, mean_q: 5.257616
 89815/100000: episode: 9165, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.039, mean reward: 0.404 [0.369, 0.473], mean action: 40.600 [15.000, 85.000], mean observation: 3.154 [-1.625, 10.309], loss: 1.519524, mae: 5.012339, mean_q: 5.261029
 89825/100000: episode: 9166, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.634, mean reward: 0.363 [0.281, 0.458], mean action: 40.800 [2.000, 68.000], mean observation: 3.159 [-1.369, 10.279], loss: 1.225263, mae: 5.011180, mean_q: 5.263827
 89835/100000: episode: 9167, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.926, mean reward: 0.393 [0.342, 0.497], mean action: 42.000 [27.000, 66.000], mean observation: 3.153 [-1.555, 10.386], loss: 0.866070, mae: 5.009892, mean_q: 5.258540
 89845/100000: episode: 9168, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.717, mean reward: 0.372 [0.297, 0.513], mean action: 42.200 [27.000, 60.000], mean observation: 3.166 [-1.632, 10.343], loss: 1.275967, mae: 5.011809, mean_q: 5.256220
 89846/100000: episode: 9169, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 35.000 [35.000, 35.000], mean observation: 3.179 [-1.647, 10.418], loss: 1.500811, mae: 5.012836, mean_q: 5.254166
 89856/100000: episode: 9170, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.320, mean reward: 0.432 [0.368, 0.569], mean action: 43.400 [3.000, 100.000], mean observation: 3.155 [-1.428, 10.325], loss: 1.141423, mae: 5.011302, mean_q: 5.247029
 89866/100000: episode: 9171, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.867, mean reward: 0.387 [0.367, 0.473], mean action: 39.300 [18.000, 86.000], mean observation: 3.173 [-1.084, 10.385], loss: 0.949258, mae: 5.010709, mean_q: 5.241170
 89876/100000: episode: 9172, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.061, mean reward: 0.406 [0.341, 0.482], mean action: 45.700 [25.000, 93.000], mean observation: 3.146 [-1.338, 10.254], loss: 1.309459, mae: 5.012375, mean_q: 5.240735
 89886/100000: episode: 9173, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.593, mean reward: 0.459 [0.411, 0.550], mean action: 38.200 [11.000, 82.000], mean observation: 3.162 [-0.899, 10.359], loss: 1.016702, mae: 5.011374, mean_q: 5.242127
 89896/100000: episode: 9174, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.240, mean reward: 0.424 [0.363, 0.501], mean action: 45.200 [26.000, 86.000], mean observation: 3.164 [-0.975, 10.411], loss: 1.281237, mae: 5.012810, mean_q: 5.244960
 89906/100000: episode: 9175, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.233, mean reward: 0.423 [0.353, 0.541], mean action: 43.700 [2.000, 78.000], mean observation: 3.159 [-1.795, 10.285], loss: 1.285947, mae: 5.013148, mean_q: 5.245909
 89916/100000: episode: 9176, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.702, mean reward: 0.370 [0.305, 0.442], mean action: 56.600 [0.000, 98.000], mean observation: 3.163 [-0.963, 10.190], loss: 1.421080, mae: 5.013959, mean_q: 5.246190
 89926/100000: episode: 9177, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.929, mean reward: 0.393 [0.317, 0.442], mean action: 36.800 [8.000, 64.000], mean observation: 3.158 [-1.257, 10.414], loss: 1.286076, mae: 5.013465, mean_q: 5.247703
 89936/100000: episode: 9178, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.061, mean reward: 0.406 [0.297, 0.511], mean action: 45.300 [26.000, 98.000], mean observation: 3.159 [-1.117, 10.274], loss: 1.260689, mae: 5.013432, mean_q: 5.252660
 89946/100000: episode: 9179, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.369, mean reward: 0.437 [0.363, 0.541], mean action: 31.600 [1.000, 35.000], mean observation: 3.156 [-2.475, 10.199], loss: 1.226725, mae: 5.013730, mean_q: 5.257498
 89956/100000: episode: 9180, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.007, mean reward: 0.401 [0.326, 0.461], mean action: 34.000 [5.000, 86.000], mean observation: 3.155 [-1.650, 10.318], loss: 1.082929, mae: 5.013673, mean_q: 5.260899
 89966/100000: episode: 9181, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.973, mean reward: 0.397 [0.328, 0.471], mean action: 59.100 [35.000, 91.000], mean observation: 3.144 [-2.235, 10.393], loss: 1.020265, mae: 5.013716, mean_q: 5.262803
 89976/100000: episode: 9182, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.776, mean reward: 0.378 [0.327, 0.460], mean action: 49.100 [31.000, 97.000], mean observation: 3.146 [-1.988, 10.162], loss: 1.022838, mae: 5.014115, mean_q: 5.265340
 89986/100000: episode: 9183, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.997, mean reward: 0.400 [0.337, 0.475], mean action: 48.300 [33.000, 75.000], mean observation: 3.157 [-1.994, 10.482], loss: 1.725178, mae: 5.017394, mean_q: 5.268851
 89996/100000: episode: 9184, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.380, mean reward: 0.438 [0.320, 0.504], mean action: 56.300 [35.000, 101.000], mean observation: 3.168 [-1.492, 10.416], loss: 1.455087, mae: 5.016275, mean_q: 5.272045
 90006/100000: episode: 9185, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.219, mean reward: 0.422 [0.356, 0.510], mean action: 37.400 [10.000, 93.000], mean observation: 3.153 [-1.140, 10.537], loss: 1.446255, mae: 5.016065, mean_q: 5.278678
 90016/100000: episode: 9186, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.947, mean reward: 0.395 [0.328, 0.600], mean action: 40.000 [20.000, 68.000], mean observation: 3.153 [-1.688, 10.372], loss: 1.163821, mae: 5.015431, mean_q: 5.279613
 90026/100000: episode: 9187, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 3.958, mean reward: 0.396 [0.351, 0.480], mean action: 39.800 [2.000, 90.000], mean observation: 3.159 [-1.558, 10.405], loss: 1.546670, mae: 5.017032, mean_q: 5.279555
 90036/100000: episode: 9188, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.965, mean reward: 0.397 [0.349, 0.445], mean action: 45.800 [28.000, 84.000], mean observation: 3.155 [-1.025, 10.298], loss: 1.286410, mae: 5.015756, mean_q: 5.280566
 90046/100000: episode: 9189, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.108, mean reward: 0.411 [0.406, 0.451], mean action: 41.500 [11.000, 80.000], mean observation: 3.163 [-1.526, 10.309], loss: 1.322848, mae: 5.015910, mean_q: 5.279439
 90056/100000: episode: 9190, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 5.062, mean reward: 0.506 [0.506, 0.506], mean action: 45.000 [21.000, 81.000], mean observation: 3.141 [-1.300, 10.353], loss: 1.202288, mae: 5.015707, mean_q: 5.281525
 90066/100000: episode: 9191, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.242, mean reward: 0.424 [0.396, 0.502], mean action: 50.800 [27.000, 94.000], mean observation: 3.163 [-1.007, 10.224], loss: 1.261495, mae: 5.016216, mean_q: 5.283052
 90076/100000: episode: 9192, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.533, mean reward: 0.453 [0.451, 0.463], mean action: 45.500 [35.000, 90.000], mean observation: 3.159 [-1.418, 10.263], loss: 1.785949, mae: 5.018238, mean_q: 5.286656
 90086/100000: episode: 9193, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.719, mean reward: 0.472 [0.468, 0.490], mean action: 44.300 [13.000, 100.000], mean observation: 3.161 [-1.474, 10.425], loss: 1.340807, mae: 5.016662, mean_q: 5.289639
 90096/100000: episode: 9194, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.026, mean reward: 0.403 [0.372, 0.445], mean action: 34.000 [18.000, 57.000], mean observation: 3.152 [-1.049, 10.385], loss: 1.331912, mae: 5.016304, mean_q: 5.289708
 90106/100000: episode: 9195, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.700, mean reward: 0.470 [0.415, 0.559], mean action: 46.600 [7.000, 94.000], mean observation: 3.136 [-2.449, 10.305], loss: 1.035467, mae: 5.015324, mean_q: 5.290246
 90116/100000: episode: 9196, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.272, mean reward: 0.427 [0.394, 0.572], mean action: 40.400 [20.000, 77.000], mean observation: 3.147 [-1.665, 10.227], loss: 1.155743, mae: 5.016458, mean_q: 5.291596
 90126/100000: episode: 9197, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.659, mean reward: 0.366 [0.291, 0.488], mean action: 51.000 [34.000, 85.000], mean observation: 3.151 [-1.579, 10.357], loss: 1.135047, mae: 5.016726, mean_q: 5.293392
 90136/100000: episode: 9198, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.402, mean reward: 0.440 [0.429, 0.492], mean action: 45.700 [19.000, 93.000], mean observation: 3.167 [-1.218, 10.366], loss: 1.106298, mae: 5.017006, mean_q: 5.295455
 90146/100000: episode: 9199, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.136, mean reward: 0.414 [0.323, 0.477], mean action: 33.100 [3.000, 81.000], mean observation: 3.154 [-1.192, 10.511], loss: 1.067005, mae: 5.017376, mean_q: 5.295726
 90156/100000: episode: 9200, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.119, mean reward: 0.412 [0.321, 0.510], mean action: 43.700 [8.000, 98.000], mean observation: 3.141 [-1.441, 10.436], loss: 1.309146, mae: 5.018745, mean_q: 5.295027
 90166/100000: episode: 9201, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.090, mean reward: 0.409 [0.323, 0.477], mean action: 44.100 [3.000, 101.000], mean observation: 3.146 [-1.753, 10.417], loss: 1.097561, mae: 5.018166, mean_q: 5.289016
 90176/100000: episode: 9202, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.570, mean reward: 0.457 [0.425, 0.539], mean action: 45.400 [35.000, 92.000], mean observation: 3.177 [-1.636, 10.327], loss: 1.293905, mae: 5.019041, mean_q: 5.283843
 90186/100000: episode: 9203, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.175, mean reward: 0.418 [0.342, 0.490], mean action: 33.000 [18.000, 35.000], mean observation: 3.158 [-1.957, 10.250], loss: 1.486578, mae: 5.020193, mean_q: 5.283547
 90196/100000: episode: 9204, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.187, mean reward: 0.419 [0.363, 0.500], mean action: 46.300 [35.000, 93.000], mean observation: 3.161 [-1.517, 10.481], loss: 1.195376, mae: 5.019110, mean_q: 5.284606
 90206/100000: episode: 9205, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.888, mean reward: 0.389 [0.357, 0.518], mean action: 52.100 [6.000, 99.000], mean observation: 3.149 [-1.378, 10.370], loss: 1.450598, mae: 5.020307, mean_q: 5.285583
 90216/100000: episode: 9206, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.158, mean reward: 0.416 [0.392, 0.454], mean action: 34.000 [16.000, 53.000], mean observation: 3.157 [-1.616, 10.321], loss: 1.076252, mae: 5.019205, mean_q: 5.288035
 90226/100000: episode: 9207, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.933, mean reward: 0.393 [0.346, 0.528], mean action: 40.300 [2.000, 99.000], mean observation: 3.148 [-1.621, 10.439], loss: 1.170047, mae: 5.019900, mean_q: 5.286664
 90236/100000: episode: 9208, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.249, mean reward: 0.425 [0.382, 0.532], mean action: 31.700 [0.000, 69.000], mean observation: 3.157 [-1.448, 10.447], loss: 1.451035, mae: 5.021253, mean_q: 5.286827
 90246/100000: episode: 9209, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.140, mean reward: 0.414 [0.351, 0.466], mean action: 55.000 [34.000, 95.000], mean observation: 3.152 [-1.288, 10.308], loss: 1.141605, mae: 5.020132, mean_q: 5.287819
 90256/100000: episode: 9210, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.042, mean reward: 0.404 [0.357, 0.459], mean action: 50.800 [5.000, 100.000], mean observation: 3.157 [-1.956, 10.328], loss: 1.204601, mae: 5.020605, mean_q: 5.284905
 90266/100000: episode: 9211, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.444, mean reward: 0.444 [0.378, 0.533], mean action: 55.100 [35.000, 101.000], mean observation: 3.164 [-1.505, 10.229], loss: 1.333688, mae: 5.021505, mean_q: 5.285009
 90276/100000: episode: 9212, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.156, mean reward: 0.416 [0.323, 0.472], mean action: 47.000 [8.000, 84.000], mean observation: 3.148 [-1.511, 10.356], loss: 1.274112, mae: 5.021571, mean_q: 5.284431
 90286/100000: episode: 9213, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.080, mean reward: 0.408 [0.352, 0.444], mean action: 55.800 [35.000, 100.000], mean observation: 3.153 [-0.982, 10.393], loss: 1.205062, mae: 5.021460, mean_q: 5.284331
 90296/100000: episode: 9214, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.754, mean reward: 0.375 [0.291, 0.471], mean action: 35.200 [6.000, 94.000], mean observation: 3.163 [-1.089, 10.285], loss: 1.257997, mae: 5.021817, mean_q: 5.286716
 90306/100000: episode: 9215, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.738, mean reward: 0.474 [0.474, 0.474], mean action: 45.300 [11.000, 92.000], mean observation: 3.147 [-1.372, 10.365], loss: 1.360074, mae: 5.022258, mean_q: 5.287447
 90316/100000: episode: 9216, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.191, mean reward: 0.419 [0.352, 0.491], mean action: 31.800 [15.000, 39.000], mean observation: 3.149 [-1.405, 10.310], loss: 1.085765, mae: 5.021314, mean_q: 5.283735
 90326/100000: episode: 9217, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.210, mean reward: 0.421 [0.324, 0.572], mean action: 27.900 [0.000, 75.000], mean observation: 3.162 [-1.310, 10.267], loss: 1.299813, mae: 5.022439, mean_q: 5.281415
 90327/100000: episode: 9218, duration: 0.025s, episode steps: 1, steps per second: 41, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 35.000 [35.000, 35.000], mean observation: 3.148 [-0.729, 10.100], loss: 1.190188, mae: 5.022103, mean_q: 5.281536
 90337/100000: episode: 9219, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.111, mean reward: 0.411 [0.321, 0.585], mean action: 41.600 [35.000, 73.000], mean observation: 3.167 [-1.443, 10.333], loss: 1.064156, mae: 5.021935, mean_q: 5.282277
 90347/100000: episode: 9220, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.756, mean reward: 0.376 [0.307, 0.471], mean action: 43.800 [17.000, 86.000], mean observation: 3.153 [-1.481, 10.257], loss: 0.932351, mae: 5.021895, mean_q: 5.283927
 90357/100000: episode: 9221, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.217, mean reward: 0.422 [0.357, 0.546], mean action: 46.500 [16.000, 94.000], mean observation: 3.163 [-1.877, 10.348], loss: 1.094426, mae: 5.023174, mean_q: 5.283975
 90367/100000: episode: 9222, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.540, mean reward: 0.454 [0.454, 0.454], mean action: 45.600 [11.000, 66.000], mean observation: 3.170 [-1.604, 10.287], loss: 1.343199, mae: 5.024614, mean_q: 5.285169
 90377/100000: episode: 9223, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.815, mean reward: 0.381 [0.283, 0.447], mean action: 46.700 [5.000, 100.000], mean observation: 3.148 [-1.572, 10.284], loss: 1.500541, mae: 5.025439, mean_q: 5.287159
 90387/100000: episode: 9224, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.246, mean reward: 0.425 [0.376, 0.502], mean action: 46.200 [23.000, 79.000], mean observation: 3.160 [-1.490, 10.404], loss: 1.301254, mae: 5.024515, mean_q: 5.288878
 90397/100000: episode: 9225, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.900, mean reward: 0.390 [0.312, 0.437], mean action: 51.600 [8.000, 82.000], mean observation: 3.159 [-1.350, 10.316], loss: 1.044470, mae: 5.023385, mean_q: 5.287893
 90407/100000: episode: 9226, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.862, mean reward: 0.386 [0.312, 0.514], mean action: 41.100 [10.000, 67.000], mean observation: 3.146 [-1.254, 10.494], loss: 1.687019, mae: 5.025868, mean_q: 5.288594
 90417/100000: episode: 9227, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.103, mean reward: 0.410 [0.308, 0.586], mean action: 42.100 [5.000, 94.000], mean observation: 3.161 [-1.759, 10.491], loss: 1.372485, mae: 5.024472, mean_q: 5.290659
 90427/100000: episode: 9228, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.354, mean reward: 0.435 [0.427, 0.471], mean action: 41.000 [30.000, 101.000], mean observation: 3.155 [-1.525, 10.247], loss: 1.176600, mae: 5.023484, mean_q: 5.291363
 90437/100000: episode: 9229, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.031, mean reward: 0.403 [0.381, 0.519], mean action: 37.300 [21.000, 57.000], mean observation: 3.148 [-1.171, 10.332], loss: 1.199090, mae: 5.023715, mean_q: 5.290700
 90447/100000: episode: 9230, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.966, mean reward: 0.397 [0.314, 0.524], mean action: 37.500 [0.000, 95.000], mean observation: 3.161 [-1.138, 10.490], loss: 1.285936, mae: 5.024504, mean_q: 5.291595
 90457/100000: episode: 9231, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.859, mean reward: 0.386 [0.330, 0.513], mean action: 37.800 [0.000, 99.000], mean observation: 3.159 [-0.909, 10.229], loss: 1.294454, mae: 5.025000, mean_q: 5.291034
 90460/100000: episode: 9232, duration: 0.061s, episode steps: 3, steps per second: 49, episode reward: 10.942, mean reward: 3.647 [0.414, 10.000], mean action: 50.667 [35.000, 59.000], mean observation: 3.140 [-1.395, 10.338], loss: 0.947143, mae: 5.023850, mean_q: 5.290470
 90470/100000: episode: 9233, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.280, mean reward: 0.428 [0.403, 0.532], mean action: 50.000 [4.000, 98.000], mean observation: 3.147 [-1.240, 10.260], loss: 1.277001, mae: 5.025510, mean_q: 5.291219
 90480/100000: episode: 9234, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 4.602, mean reward: 0.460 [0.399, 0.552], mean action: 36.800 [31.000, 50.000], mean observation: 3.156 [-0.806, 10.432], loss: 1.010736, mae: 5.024779, mean_q: 5.290864
 90490/100000: episode: 9235, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.001, mean reward: 0.400 [0.342, 0.460], mean action: 40.400 [35.000, 66.000], mean observation: 3.156 [-1.108, 10.322], loss: 1.272363, mae: 5.026484, mean_q: 5.289103
 90500/100000: episode: 9236, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.941, mean reward: 0.394 [0.344, 0.556], mean action: 54.100 [9.000, 89.000], mean observation: 3.159 [-1.261, 10.354], loss: 1.143976, mae: 5.026164, mean_q: 5.286614
 90506/100000: episode: 9237, duration: 0.109s, episode steps: 6, steps per second: 55, episode reward: 12.117, mean reward: 2.020 [0.423, 10.000], mean action: 42.167 [26.000, 58.000], mean observation: 3.156 [-1.669, 10.183], loss: 0.991926, mae: 5.025801, mean_q: 5.287487
 90516/100000: episode: 9238, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.331, mean reward: 0.433 [0.327, 0.502], mean action: 46.300 [20.000, 81.000], mean observation: 3.151 [-2.189, 10.491], loss: 1.306547, mae: 5.027538, mean_q: 5.288982
 90526/100000: episode: 9239, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.290, mean reward: 0.429 [0.384, 0.531], mean action: 50.400 [22.000, 88.000], mean observation: 3.158 [-1.182, 10.458], loss: 1.243141, mae: 5.027421, mean_q: 5.287853
 90536/100000: episode: 9240, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.051, mean reward: 0.405 [0.314, 0.440], mean action: 38.800 [9.000, 77.000], mean observation: 3.154 [-1.744, 10.207], loss: 1.418226, mae: 5.028258, mean_q: 5.286520
 90546/100000: episode: 9241, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.735, mean reward: 0.374 [0.357, 0.399], mean action: 48.400 [35.000, 80.000], mean observation: 3.148 [-1.228, 10.463], loss: 1.395340, mae: 5.028258, mean_q: 5.287867
 90556/100000: episode: 9242, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.981, mean reward: 0.398 [0.391, 0.434], mean action: 40.500 [31.000, 65.000], mean observation: 3.146 [-1.753, 10.216], loss: 1.274599, mae: 5.027726, mean_q: 5.290147
 90566/100000: episode: 9243, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.929, mean reward: 0.393 [0.318, 0.506], mean action: 49.000 [35.000, 95.000], mean observation: 3.149 [-1.381, 10.361], loss: 1.477507, mae: 5.028667, mean_q: 5.291158
 90576/100000: episode: 9244, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.361, mean reward: 0.436 [0.322, 0.532], mean action: 41.200 [7.000, 99.000], mean observation: 3.161 [-1.261, 10.286], loss: 1.329614, mae: 5.028259, mean_q: 5.284665
 90586/100000: episode: 9245, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.403, mean reward: 0.440 [0.436, 0.477], mean action: 48.900 [31.000, 85.000], mean observation: 3.148 [-2.228, 10.319], loss: 1.005051, mae: 5.027101, mean_q: 5.280840
 90596/100000: episode: 9246, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.683, mean reward: 0.368 [0.297, 0.431], mean action: 40.900 [21.000, 97.000], mean observation: 3.157 [-1.535, 10.281], loss: 1.219931, mae: 5.028048, mean_q: 5.279328
 90606/100000: episode: 9247, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.416, mean reward: 0.442 [0.332, 0.533], mean action: 41.800 [0.000, 87.000], mean observation: 3.157 [-0.882, 10.337], loss: 1.066769, mae: 5.027918, mean_q: 5.280481
 90616/100000: episode: 9248, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.069, mean reward: 0.407 [0.356, 0.467], mean action: 38.700 [7.000, 68.000], mean observation: 3.154 [-1.549, 10.446], loss: 1.153925, mae: 5.028794, mean_q: 5.283252
 90626/100000: episode: 9249, duration: 0.153s, episode steps: 10, steps per second: 66, episode reward: 4.310, mean reward: 0.431 [0.378, 0.508], mean action: 50.700 [33.000, 90.000], mean observation: 3.160 [-1.650, 10.324], loss: 1.512360, mae: 5.030361, mean_q: 5.283218
 90636/100000: episode: 9250, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.542, mean reward: 0.454 [0.423, 0.551], mean action: 49.400 [11.000, 86.000], mean observation: 3.159 [-1.670, 10.291], loss: 1.622794, mae: 5.031005, mean_q: 5.278307
 90646/100000: episode: 9251, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.037, mean reward: 0.404 [0.344, 0.524], mean action: 46.200 [35.000, 86.000], mean observation: 3.153 [-1.521, 10.278], loss: 1.005425, mae: 5.028465, mean_q: 5.277781
 90656/100000: episode: 9252, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.631, mean reward: 0.363 [0.319, 0.439], mean action: 48.500 [18.000, 94.000], mean observation: 3.170 [-1.338, 10.337], loss: 1.360271, mae: 5.030396, mean_q: 5.277375
 90666/100000: episode: 9253, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 5.628, mean reward: 0.563 [0.563, 0.563], mean action: 47.700 [32.000, 93.000], mean observation: 3.141 [-1.889, 10.239], loss: 1.387388, mae: 5.030412, mean_q: 5.273536
 90676/100000: episode: 9254, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.126, mean reward: 0.413 [0.317, 0.543], mean action: 29.700 [4.000, 55.000], mean observation: 3.154 [-1.558, 10.412], loss: 1.025082, mae: 5.029309, mean_q: 5.271193
 90686/100000: episode: 9255, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.997, mean reward: 0.400 [0.334, 0.501], mean action: 45.400 [10.000, 91.000], mean observation: 3.156 [-1.410, 10.457], loss: 1.022548, mae: 5.029382, mean_q: 5.268143
 90696/100000: episode: 9256, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.430, mean reward: 0.443 [0.388, 0.489], mean action: 47.500 [20.000, 98.000], mean observation: 3.168 [-1.241, 10.384], loss: 0.988990, mae: 5.029731, mean_q: 5.269630
 90706/100000: episode: 9257, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.333, mean reward: 0.433 [0.424, 0.474], mean action: 40.400 [35.000, 62.000], mean observation: 3.157 [-0.851, 10.260], loss: 1.268766, mae: 5.031199, mean_q: 5.272502
 90716/100000: episode: 9258, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.888, mean reward: 0.389 [0.277, 0.541], mean action: 48.000 [24.000, 92.000], mean observation: 3.163 [-1.597, 10.328], loss: 1.384894, mae: 5.031958, mean_q: 5.274428
 90726/100000: episode: 9259, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.158, mean reward: 0.416 [0.393, 0.536], mean action: 49.000 [14.000, 90.000], mean observation: 3.145 [-1.039, 10.262], loss: 1.172525, mae: 5.031184, mean_q: 5.275016
 90736/100000: episode: 9260, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.233, mean reward: 0.423 [0.406, 0.478], mean action: 44.000 [17.000, 85.000], mean observation: 3.155 [-1.404, 10.360], loss: 1.297158, mae: 5.031827, mean_q: 5.272607
 90746/100000: episode: 9261, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.066, mean reward: 0.407 [0.308, 0.578], mean action: 32.300 [8.000, 58.000], mean observation: 3.153 [-1.275, 10.201], loss: 1.350954, mae: 5.032376, mean_q: 5.272948
 90756/100000: episode: 9262, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.341, mean reward: 0.434 [0.325, 0.585], mean action: 48.200 [11.000, 101.000], mean observation: 3.154 [-1.055, 10.300], loss: 1.023954, mae: 5.031338, mean_q: 5.275505
 90766/100000: episode: 9263, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.037, mean reward: 0.404 [0.344, 0.576], mean action: 42.400 [20.000, 92.000], mean observation: 3.154 [-1.591, 10.214], loss: 1.298173, mae: 5.032760, mean_q: 5.278762
 90776/100000: episode: 9264, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.051, mean reward: 0.405 [0.330, 0.524], mean action: 49.400 [31.000, 83.000], mean observation: 3.150 [-1.324, 10.262], loss: 1.433508, mae: 5.033631, mean_q: 5.280522
 90786/100000: episode: 9265, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.006, mean reward: 0.401 [0.318, 0.531], mean action: 38.500 [2.000, 86.000], mean observation: 3.155 [-1.594, 10.337], loss: 1.133973, mae: 5.032431, mean_q: 5.281409
 90796/100000: episode: 9266, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.160, mean reward: 0.416 [0.396, 0.542], mean action: 42.600 [15.000, 86.000], mean observation: 3.157 [-1.703, 10.393], loss: 1.405256, mae: 5.033467, mean_q: 5.282236
 90806/100000: episode: 9267, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.942, mean reward: 0.394 [0.312, 0.481], mean action: 37.100 [12.000, 79.000], mean observation: 3.165 [-1.769, 10.339], loss: 0.957925, mae: 5.031950, mean_q: 5.282527
 90816/100000: episode: 9268, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.956, mean reward: 0.396 [0.353, 0.476], mean action: 44.100 [9.000, 100.000], mean observation: 3.153 [-1.317, 10.343], loss: 1.264103, mae: 5.033355, mean_q: 5.281251
 90826/100000: episode: 9269, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.974, mean reward: 0.397 [0.308, 0.482], mean action: 40.100 [3.000, 97.000], mean observation: 3.153 [-1.473, 10.326], loss: 1.131203, mae: 5.033238, mean_q: 5.287334
 90836/100000: episode: 9270, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.213, mean reward: 0.421 [0.409, 0.502], mean action: 39.900 [35.000, 84.000], mean observation: 3.159 [-1.064, 10.239], loss: 1.396992, mae: 5.034336, mean_q: 5.290218
 90846/100000: episode: 9271, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.882, mean reward: 0.388 [0.362, 0.429], mean action: 44.000 [30.000, 96.000], mean observation: 3.155 [-1.462, 10.264], loss: 1.376600, mae: 5.034480, mean_q: 5.290937
 90856/100000: episode: 9272, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.342, mean reward: 0.434 [0.361, 0.532], mean action: 39.400 [1.000, 96.000], mean observation: 3.158 [-1.095, 10.333], loss: 1.293799, mae: 5.034149, mean_q: 5.290159
 90866/100000: episode: 9273, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.269, mean reward: 0.427 [0.297, 0.553], mean action: 42.400 [8.000, 82.000], mean observation: 3.158 [-1.381, 10.466], loss: 1.076568, mae: 5.033471, mean_q: 5.284879
 90876/100000: episode: 9274, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.064, mean reward: 0.406 [0.379, 0.461], mean action: 38.800 [5.000, 70.000], mean observation: 3.147 [-1.444, 10.285], loss: 1.061373, mae: 5.033563, mean_q: 5.277295
 90886/100000: episode: 9275, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.453, mean reward: 0.445 [0.425, 0.499], mean action: 27.800 [2.000, 35.000], mean observation: 3.168 [-1.316, 10.351], loss: 0.756341, mae: 5.032699, mean_q: 5.270953
 90896/100000: episode: 9276, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.273, mean reward: 0.427 [0.425, 0.446], mean action: 51.800 [16.000, 92.000], mean observation: 3.153 [-1.206, 10.210], loss: 1.583043, mae: 5.036465, mean_q: 5.264850
 90906/100000: episode: 9277, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.466, mean reward: 0.447 [0.376, 0.493], mean action: 48.500 [7.000, 83.000], mean observation: 3.166 [-1.139, 10.226], loss: 1.369924, mae: 5.035951, mean_q: 5.258602
 90916/100000: episode: 9278, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.118, mean reward: 0.412 [0.333, 0.528], mean action: 37.800 [13.000, 80.000], mean observation: 3.158 [-1.503, 10.417], loss: 1.082902, mae: 5.034711, mean_q: 5.257059
 90926/100000: episode: 9279, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.233, mean reward: 0.423 [0.328, 0.574], mean action: 39.900 [2.000, 76.000], mean observation: 3.152 [-1.424, 10.617], loss: 0.737446, mae: 5.033615, mean_q: 5.258176
 90936/100000: episode: 9280, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.985, mean reward: 0.399 [0.340, 0.595], mean action: 48.400 [9.000, 85.000], mean observation: 3.165 [-1.156, 10.465], loss: 1.248216, mae: 5.036596, mean_q: 5.260969
 90946/100000: episode: 9281, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.603, mean reward: 0.360 [0.329, 0.505], mean action: 29.000 [1.000, 43.000], mean observation: 3.160 [-1.341, 10.423], loss: 1.242528, mae: 5.036891, mean_q: 5.261614
 90956/100000: episode: 9282, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.254, mean reward: 0.425 [0.425, 0.426], mean action: 51.700 [18.000, 97.000], mean observation: 3.148 [-1.430, 10.384], loss: 1.390049, mae: 5.037511, mean_q: 5.262104
 90966/100000: episode: 9283, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.577, mean reward: 0.458 [0.386, 0.497], mean action: 41.400 [21.000, 70.000], mean observation: 3.152 [-1.908, 10.366], loss: 1.366599, mae: 5.037427, mean_q: 5.264289
 90976/100000: episode: 9284, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.112, mean reward: 0.411 [0.411, 0.411], mean action: 39.300 [33.000, 81.000], mean observation: 3.142 [-1.215, 10.319], loss: 1.527580, mae: 5.037724, mean_q: 5.262860
 90986/100000: episode: 9285, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.093, mean reward: 0.409 [0.366, 0.459], mean action: 31.000 [6.000, 40.000], mean observation: 3.163 [-1.826, 10.452], loss: 1.263296, mae: 5.036138, mean_q: 5.261473
 90996/100000: episode: 9286, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.042, mean reward: 0.404 [0.331, 0.511], mean action: 34.900 [4.000, 96.000], mean observation: 3.153 [-1.117, 10.241], loss: 1.267752, mae: 5.036226, mean_q: 5.261571
 91006/100000: episode: 9287, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.172, mean reward: 0.417 [0.388, 0.460], mean action: 36.500 [24.000, 58.000], mean observation: 3.148 [-1.756, 10.185], loss: 1.260752, mae: 5.035940, mean_q: 5.263243
 91016/100000: episode: 9288, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.155, mean reward: 0.416 [0.295, 0.520], mean action: 41.800 [19.000, 96.000], mean observation: 3.136 [-1.464, 10.179], loss: 1.232761, mae: 5.036026, mean_q: 5.266213
 91026/100000: episode: 9289, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.394, mean reward: 0.439 [0.429, 0.476], mean action: 43.500 [35.000, 86.000], mean observation: 3.163 [-1.379, 10.449], loss: 0.977358, mae: 5.035306, mean_q: 5.267212
 91036/100000: episode: 9290, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.926, mean reward: 0.393 [0.346, 0.497], mean action: 50.000 [9.000, 85.000], mean observation: 3.144 [-1.805, 10.366], loss: 0.894628, mae: 5.035272, mean_q: 5.267682
 91046/100000: episode: 9291, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.062, mean reward: 0.406 [0.335, 0.457], mean action: 37.200 [2.000, 76.000], mean observation: 3.146 [-1.140, 10.353], loss: 1.349797, mae: 5.037493, mean_q: 5.269302
 91056/100000: episode: 9292, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.318, mean reward: 0.432 [0.388, 0.492], mean action: 46.800 [35.000, 80.000], mean observation: 3.153 [-1.091, 10.341], loss: 1.261817, mae: 5.037532, mean_q: 5.268773
 91057/100000: episode: 9293, duration: 0.022s, episode steps: 1, steps per second: 45, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 5.000 [5.000, 5.000], mean observation: 3.170 [-0.939, 10.660], loss: 0.787758, mae: 5.035957, mean_q: 5.267657
 91067/100000: episode: 9294, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.024, mean reward: 0.402 [0.363, 0.488], mean action: 34.500 [4.000, 101.000], mean observation: 3.151 [-1.377, 10.245], loss: 1.094493, mae: 5.037005, mean_q: 5.267093
 91077/100000: episode: 9295, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.292, mean reward: 0.429 [0.359, 0.515], mean action: 30.000 [1.000, 35.000], mean observation: 3.153 [-1.752, 10.438], loss: 1.161486, mae: 5.037468, mean_q: 5.264972
 91087/100000: episode: 9296, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.875, mean reward: 0.388 [0.315, 0.553], mean action: 48.000 [35.000, 78.000], mean observation: 3.148 [-1.288, 10.346], loss: 1.401586, mae: 5.038613, mean_q: 5.262540
 91097/100000: episode: 9297, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.008, mean reward: 0.401 [0.331, 0.470], mean action: 52.600 [22.000, 97.000], mean observation: 3.168 [-1.155, 10.336], loss: 1.297741, mae: 5.038461, mean_q: 5.259876
 91107/100000: episode: 9298, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.872, mean reward: 0.387 [0.297, 0.466], mean action: 37.700 [25.000, 86.000], mean observation: 3.151 [-1.639, 10.244], loss: 1.108625, mae: 5.037676, mean_q: 5.260673
 91117/100000: episode: 9299, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.263, mean reward: 0.426 [0.365, 0.552], mean action: 31.800 [4.000, 52.000], mean observation: 3.157 [-2.378, 10.398], loss: 1.243061, mae: 5.038300, mean_q: 5.263087
 91127/100000: episode: 9300, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.019, mean reward: 0.402 [0.401, 0.407], mean action: 46.900 [35.000, 98.000], mean observation: 3.155 [-1.710, 10.281], loss: 1.188490, mae: 5.037893, mean_q: 5.259246
 91137/100000: episode: 9301, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.528, mean reward: 0.353 [0.311, 0.420], mean action: 49.200 [35.000, 93.000], mean observation: 3.162 [-1.658, 10.284], loss: 1.153899, mae: 5.037801, mean_q: 5.258059
 91147/100000: episode: 9302, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.056, mean reward: 0.406 [0.331, 0.458], mean action: 38.400 [3.000, 87.000], mean observation: 3.144 [-1.295, 10.285], loss: 0.978997, mae: 5.036999, mean_q: 5.259762
 91157/100000: episode: 9303, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.742, mean reward: 0.374 [0.308, 0.465], mean action: 38.200 [19.000, 58.000], mean observation: 3.162 [-0.892, 10.359], loss: 0.958876, mae: 5.037426, mean_q: 5.262611
 91167/100000: episode: 9304, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.372, mean reward: 0.437 [0.408, 0.549], mean action: 45.600 [12.000, 101.000], mean observation: 3.153 [-1.598, 10.524], loss: 1.103481, mae: 5.038667, mean_q: 5.265152
 91177/100000: episode: 9305, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.824, mean reward: 0.382 [0.341, 0.462], mean action: 35.700 [28.000, 49.000], mean observation: 3.157 [-1.689, 10.445], loss: 1.198598, mae: 5.039211, mean_q: 5.265601
 91179/100000: episode: 9306, duration: 0.045s, episode steps: 2, steps per second: 44, episode reward: 10.495, mean reward: 5.247 [0.495, 10.000], mean action: 33.500 [21.000, 46.000], mean observation: 3.158 [-0.566, 10.255], loss: 1.534764, mae: 5.040676, mean_q: 5.266846
 91189/100000: episode: 9307, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.075, mean reward: 0.407 [0.281, 0.513], mean action: 43.300 [13.000, 94.000], mean observation: 3.156 [-1.891, 10.288], loss: 1.586433, mae: 5.040910, mean_q: 5.267026
 91199/100000: episode: 9308, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.570, mean reward: 0.457 [0.442, 0.509], mean action: 43.600 [34.000, 88.000], mean observation: 3.161 [-0.959, 10.340], loss: 1.590750, mae: 5.040466, mean_q: 5.266500
 91209/100000: episode: 9309, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 3.880, mean reward: 0.388 [0.334, 0.503], mean action: 45.600 [14.000, 96.000], mean observation: 3.160 [-1.282, 10.324], loss: 1.157514, mae: 5.038390, mean_q: 5.266033
 91219/100000: episode: 9310, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.203, mean reward: 0.420 [0.395, 0.493], mean action: 39.300 [7.000, 78.000], mean observation: 3.149 [-1.695, 10.196], loss: 1.556124, mae: 5.039649, mean_q: 5.266410
 91227/100000: episode: 9311, duration: 0.122s, episode steps: 8, steps per second: 65, episode reward: 12.679, mean reward: 1.585 [0.383, 10.000], mean action: 52.875 [35.000, 101.000], mean observation: 3.147 [-1.691, 10.253], loss: 1.160204, mae: 5.037779, mean_q: 5.266383
 91237/100000: episode: 9312, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.096, mean reward: 0.410 [0.374, 0.542], mean action: 39.800 [10.000, 99.000], mean observation: 3.159 [-1.834, 10.458], loss: 1.133314, mae: 5.037825, mean_q: 5.264246
 91247/100000: episode: 9313, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.786, mean reward: 0.479 [0.431, 0.557], mean action: 40.400 [2.000, 101.000], mean observation: 3.150 [-1.519, 10.259], loss: 1.198906, mae: 5.038542, mean_q: 5.257894
 91257/100000: episode: 9314, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.869, mean reward: 0.387 [0.370, 0.420], mean action: 43.800 [14.000, 99.000], mean observation: 3.147 [-1.385, 10.237], loss: 1.098847, mae: 5.038421, mean_q: 5.254229
 91267/100000: episode: 9315, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.765, mean reward: 0.377 [0.348, 0.409], mean action: 53.100 [34.000, 101.000], mean observation: 3.161 [-1.433, 10.255], loss: 1.089265, mae: 5.038500, mean_q: 5.250165
 91277/100000: episode: 9316, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.831, mean reward: 0.383 [0.293, 0.505], mean action: 32.000 [14.000, 44.000], mean observation: 3.155 [-1.338, 10.366], loss: 1.207079, mae: 5.039253, mean_q: 5.247813
 91287/100000: episode: 9317, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.459, mean reward: 0.446 [0.445, 0.457], mean action: 44.400 [19.000, 95.000], mean observation: 3.172 [-1.996, 10.271], loss: 1.362397, mae: 5.040152, mean_q: 5.247638
 91297/100000: episode: 9318, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.105, mean reward: 0.411 [0.400, 0.438], mean action: 51.900 [35.000, 83.000], mean observation: 3.169 [-1.176, 10.290], loss: 0.889808, mae: 5.038384, mean_q: 5.249933
 91307/100000: episode: 9319, duration: 0.209s, episode steps: 10, steps per second: 48, episode reward: 3.881, mean reward: 0.388 [0.314, 0.459], mean action: 35.400 [19.000, 70.000], mean observation: 3.163 [-1.192, 10.310], loss: 1.213309, mae: 5.039899, mean_q: 5.245416
 91317/100000: episode: 9320, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.203, mean reward: 0.420 [0.416, 0.443], mean action: 43.400 [17.000, 93.000], mean observation: 3.142 [-1.226, 10.231], loss: 1.428419, mae: 5.040953, mean_q: 5.239141
 91327/100000: episode: 9321, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 3.671, mean reward: 0.367 [0.287, 0.488], mean action: 41.600 [16.000, 101.000], mean observation: 3.154 [-1.012, 10.437], loss: 1.214316, mae: 5.039920, mean_q: 5.239017
 91337/100000: episode: 9322, duration: 0.220s, episode steps: 10, steps per second: 45, episode reward: 4.017, mean reward: 0.402 [0.328, 0.453], mean action: 26.900 [16.000, 87.000], mean observation: 3.150 [-2.094, 10.300], loss: 1.522152, mae: 5.040948, mean_q: 5.241310
 91347/100000: episode: 9323, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 3.924, mean reward: 0.392 [0.317, 0.536], mean action: 27.600 [16.000, 96.000], mean observation: 3.151 [-1.299, 10.532], loss: 1.249841, mae: 5.039864, mean_q: 5.243404
 91357/100000: episode: 9324, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.273, mean reward: 0.427 [0.345, 0.497], mean action: 39.200 [1.000, 75.000], mean observation: 3.156 [-1.347, 10.357], loss: 1.393228, mae: 5.040097, mean_q: 5.242215
 91367/100000: episode: 9325, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.797, mean reward: 0.380 [0.367, 0.465], mean action: 48.600 [11.000, 94.000], mean observation: 3.158 [-1.301, 10.294], loss: 1.089299, mae: 5.038743, mean_q: 5.241029
 91377/100000: episode: 9326, duration: 0.215s, episode steps: 10, steps per second: 47, episode reward: 4.366, mean reward: 0.437 [0.421, 0.513], mean action: 37.200 [14.000, 90.000], mean observation: 3.165 [-1.331, 10.301], loss: 1.245145, mae: 5.039468, mean_q: 5.240025
 91387/100000: episode: 9327, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.206, mean reward: 0.421 [0.386, 0.460], mean action: 50.700 [26.000, 99.000], mean observation: 3.158 [-1.136, 10.282], loss: 1.206702, mae: 5.039229, mean_q: 5.236158
 91397/100000: episode: 9328, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.785, mean reward: 0.379 [0.345, 0.478], mean action: 45.200 [35.000, 94.000], mean observation: 3.165 [-1.253, 10.284], loss: 1.284357, mae: 5.039636, mean_q: 5.233604
 91407/100000: episode: 9329, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.516, mean reward: 0.352 [0.332, 0.392], mean action: 45.000 [5.000, 101.000], mean observation: 3.160 [-0.928, 10.186], loss: 0.783680, mae: 5.037817, mean_q: 5.230007
 91417/100000: episode: 9330, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.024, mean reward: 0.402 [0.341, 0.531], mean action: 42.800 [9.000, 93.000], mean observation: 3.162 [-1.510, 10.301], loss: 1.098449, mae: 5.039428, mean_q: 5.226531
 91427/100000: episode: 9331, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.865, mean reward: 0.486 [0.413, 0.504], mean action: 32.000 [0.000, 87.000], mean observation: 3.145 [-1.189, 10.349], loss: 1.147623, mae: 5.040007, mean_q: 5.227437
 91437/100000: episode: 9332, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.827, mean reward: 0.383 [0.341, 0.431], mean action: 49.100 [21.000, 98.000], mean observation: 3.170 [-1.960, 10.316], loss: 1.347956, mae: 5.041058, mean_q: 5.229575
 91447/100000: episode: 9333, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.191, mean reward: 0.419 [0.407, 0.491], mean action: 31.600 [4.000, 98.000], mean observation: 3.160 [-1.079, 10.339], loss: 1.567202, mae: 5.041575, mean_q: 5.229550
 91457/100000: episode: 9334, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.839, mean reward: 0.384 [0.349, 0.495], mean action: 44.100 [4.000, 99.000], mean observation: 3.160 [-1.289, 10.390], loss: 1.303157, mae: 5.040475, mean_q: 5.230342
 91460/100000: episode: 9335, duration: 0.068s, episode steps: 3, steps per second: 44, episode reward: 10.891, mean reward: 3.630 [0.391, 10.000], mean action: 23.000 [4.000, 61.000], mean observation: 3.157 [-0.849, 10.221], loss: 1.351776, mae: 5.040387, mean_q: 5.230743
 91466/100000: episode: 9336, duration: 0.120s, episode steps: 6, steps per second: 50, episode reward: 12.087, mean reward: 2.015 [0.304, 10.000], mean action: 15.667 [4.000, 40.000], mean observation: 3.162 [-1.113, 10.676], loss: 1.032542, mae: 5.039038, mean_q: 5.231806
 91476/100000: episode: 9337, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.970, mean reward: 0.397 [0.336, 0.450], mean action: 32.400 [1.000, 88.000], mean observation: 3.157 [-1.016, 10.240], loss: 1.138528, mae: 5.039664, mean_q: 5.236802
 91485/100000: episode: 9338, duration: 0.156s, episode steps: 9, steps per second: 58, episode reward: 13.544, mean reward: 1.505 [0.419, 10.000], mean action: 35.667 [4.000, 87.000], mean observation: 3.159 [-1.287, 10.500], loss: 1.372928, mae: 5.040677, mean_q: 5.240800
 91495/100000: episode: 9339, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.676, mean reward: 0.368 [0.336, 0.382], mean action: 41.100 [4.000, 92.000], mean observation: 3.157 [-1.696, 10.295], loss: 1.005947, mae: 5.039439, mean_q: 5.243178
 91505/100000: episode: 9340, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.053, mean reward: 0.405 [0.359, 0.477], mean action: 28.800 [4.000, 95.000], mean observation: 3.152 [-1.290, 10.229], loss: 1.218907, mae: 5.040444, mean_q: 5.244543
 91515/100000: episode: 9341, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.146, mean reward: 0.415 [0.358, 0.460], mean action: 19.300 [4.000, 75.000], mean observation: 3.155 [-1.467, 10.280], loss: 1.227037, mae: 5.040722, mean_q: 5.246036
 91525/100000: episode: 9342, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.949, mean reward: 0.395 [0.312, 0.449], mean action: 27.300 [4.000, 72.000], mean observation: 3.151 [-1.395, 10.490], loss: 1.447463, mae: 5.041635, mean_q: 5.246689
 91535/100000: episode: 9343, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.474, mean reward: 0.447 [0.309, 0.585], mean action: 35.300 [4.000, 83.000], mean observation: 3.151 [-1.386, 10.199], loss: 1.105480, mae: 5.040107, mean_q: 5.244382
 91545/100000: episode: 9344, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.270, mean reward: 0.427 [0.350, 0.543], mean action: 34.000 [4.000, 97.000], mean observation: 3.157 [-2.121, 10.225], loss: 1.315545, mae: 5.040988, mean_q: 5.244756
 91555/100000: episode: 9345, duration: 0.220s, episode steps: 10, steps per second: 45, episode reward: 4.244, mean reward: 0.424 [0.322, 0.528], mean action: 5.400 [4.000, 18.000], mean observation: 3.153 [-1.899, 10.361], loss: 1.014953, mae: 5.040087, mean_q: 5.246066
 91565/100000: episode: 9346, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.498, mean reward: 0.450 [0.321, 0.523], mean action: 8.700 [4.000, 51.000], mean observation: 3.148 [-1.879, 10.252], loss: 1.240741, mae: 5.041057, mean_q: 5.247214
 91575/100000: episode: 9347, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.543, mean reward: 0.354 [0.307, 0.461], mean action: 34.900 [2.000, 85.000], mean observation: 3.158 [-1.087, 10.348], loss: 1.485811, mae: 5.042067, mean_q: 5.248688
 91585/100000: episode: 9348, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.089, mean reward: 0.409 [0.334, 0.509], mean action: 41.700 [4.000, 85.000], mean observation: 3.162 [-1.318, 10.286], loss: 0.936721, mae: 5.039653, mean_q: 5.250033
 91589/100000: episode: 9349, duration: 0.085s, episode steps: 4, steps per second: 47, episode reward: 11.371, mean reward: 2.843 [0.457, 10.000], mean action: 35.500 [4.000, 87.000], mean observation: 3.145 [-0.969, 10.286], loss: 1.135015, mae: 5.040649, mean_q: 5.251189
 91599/100000: episode: 9350, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.053, mean reward: 0.405 [0.359, 0.485], mean action: 43.700 [4.000, 91.000], mean observation: 3.166 [-1.623, 10.435], loss: 1.240580, mae: 5.041255, mean_q: 5.252280
 91609/100000: episode: 9351, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.301, mean reward: 0.430 [0.341, 0.547], mean action: 18.100 [4.000, 41.000], mean observation: 3.157 [-2.052, 10.299], loss: 1.060969, mae: 5.040567, mean_q: 5.254011
 91619/100000: episode: 9352, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.069, mean reward: 0.407 [0.301, 0.529], mean action: 31.500 [4.000, 94.000], mean observation: 3.155 [-1.377, 10.315], loss: 1.311104, mae: 5.041734, mean_q: 5.255557
 91629/100000: episode: 9353, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 3.854, mean reward: 0.385 [0.284, 0.481], mean action: 13.200 [4.000, 88.000], mean observation: 3.152 [-0.934, 10.243], loss: 1.524000, mae: 5.042666, mean_q: 5.257150
 91639/100000: episode: 9354, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.068, mean reward: 0.407 [0.391, 0.441], mean action: 15.400 [3.000, 63.000], mean observation: 3.158 [-1.101, 10.382], loss: 1.374771, mae: 5.041868, mean_q: 5.260253
 91649/100000: episode: 9355, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.439, mean reward: 0.444 [0.337, 0.537], mean action: 33.800 [1.000, 85.000], mean observation: 3.158 [-1.245, 10.470], loss: 1.094734, mae: 5.040824, mean_q: 5.265246
 91659/100000: episode: 9356, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.855, mean reward: 0.386 [0.315, 0.497], mean action: 45.300 [4.000, 92.000], mean observation: 3.153 [-1.195, 10.241], loss: 1.203998, mae: 5.041418, mean_q: 5.271279
 91669/100000: episode: 9357, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.902, mean reward: 0.390 [0.313, 0.493], mean action: 24.500 [4.000, 76.000], mean observation: 3.148 [-1.784, 10.307], loss: 1.110536, mae: 5.041268, mean_q: 5.275314
 91679/100000: episode: 9358, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.285, mean reward: 0.428 [0.376, 0.481], mean action: 31.700 [4.000, 97.000], mean observation: 3.163 [-1.318, 10.381], loss: 1.160481, mae: 5.041661, mean_q: 5.277644
 91689/100000: episode: 9359, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.265, mean reward: 0.426 [0.332, 0.508], mean action: 43.000 [4.000, 101.000], mean observation: 3.154 [-1.433, 10.312], loss: 1.212008, mae: 5.042063, mean_q: 5.279843
 91699/100000: episode: 9360, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.074, mean reward: 0.407 [0.339, 0.540], mean action: 27.500 [4.000, 100.000], mean observation: 3.164 [-1.145, 10.308], loss: 1.408918, mae: 5.042829, mean_q: 5.281448
 91709/100000: episode: 9361, duration: 0.221s, episode steps: 10, steps per second: 45, episode reward: 4.271, mean reward: 0.427 [0.413, 0.465], mean action: 16.200 [4.000, 96.000], mean observation: 3.167 [-1.213, 10.267], loss: 1.264857, mae: 5.042222, mean_q: 5.282454
 91719/100000: episode: 9362, duration: 0.232s, episode steps: 10, steps per second: 43, episode reward: 3.959, mean reward: 0.396 [0.320, 0.450], mean action: 12.800 [4.000, 57.000], mean observation: 3.155 [-1.519, 10.366], loss: 1.241842, mae: 5.042635, mean_q: 5.279994
 91729/100000: episode: 9363, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.111, mean reward: 0.411 [0.393, 0.527], mean action: 26.400 [4.000, 100.000], mean observation: 3.165 [-1.313, 10.352], loss: 1.229633, mae: 5.042623, mean_q: 5.276400
 91739/100000: episode: 9364, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 4.239, mean reward: 0.424 [0.378, 0.451], mean action: 21.500 [0.000, 91.000], mean observation: 3.156 [-1.678, 10.330], loss: 1.022757, mae: 5.042164, mean_q: 5.274804
 91749/100000: episode: 9365, duration: 0.215s, episode steps: 10, steps per second: 47, episode reward: 4.072, mean reward: 0.407 [0.308, 0.474], mean action: 12.600 [4.000, 69.000], mean observation: 3.164 [-1.175, 10.533], loss: 1.028628, mae: 5.042613, mean_q: 5.275063
 91759/100000: episode: 9366, duration: 0.227s, episode steps: 10, steps per second: 44, episode reward: 4.145, mean reward: 0.415 [0.335, 0.577], mean action: 10.100 [2.000, 52.000], mean observation: 3.146 [-1.135, 10.367], loss: 1.365434, mae: 5.044263, mean_q: 5.276327
 91769/100000: episode: 9367, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.042, mean reward: 0.404 [0.334, 0.510], mean action: 40.000 [4.000, 100.000], mean observation: 3.151 [-1.418, 10.465], loss: 1.540407, mae: 5.044967, mean_q: 5.277557
 91779/100000: episode: 9368, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.148, mean reward: 0.415 [0.334, 0.499], mean action: 31.900 [4.000, 101.000], mean observation: 3.151 [-1.140, 10.244], loss: 1.327967, mae: 5.044195, mean_q: 5.275773
 91789/100000: episode: 9369, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.367, mean reward: 0.437 [0.362, 0.498], mean action: 23.600 [4.000, 71.000], mean observation: 3.154 [-1.765, 10.333], loss: 1.473837, mae: 5.044702, mean_q: 5.275819
 91799/100000: episode: 9370, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.994, mean reward: 0.399 [0.322, 0.458], mean action: 34.500 [4.000, 98.000], mean observation: 3.152 [-1.112, 10.321], loss: 1.287663, mae: 5.043817, mean_q: 5.274146
 91809/100000: episode: 9371, duration: 0.222s, episode steps: 10, steps per second: 45, episode reward: 4.473, mean reward: 0.447 [0.376, 0.494], mean action: 13.000 [4.000, 85.000], mean observation: 3.157 [-1.311, 10.317], loss: 1.123983, mae: 5.043444, mean_q: 5.271991
 91819/100000: episode: 9372, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 4.153, mean reward: 0.415 [0.334, 0.536], mean action: 21.900 [4.000, 52.000], mean observation: 3.174 [-1.559, 10.371], loss: 1.224849, mae: 5.043959, mean_q: 5.271619
 91829/100000: episode: 9373, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.736, mean reward: 0.374 [0.325, 0.492], mean action: 33.500 [4.000, 78.000], mean observation: 3.164 [-1.489, 10.199], loss: 1.050504, mae: 5.043792, mean_q: 5.273204
 91839/100000: episode: 9374, duration: 0.220s, episode steps: 10, steps per second: 45, episode reward: 4.203, mean reward: 0.420 [0.358, 0.519], mean action: 21.800 [4.000, 90.000], mean observation: 3.147 [-1.275, 10.488], loss: 1.226674, mae: 5.044572, mean_q: 5.273502
 91849/100000: episode: 9375, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.937, mean reward: 0.394 [0.359, 0.424], mean action: 24.200 [4.000, 96.000], mean observation: 3.157 [-1.479, 10.327], loss: 1.033268, mae: 5.044055, mean_q: 5.268760
 91859/100000: episode: 9376, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.142, mean reward: 0.414 [0.336, 0.560], mean action: 34.800 [4.000, 99.000], mean observation: 3.153 [-1.812, 10.261], loss: 1.043158, mae: 5.044402, mean_q: 5.268276
 91869/100000: episode: 9377, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.889, mean reward: 0.389 [0.330, 0.524], mean action: 40.300 [4.000, 98.000], mean observation: 3.148 [-1.668, 10.436], loss: 1.454567, mae: 5.046269, mean_q: 5.269930
 91879/100000: episode: 9378, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.028, mean reward: 0.403 [0.330, 0.477], mean action: 28.700 [4.000, 89.000], mean observation: 3.146 [-2.056, 10.324], loss: 0.919243, mae: 5.044194, mean_q: 5.272750
 91889/100000: episode: 9379, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.582, mean reward: 0.458 [0.411, 0.545], mean action: 26.800 [4.000, 83.000], mean observation: 3.157 [-1.469, 10.322], loss: 1.098206, mae: 5.045062, mean_q: 5.277404
 91899/100000: episode: 9380, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.976, mean reward: 0.398 [0.367, 0.456], mean action: 38.500 [4.000, 94.000], mean observation: 3.160 [-1.329, 10.290], loss: 1.266195, mae: 5.045991, mean_q: 5.280699
 91909/100000: episode: 9381, duration: 0.222s, episode steps: 10, steps per second: 45, episode reward: 4.152, mean reward: 0.415 [0.368, 0.523], mean action: 16.400 [4.000, 93.000], mean observation: 3.168 [-1.600, 10.565], loss: 1.517682, mae: 5.047403, mean_q: 5.282930
 91913/100000: episode: 9382, duration: 0.092s, episode steps: 4, steps per second: 43, episode reward: 11.148, mean reward: 2.787 [0.359, 10.000], mean action: 31.750 [4.000, 88.000], mean observation: 3.151 [-0.632, 10.182], loss: 1.157699, mae: 5.045941, mean_q: 5.281448
 91923/100000: episode: 9383, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.942, mean reward: 0.394 [0.293, 0.456], mean action: 36.500 [4.000, 95.000], mean observation: 3.160 [-1.731, 10.208], loss: 1.760998, mae: 5.048197, mean_q: 5.277729
 91933/100000: episode: 9384, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.982, mean reward: 0.398 [0.297, 0.475], mean action: 26.600 [4.000, 87.000], mean observation: 3.173 [-1.284, 10.376], loss: 1.154053, mae: 5.045846, mean_q: 5.279036
 91943/100000: episode: 9385, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.586, mean reward: 0.459 [0.433, 0.526], mean action: 27.600 [4.000, 83.000], mean observation: 3.148 [-1.271, 10.317], loss: 1.491693, mae: 5.047076, mean_q: 5.280738
 91953/100000: episode: 9386, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.549, mean reward: 0.455 [0.331, 0.541], mean action: 22.800 [4.000, 75.000], mean observation: 3.147 [-1.817, 10.188], loss: 1.438281, mae: 5.047137, mean_q: 5.282682
 91963/100000: episode: 9387, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 4.569, mean reward: 0.457 [0.356, 0.545], mean action: 22.400 [4.000, 57.000], mean observation: 3.152 [-2.293, 10.373], loss: 0.990169, mae: 5.045233, mean_q: 5.282008
 91973/100000: episode: 9388, duration: 0.215s, episode steps: 10, steps per second: 46, episode reward: 4.166, mean reward: 0.417 [0.334, 0.507], mean action: 21.900 [4.000, 91.000], mean observation: 3.149 [-1.295, 10.318], loss: 1.312502, mae: 5.046836, mean_q: 5.280890
 91983/100000: episode: 9389, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 4.152, mean reward: 0.415 [0.349, 0.488], mean action: 18.300 [4.000, 72.000], mean observation: 3.153 [-1.948, 10.396], loss: 1.324395, mae: 5.047054, mean_q: 5.279117
 91993/100000: episode: 9390, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.052, mean reward: 0.405 [0.344, 0.463], mean action: 36.000 [4.000, 98.000], mean observation: 3.154 [-1.891, 10.262], loss: 1.257810, mae: 5.046789, mean_q: 5.280282
 92003/100000: episode: 9391, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.421, mean reward: 0.442 [0.314, 0.547], mean action: 21.000 [4.000, 90.000], mean observation: 3.160 [-1.172, 10.462], loss: 1.258449, mae: 5.046853, mean_q: 5.281985
 92013/100000: episode: 9392, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.239, mean reward: 0.424 [0.277, 0.538], mean action: 25.400 [4.000, 84.000], mean observation: 3.152 [-2.086, 10.475], loss: 0.871153, mae: 5.045705, mean_q: 5.283299
 92023/100000: episode: 9393, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.629, mean reward: 0.363 [0.319, 0.447], mean action: 27.600 [4.000, 96.000], mean observation: 3.152 [-1.377, 10.309], loss: 1.278901, mae: 5.047826, mean_q: 5.284373
 92033/100000: episode: 9394, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.934, mean reward: 0.393 [0.330, 0.438], mean action: 33.200 [4.000, 77.000], mean observation: 3.157 [-2.007, 10.262], loss: 1.552093, mae: 5.049124, mean_q: 5.285395
 92038/100000: episode: 9395, duration: 0.088s, episode steps: 5, steps per second: 57, episode reward: 11.515, mean reward: 2.303 [0.341, 10.000], mean action: 34.800 [4.000, 69.000], mean observation: 3.138 [-1.916, 10.186], loss: 1.337723, mae: 5.048038, mean_q: 5.285876
 92048/100000: episode: 9396, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.160, mean reward: 0.416 [0.320, 0.509], mean action: 19.200 [4.000, 95.000], mean observation: 3.160 [-1.482, 10.276], loss: 1.132798, mae: 5.047431, mean_q: 5.284702
 92058/100000: episode: 9397, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 4.299, mean reward: 0.430 [0.346, 0.536], mean action: 30.700 [2.000, 98.000], mean observation: 3.156 [-1.826, 10.358], loss: 1.080770, mae: 5.047595, mean_q: 5.284255
 92068/100000: episode: 9398, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.648, mean reward: 0.465 [0.395, 0.585], mean action: 29.600 [4.000, 77.000], mean observation: 3.162 [-1.550, 10.270], loss: 1.421589, mae: 5.049302, mean_q: 5.280217
 92078/100000: episode: 9399, duration: 0.222s, episode steps: 10, steps per second: 45, episode reward: 4.389, mean reward: 0.439 [0.342, 0.532], mean action: 12.000 [0.000, 79.000], mean observation: 3.158 [-1.624, 10.306], loss: 1.260880, mae: 5.048693, mean_q: 5.279292
 92088/100000: episode: 9400, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.205, mean reward: 0.421 [0.332, 0.511], mean action: 30.900 [2.000, 96.000], mean observation: 3.161 [-1.415, 10.346], loss: 1.070486, mae: 5.048181, mean_q: 5.284544
 92098/100000: episode: 9401, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.101, mean reward: 0.410 [0.340, 0.518], mean action: 33.200 [4.000, 101.000], mean observation: 3.156 [-1.589, 10.372], loss: 1.190714, mae: 5.048749, mean_q: 5.288708
 92108/100000: episode: 9402, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.070, mean reward: 0.407 [0.352, 0.476], mean action: 17.200 [4.000, 66.000], mean observation: 3.159 [-1.107, 10.433], loss: 1.092037, mae: 5.048615, mean_q: 5.287713
 92118/100000: episode: 9403, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.195, mean reward: 0.419 [0.309, 0.555], mean action: 14.300 [4.000, 46.000], mean observation: 3.164 [-1.815, 10.353], loss: 1.132050, mae: 5.049372, mean_q: 5.284966
 92128/100000: episode: 9404, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.985, mean reward: 0.399 [0.305, 0.485], mean action: 7.700 [3.000, 42.000], mean observation: 3.158 [-1.335, 10.330], loss: 1.029727, mae: 5.049313, mean_q: 5.282839
 92138/100000: episode: 9405, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 3.759, mean reward: 0.376 [0.320, 0.443], mean action: 24.200 [4.000, 99.000], mean observation: 3.157 [-2.046, 10.393], loss: 0.992787, mae: 5.049503, mean_q: 5.283724
 92148/100000: episode: 9406, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 3.842, mean reward: 0.384 [0.332, 0.444], mean action: 29.800 [4.000, 90.000], mean observation: 3.154 [-1.354, 10.348], loss: 1.074686, mae: 5.050257, mean_q: 5.286082
 92158/100000: episode: 9407, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.322, mean reward: 0.432 [0.342, 0.503], mean action: 37.800 [4.000, 93.000], mean observation: 3.153 [-1.744, 10.377], loss: 0.995435, mae: 5.050684, mean_q: 5.283269
 92168/100000: episode: 9408, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.770, mean reward: 0.377 [0.328, 0.429], mean action: 18.400 [4.000, 66.000], mean observation: 3.150 [-1.381, 10.232], loss: 1.441223, mae: 5.052570, mean_q: 5.277347
 92178/100000: episode: 9409, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.873, mean reward: 0.487 [0.392, 0.594], mean action: 43.000 [4.000, 100.000], mean observation: 3.169 [-0.894, 10.509], loss: 1.482183, mae: 5.052541, mean_q: 5.276229
 92188/100000: episode: 9410, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.214, mean reward: 0.421 [0.390, 0.545], mean action: 26.100 [4.000, 85.000], mean observation: 3.152 [-1.619, 10.283], loss: 1.121960, mae: 5.051305, mean_q: 5.274724
 92198/100000: episode: 9411, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.229, mean reward: 0.423 [0.317, 0.525], mean action: 37.200 [4.000, 99.000], mean observation: 3.159 [-1.511, 10.562], loss: 1.281875, mae: 5.052062, mean_q: 5.274585
 92208/100000: episode: 9412, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.103, mean reward: 0.410 [0.343, 0.500], mean action: 21.100 [4.000, 96.000], mean observation: 3.151 [-2.163, 10.320], loss: 1.385960, mae: 5.052640, mean_q: 5.276361
 92218/100000: episode: 9413, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.032, mean reward: 0.403 [0.338, 0.515], mean action: 35.700 [1.000, 97.000], mean observation: 3.152 [-1.479, 10.160], loss: 1.080781, mae: 5.051840, mean_q: 5.278730
 92228/100000: episode: 9414, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.782, mean reward: 0.378 [0.295, 0.489], mean action: 37.200 [4.000, 96.000], mean observation: 3.163 [-1.980, 10.386], loss: 0.817853, mae: 5.051073, mean_q: 5.280385
 92238/100000: episode: 9415, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.172, mean reward: 0.417 [0.350, 0.538], mean action: 20.800 [3.000, 71.000], mean observation: 3.154 [-1.501, 10.335], loss: 1.375155, mae: 5.053463, mean_q: 5.280227
 92248/100000: episode: 9416, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.402, mean reward: 0.440 [0.341, 0.579], mean action: 17.700 [4.000, 86.000], mean observation: 3.148 [-1.813, 10.338], loss: 0.995099, mae: 5.052391, mean_q: 5.276637
 92258/100000: episode: 9417, duration: 0.213s, episode steps: 10, steps per second: 47, episode reward: 4.179, mean reward: 0.418 [0.328, 0.493], mean action: 8.100 [4.000, 45.000], mean observation: 3.166 [-1.833, 10.431], loss: 1.525983, mae: 5.054706, mean_q: 5.273271
 92268/100000: episode: 9418, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.759, mean reward: 0.376 [0.297, 0.454], mean action: 49.200 [4.000, 101.000], mean observation: 3.147 [-1.869, 10.362], loss: 1.638430, mae: 5.055175, mean_q: 5.269520
 92278/100000: episode: 9419, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.038, mean reward: 0.404 [0.337, 0.441], mean action: 29.200 [4.000, 92.000], mean observation: 3.154 [-1.761, 10.234], loss: 1.020366, mae: 5.052620, mean_q: 5.268739
 92288/100000: episode: 9420, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.140, mean reward: 0.414 [0.341, 0.473], mean action: 28.100 [4.000, 101.000], mean observation: 3.157 [-1.580, 10.233], loss: 1.193205, mae: 5.053615, mean_q: 5.270888
 92298/100000: episode: 9421, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.158, mean reward: 0.416 [0.366, 0.566], mean action: 29.400 [4.000, 93.000], mean observation: 3.154 [-1.380, 10.426], loss: 1.132966, mae: 5.053259, mean_q: 5.273525
 92308/100000: episode: 9422, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.408, mean reward: 0.441 [0.394, 0.546], mean action: 21.300 [4.000, 67.000], mean observation: 3.164 [-1.768, 10.459], loss: 1.097739, mae: 5.053469, mean_q: 5.275675
 92318/100000: episode: 9423, duration: 0.215s, episode steps: 10, steps per second: 47, episode reward: 4.115, mean reward: 0.411 [0.352, 0.481], mean action: 23.800 [2.000, 94.000], mean observation: 3.157 [-1.066, 10.326], loss: 1.239792, mae: 5.054184, mean_q: 5.276219
 92328/100000: episode: 9424, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.744, mean reward: 0.474 [0.378, 0.531], mean action: 23.300 [2.000, 92.000], mean observation: 3.159 [-1.446, 10.583], loss: 1.302737, mae: 5.054641, mean_q: 5.270530
 92338/100000: episode: 9425, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.227, mean reward: 0.423 [0.304, 0.585], mean action: 22.000 [4.000, 72.000], mean observation: 3.164 [-1.471, 10.452], loss: 1.310588, mae: 5.054944, mean_q: 5.267591
 92348/100000: episode: 9426, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.574, mean reward: 0.357 [0.303, 0.424], mean action: 37.800 [8.000, 97.000], mean observation: 3.160 [-2.077, 10.277], loss: 1.129113, mae: 5.054337, mean_q: 5.268400
 92358/100000: episode: 9427, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 3.983, mean reward: 0.398 [0.335, 0.510], mean action: 37.100 [7.000, 101.000], mean observation: 3.148 [-1.319, 10.520], loss: 1.585361, mae: 5.056052, mean_q: 5.269164
 92368/100000: episode: 9428, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.737, mean reward: 0.374 [0.333, 0.419], mean action: 58.000 [20.000, 99.000], mean observation: 3.161 [-1.798, 10.262], loss: 1.324412, mae: 5.054977, mean_q: 5.271033
 92378/100000: episode: 9429, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.234, mean reward: 0.423 [0.335, 0.475], mean action: 34.900 [11.000, 92.000], mean observation: 3.152 [-1.411, 10.565], loss: 1.441387, mae: 5.055423, mean_q: 5.273456
 92388/100000: episode: 9430, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.223, mean reward: 0.422 [0.337, 0.502], mean action: 23.100 [11.000, 69.000], mean observation: 3.157 [-2.116, 10.447], loss: 1.021826, mae: 5.054317, mean_q: 5.274868
 92398/100000: episode: 9431, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.274, mean reward: 0.427 [0.382, 0.565], mean action: 41.000 [20.000, 100.000], mean observation: 3.170 [-1.656, 10.430], loss: 1.084633, mae: 5.054629, mean_q: 5.276954
 92408/100000: episode: 9432, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 3.980, mean reward: 0.398 [0.368, 0.456], mean action: 32.300 [20.000, 90.000], mean observation: 3.162 [-1.446, 10.321], loss: 1.429233, mae: 5.055973, mean_q: 5.278461
 92418/100000: episode: 9433, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.594, mean reward: 0.459 [0.357, 0.574], mean action: 48.300 [20.000, 95.000], mean observation: 3.156 [-0.947, 10.269], loss: 1.159656, mae: 5.054845, mean_q: 5.279962
 92424/100000: episode: 9434, duration: 0.117s, episode steps: 6, steps per second: 51, episode reward: 12.125, mean reward: 2.021 [0.417, 10.000], mean action: 35.833 [1.000, 61.000], mean observation: 3.162 [-1.007, 10.560], loss: 1.585678, mae: 5.056653, mean_q: 5.281205
 92434/100000: episode: 9435, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.144, mean reward: 0.414 [0.385, 0.449], mean action: 26.400 [2.000, 95.000], mean observation: 3.145 [-1.582, 10.317], loss: 1.438027, mae: 5.055650, mean_q: 5.281805
 92444/100000: episode: 9436, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.286, mean reward: 0.429 [0.399, 0.479], mean action: 41.100 [9.000, 86.000], mean observation: 3.146 [-1.789, 10.425], loss: 1.046074, mae: 5.054080, mean_q: 5.282197
 92454/100000: episode: 9437, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.348, mean reward: 0.435 [0.395, 0.485], mean action: 56.000 [0.000, 100.000], mean observation: 3.164 [-1.484, 10.365], loss: 1.133130, mae: 5.054301, mean_q: 5.278425
 92464/100000: episode: 9438, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.824, mean reward: 0.382 [0.324, 0.468], mean action: 41.600 [4.000, 88.000], mean observation: 3.154 [-1.809, 10.417], loss: 1.053613, mae: 5.054743, mean_q: 5.278924
 92474/100000: episode: 9439, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 3.994, mean reward: 0.399 [0.353, 0.548], mean action: 36.300 [11.000, 79.000], mean observation: 3.155 [-1.869, 10.355], loss: 1.527136, mae: 5.056636, mean_q: 5.276362
 92484/100000: episode: 9440, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.137, mean reward: 0.414 [0.371, 0.462], mean action: 32.100 [12.000, 89.000], mean observation: 3.159 [-1.927, 10.327], loss: 1.586208, mae: 5.056398, mean_q: 5.269591
 92494/100000: episode: 9441, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.839, mean reward: 0.384 [0.297, 0.498], mean action: 30.400 [5.000, 95.000], mean observation: 3.157 [-1.184, 10.408], loss: 1.320437, mae: 5.055192, mean_q: 5.272550
 92504/100000: episode: 9442, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 13.591, mean reward: 1.359 [0.334, 10.000], mean action: 22.800 [0.000, 72.000], mean observation: 3.158 [-1.182, 10.223], loss: 1.394207, mae: 5.055318, mean_q: 5.274692
 92514/100000: episode: 9443, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.906, mean reward: 0.391 [0.315, 0.446], mean action: 31.500 [9.000, 95.000], mean observation: 3.146 [-1.515, 10.264], loss: 1.410723, mae: 5.054970, mean_q: 5.276639
 92524/100000: episode: 9444, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.501, mean reward: 0.450 [0.383, 0.525], mean action: 32.200 [13.000, 75.000], mean observation: 3.155 [-1.066, 10.351], loss: 1.536195, mae: 5.055193, mean_q: 5.277308
 92534/100000: episode: 9445, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.237, mean reward: 0.424 [0.386, 0.520], mean action: 33.700 [8.000, 100.000], mean observation: 3.142 [-1.510, 10.288], loss: 1.230386, mae: 5.053733, mean_q: 5.273871
 92544/100000: episode: 9446, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.185, mean reward: 0.419 [0.387, 0.491], mean action: 45.400 [17.000, 101.000], mean observation: 3.162 [-1.469, 10.479], loss: 1.127490, mae: 5.053536, mean_q: 5.273757
 92554/100000: episode: 9447, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.149, mean reward: 0.415 [0.323, 0.470], mean action: 38.900 [17.000, 88.000], mean observation: 3.154 [-1.883, 10.261], loss: 1.379855, mae: 5.054741, mean_q: 5.275472
 92564/100000: episode: 9448, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.107, mean reward: 0.411 [0.326, 0.495], mean action: 24.200 [0.000, 80.000], mean observation: 3.150 [-2.632, 10.195], loss: 1.344389, mae: 5.054445, mean_q: 5.276992
 92574/100000: episode: 9449, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.160, mean reward: 0.416 [0.309, 0.483], mean action: 47.400 [9.000, 99.000], mean observation: 3.150 [-1.804, 10.299], loss: 1.442537, mae: 5.054715, mean_q: 5.278178
 92584/100000: episode: 9450, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.959, mean reward: 0.396 [0.345, 0.556], mean action: 23.400 [0.000, 68.000], mean observation: 3.160 [-1.838, 10.395], loss: 1.421106, mae: 5.054699, mean_q: 5.278732
 92594/100000: episode: 9451, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.222, mean reward: 0.422 [0.373, 0.469], mean action: 34.400 [3.000, 97.000], mean observation: 3.151 [-1.462, 10.246], loss: 1.193097, mae: 5.053599, mean_q: 5.275647
 92604/100000: episode: 9452, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.319, mean reward: 0.432 [0.420, 0.516], mean action: 29.200 [13.000, 91.000], mean observation: 3.161 [-1.085, 10.229], loss: 1.201324, mae: 5.053433, mean_q: 5.275676
 92614/100000: episode: 9453, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.309, mean reward: 0.431 [0.332, 0.501], mean action: 54.700 [20.000, 97.000], mean observation: 3.146 [-2.021, 10.200], loss: 1.325839, mae: 5.053883, mean_q: 5.272590
 92624/100000: episode: 9454, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.082, mean reward: 0.408 [0.344, 0.508], mean action: 31.400 [20.000, 73.000], mean observation: 3.164 [-1.463, 10.457], loss: 1.356337, mae: 5.053839, mean_q: 5.266903
 92634/100000: episode: 9455, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.545, mean reward: 0.355 [0.273, 0.467], mean action: 24.000 [0.000, 56.000], mean observation: 3.169 [-1.120, 10.285], loss: 1.289716, mae: 5.053377, mean_q: 5.264678
 92644/100000: episode: 9456, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.982, mean reward: 0.398 [0.335, 0.439], mean action: 50.200 [9.000, 82.000], mean observation: 3.158 [-1.395, 10.338], loss: 1.352255, mae: 5.053537, mean_q: 5.267486
 92654/100000: episode: 9457, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 3.816, mean reward: 0.382 [0.251, 0.466], mean action: 51.000 [9.000, 95.000], mean observation: 3.150 [-1.258, 10.325], loss: 1.088048, mae: 5.052837, mean_q: 5.271876
 92664/100000: episode: 9458, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.269, mean reward: 0.427 [0.363, 0.518], mean action: 63.100 [0.000, 95.000], mean observation: 3.153 [-0.956, 10.255], loss: 1.049266, mae: 5.052888, mean_q: 5.273528
 92674/100000: episode: 9459, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.595, mean reward: 0.459 [0.357, 0.524], mean action: 56.100 [1.000, 93.000], mean observation: 3.157 [-0.950, 10.271], loss: 1.191014, mae: 5.053641, mean_q: 5.276011
 92684/100000: episode: 9460, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.099, mean reward: 0.410 [0.353, 0.575], mean action: 49.500 [0.000, 68.000], mean observation: 3.156 [-1.612, 10.270], loss: 1.210583, mae: 5.053768, mean_q: 5.279105
 92694/100000: episode: 9461, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.398, mean reward: 0.340 [0.278, 0.405], mean action: 62.100 [22.000, 96.000], mean observation: 3.162 [-1.226, 10.333], loss: 1.467169, mae: 5.054716, mean_q: 5.281874
 92704/100000: episode: 9462, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 4.319, mean reward: 0.432 [0.373, 0.512], mean action: 53.400 [12.000, 97.000], mean observation: 3.165 [-1.968, 10.301], loss: 1.230360, mae: 5.054090, mean_q: 5.283307
 92714/100000: episode: 9463, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.701, mean reward: 0.370 [0.348, 0.440], mean action: 59.200 [11.000, 97.000], mean observation: 3.158 [-0.914, 10.330], loss: 0.947000, mae: 5.052999, mean_q: 5.284383
 92724/100000: episode: 9464, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.916, mean reward: 0.392 [0.303, 0.470], mean action: 57.600 [20.000, 67.000], mean observation: 3.157 [-1.192, 10.388], loss: 1.061736, mae: 5.053882, mean_q: 5.284921
 92734/100000: episode: 9465, duration: 0.120s, episode steps: 10, steps per second: 84, episode reward: 3.795, mean reward: 0.380 [0.320, 0.467], mean action: 63.300 [38.000, 67.000], mean observation: 3.147 [-1.446, 10.199], loss: 1.210371, mae: 5.054310, mean_q: 5.285948
 92744/100000: episode: 9466, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.439, mean reward: 0.444 [0.357, 0.577], mean action: 54.900 [6.000, 69.000], mean observation: 3.148 [-1.394, 10.279], loss: 1.327812, mae: 5.055040, mean_q: 5.288182
 92754/100000: episode: 9467, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.112, mean reward: 0.411 [0.287, 0.502], mean action: 60.500 [10.000, 93.000], mean observation: 3.173 [-1.325, 10.528], loss: 1.262023, mae: 5.055049, mean_q: 5.290175
 92764/100000: episode: 9468, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.202, mean reward: 0.420 [0.403, 0.464], mean action: 50.500 [21.000, 76.000], mean observation: 3.160 [-1.367, 10.344], loss: 1.025148, mae: 5.054382, mean_q: 5.287076
 92774/100000: episode: 9469, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 3.896, mean reward: 0.390 [0.364, 0.419], mean action: 55.300 [4.000, 87.000], mean observation: 3.179 [-1.409, 10.340], loss: 1.190268, mae: 5.055422, mean_q: 5.285407
 92784/100000: episode: 9470, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.880, mean reward: 0.388 [0.293, 0.574], mean action: 51.300 [8.000, 67.000], mean observation: 3.163 [-1.200, 10.347], loss: 0.951658, mae: 5.054829, mean_q: 5.286065
 92794/100000: episode: 9471, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.687, mean reward: 0.369 [0.307, 0.457], mean action: 65.400 [4.000, 93.000], mean observation: 3.149 [-1.172, 10.292], loss: 1.164156, mae: 5.056024, mean_q: 5.284448
 92804/100000: episode: 9472, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.387, mean reward: 0.439 [0.366, 0.571], mean action: 72.200 [50.000, 93.000], mean observation: 3.174 [-1.271, 10.424], loss: 1.329857, mae: 5.056914, mean_q: 5.282105
 92814/100000: episode: 9473, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.734, mean reward: 0.473 [0.455, 0.546], mean action: 64.200 [20.000, 100.000], mean observation: 3.164 [-1.405, 10.289], loss: 0.989151, mae: 5.055958, mean_q: 5.282046
 92824/100000: episode: 9474, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.774, mean reward: 0.377 [0.326, 0.487], mean action: 46.700 [5.000, 67.000], mean observation: 3.147 [-1.586, 10.282], loss: 1.065018, mae: 5.056512, mean_q: 5.279129
 92834/100000: episode: 9475, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.407, mean reward: 0.441 [0.431, 0.478], mean action: 52.600 [19.000, 78.000], mean observation: 3.171 [-0.960, 10.283], loss: 1.221910, mae: 5.057522, mean_q: 5.276900
 92844/100000: episode: 9476, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.048, mean reward: 0.405 [0.348, 0.433], mean action: 58.400 [14.000, 90.000], mean observation: 3.159 [-2.411, 10.309], loss: 1.365971, mae: 5.057889, mean_q: 5.272156
 92854/100000: episode: 9477, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.277, mean reward: 0.428 [0.333, 0.505], mean action: 63.200 [11.000, 88.000], mean observation: 3.151 [-1.831, 10.223], loss: 1.305429, mae: 5.057405, mean_q: 5.264682
 92864/100000: episode: 9478, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.664, mean reward: 0.466 [0.427, 0.476], mean action: 69.400 [43.000, 98.000], mean observation: 3.169 [-1.647, 10.334], loss: 1.215836, mae: 5.056684, mean_q: 5.263053
 92874/100000: episode: 9479, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 4.456, mean reward: 0.446 [0.349, 0.474], mean action: 51.000 [6.000, 67.000], mean observation: 3.166 [-1.319, 10.262], loss: 1.195380, mae: 5.056729, mean_q: 5.263786
 92884/100000: episode: 9480, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.001, mean reward: 0.400 [0.296, 0.499], mean action: 51.200 [0.000, 96.000], mean observation: 3.162 [-0.530, 10.238], loss: 1.195247, mae: 5.056855, mean_q: 5.266129
 92894/100000: episode: 9481, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.295, mean reward: 0.430 [0.402, 0.542], mean action: 66.400 [23.000, 100.000], mean observation: 3.177 [-1.382, 10.337], loss: 1.652833, mae: 5.058295, mean_q: 5.266132
 92904/100000: episode: 9482, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.048, mean reward: 0.405 [0.370, 0.512], mean action: 69.200 [28.000, 100.000], mean observation: 3.158 [-1.096, 10.185], loss: 1.315731, mae: 5.056686, mean_q: 5.264154
 92914/100000: episode: 9483, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.998, mean reward: 0.400 [0.344, 0.453], mean action: 86.000 [56.000, 100.000], mean observation: 3.152 [-1.685, 10.262], loss: 1.196808, mae: 5.056237, mean_q: 5.265269
 92924/100000: episode: 9484, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 3.947, mean reward: 0.395 [0.347, 0.518], mean action: 71.400 [29.000, 100.000], mean observation: 3.150 [-1.953, 10.214], loss: 0.892394, mae: 5.055490, mean_q: 5.266757
 92934/100000: episode: 9485, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 4.294, mean reward: 0.429 [0.400, 0.451], mean action: 86.800 [52.000, 100.000], mean observation: 3.148 [-1.162, 10.331], loss: 1.017277, mae: 5.056320, mean_q: 5.268690
 92944/100000: episode: 9486, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 4.347, mean reward: 0.435 [0.338, 0.476], mean action: 62.300 [2.000, 100.000], mean observation: 3.160 [-1.007, 10.305], loss: 1.049423, mae: 5.057029, mean_q: 5.270385
 92954/100000: episode: 9487, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.805, mean reward: 0.380 [0.333, 0.405], mean action: 68.400 [15.000, 100.000], mean observation: 3.142 [-1.188, 10.194], loss: 1.228115, mae: 5.057864, mean_q: 5.271743
 92964/100000: episode: 9488, duration: 0.096s, episode steps: 10, steps per second: 105, episode reward: 3.903, mean reward: 0.390 [0.360, 0.470], mean action: 88.900 [42.000, 100.000], mean observation: 3.162 [-1.561, 10.338], loss: 0.867180, mae: 5.056586, mean_q: 5.273396
 92974/100000: episode: 9489, duration: 0.097s, episode steps: 10, steps per second: 104, episode reward: 3.632, mean reward: 0.363 [0.341, 0.439], mean action: 86.600 [31.000, 101.000], mean observation: 3.144 [-1.254, 10.438], loss: 1.327857, mae: 5.058640, mean_q: 5.275250
 92984/100000: episode: 9490, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.787, mean reward: 0.379 [0.294, 0.505], mean action: 47.700 [1.000, 100.000], mean observation: 3.158 [-1.809, 10.410], loss: 1.080084, mae: 5.057880, mean_q: 5.277146
 92994/100000: episode: 9491, duration: 0.102s, episode steps: 10, steps per second: 98, episode reward: 3.622, mean reward: 0.362 [0.324, 0.407], mean action: 83.300 [32.000, 100.000], mean observation: 3.152 [-1.600, 10.330], loss: 1.515672, mae: 5.059700, mean_q: 5.279294
 93004/100000: episode: 9492, duration: 0.112s, episode steps: 10, steps per second: 90, episode reward: 3.599, mean reward: 0.360 [0.353, 0.424], mean action: 78.600 [12.000, 100.000], mean observation: 3.163 [-0.663, 10.258], loss: 1.059908, mae: 5.057985, mean_q: 5.281729
 93014/100000: episode: 9493, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.542, mean reward: 0.454 [0.402, 0.569], mean action: 70.400 [0.000, 100.000], mean observation: 3.161 [-1.010, 10.293], loss: 1.052930, mae: 5.058070, mean_q: 5.283825
 93024/100000: episode: 9494, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 3.989, mean reward: 0.399 [0.356, 0.513], mean action: 81.200 [43.000, 100.000], mean observation: 3.151 [-1.582, 10.229], loss: 1.354154, mae: 5.059195, mean_q: 5.285564
 93034/100000: episode: 9495, duration: 0.087s, episode steps: 10, steps per second: 115, episode reward: 3.245, mean reward: 0.325 [0.317, 0.396], mean action: 100.000 [100.000, 100.000], mean observation: 3.138 [-1.121, 10.123], loss: 1.067379, mae: 5.058280, mean_q: 5.286561
 93044/100000: episode: 9496, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.949, mean reward: 0.395 [0.318, 0.435], mean action: 64.100 [4.000, 100.000], mean observation: 3.159 [-1.022, 10.386], loss: 1.315226, mae: 5.059193, mean_q: 5.287929
 93054/100000: episode: 9497, duration: 0.131s, episode steps: 10, steps per second: 77, episode reward: 4.200, mean reward: 0.420 [0.387, 0.443], mean action: 64.200 [13.000, 100.000], mean observation: 3.157 [-1.555, 10.259], loss: 1.089488, mae: 5.058417, mean_q: 5.289813
 93064/100000: episode: 9498, duration: 0.084s, episode steps: 10, steps per second: 118, episode reward: 3.667, mean reward: 0.367 [0.364, 0.369], mean action: 91.100 [11.000, 100.000], mean observation: 3.150 [-1.178, 10.397], loss: 1.329569, mae: 5.059367, mean_q: 5.291121
 93074/100000: episode: 9499, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.218, mean reward: 0.422 [0.388, 0.505], mean action: 74.500 [10.000, 100.000], mean observation: 3.161 [-0.765, 10.237], loss: 1.300974, mae: 5.059213, mean_q: 5.292332
 93084/100000: episode: 9500, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 3.834, mean reward: 0.383 [0.313, 0.467], mean action: 69.700 [10.000, 100.000], mean observation: 3.176 [-1.233, 10.296], loss: 1.245766, mae: 5.059079, mean_q: 5.293832
 93094/100000: episode: 9501, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.857, mean reward: 0.386 [0.343, 0.420], mean action: 75.900 [6.000, 100.000], mean observation: 3.144 [-1.201, 10.424], loss: 1.294559, mae: 5.059261, mean_q: 5.295547
 93104/100000: episode: 9502, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.089, mean reward: 0.409 [0.302, 0.560], mean action: 62.900 [13.000, 100.000], mean observation: 3.148 [-2.188, 10.391], loss: 1.466386, mae: 5.059951, mean_q: 5.298423
 93114/100000: episode: 9503, duration: 0.095s, episode steps: 10, steps per second: 106, episode reward: 4.266, mean reward: 0.427 [0.419, 0.497], mean action: 89.900 [40.000, 100.000], mean observation: 3.161 [-0.884, 10.254], loss: 1.466502, mae: 5.059525, mean_q: 5.300098
 93124/100000: episode: 9504, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 3.928, mean reward: 0.393 [0.363, 0.447], mean action: 79.500 [9.000, 100.000], mean observation: 3.169 [-1.150, 10.424], loss: 1.311192, mae: 5.058794, mean_q: 5.301803
 93134/100000: episode: 9505, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.114, mean reward: 0.411 [0.353, 0.494], mean action: 77.000 [8.000, 100.000], mean observation: 3.153 [-0.858, 10.434], loss: 1.259908, mae: 5.058868, mean_q: 5.302873
 93144/100000: episode: 9506, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 5.123, mean reward: 0.512 [0.460, 0.591], mean action: 84.700 [40.000, 100.000], mean observation: 3.154 [-1.122, 10.244], loss: 1.413579, mae: 5.059518, mean_q: 5.301307
 93154/100000: episode: 9507, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 3.913, mean reward: 0.391 [0.318, 0.525], mean action: 66.600 [13.000, 100.000], mean observation: 3.160 [-1.464, 10.229], loss: 1.218704, mae: 5.058544, mean_q: 5.297135
 93164/100000: episode: 9508, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.306, mean reward: 0.431 [0.357, 0.533], mean action: 69.100 [6.000, 100.000], mean observation: 3.159 [-0.526, 10.242], loss: 1.262513, mae: 5.058632, mean_q: 5.296823
 93174/100000: episode: 9509, duration: 0.102s, episode steps: 10, steps per second: 98, episode reward: 3.948, mean reward: 0.395 [0.359, 0.438], mean action: 86.800 [72.000, 100.000], mean observation: 3.173 [-1.076, 10.231], loss: 1.119843, mae: 5.058315, mean_q: 5.297905
 93184/100000: episode: 9510, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 4.537, mean reward: 0.454 [0.430, 0.510], mean action: 86.300 [31.000, 100.000], mean observation: 3.151 [-1.074, 10.205], loss: 1.505981, mae: 5.060032, mean_q: 5.299558
 93194/100000: episode: 9511, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 4.015, mean reward: 0.402 [0.343, 0.503], mean action: 85.500 [24.000, 101.000], mean observation: 3.147 [-1.242, 10.410], loss: 1.356957, mae: 5.059042, mean_q: 5.301359
 93204/100000: episode: 9512, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.064, mean reward: 0.406 [0.406, 0.407], mean action: 76.600 [9.000, 100.000], mean observation: 3.155 [-1.314, 10.268], loss: 1.027895, mae: 5.057795, mean_q: 5.303021
 93214/100000: episode: 9513, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.193, mean reward: 0.419 [0.362, 0.520], mean action: 61.500 [25.000, 100.000], mean observation: 3.165 [-0.953, 10.410], loss: 1.457119, mae: 5.059798, mean_q: 5.304946
 93224/100000: episode: 9514, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.893, mean reward: 0.389 [0.355, 0.458], mean action: 60.900 [8.000, 100.000], mean observation: 3.157 [-1.314, 10.226], loss: 1.167428, mae: 5.058717, mean_q: 5.306676
 93234/100000: episode: 9515, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 4.554, mean reward: 0.455 [0.450, 0.464], mean action: 87.200 [20.000, 100.000], mean observation: 3.166 [-1.326, 10.367], loss: 1.041575, mae: 5.058308, mean_q: 5.308314
 93244/100000: episode: 9516, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.970, mean reward: 0.397 [0.387, 0.417], mean action: 75.400 [6.000, 100.000], mean observation: 3.162 [-1.062, 10.257], loss: 1.160808, mae: 5.059268, mean_q: 5.305058
 93245/100000: episode: 9517, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 27.000 [27.000, 27.000], mean observation: 3.164 [-1.606, 10.667], loss: 1.483987, mae: 5.060435, mean_q: 5.303714
 93255/100000: episode: 9518, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 3.586, mean reward: 0.359 [0.303, 0.437], mean action: 75.500 [21.000, 100.000], mean observation: 3.165 [-1.185, 10.477], loss: 1.221742, mae: 5.059935, mean_q: 5.303972
 93265/100000: episode: 9519, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.791, mean reward: 0.379 [0.367, 0.405], mean action: 60.800 [14.000, 100.000], mean observation: 3.159 [-1.250, 10.298], loss: 0.929886, mae: 5.059139, mean_q: 5.305453
 93275/100000: episode: 9520, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.804, mean reward: 0.380 [0.380, 0.380], mean action: 66.100 [16.000, 101.000], mean observation: 3.170 [-1.372, 10.294], loss: 1.238855, mae: 5.060475, mean_q: 5.308149
 93285/100000: episode: 9521, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.402, mean reward: 0.440 [0.435, 0.462], mean action: 83.400 [34.000, 100.000], mean observation: 3.159 [-1.286, 10.221], loss: 1.453636, mae: 5.061813, mean_q: 5.308885
 93295/100000: episode: 9522, duration: 0.123s, episode steps: 10, steps per second: 82, episode reward: 4.197, mean reward: 0.420 [0.417, 0.446], mean action: 73.900 [23.000, 100.000], mean observation: 3.176 [-0.821, 10.308], loss: 1.095368, mae: 5.060556, mean_q: 5.305674
 93305/100000: episode: 9523, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 4.006, mean reward: 0.401 [0.352, 0.495], mean action: 62.800 [14.000, 100.000], mean observation: 3.153 [-1.042, 10.337], loss: 1.247879, mae: 5.061282, mean_q: 5.300312
 93315/100000: episode: 9524, duration: 0.093s, episode steps: 10, steps per second: 108, episode reward: 4.662, mean reward: 0.466 [0.339, 0.521], mean action: 88.500 [14.000, 100.000], mean observation: 3.155 [-0.532, 10.328], loss: 1.247027, mae: 5.061477, mean_q: 5.297378
 93325/100000: episode: 9525, duration: 0.117s, episode steps: 10, steps per second: 86, episode reward: 3.974, mean reward: 0.397 [0.377, 0.448], mean action: 82.400 [34.000, 100.000], mean observation: 3.153 [-1.308, 10.279], loss: 1.075160, mae: 5.060752, mean_q: 5.292626
 93326/100000: episode: 9526, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 75.000 [75.000, 75.000], mean observation: 3.168 [-0.674, 10.214], loss: 0.840726, mae: 5.060045, mean_q: 5.291466
 93336/100000: episode: 9527, duration: 0.109s, episode steps: 10, steps per second: 91, episode reward: 4.429, mean reward: 0.443 [0.401, 0.535], mean action: 84.900 [8.000, 100.000], mean observation: 3.149 [-2.642, 10.127], loss: 1.133409, mae: 5.061709, mean_q: 5.291579
 93346/100000: episode: 9528, duration: 0.079s, episode steps: 10, steps per second: 126, episode reward: 4.011, mean reward: 0.401 [0.401, 0.401], mean action: 99.200 [94.000, 100.000], mean observation: 3.181 [-1.293, 10.332], loss: 1.832440, mae: 5.064504, mean_q: 5.284219
 93356/100000: episode: 9529, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 3.575, mean reward: 0.357 [0.316, 0.456], mean action: 79.700 [36.000, 100.000], mean observation: 3.161 [-1.371, 10.230], loss: 1.246588, mae: 5.061761, mean_q: 5.278126
 93366/100000: episode: 9530, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.083, mean reward: 0.408 [0.353, 0.518], mean action: 61.800 [7.000, 100.000], mean observation: 3.159 [-1.255, 10.313], loss: 1.265476, mae: 5.061977, mean_q: 5.277049
 93376/100000: episode: 9531, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 5.335, mean reward: 0.534 [0.359, 0.577], mean action: 88.500 [11.000, 100.000], mean observation: 3.152 [-1.285, 10.419], loss: 1.542091, mae: 5.063111, mean_q: 5.275609
 93386/100000: episode: 9532, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 4.241, mean reward: 0.424 [0.351, 0.493], mean action: 82.300 [38.000, 100.000], mean observation: 3.169 [-0.836, 10.297], loss: 1.088668, mae: 5.061158, mean_q: 5.271827
 93396/100000: episode: 9533, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.087, mean reward: 0.409 [0.382, 0.455], mean action: 65.500 [34.000, 100.000], mean observation: 3.165 [-1.385, 10.255], loss: 1.142779, mae: 5.061316, mean_q: 5.267402
 93406/100000: episode: 9534, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.827, mean reward: 0.383 [0.342, 0.498], mean action: 46.600 [10.000, 96.000], mean observation: 3.154 [-1.466, 10.344], loss: 1.370911, mae: 5.062186, mean_q: 5.267835
 93416/100000: episode: 9535, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.240, mean reward: 0.424 [0.324, 0.458], mean action: 50.100 [3.000, 100.000], mean observation: 3.152 [-1.238, 10.298], loss: 1.280272, mae: 5.062097, mean_q: 5.267365
 93426/100000: episode: 9536, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 4.659, mean reward: 0.466 [0.450, 0.483], mean action: 78.200 [13.000, 100.000], mean observation: 3.168 [-1.006, 10.360], loss: 1.386788, mae: 5.062031, mean_q: 5.269500
 93436/100000: episode: 9537, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.723, mean reward: 0.372 [0.339, 0.444], mean action: 68.700 [11.000, 100.000], mean observation: 3.152 [-1.197, 10.358], loss: 1.446428, mae: 5.062227, mean_q: 5.271172
 93446/100000: episode: 9538, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.535, mean reward: 0.453 [0.278, 0.490], mean action: 71.500 [3.000, 100.000], mean observation: 3.163 [-1.332, 10.409], loss: 1.258838, mae: 5.061351, mean_q: 5.272271
 93456/100000: episode: 9539, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 3.799, mean reward: 0.380 [0.340, 0.432], mean action: 86.900 [36.000, 100.000], mean observation: 3.149 [-1.055, 10.313], loss: 1.062067, mae: 5.060340, mean_q: 5.273534
 93466/100000: episode: 9540, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 3.915, mean reward: 0.392 [0.369, 0.427], mean action: 77.900 [23.000, 100.000], mean observation: 3.150 [-2.305, 10.403], loss: 1.619057, mae: 5.062354, mean_q: 5.273411
 93476/100000: episode: 9541, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 4.160, mean reward: 0.416 [0.351, 0.481], mean action: 55.500 [0.000, 101.000], mean observation: 3.170 [-1.467, 10.374], loss: 1.242620, mae: 5.060673, mean_q: 5.269944
 93486/100000: episode: 9542, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.958, mean reward: 0.396 [0.364, 0.450], mean action: 37.900 [7.000, 101.000], mean observation: 3.164 [-1.658, 10.366], loss: 1.050327, mae: 5.059859, mean_q: 5.268888
 93496/100000: episode: 9543, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.000, mean reward: 0.400 [0.370, 0.459], mean action: 33.800 [2.000, 89.000], mean observation: 3.146 [-1.483, 10.378], loss: 1.311302, mae: 5.060695, mean_q: 5.270155
 93506/100000: episode: 9544, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 4.537, mean reward: 0.454 [0.366, 0.504], mean action: 27.800 [6.000, 35.000], mean observation: 3.161 [-2.791, 10.329], loss: 1.212174, mae: 5.060102, mean_q: 5.272683
 93516/100000: episode: 9545, duration: 0.228s, episode steps: 10, steps per second: 44, episode reward: 3.665, mean reward: 0.367 [0.323, 0.388], mean action: 33.600 [4.000, 55.000], mean observation: 3.163 [-1.344, 10.348], loss: 1.125866, mae: 5.060088, mean_q: 5.273450
 93526/100000: episode: 9546, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.687, mean reward: 0.369 [0.332, 0.406], mean action: 45.100 [30.000, 79.000], mean observation: 3.157 [-1.814, 10.353], loss: 1.256683, mae: 5.060576, mean_q: 5.275300
 93536/100000: episode: 9547, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.827, mean reward: 0.383 [0.289, 0.510], mean action: 35.500 [0.000, 87.000], mean observation: 3.170 [-1.607, 10.433], loss: 1.218701, mae: 5.060497, mean_q: 5.275985
 93546/100000: episode: 9548, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.782, mean reward: 0.378 [0.349, 0.415], mean action: 51.400 [5.000, 98.000], mean observation: 3.151 [-1.330, 10.242], loss: 1.094584, mae: 5.059948, mean_q: 5.274907
 93556/100000: episode: 9549, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.267, mean reward: 0.427 [0.322, 0.571], mean action: 34.700 [32.000, 35.000], mean observation: 3.167 [-1.389, 10.478], loss: 1.235298, mae: 5.060822, mean_q: 5.274500
 93566/100000: episode: 9550, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.626, mean reward: 0.363 [0.308, 0.450], mean action: 49.100 [22.000, 98.000], mean observation: 3.144 [-1.829, 10.290], loss: 1.326893, mae: 5.061406, mean_q: 5.274398
 93576/100000: episode: 9551, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.050, mean reward: 0.405 [0.317, 0.479], mean action: 39.400 [19.000, 57.000], mean observation: 3.165 [-2.152, 10.340], loss: 1.061232, mae: 5.060627, mean_q: 5.270412
 93586/100000: episode: 9552, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.186, mean reward: 0.419 [0.419, 0.419], mean action: 58.000 [16.000, 100.000], mean observation: 3.173 [-2.704, 10.365], loss: 0.891155, mae: 5.060337, mean_q: 5.268934
 93596/100000: episode: 9553, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.737, mean reward: 0.474 [0.383, 0.575], mean action: 58.300 [11.000, 100.000], mean observation: 3.153 [-1.542, 10.323], loss: 1.495279, mae: 5.063050, mean_q: 5.269113
 93606/100000: episode: 9554, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.089, mean reward: 0.409 [0.338, 0.457], mean action: 74.700 [17.000, 101.000], mean observation: 3.146 [-1.826, 10.414], loss: 1.136262, mae: 5.061785, mean_q: 5.266606
 93616/100000: episode: 9555, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.379, mean reward: 0.438 [0.384, 0.482], mean action: 74.900 [5.000, 100.000], mean observation: 3.157 [-1.006, 10.337], loss: 1.234067, mae: 5.062168, mean_q: 5.266887
 93626/100000: episode: 9556, duration: 0.098s, episode steps: 10, steps per second: 102, episode reward: 4.129, mean reward: 0.413 [0.400, 0.459], mean action: 91.600 [59.000, 100.000], mean observation: 3.177 [-0.612, 10.243], loss: 1.413314, mae: 5.062707, mean_q: 5.266829
 93636/100000: episode: 9557, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.025, mean reward: 0.403 [0.327, 0.429], mean action: 75.000 [18.000, 100.000], mean observation: 3.155 [-0.926, 10.379], loss: 1.256500, mae: 5.062510, mean_q: 5.264258
 93637/100000: episode: 9558, duration: 0.028s, episode steps: 1, steps per second: 36, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 100.000 [100.000, 100.000], mean observation: 3.154 [-0.408, 10.275], loss: 2.150887, mae: 5.065258, mean_q: 5.264600
 93647/100000: episode: 9559, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.687, mean reward: 0.369 [0.336, 0.559], mean action: 65.200 [5.000, 100.000], mean observation: 3.160 [-0.901, 10.359], loss: 1.305232, mae: 5.062492, mean_q: 5.265270
 93657/100000: episode: 9560, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 3.188, mean reward: 0.319 [0.275, 0.394], mean action: 77.800 [2.000, 100.000], mean observation: 3.160 [-1.192, 10.471], loss: 1.223984, mae: 5.061931, mean_q: 5.266709
 93667/100000: episode: 9561, duration: 0.102s, episode steps: 10, steps per second: 98, episode reward: 4.492, mean reward: 0.449 [0.444, 0.455], mean action: 86.100 [11.000, 100.000], mean observation: 3.157 [-0.558, 10.249], loss: 1.025385, mae: 5.061356, mean_q: 5.268339
 93677/100000: episode: 9562, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 3.806, mean reward: 0.381 [0.338, 0.435], mean action: 73.500 [9.000, 100.000], mean observation: 3.162 [-0.961, 10.441], loss: 1.153739, mae: 5.062052, mean_q: 5.270045
 93687/100000: episode: 9563, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 3.966, mean reward: 0.397 [0.349, 0.428], mean action: 63.000 [11.000, 101.000], mean observation: 3.164 [-1.215, 10.403], loss: 0.896273, mae: 5.061238, mean_q: 5.271335
 93697/100000: episode: 9564, duration: 0.092s, episode steps: 10, steps per second: 108, episode reward: 5.421, mean reward: 0.542 [0.542, 0.542], mean action: 85.500 [41.000, 100.000], mean observation: 3.142 [-1.282, 10.294], loss: 1.406723, mae: 5.063373, mean_q: 5.272752
 93707/100000: episode: 9565, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 3.597, mean reward: 0.360 [0.320, 0.404], mean action: 71.300 [16.000, 100.000], mean observation: 3.152 [-1.316, 10.257], loss: 1.446122, mae: 5.063629, mean_q: 5.274108
 93717/100000: episode: 9566, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 4.258, mean reward: 0.426 [0.396, 0.452], mean action: 85.700 [25.000, 100.000], mean observation: 3.157 [-1.396, 10.248], loss: 1.270480, mae: 5.062974, mean_q: 5.272555
 93727/100000: episode: 9567, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.555, mean reward: 0.455 [0.364, 0.501], mean action: 58.300 [2.000, 100.000], mean observation: 3.139 [-1.517, 10.173], loss: 1.329344, mae: 5.063053, mean_q: 5.268903
 93737/100000: episode: 9568, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.073, mean reward: 0.407 [0.325, 0.493], mean action: 48.000 [3.000, 67.000], mean observation: 3.156 [-1.282, 10.166], loss: 1.185857, mae: 5.062431, mean_q: 5.264748
 93747/100000: episode: 9569, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.973, mean reward: 0.397 [0.367, 0.497], mean action: 50.100 [0.000, 75.000], mean observation: 3.159 [-0.890, 10.309], loss: 1.403905, mae: 5.063070, mean_q: 5.263919
 93757/100000: episode: 9570, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.149, mean reward: 0.415 [0.343, 0.469], mean action: 52.400 [1.000, 99.000], mean observation: 3.156 [-1.349, 10.354], loss: 1.342486, mae: 5.062743, mean_q: 5.259065
 93767/100000: episode: 9571, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 4.534, mean reward: 0.453 [0.375, 0.542], mean action: 29.700 [14.000, 70.000], mean observation: 3.147 [-1.072, 10.394], loss: 1.473609, mae: 5.063106, mean_q: 5.254877
 93777/100000: episode: 9572, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 5.493, mean reward: 0.549 [0.549, 0.549], mean action: 38.200 [23.000, 90.000], mean observation: 3.163 [-1.544, 10.421], loss: 1.228332, mae: 5.061846, mean_q: 5.254387
 93787/100000: episode: 9573, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.535, mean reward: 0.453 [0.346, 0.540], mean action: 27.800 [20.000, 52.000], mean observation: 3.156 [-1.651, 10.323], loss: 1.254934, mae: 5.061838, mean_q: 5.254915
 93797/100000: episode: 9574, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 3.937, mean reward: 0.394 [0.321, 0.483], mean action: 24.700 [1.000, 78.000], mean observation: 3.154 [-1.866, 10.368], loss: 1.216957, mae: 5.061887, mean_q: 5.255509
 93807/100000: episode: 9575, duration: 0.216s, episode steps: 10, steps per second: 46, episode reward: 3.873, mean reward: 0.387 [0.295, 0.492], mean action: 25.300 [0.000, 73.000], mean observation: 3.161 [-1.155, 10.297], loss: 1.197785, mae: 5.061908, mean_q: 5.256697
 93817/100000: episode: 9576, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.293, mean reward: 0.429 [0.352, 0.514], mean action: 31.900 [7.000, 90.000], mean observation: 3.168 [-1.717, 10.458], loss: 1.590325, mae: 5.063500, mean_q: 5.257970
 93827/100000: episode: 9577, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 4.096, mean reward: 0.410 [0.377, 0.508], mean action: 62.300 [3.000, 100.000], mean observation: 3.164 [-1.395, 10.286], loss: 1.028268, mae: 5.061264, mean_q: 5.259322
 93837/100000: episode: 9578, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.013, mean reward: 0.401 [0.373, 0.479], mean action: 74.400 [35.000, 100.000], mean observation: 3.157 [-1.212, 10.316], loss: 1.182511, mae: 5.061958, mean_q: 5.261252
 93847/100000: episode: 9579, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 3.835, mean reward: 0.383 [0.367, 0.428], mean action: 77.100 [11.000, 100.000], mean observation: 3.154 [-1.143, 10.300], loss: 1.126092, mae: 5.061985, mean_q: 5.262938
 93857/100000: episode: 9580, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 4.181, mean reward: 0.418 [0.392, 0.478], mean action: 69.900 [20.000, 100.000], mean observation: 3.154 [-1.226, 10.295], loss: 1.065164, mae: 5.062039, mean_q: 5.264800
 93867/100000: episode: 9581, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.682, mean reward: 0.468 [0.468, 0.468], mean action: 67.800 [3.000, 100.000], mean observation: 3.157 [-1.354, 10.346], loss: 1.330783, mae: 5.063504, mean_q: 5.266619
 93877/100000: episode: 9582, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.253, mean reward: 0.425 [0.416, 0.467], mean action: 70.500 [6.000, 100.000], mean observation: 3.157 [-1.170, 10.324], loss: 1.340152, mae: 5.063286, mean_q: 5.267905
 93887/100000: episode: 9583, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.284, mean reward: 0.428 [0.348, 0.523], mean action: 52.900 [10.000, 100.000], mean observation: 3.158 [-1.792, 10.287], loss: 1.142315, mae: 5.062408, mean_q: 5.266307
 93897/100000: episode: 9584, duration: 0.086s, episode steps: 10, steps per second: 116, episode reward: 3.607, mean reward: 0.361 [0.322, 0.449], mean action: 92.100 [52.000, 100.000], mean observation: 3.159 [-1.012, 10.126], loss: 1.121340, mae: 5.062653, mean_q: 5.266694
 93907/100000: episode: 9585, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 5.891, mean reward: 0.589 [0.589, 0.589], mean action: 77.800 [38.000, 100.000], mean observation: 3.158 [-1.277, 10.279], loss: 1.120021, mae: 5.062783, mean_q: 5.268163
 93917/100000: episode: 9586, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.165, mean reward: 0.416 [0.345, 0.531], mean action: 75.000 [10.000, 100.000], mean observation: 3.153 [-0.994, 10.417], loss: 1.319319, mae: 5.063740, mean_q: 5.266519
 93927/100000: episode: 9587, duration: 0.095s, episode steps: 10, steps per second: 105, episode reward: 3.864, mean reward: 0.386 [0.335, 0.475], mean action: 88.100 [31.000, 100.000], mean observation: 3.169 [-1.138, 10.207], loss: 1.090548, mae: 5.063239, mean_q: 5.266269
 93937/100000: episode: 9588, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 4.216, mean reward: 0.422 [0.413, 0.450], mean action: 64.400 [23.000, 100.000], mean observation: 3.153 [-1.798, 10.355], loss: 1.166488, mae: 5.063649, mean_q: 5.265467
 93947/100000: episode: 9589, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.347, mean reward: 0.435 [0.435, 0.435], mean action: 40.300 [14.000, 95.000], mean observation: 3.149 [-2.476, 10.375], loss: 1.240749, mae: 5.064254, mean_q: 5.265735
 93957/100000: episode: 9590, duration: 0.113s, episode steps: 10, steps per second: 88, episode reward: 3.535, mean reward: 0.353 [0.316, 0.525], mean action: 77.500 [8.000, 100.000], mean observation: 3.154 [-1.098, 10.250], loss: 1.415006, mae: 5.065170, mean_q: 5.262596
 93967/100000: episode: 9591, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 3.820, mean reward: 0.382 [0.347, 0.439], mean action: 79.700 [4.000, 100.000], mean observation: 3.167 [-1.280, 10.353], loss: 1.451563, mae: 5.065107, mean_q: 5.264375
 93977/100000: episode: 9592, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.268, mean reward: 0.427 [0.375, 0.489], mean action: 79.100 [38.000, 100.000], mean observation: 3.156 [-0.851, 10.284], loss: 1.122661, mae: 5.063603, mean_q: 5.266341
 93987/100000: episode: 9593, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 3.842, mean reward: 0.384 [0.377, 0.402], mean action: 91.100 [29.000, 100.000], mean observation: 3.161 [-1.470, 10.247], loss: 0.915842, mae: 5.063001, mean_q: 5.267863
 93997/100000: episode: 9594, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 3.690, mean reward: 0.369 [0.282, 0.400], mean action: 81.100 [2.000, 100.000], mean observation: 3.153 [-1.427, 10.259], loss: 1.192718, mae: 5.064255, mean_q: 5.269980
 94007/100000: episode: 9595, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 4.164, mean reward: 0.416 [0.346, 0.488], mean action: 84.000 [40.000, 100.000], mean observation: 3.160 [-0.821, 10.549], loss: 1.061874, mae: 5.063817, mean_q: 5.271804
 94017/100000: episode: 9596, duration: 0.113s, episode steps: 10, steps per second: 88, episode reward: 3.894, mean reward: 0.389 [0.389, 0.389], mean action: 90.200 [5.000, 100.000], mean observation: 3.154 [-1.379, 10.322], loss: 1.392224, mae: 5.065316, mean_q: 5.273006
 94027/100000: episode: 9597, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 4.240, mean reward: 0.424 [0.412, 0.448], mean action: 72.000 [16.000, 100.000], mean observation: 3.142 [-1.409, 10.339], loss: 1.118158, mae: 5.064002, mean_q: 5.271135
 94037/100000: episode: 9598, duration: 0.116s, episode steps: 10, steps per second: 87, episode reward: 3.462, mean reward: 0.346 [0.326, 0.375], mean action: 73.600 [14.000, 100.000], mean observation: 3.161 [-1.066, 10.355], loss: 1.340800, mae: 5.065222, mean_q: 5.268939
 94047/100000: episode: 9599, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 4.095, mean reward: 0.409 [0.393, 0.511], mean action: 81.200 [24.000, 100.000], mean observation: 3.142 [-1.146, 10.257], loss: 0.966777, mae: 5.063684, mean_q: 5.266150
 94057/100000: episode: 9600, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.945, mean reward: 0.395 [0.383, 0.427], mean action: 76.300 [22.000, 100.000], mean observation: 3.150 [-0.723, 10.244], loss: 1.118736, mae: 5.064448, mean_q: 5.264458
 94067/100000: episode: 9601, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.544, mean reward: 0.454 [0.432, 0.470], mean action: 58.400 [3.000, 101.000], mean observation: 3.160 [-1.685, 10.190], loss: 1.177569, mae: 5.064733, mean_q: 5.265899
 94077/100000: episode: 9602, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 4.647, mean reward: 0.465 [0.368, 0.544], mean action: 67.000 [2.000, 100.000], mean observation: 3.163 [-1.581, 10.309], loss: 1.462571, mae: 5.065741, mean_q: 5.268181
 94087/100000: episode: 9603, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.076, mean reward: 0.408 [0.399, 0.450], mean action: 47.200 [7.000, 75.000], mean observation: 3.160 [-1.021, 10.408], loss: 1.322648, mae: 5.065010, mean_q: 5.270358
 94093/100000: episode: 9604, duration: 0.088s, episode steps: 6, steps per second: 68, episode reward: 11.754, mean reward: 1.959 [0.339, 10.000], mean action: 64.000 [30.000, 94.000], mean observation: 3.163 [-1.358, 10.329], loss: 1.630936, mae: 5.065985, mean_q: 5.271600
 94103/100000: episode: 9605, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 4.216, mean reward: 0.422 [0.390, 0.443], mean action: 72.300 [2.000, 100.000], mean observation: 3.164 [-0.851, 10.276], loss: 1.110512, mae: 5.064112, mean_q: 5.273438
 94113/100000: episode: 9606, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.155, mean reward: 0.416 [0.306, 0.558], mean action: 68.300 [4.000, 100.000], mean observation: 3.152 [-1.271, 10.328], loss: 1.503886, mae: 5.065795, mean_q: 5.275887
 94123/100000: episode: 9607, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.352, mean reward: 0.435 [0.369, 0.485], mean action: 75.900 [12.000, 100.000], mean observation: 3.154 [-1.623, 10.300], loss: 1.305319, mae: 5.065000, mean_q: 5.278003
 94133/100000: episode: 9608, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.332, mean reward: 0.433 [0.394, 0.458], mean action: 74.800 [4.000, 100.000], mean observation: 3.161 [-2.416, 10.322], loss: 1.368076, mae: 5.065198, mean_q: 5.279754
 94143/100000: episode: 9609, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.629, mean reward: 0.463 [0.391, 0.589], mean action: 67.300 [17.000, 100.000], mean observation: 3.160 [-1.049, 10.265], loss: 1.684879, mae: 5.066289, mean_q: 5.280489
 94153/100000: episode: 9610, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 4.929, mean reward: 0.493 [0.493, 0.493], mean action: 71.200 [4.000, 100.000], mean observation: 3.167 [-0.729, 10.466], loss: 1.061178, mae: 5.063911, mean_q: 5.275967
 94163/100000: episode: 9611, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.744, mean reward: 0.474 [0.452, 0.529], mean action: 66.900 [4.000, 100.000], mean observation: 3.144 [-1.195, 10.266], loss: 1.162926, mae: 5.064308, mean_q: 5.273841
 94173/100000: episode: 9612, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.680, mean reward: 0.468 [0.393, 0.508], mean action: 74.200 [9.000, 100.000], mean observation: 3.156 [-1.650, 10.463], loss: 1.601882, mae: 5.066127, mean_q: 5.274428
 94183/100000: episode: 9613, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 3.848, mean reward: 0.385 [0.331, 0.492], mean action: 71.600 [20.000, 100.000], mean observation: 3.145 [-0.944, 10.426], loss: 1.217735, mae: 5.064361, mean_q: 5.275499
 94193/100000: episode: 9614, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.187, mean reward: 0.419 [0.379, 0.519], mean action: 60.000 [7.000, 100.000], mean observation: 3.168 [-1.160, 10.287], loss: 1.421464, mae: 5.065056, mean_q: 5.269768
 94203/100000: episode: 9615, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.744, mean reward: 0.474 [0.351, 0.522], mean action: 68.600 [11.000, 100.000], mean observation: 3.156 [-0.838, 10.242], loss: 1.375163, mae: 5.064176, mean_q: 5.263136
 94213/100000: episode: 9616, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.034, mean reward: 0.403 [0.396, 0.446], mean action: 52.100 [35.000, 93.000], mean observation: 3.150 [-1.387, 10.500], loss: 1.075836, mae: 5.063074, mean_q: 5.259994
 94223/100000: episode: 9617, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.676, mean reward: 0.368 [0.316, 0.499], mean action: 52.400 [10.000, 97.000], mean observation: 3.145 [-1.504, 10.320], loss: 0.872053, mae: 5.062212, mean_q: 5.257007
 94233/100000: episode: 9618, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.682, mean reward: 0.368 [0.353, 0.404], mean action: 51.500 [34.000, 98.000], mean observation: 3.165 [-1.619, 10.319], loss: 1.128338, mae: 5.063476, mean_q: 5.257199
 94243/100000: episode: 9619, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.507, mean reward: 0.451 [0.341, 0.523], mean action: 62.800 [32.000, 101.000], mean observation: 3.139 [-1.693, 10.255], loss: 1.148682, mae: 5.063783, mean_q: 5.258780
 94253/100000: episode: 9620, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.173, mean reward: 0.417 [0.359, 0.589], mean action: 49.100 [11.000, 88.000], mean observation: 3.163 [-1.416, 10.277], loss: 1.115153, mae: 5.063735, mean_q: 5.262068
 94263/100000: episode: 9621, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.919, mean reward: 0.392 [0.343, 0.570], mean action: 42.500 [22.000, 83.000], mean observation: 3.159 [-1.400, 10.228], loss: 1.014355, mae: 5.063560, mean_q: 5.264453
 94273/100000: episode: 9622, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.925, mean reward: 0.392 [0.372, 0.426], mean action: 58.700 [40.000, 100.000], mean observation: 3.169 [-2.045, 10.312], loss: 1.253601, mae: 5.064682, mean_q: 5.265342
 94283/100000: episode: 9623, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.940, mean reward: 0.394 [0.333, 0.477], mean action: 36.400 [16.000, 54.000], mean observation: 3.173 [-1.381, 10.253], loss: 1.255691, mae: 5.064744, mean_q: 5.264363
 94293/100000: episode: 9624, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 5.511, mean reward: 0.551 [0.530, 0.560], mean action: 46.100 [4.000, 59.000], mean observation: 3.151 [-1.213, 10.258], loss: 1.202490, mae: 5.064311, mean_q: 5.265727
 94303/100000: episode: 9625, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.216, mean reward: 0.422 [0.399, 0.460], mean action: 58.300 [16.000, 91.000], mean observation: 3.159 [-1.396, 10.281], loss: 1.416142, mae: 5.065166, mean_q: 5.270144
 94313/100000: episode: 9626, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.828, mean reward: 0.383 [0.289, 0.485], mean action: 52.600 [15.000, 82.000], mean observation: 3.143 [-1.484, 10.262], loss: 1.122703, mae: 5.063573, mean_q: 5.273925
 94323/100000: episode: 9627, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.864, mean reward: 0.486 [0.486, 0.486], mean action: 62.300 [50.000, 93.000], mean observation: 3.161 [-1.619, 10.245], loss: 1.197240, mae: 5.063870, mean_q: 5.276697
 94333/100000: episode: 9628, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.704, mean reward: 0.470 [0.396, 0.527], mean action: 51.800 [12.000, 89.000], mean observation: 3.156 [-1.326, 10.315], loss: 1.135517, mae: 5.063660, mean_q: 5.278590
 94343/100000: episode: 9629, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.121, mean reward: 0.412 [0.395, 0.457], mean action: 54.600 [10.000, 88.000], mean observation: 3.151 [-1.552, 10.255], loss: 1.550994, mae: 5.065606, mean_q: 5.280879
 94353/100000: episode: 9630, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 3.615, mean reward: 0.361 [0.317, 0.459], mean action: 56.300 [46.000, 69.000], mean observation: 3.171 [-1.619, 10.234], loss: 1.640919, mae: 5.065533, mean_q: 5.282474
 94363/100000: episode: 9631, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 4.616, mean reward: 0.462 [0.429, 0.546], mean action: 42.600 [13.000, 86.000], mean observation: 3.143 [-2.053, 10.352], loss: 1.332278, mae: 5.064057, mean_q: 5.279074
 94373/100000: episode: 9632, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.221, mean reward: 0.422 [0.332, 0.531], mean action: 41.000 [0.000, 78.000], mean observation: 3.149 [-1.141, 10.223], loss: 1.399495, mae: 5.064069, mean_q: 5.274519
 94383/100000: episode: 9633, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.387, mean reward: 0.439 [0.344, 0.473], mean action: 44.400 [9.000, 98.000], mean observation: 3.170 [-1.390, 10.448], loss: 1.119212, mae: 5.063053, mean_q: 5.273808
 94393/100000: episode: 9634, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.504, mean reward: 0.350 [0.310, 0.422], mean action: 63.600 [51.000, 101.000], mean observation: 3.156 [-2.361, 10.341], loss: 1.232893, mae: 5.063684, mean_q: 5.274406
 94403/100000: episode: 9635, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.924, mean reward: 0.392 [0.336, 0.486], mean action: 42.900 [4.000, 69.000], mean observation: 3.157 [-0.997, 10.330], loss: 1.366614, mae: 5.064317, mean_q: 5.275409
 94413/100000: episode: 9636, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.059, mean reward: 0.406 [0.334, 0.455], mean action: 40.900 [11.000, 61.000], mean observation: 3.156 [-1.424, 10.205], loss: 1.414456, mae: 5.064456, mean_q: 5.276799
 94423/100000: episode: 9637, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 3.873, mean reward: 0.387 [0.337, 0.447], mean action: 46.400 [14.000, 72.000], mean observation: 3.158 [-1.297, 10.405], loss: 1.054323, mae: 5.063023, mean_q: 5.279155
 94433/100000: episode: 9638, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.122, mean reward: 0.412 [0.375, 0.536], mean action: 54.500 [11.000, 74.000], mean observation: 3.149 [-1.118, 10.289], loss: 1.458967, mae: 5.064544, mean_q: 5.280229
 94443/100000: episode: 9639, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 3.759, mean reward: 0.376 [0.372, 0.382], mean action: 67.200 [46.000, 99.000], mean observation: 3.154 [-1.746, 10.281], loss: 1.256948, mae: 5.063603, mean_q: 5.280570
 94453/100000: episode: 9640, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.268, mean reward: 0.427 [0.344, 0.448], mean action: 50.500 [23.000, 62.000], mean observation: 3.148 [-1.175, 10.446], loss: 1.211872, mae: 5.063153, mean_q: 5.280837
 94463/100000: episode: 9641, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.189, mean reward: 0.419 [0.401, 0.535], mean action: 57.300 [56.000, 69.000], mean observation: 3.166 [-1.446, 10.276], loss: 1.324822, mae: 5.063544, mean_q: 5.282147
 94473/100000: episode: 9642, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.678, mean reward: 0.368 [0.337, 0.389], mean action: 64.900 [56.000, 98.000], mean observation: 3.144 [-1.981, 10.248], loss: 1.153171, mae: 5.062941, mean_q: 5.283226
 94483/100000: episode: 9643, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.945, mean reward: 0.395 [0.324, 0.472], mean action: 55.700 [19.000, 90.000], mean observation: 3.147 [-1.949, 10.249], loss: 1.258345, mae: 5.063304, mean_q: 5.284454
 94493/100000: episode: 9644, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.252, mean reward: 0.425 [0.309, 0.596], mean action: 54.900 [4.000, 97.000], mean observation: 3.149 [-1.212, 10.542], loss: 1.066921, mae: 5.063041, mean_q: 5.285253
 94503/100000: episode: 9645, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.174, mean reward: 0.417 [0.361, 0.506], mean action: 44.600 [2.000, 79.000], mean observation: 3.153 [-2.480, 10.398], loss: 1.360946, mae: 5.064464, mean_q: 5.287404
 94513/100000: episode: 9646, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.116, mean reward: 0.412 [0.360, 0.455], mean action: 45.400 [4.000, 59.000], mean observation: 3.155 [-1.460, 10.232], loss: 1.260316, mae: 5.064286, mean_q: 5.290088
 94523/100000: episode: 9647, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.294, mean reward: 0.429 [0.330, 0.502], mean action: 46.600 [7.000, 66.000], mean observation: 3.165 [-1.664, 10.316], loss: 1.335069, mae: 5.064576, mean_q: 5.292828
 94533/100000: episode: 9648, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.889, mean reward: 0.389 [0.332, 0.465], mean action: 50.500 [5.000, 79.000], mean observation: 3.153 [-2.832, 10.373], loss: 0.977236, mae: 5.062998, mean_q: 5.288999
 94543/100000: episode: 9649, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.541, mean reward: 0.354 [0.313, 0.421], mean action: 54.800 [20.000, 76.000], mean observation: 3.153 [-1.504, 10.244], loss: 1.099352, mae: 5.063635, mean_q: 5.280054
 94553/100000: episode: 9650, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 4.382, mean reward: 0.438 [0.437, 0.452], mean action: 55.300 [32.000, 77.000], mean observation: 3.153 [-1.411, 10.343], loss: 1.130620, mae: 5.063748, mean_q: 5.272653
 94563/100000: episode: 9651, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.745, mean reward: 0.375 [0.280, 0.511], mean action: 38.200 [2.000, 56.000], mean observation: 3.153 [-1.203, 10.461], loss: 1.119944, mae: 5.063976, mean_q: 5.270907
 94573/100000: episode: 9652, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.165, mean reward: 0.416 [0.389, 0.475], mean action: 48.100 [10.000, 80.000], mean observation: 3.146 [-1.169, 10.397], loss: 1.368111, mae: 5.065377, mean_q: 5.271829
 94583/100000: episode: 9653, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.735, mean reward: 0.474 [0.348, 0.519], mean action: 53.600 [6.000, 80.000], mean observation: 3.148 [-1.909, 10.324], loss: 1.219594, mae: 5.064826, mean_q: 5.272970
 94593/100000: episode: 9654, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.251, mean reward: 0.425 [0.330, 0.516], mean action: 48.000 [7.000, 74.000], mean observation: 3.154 [-1.223, 10.373], loss: 1.289653, mae: 5.065210, mean_q: 5.274660
 94603/100000: episode: 9655, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.024, mean reward: 0.402 [0.384, 0.437], mean action: 41.100 [3.000, 101.000], mean observation: 3.157 [-1.443, 10.267], loss: 1.195797, mae: 5.064918, mean_q: 5.277249
 94613/100000: episode: 9656, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.304, mean reward: 0.430 [0.311, 0.556], mean action: 39.600 [9.000, 56.000], mean observation: 3.156 [-1.789, 10.247], loss: 1.421513, mae: 5.066175, mean_q: 5.279520
 94623/100000: episode: 9657, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.290, mean reward: 0.429 [0.380, 0.536], mean action: 56.500 [30.000, 98.000], mean observation: 3.170 [-1.295, 10.303], loss: 1.119196, mae: 5.065048, mean_q: 5.280332
 94633/100000: episode: 9658, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.448, mean reward: 0.345 [0.289, 0.418], mean action: 49.400 [40.000, 89.000], mean observation: 3.157 [-1.103, 10.504], loss: 1.350815, mae: 5.066194, mean_q: 5.281497
 94643/100000: episode: 9659, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.061, mean reward: 0.406 [0.343, 0.486], mean action: 54.600 [30.000, 85.000], mean observation: 3.155 [-1.054, 10.343], loss: 1.197028, mae: 5.065439, mean_q: 5.279947
 94653/100000: episode: 9660, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.552, mean reward: 0.455 [0.398, 0.506], mean action: 52.200 [7.000, 96.000], mean observation: 3.165 [-1.569, 10.415], loss: 1.340229, mae: 5.066233, mean_q: 5.278423
 94663/100000: episode: 9661, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.324, mean reward: 0.432 [0.314, 0.492], mean action: 49.600 [15.000, 82.000], mean observation: 3.153 [-1.024, 10.329], loss: 1.508724, mae: 5.066966, mean_q: 5.278180
 94673/100000: episode: 9662, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 4.227, mean reward: 0.423 [0.346, 0.572], mean action: 39.600 [8.000, 69.000], mean observation: 3.161 [-1.513, 10.459], loss: 1.097007, mae: 5.065410, mean_q: 5.274489
 94683/100000: episode: 9663, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.208, mean reward: 0.421 [0.334, 0.517], mean action: 50.800 [28.000, 83.000], mean observation: 3.163 [-1.532, 10.295], loss: 1.185755, mae: 5.065671, mean_q: 5.272405
 94693/100000: episode: 9664, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.261, mean reward: 0.426 [0.325, 0.525], mean action: 44.800 [36.000, 92.000], mean observation: 3.157 [-1.643, 10.271], loss: 1.551704, mae: 5.067649, mean_q: 5.272429
 94703/100000: episode: 9665, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.363, mean reward: 0.436 [0.337, 0.516], mean action: 47.300 [11.000, 67.000], mean observation: 3.172 [-1.416, 10.390], loss: 1.404707, mae: 5.067155, mean_q: 5.273913
 94713/100000: episode: 9666, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.116, mean reward: 0.412 [0.339, 0.544], mean action: 51.500 [5.000, 98.000], mean observation: 3.172 [-1.471, 10.580], loss: 1.410629, mae: 5.066842, mean_q: 5.275414
 94723/100000: episode: 9667, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.846, mean reward: 0.385 [0.345, 0.504], mean action: 55.700 [53.000, 56.000], mean observation: 3.143 [-1.847, 10.348], loss: 1.506477, mae: 5.067146, mean_q: 5.276671
 94733/100000: episode: 9668, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.881, mean reward: 0.388 [0.308, 0.453], mean action: 46.700 [5.000, 92.000], mean observation: 3.156 [-1.280, 10.330], loss: 1.228284, mae: 5.065771, mean_q: 5.278636
 94743/100000: episode: 9669, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.727, mean reward: 0.373 [0.310, 0.418], mean action: 53.800 [33.000, 67.000], mean observation: 3.162 [-1.190, 10.339], loss: 1.128166, mae: 5.065441, mean_q: 5.281672
 94753/100000: episode: 9670, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.904, mean reward: 0.390 [0.359, 0.451], mean action: 56.900 [56.000, 65.000], mean observation: 3.178 [-1.353, 10.237], loss: 1.245515, mae: 5.066112, mean_q: 5.284284
 94763/100000: episode: 9671, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.272, mean reward: 0.427 [0.417, 0.517], mean action: 59.600 [9.000, 99.000], mean observation: 3.156 [-1.482, 10.278], loss: 1.560053, mae: 5.067449, mean_q: 5.287353
 94773/100000: episode: 9672, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.922, mean reward: 0.392 [0.373, 0.463], mean action: 58.500 [42.000, 95.000], mean observation: 3.163 [-1.228, 10.302], loss: 1.316971, mae: 5.066350, mean_q: 5.288836
 94783/100000: episode: 9673, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.249, mean reward: 0.425 [0.348, 0.479], mean action: 49.100 [1.000, 82.000], mean observation: 3.158 [-1.493, 10.282], loss: 1.387572, mae: 5.066231, mean_q: 5.285524
 94793/100000: episode: 9674, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.593, mean reward: 0.359 [0.287, 0.437], mean action: 57.300 [15.000, 91.000], mean observation: 3.148 [-1.086, 10.390], loss: 1.235629, mae: 5.065562, mean_q: 5.277965
 94803/100000: episode: 9675, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.956, mean reward: 0.396 [0.343, 0.433], mean action: 51.800 [22.000, 92.000], mean observation: 3.162 [-1.155, 10.279], loss: 1.234996, mae: 5.065467, mean_q: 5.276834
 94813/100000: episode: 9676, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.114, mean reward: 0.411 [0.320, 0.466], mean action: 45.800 [4.000, 86.000], mean observation: 3.151 [-1.012, 10.316], loss: 1.089730, mae: 5.064837, mean_q: 5.278007
 94823/100000: episode: 9677, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.754, mean reward: 0.475 [0.464, 0.522], mean action: 66.500 [56.000, 93.000], mean observation: 3.145 [-1.448, 10.232], loss: 1.218778, mae: 5.065433, mean_q: 5.274571
 94833/100000: episode: 9678, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 3.688, mean reward: 0.369 [0.335, 0.431], mean action: 60.000 [49.000, 82.000], mean observation: 3.170 [-1.701, 10.366], loss: 1.411443, mae: 5.066071, mean_q: 5.273987
 94843/100000: episode: 9679, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.357, mean reward: 0.436 [0.353, 0.494], mean action: 54.400 [7.000, 96.000], mean observation: 3.146 [-1.042, 10.371], loss: 1.237333, mae: 5.065385, mean_q: 5.273275
 94853/100000: episode: 9680, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.651, mean reward: 0.465 [0.465, 0.465], mean action: 50.100 [37.000, 78.000], mean observation: 3.161 [-1.636, 10.387], loss: 1.114074, mae: 5.064884, mean_q: 5.272210
 94863/100000: episode: 9681, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.799, mean reward: 0.480 [0.480, 0.483], mean action: 60.700 [43.000, 100.000], mean observation: 3.128 [-1.136, 10.363], loss: 1.217271, mae: 5.065552, mean_q: 5.273110
 94873/100000: episode: 9682, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.000, mean reward: 0.400 [0.306, 0.511], mean action: 42.100 [16.000, 67.000], mean observation: 3.152 [-1.290, 10.345], loss: 1.411157, mae: 5.066235, mean_q: 5.274216
 94883/100000: episode: 9683, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.337, mean reward: 0.434 [0.404, 0.501], mean action: 51.600 [6.000, 83.000], mean observation: 3.162 [-1.583, 10.421], loss: 1.054702, mae: 5.064998, mean_q: 5.275412
 94893/100000: episode: 9684, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.577, mean reward: 0.358 [0.298, 0.443], mean action: 51.400 [32.000, 90.000], mean observation: 3.164 [-2.307, 10.333], loss: 1.123844, mae: 5.065324, mean_q: 5.276419
 94903/100000: episode: 9685, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.230, mean reward: 0.423 [0.403, 0.490], mean action: 41.200 [8.000, 49.000], mean observation: 3.158 [-1.892, 10.338], loss: 1.271474, mae: 5.066168, mean_q: 5.277638
 94913/100000: episode: 9686, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.576, mean reward: 0.458 [0.432, 0.529], mean action: 54.800 [35.000, 93.000], mean observation: 3.171 [-1.256, 10.320], loss: 1.439712, mae: 5.066715, mean_q: 5.275341
 94923/100000: episode: 9687, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.722, mean reward: 0.372 [0.343, 0.446], mean action: 43.900 [16.000, 83.000], mean observation: 3.154 [-1.833, 10.439], loss: 0.871067, mae: 5.064677, mean_q: 5.271986
 94933/100000: episode: 9688, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.140, mean reward: 0.414 [0.356, 0.529], mean action: 50.900 [21.000, 73.000], mean observation: 3.176 [-1.888, 10.553], loss: 1.332774, mae: 5.066943, mean_q: 5.271482
 94943/100000: episode: 9689, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.583, mean reward: 0.458 [0.331, 0.489], mean action: 57.000 [9.000, 91.000], mean observation: 3.167 [-1.736, 10.448], loss: 0.927777, mae: 5.065655, mean_q: 5.271718
 94953/100000: episode: 9690, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 5.006, mean reward: 0.501 [0.349, 0.534], mean action: 54.300 [21.000, 100.000], mean observation: 3.145 [-1.480, 10.393], loss: 1.252949, mae: 5.067081, mean_q: 5.272392
 94958/100000: episode: 9691, duration: 0.099s, episode steps: 5, steps per second: 50, episode reward: 11.624, mean reward: 2.325 [0.351, 10.000], mean action: 37.000 [16.000, 49.000], mean observation: 3.173 [-1.031, 10.154], loss: 1.430474, mae: 5.068021, mean_q: 5.269574
 94968/100000: episode: 9692, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.248, mean reward: 0.425 [0.418, 0.473], mean action: 69.400 [43.000, 99.000], mean observation: 3.168 [-1.026, 10.331], loss: 1.496329, mae: 5.068061, mean_q: 5.269032
 94978/100000: episode: 9693, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.201, mean reward: 0.420 [0.357, 0.496], mean action: 53.800 [1.000, 90.000], mean observation: 3.168 [-1.103, 10.365], loss: 1.475565, mae: 5.067685, mean_q: 5.270655
 94988/100000: episode: 9694, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 4.060, mean reward: 0.406 [0.383, 0.477], mean action: 69.300 [49.000, 88.000], mean observation: 3.143 [-1.643, 10.300], loss: 1.477349, mae: 5.067314, mean_q: 5.272067
 94998/100000: episode: 9695, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.083, mean reward: 0.408 [0.390, 0.429], mean action: 73.100 [13.000, 99.000], mean observation: 3.150 [-1.945, 10.425], loss: 0.806810, mae: 5.064788, mean_q: 5.273478
 95008/100000: episode: 9696, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.085, mean reward: 0.409 [0.374, 0.463], mean action: 46.900 [4.000, 95.000], mean observation: 3.160 [-0.623, 10.257], loss: 1.135505, mae: 5.066285, mean_q: 5.271654
 95018/100000: episode: 9697, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.596, mean reward: 0.460 [0.440, 0.547], mean action: 55.000 [18.000, 85.000], mean observation: 3.162 [-1.179, 10.390], loss: 1.502002, mae: 5.067659, mean_q: 5.266063
 95028/100000: episode: 9698, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.524, mean reward: 0.352 [0.333, 0.366], mean action: 64.000 [6.000, 97.000], mean observation: 3.155 [-1.204, 10.304], loss: 0.753175, mae: 5.064895, mean_q: 5.265272
 95038/100000: episode: 9699, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.993, mean reward: 0.399 [0.339, 0.487], mean action: 64.700 [16.000, 95.000], mean observation: 3.138 [-1.543, 10.224], loss: 1.109462, mae: 5.066637, mean_q: 5.265981
 95048/100000: episode: 9700, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.826, mean reward: 0.383 [0.342, 0.499], mean action: 57.000 [23.000, 97.000], mean observation: 3.158 [-1.365, 10.239], loss: 1.431269, mae: 5.068023, mean_q: 5.267066
 95058/100000: episode: 9701, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.972, mean reward: 0.397 [0.334, 0.574], mean action: 61.700 [29.000, 67.000], mean observation: 3.166 [-1.920, 10.261], loss: 1.201761, mae: 5.067036, mean_q: 5.267894
 95068/100000: episode: 9702, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.658, mean reward: 0.366 [0.344, 0.412], mean action: 61.100 [1.000, 74.000], mean observation: 3.157 [-0.892, 10.414], loss: 1.266516, mae: 5.067504, mean_q: 5.269444
 95078/100000: episode: 9703, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.597, mean reward: 0.360 [0.353, 0.401], mean action: 65.900 [36.000, 97.000], mean observation: 3.161 [-1.277, 10.264], loss: 1.485059, mae: 5.068501, mean_q: 5.270555
 95088/100000: episode: 9704, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 3.812, mean reward: 0.381 [0.316, 0.533], mean action: 59.800 [2.000, 101.000], mean observation: 3.162 [-1.264, 10.418], loss: 1.020508, mae: 5.066323, mean_q: 5.267448
 95098/100000: episode: 9705, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 5.456, mean reward: 0.546 [0.546, 0.546], mean action: 54.000 [5.000, 94.000], mean observation: 3.166 [-1.174, 10.291], loss: 1.365979, mae: 5.067537, mean_q: 5.262459
 95108/100000: episode: 9706, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.380, mean reward: 0.338 [0.303, 0.394], mean action: 56.400 [12.000, 99.000], mean observation: 3.151 [-1.569, 10.220], loss: 1.243482, mae: 5.066836, mean_q: 5.262216
 95118/100000: episode: 9707, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.736, mean reward: 0.374 [0.324, 0.450], mean action: 50.700 [20.000, 100.000], mean observation: 3.147 [-1.033, 10.249], loss: 1.328911, mae: 5.067111, mean_q: 5.263799
 95128/100000: episode: 9708, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.249, mean reward: 0.425 [0.361, 0.460], mean action: 44.900 [15.000, 89.000], mean observation: 3.160 [-1.639, 10.284], loss: 1.371157, mae: 5.067044, mean_q: 5.265996
 95138/100000: episode: 9709, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.735, mean reward: 0.374 [0.337, 0.420], mean action: 44.300 [5.000, 90.000], mean observation: 3.159 [-1.287, 10.244], loss: 1.164423, mae: 5.065976, mean_q: 5.268770
 95148/100000: episode: 9710, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.416, mean reward: 0.442 [0.368, 0.483], mean action: 39.000 [4.000, 70.000], mean observation: 3.172 [-1.166, 10.434], loss: 1.370101, mae: 5.066920, mean_q: 5.272683
 95158/100000: episode: 9711, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.916, mean reward: 0.392 [0.348, 0.434], mean action: 57.400 [25.000, 94.000], mean observation: 3.147 [-1.362, 10.356], loss: 1.203595, mae: 5.066460, mean_q: 5.280245
 95168/100000: episode: 9712, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.204, mean reward: 0.420 [0.350, 0.483], mean action: 54.200 [10.000, 94.000], mean observation: 3.163 [-0.715, 10.275], loss: 1.615011, mae: 5.068070, mean_q: 5.284602
 95178/100000: episode: 9713, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.497, mean reward: 0.450 [0.327, 0.516], mean action: 41.200 [4.000, 81.000], mean observation: 3.160 [-1.111, 10.311], loss: 1.465184, mae: 5.067022, mean_q: 5.282136
 95188/100000: episode: 9714, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.224, mean reward: 0.422 [0.387, 0.577], mean action: 55.500 [15.000, 90.000], mean observation: 3.170 [-1.473, 10.257], loss: 1.798594, mae: 5.067963, mean_q: 5.281439
 95198/100000: episode: 9715, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.275, mean reward: 0.427 [0.376, 0.464], mean action: 46.200 [0.000, 65.000], mean observation: 3.149 [-1.210, 10.253], loss: 1.645128, mae: 5.066821, mean_q: 5.279831
 95208/100000: episode: 9716, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.107, mean reward: 0.411 [0.295, 0.526], mean action: 41.400 [9.000, 91.000], mean observation: 3.157 [-1.847, 10.365], loss: 1.372824, mae: 5.065112, mean_q: 5.275737
 95218/100000: episode: 9717, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 3.921, mean reward: 0.392 [0.318, 0.466], mean action: 55.700 [0.000, 97.000], mean observation: 3.162 [-1.347, 10.376], loss: 1.135950, mae: 5.064002, mean_q: 5.275247
 95228/100000: episode: 9718, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.401, mean reward: 0.440 [0.426, 0.466], mean action: 58.500 [4.000, 97.000], mean observation: 3.163 [-1.389, 10.387], loss: 1.304308, mae: 5.064636, mean_q: 5.276282
 95238/100000: episode: 9719, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 4.093, mean reward: 0.409 [0.357, 0.517], mean action: 48.400 [16.000, 84.000], mean observation: 3.165 [-1.106, 10.235], loss: 1.332525, mae: 5.064728, mean_q: 5.272897
 95248/100000: episode: 9720, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 3.777, mean reward: 0.378 [0.360, 0.445], mean action: 54.500 [37.000, 87.000], mean observation: 3.152 [-0.933, 10.276], loss: 1.298766, mae: 5.064485, mean_q: 5.270679
 95258/100000: episode: 9721, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.921, mean reward: 0.492 [0.412, 0.583], mean action: 62.200 [28.000, 76.000], mean observation: 3.170 [-1.411, 10.239], loss: 1.193009, mae: 5.063756, mean_q: 5.271141
 95268/100000: episode: 9722, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.916, mean reward: 0.392 [0.313, 0.503], mean action: 49.600 [16.000, 100.000], mean observation: 3.155 [-1.392, 10.217], loss: 1.295416, mae: 5.064464, mean_q: 5.270590
 95278/100000: episode: 9723, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.032, mean reward: 0.403 [0.302, 0.490], mean action: 49.700 [5.000, 68.000], mean observation: 3.172 [-1.447, 10.274], loss: 1.389892, mae: 5.064767, mean_q: 5.267422
 95288/100000: episode: 9724, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.089, mean reward: 0.409 [0.355, 0.497], mean action: 57.400 [8.000, 69.000], mean observation: 3.149 [-1.399, 10.318], loss: 1.177127, mae: 5.063825, mean_q: 5.267992
 95298/100000: episode: 9725, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.110, mean reward: 0.411 [0.374, 0.513], mean action: 49.000 [7.000, 67.000], mean observation: 3.154 [-1.055, 10.296], loss: 0.954006, mae: 5.063303, mean_q: 5.268098
 95308/100000: episode: 9726, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.278, mean reward: 0.428 [0.375, 0.504], mean action: 48.200 [7.000, 93.000], mean observation: 3.150 [-1.479, 10.253], loss: 1.183453, mae: 5.064395, mean_q: 5.267727
 95318/100000: episode: 9727, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.642, mean reward: 0.464 [0.371, 0.506], mean action: 41.800 [2.000, 93.000], mean observation: 3.160 [-1.788, 10.362], loss: 1.303205, mae: 5.065059, mean_q: 5.268551
 95328/100000: episode: 9728, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.164, mean reward: 0.416 [0.413, 0.435], mean action: 52.800 [23.000, 88.000], mean observation: 3.156 [-1.686, 10.332], loss: 1.126102, mae: 5.064381, mean_q: 5.265924
 95338/100000: episode: 9729, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 4.064, mean reward: 0.406 [0.342, 0.506], mean action: 41.600 [4.000, 49.000], mean observation: 3.160 [-1.922, 10.255], loss: 1.501327, mae: 5.065895, mean_q: 5.265832
 95348/100000: episode: 9730, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.789, mean reward: 0.479 [0.400, 0.488], mean action: 47.500 [5.000, 84.000], mean observation: 3.141 [-1.224, 10.278], loss: 0.993368, mae: 5.063897, mean_q: 5.266307
 95358/100000: episode: 9731, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.483, mean reward: 0.348 [0.293, 0.407], mean action: 57.900 [9.000, 89.000], mean observation: 3.154 [-1.206, 10.325], loss: 1.397225, mae: 5.065549, mean_q: 5.266837
 95367/100000: episode: 9732, duration: 0.167s, episode steps: 9, steps per second: 54, episode reward: 13.074, mean reward: 1.453 [0.363, 10.000], mean action: 44.667 [5.000, 98.000], mean observation: 3.160 [-1.400, 10.325], loss: 1.311468, mae: 5.065053, mean_q: 5.268104
 95377/100000: episode: 9733, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.188, mean reward: 0.419 [0.346, 0.464], mean action: 55.500 [23.000, 96.000], mean observation: 3.165 [-1.976, 10.286], loss: 1.299124, mae: 5.065048, mean_q: 5.266791
 95387/100000: episode: 9734, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 5.017, mean reward: 0.502 [0.392, 0.533], mean action: 53.300 [27.000, 90.000], mean observation: 3.168 [-1.541, 10.359], loss: 1.390779, mae: 5.065248, mean_q: 5.266685
 95397/100000: episode: 9735, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.845, mean reward: 0.384 [0.337, 0.420], mean action: 55.300 [14.000, 89.000], mean observation: 3.150 [-1.495, 10.313], loss: 1.334090, mae: 5.064778, mean_q: 5.268206
 95407/100000: episode: 9736, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.951, mean reward: 0.395 [0.395, 0.397], mean action: 41.300 [15.000, 77.000], mean observation: 3.149 [-0.937, 10.220], loss: 1.241299, mae: 5.064349, mean_q: 5.270175
 95417/100000: episode: 9737, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.923, mean reward: 0.392 [0.319, 0.473], mean action: 46.300 [5.000, 97.000], mean observation: 3.148 [-1.173, 10.271], loss: 1.214739, mae: 5.064107, mean_q: 5.271870
 95427/100000: episode: 9738, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.887, mean reward: 0.389 [0.357, 0.522], mean action: 50.100 [28.000, 80.000], mean observation: 3.157 [-1.236, 10.327], loss: 1.160350, mae: 5.063868, mean_q: 5.274185
 95437/100000: episode: 9739, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 3.824, mean reward: 0.382 [0.293, 0.497], mean action: 54.100 [8.000, 101.000], mean observation: 3.152 [-1.152, 10.221], loss: 1.360468, mae: 5.064539, mean_q: 5.276526
 95447/100000: episode: 9740, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.611, mean reward: 0.461 [0.446, 0.597], mean action: 50.900 [27.000, 99.000], mean observation: 3.150 [-1.382, 10.250], loss: 1.292348, mae: 5.064204, mean_q: 5.276269
 95457/100000: episode: 9741, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.401, mean reward: 0.440 [0.312, 0.482], mean action: 59.600 [26.000, 98.000], mean observation: 3.149 [-1.003, 10.250], loss: 1.187711, mae: 5.063783, mean_q: 5.273894
 95467/100000: episode: 9742, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.872, mean reward: 0.387 [0.304, 0.479], mean action: 49.100 [32.000, 66.000], mean observation: 3.160 [-1.565, 10.308], loss: 1.284791, mae: 5.063899, mean_q: 5.274978
 95477/100000: episode: 9743, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.790, mean reward: 0.379 [0.293, 0.513], mean action: 54.800 [19.000, 97.000], mean observation: 3.165 [-1.031, 10.424], loss: 0.994698, mae: 5.062901, mean_q: 5.272233
 95487/100000: episode: 9744, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.766, mean reward: 0.477 [0.453, 0.529], mean action: 48.400 [7.000, 72.000], mean observation: 3.156 [-1.689, 10.463], loss: 1.323212, mae: 5.064151, mean_q: 5.271298
 95497/100000: episode: 9745, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.445, mean reward: 0.445 [0.349, 0.524], mean action: 46.500 [5.000, 82.000], mean observation: 3.170 [-1.211, 10.249], loss: 1.279306, mae: 5.064046, mean_q: 5.272077
 95507/100000: episode: 9746, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.905, mean reward: 0.491 [0.480, 0.532], mean action: 59.400 [32.000, 96.000], mean observation: 3.153 [-1.497, 10.381], loss: 1.056600, mae: 5.063222, mean_q: 5.274931
 95517/100000: episode: 9747, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.993, mean reward: 0.399 [0.315, 0.507], mean action: 60.400 [41.000, 97.000], mean observation: 3.158 [-1.171, 10.277], loss: 1.463352, mae: 5.065067, mean_q: 5.277053
 95527/100000: episode: 9748, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 4.312, mean reward: 0.431 [0.431, 0.431], mean action: 67.700 [49.000, 101.000], mean observation: 3.149 [-0.621, 10.200], loss: 0.978691, mae: 5.063117, mean_q: 5.279424
 95537/100000: episode: 9749, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 4.843, mean reward: 0.484 [0.484, 0.484], mean action: 64.500 [46.000, 95.000], mean observation: 3.168 [-1.548, 10.389], loss: 1.472856, mae: 5.065311, mean_q: 5.282045
 95547/100000: episode: 9750, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.851, mean reward: 0.385 [0.372, 0.442], mean action: 42.500 [4.000, 83.000], mean observation: 3.150 [-1.622, 10.396], loss: 1.157109, mae: 5.064091, mean_q: 5.285328
 95557/100000: episode: 9751, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 3.844, mean reward: 0.384 [0.295, 0.430], mean action: 43.100 [3.000, 63.000], mean observation: 3.146 [-1.650, 10.273], loss: 1.279119, mae: 5.064281, mean_q: 5.288055
 95567/100000: episode: 9752, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 3.993, mean reward: 0.399 [0.322, 0.487], mean action: 55.300 [24.000, 99.000], mean observation: 3.155 [-1.382, 10.346], loss: 0.814609, mae: 5.062534, mean_q: 5.289821
 95577/100000: episode: 9753, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 4.082, mean reward: 0.408 [0.374, 0.524], mean action: 52.700 [10.000, 100.000], mean observation: 3.147 [-1.091, 10.344], loss: 1.522270, mae: 5.065638, mean_q: 5.291593
 95587/100000: episode: 9754, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.697, mean reward: 0.370 [0.300, 0.507], mean action: 51.200 [12.000, 98.000], mean observation: 3.161 [-1.247, 10.403], loss: 1.133583, mae: 5.064065, mean_q: 5.293328
 95597/100000: episode: 9755, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.039, mean reward: 0.404 [0.367, 0.477], mean action: 51.300 [1.000, 101.000], mean observation: 3.159 [-1.277, 10.328], loss: 1.223528, mae: 5.064551, mean_q: 5.292652
 95607/100000: episode: 9756, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.842, mean reward: 0.384 [0.342, 0.422], mean action: 42.000 [8.000, 92.000], mean observation: 3.156 [-1.483, 10.388], loss: 1.040729, mae: 5.064065, mean_q: 5.289374
 95617/100000: episode: 9757, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.035, mean reward: 0.403 [0.323, 0.438], mean action: 54.400 [28.000, 88.000], mean observation: 3.180 [-1.327, 10.314], loss: 1.323789, mae: 5.065439, mean_q: 5.285513
 95627/100000: episode: 9758, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.253, mean reward: 0.425 [0.326, 0.530], mean action: 48.400 [1.000, 100.000], mean observation: 3.158 [-1.560, 10.515], loss: 1.546522, mae: 5.066414, mean_q: 5.278867
 95637/100000: episode: 9759, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.865, mean reward: 0.386 [0.289, 0.490], mean action: 47.400 [1.000, 95.000], mean observation: 3.156 [-1.238, 10.361], loss: 1.022096, mae: 5.064267, mean_q: 5.272552
 95647/100000: episode: 9760, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.908, mean reward: 0.391 [0.341, 0.448], mean action: 50.100 [31.000, 91.000], mean observation: 3.161 [-1.141, 10.319], loss: 1.271594, mae: 5.065356, mean_q: 5.271475
 95657/100000: episode: 9761, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.013, mean reward: 0.401 [0.337, 0.490], mean action: 53.200 [8.000, 86.000], mean observation: 3.146 [-1.214, 10.289], loss: 1.222185, mae: 5.065418, mean_q: 5.271860
 95667/100000: episode: 9762, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.404, mean reward: 0.440 [0.355, 0.596], mean action: 41.900 [0.000, 66.000], mean observation: 3.152 [-1.043, 10.357], loss: 1.338448, mae: 5.065993, mean_q: 5.270562
 95677/100000: episode: 9763, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.962, mean reward: 0.396 [0.356, 0.491], mean action: 50.000 [3.000, 100.000], mean observation: 3.164 [-1.172, 10.399], loss: 0.917963, mae: 5.064680, mean_q: 5.267890
 95687/100000: episode: 9764, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.833, mean reward: 0.383 [0.336, 0.446], mean action: 56.500 [49.000, 75.000], mean observation: 3.158 [-1.472, 10.358], loss: 1.047924, mae: 5.065259, mean_q: 5.268703
 95697/100000: episode: 9765, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.330, mean reward: 0.433 [0.296, 0.501], mean action: 48.100 [33.000, 57.000], mean observation: 3.169 [-1.314, 10.297], loss: 1.453252, mae: 5.067103, mean_q: 5.269460
 95707/100000: episode: 9766, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.566, mean reward: 0.457 [0.452, 0.501], mean action: 54.700 [49.000, 95.000], mean observation: 3.137 [-1.927, 10.276], loss: 1.336092, mae: 5.066581, mean_q: 5.266488
 95717/100000: episode: 9767, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.652, mean reward: 0.465 [0.415, 0.473], mean action: 49.700 [1.000, 81.000], mean observation: 3.153 [-1.112, 10.264], loss: 1.266394, mae: 5.066264, mean_q: 5.265464
 95721/100000: episode: 9768, duration: 0.071s, episode steps: 4, steps per second: 56, episode reward: 11.362, mean reward: 2.840 [0.430, 10.000], mean action: 43.750 [5.000, 86.000], mean observation: 3.139 [-0.659, 10.163], loss: 0.986001, mae: 5.065164, mean_q: 5.263306
 95731/100000: episode: 9769, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.848, mean reward: 0.385 [0.336, 0.443], mean action: 45.200 [6.000, 67.000], mean observation: 3.160 [-2.161, 10.351], loss: 1.046134, mae: 5.065382, mean_q: 5.262540
 95741/100000: episode: 9770, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 3.813, mean reward: 0.381 [0.328, 0.450], mean action: 53.300 [10.000, 94.000], mean observation: 3.150 [-1.427, 10.370], loss: 1.300280, mae: 5.066687, mean_q: 5.262789
 95751/100000: episode: 9771, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.973, mean reward: 0.397 [0.372, 0.446], mean action: 49.400 [48.000, 54.000], mean observation: 3.153 [-1.578, 10.321], loss: 1.161505, mae: 5.066377, mean_q: 5.263824
 95761/100000: episode: 9772, duration: 0.150s, episode steps: 10, steps per second: 66, episode reward: 3.994, mean reward: 0.399 [0.335, 0.542], mean action: 57.400 [15.000, 99.000], mean observation: 3.158 [-1.952, 10.281], loss: 1.059660, mae: 5.066262, mean_q: 5.266012
 95771/100000: episode: 9773, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.822, mean reward: 0.382 [0.305, 0.439], mean action: 54.100 [17.000, 99.000], mean observation: 3.157 [-1.513, 10.367], loss: 1.093701, mae: 5.067056, mean_q: 5.268562
 95777/100000: episode: 9774, duration: 0.109s, episode steps: 6, steps per second: 55, episode reward: 11.977, mean reward: 1.996 [0.347, 10.000], mean action: 50.500 [49.000, 55.000], mean observation: 3.169 [-0.974, 10.215], loss: 1.452784, mae: 5.068853, mean_q: 5.270017
 95787/100000: episode: 9775, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 3.719, mean reward: 0.372 [0.315, 0.435], mean action: 56.300 [42.000, 100.000], mean observation: 3.147 [-1.356, 10.321], loss: 1.264247, mae: 5.068217, mean_q: 5.271596
 95797/100000: episode: 9776, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 5.031, mean reward: 0.503 [0.503, 0.503], mean action: 49.100 [46.000, 53.000], mean observation: 3.156 [-0.923, 10.423], loss: 1.179676, mae: 5.067552, mean_q: 5.273540
 95807/100000: episode: 9777, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.381, mean reward: 0.438 [0.277, 0.490], mean action: 48.600 [6.000, 88.000], mean observation: 3.158 [-0.704, 10.354], loss: 1.377892, mae: 5.068201, mean_q: 5.275916
 95817/100000: episode: 9778, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.143, mean reward: 0.414 [0.397, 0.489], mean action: 46.700 [15.000, 98.000], mean observation: 3.150 [-1.809, 10.274], loss: 1.380572, mae: 5.068521, mean_q: 5.278094
 95827/100000: episode: 9779, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.245, mean reward: 0.424 [0.408, 0.475], mean action: 55.900 [49.000, 87.000], mean observation: 3.156 [-2.492, 10.402], loss: 0.954261, mae: 5.067056, mean_q: 5.279868
 95837/100000: episode: 9780, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.909, mean reward: 0.491 [0.484, 0.520], mean action: 42.100 [11.000, 80.000], mean observation: 3.142 [-1.202, 10.323], loss: 1.480382, mae: 5.069572, mean_q: 5.280269
 95847/100000: episode: 9781, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.857, mean reward: 0.386 [0.347, 0.486], mean action: 56.300 [6.000, 97.000], mean observation: 3.169 [-1.153, 10.283], loss: 1.050716, mae: 5.067798, mean_q: 5.277764
 95857/100000: episode: 9782, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.110, mean reward: 0.411 [0.318, 0.488], mean action: 44.100 [14.000, 63.000], mean observation: 3.159 [-1.275, 10.339], loss: 1.186407, mae: 5.068749, mean_q: 5.274928
 95867/100000: episode: 9783, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.105, mean reward: 0.411 [0.306, 0.552], mean action: 44.400 [5.000, 99.000], mean observation: 3.155 [-1.758, 10.195], loss: 1.308494, mae: 5.069273, mean_q: 5.272799
 95877/100000: episode: 9784, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 4.120, mean reward: 0.412 [0.379, 0.474], mean action: 60.200 [15.000, 100.000], mean observation: 3.156 [-1.556, 10.280], loss: 1.504919, mae: 5.069971, mean_q: 5.270350
 95887/100000: episode: 9785, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.812, mean reward: 0.481 [0.367, 0.528], mean action: 41.600 [14.000, 49.000], mean observation: 3.159 [-2.126, 10.287], loss: 1.384589, mae: 5.069140, mean_q: 5.266532
 95897/100000: episode: 9786, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.797, mean reward: 0.380 [0.319, 0.493], mean action: 51.700 [49.000, 76.000], mean observation: 3.146 [-1.012, 10.232], loss: 1.373809, mae: 5.069187, mean_q: 5.264183
 95907/100000: episode: 9787, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.933, mean reward: 0.393 [0.322, 0.458], mean action: 36.500 [4.000, 49.000], mean observation: 3.154 [-1.383, 10.383], loss: 1.177420, mae: 5.068143, mean_q: 5.259445
 95917/100000: episode: 9788, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.827, mean reward: 0.383 [0.316, 0.452], mean action: 42.900 [14.000, 68.000], mean observation: 3.152 [-2.080, 10.422], loss: 1.175427, mae: 5.068205, mean_q: 5.256893
 95927/100000: episode: 9789, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.239, mean reward: 0.424 [0.376, 0.499], mean action: 51.300 [22.000, 86.000], mean observation: 3.160 [-1.676, 10.205], loss: 1.259966, mae: 5.068785, mean_q: 5.258458
 95937/100000: episode: 9790, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.992, mean reward: 0.399 [0.365, 0.502], mean action: 55.100 [49.000, 82.000], mean observation: 3.162 [-0.975, 10.303], loss: 1.306888, mae: 5.069200, mean_q: 5.261951
 95947/100000: episode: 9791, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.592, mean reward: 0.459 [0.453, 0.485], mean action: 42.700 [14.000, 93.000], mean observation: 3.157 [-1.117, 10.279], loss: 1.006586, mae: 5.067822, mean_q: 5.259267
 95957/100000: episode: 9792, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.028, mean reward: 0.403 [0.356, 0.528], mean action: 40.700 [2.000, 94.000], mean observation: 3.150 [-0.974, 10.404], loss: 1.252612, mae: 5.068758, mean_q: 5.260738
 95967/100000: episode: 9793, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.445, mean reward: 0.444 [0.423, 0.468], mean action: 45.800 [13.000, 84.000], mean observation: 3.161 [-1.011, 10.263], loss: 1.093047, mae: 5.068223, mean_q: 5.261812
 95977/100000: episode: 9794, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 3.949, mean reward: 0.395 [0.361, 0.468], mean action: 40.400 [16.000, 77.000], mean observation: 3.151 [-1.159, 10.373], loss: 1.127210, mae: 5.068806, mean_q: 5.262315
 95987/100000: episode: 9795, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.445, mean reward: 0.445 [0.415, 0.515], mean action: 48.600 [6.000, 86.000], mean observation: 3.156 [-1.246, 10.341], loss: 1.363503, mae: 5.069747, mean_q: 5.260564
 95997/100000: episode: 9796, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.412, mean reward: 0.441 [0.355, 0.565], mean action: 47.400 [2.000, 89.000], mean observation: 3.155 [-1.010, 10.279], loss: 1.324945, mae: 5.069708, mean_q: 5.258302
 96007/100000: episode: 9797, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.795, mean reward: 0.380 [0.331, 0.447], mean action: 43.600 [18.000, 76.000], mean observation: 3.152 [-1.056, 10.375], loss: 1.314695, mae: 5.069448, mean_q: 5.258335
 96017/100000: episode: 9798, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.015, mean reward: 0.402 [0.311, 0.508], mean action: 42.500 [8.000, 101.000], mean observation: 3.155 [-1.252, 10.393], loss: 1.155132, mae: 5.068574, mean_q: 5.255330
 96027/100000: episode: 9799, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.347, mean reward: 0.435 [0.411, 0.490], mean action: 48.900 [20.000, 87.000], mean observation: 3.147 [-1.618, 10.395], loss: 1.044116, mae: 5.068360, mean_q: 5.253480
 96037/100000: episode: 9800, duration: 0.208s, episode steps: 10, steps per second: 48, episode reward: 3.840, mean reward: 0.384 [0.292, 0.461], mean action: 42.600 [13.000, 90.000], mean observation: 3.161 [-1.730, 10.320], loss: 1.180926, mae: 5.068954, mean_q: 5.253081
 96047/100000: episode: 9801, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.160, mean reward: 0.416 [0.357, 0.522], mean action: 49.300 [24.000, 94.000], mean observation: 3.164 [-1.625, 10.281], loss: 0.862574, mae: 5.067963, mean_q: 5.253862
 96057/100000: episode: 9802, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.480, mean reward: 0.448 [0.382, 0.510], mean action: 48.300 [14.000, 101.000], mean observation: 3.154 [-1.763, 10.217], loss: 1.263044, mae: 5.069569, mean_q: 5.255125
 96067/100000: episode: 9803, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.749, mean reward: 0.375 [0.313, 0.434], mean action: 38.700 [21.000, 68.000], mean observation: 3.159 [-1.183, 10.293], loss: 1.330011, mae: 5.069989, mean_q: 5.256232
 96077/100000: episode: 9804, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.240, mean reward: 0.424 [0.407, 0.448], mean action: 47.200 [31.000, 76.000], mean observation: 3.151 [-2.329, 10.457], loss: 1.254409, mae: 5.069828, mean_q: 5.257429
 96087/100000: episode: 9805, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.234, mean reward: 0.423 [0.396, 0.473], mean action: 45.100 [21.000, 78.000], mean observation: 3.162 [-0.946, 10.250], loss: 0.945093, mae: 5.068629, mean_q: 5.259068
 96097/100000: episode: 9806, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.778, mean reward: 0.378 [0.295, 0.449], mean action: 56.700 [40.000, 99.000], mean observation: 3.169 [-1.254, 10.486], loss: 1.252252, mae: 5.070180, mean_q: 5.261817
 96107/100000: episode: 9807, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 3.875, mean reward: 0.388 [0.318, 0.534], mean action: 32.100 [3.000, 40.000], mean observation: 3.153 [-1.372, 10.448], loss: 1.362293, mae: 5.070762, mean_q: 5.261610
 96117/100000: episode: 9808, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.801, mean reward: 0.380 [0.336, 0.459], mean action: 44.600 [6.000, 97.000], mean observation: 3.159 [-1.541, 10.230], loss: 1.370342, mae: 5.070634, mean_q: 5.259795
 96127/100000: episode: 9809, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.218, mean reward: 0.422 [0.372, 0.588], mean action: 48.300 [9.000, 94.000], mean observation: 3.158 [-1.688, 10.264], loss: 1.119317, mae: 5.069778, mean_q: 5.259697
 96137/100000: episode: 9810, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 3.878, mean reward: 0.388 [0.290, 0.441], mean action: 36.300 [7.000, 73.000], mean observation: 3.155 [-1.104, 10.469], loss: 1.254201, mae: 5.070213, mean_q: 5.260020
 96147/100000: episode: 9811, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.058, mean reward: 0.406 [0.323, 0.480], mean action: 43.700 [6.000, 98.000], mean observation: 3.139 [-1.321, 10.309], loss: 1.200321, mae: 5.070253, mean_q: 5.261292
 96157/100000: episode: 9812, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 5.458, mean reward: 0.546 [0.546, 0.546], mean action: 46.100 [40.000, 75.000], mean observation: 3.171 [-1.516, 10.323], loss: 1.332623, mae: 5.070963, mean_q: 5.262734
 96167/100000: episode: 9813, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.051, mean reward: 0.405 [0.356, 0.478], mean action: 47.900 [2.000, 101.000], mean observation: 3.159 [-1.448, 10.325], loss: 1.056574, mae: 5.069750, mean_q: 5.261937
 96177/100000: episode: 9814, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.865, mean reward: 0.387 [0.309, 0.511], mean action: 43.100 [22.000, 66.000], mean observation: 3.151 [-1.353, 10.358], loss: 1.392848, mae: 5.071108, mean_q: 5.260508
 96187/100000: episode: 9815, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 4.252, mean reward: 0.425 [0.385, 0.518], mean action: 40.900 [17.000, 90.000], mean observation: 3.157 [-1.603, 10.318], loss: 1.219917, mae: 5.070447, mean_q: 5.261681
 96197/100000: episode: 9816, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.772, mean reward: 0.377 [0.358, 0.492], mean action: 51.800 [27.000, 90.000], mean observation: 3.166 [-1.049, 10.280], loss: 1.286134, mae: 5.070571, mean_q: 5.263457
 96207/100000: episode: 9817, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.001, mean reward: 0.400 [0.355, 0.561], mean action: 42.800 [26.000, 82.000], mean observation: 3.148 [-1.078, 10.391], loss: 0.786995, mae: 5.068785, mean_q: 5.264947
 96217/100000: episode: 9818, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.096, mean reward: 0.410 [0.369, 0.483], mean action: 38.200 [16.000, 63.000], mean observation: 3.138 [-1.382, 10.448], loss: 1.516114, mae: 5.071752, mean_q: 5.262204
 96227/100000: episode: 9819, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 3.521, mean reward: 0.352 [0.304, 0.425], mean action: 51.400 [4.000, 97.000], mean observation: 3.146 [-1.090, 10.330], loss: 1.300921, mae: 5.070769, mean_q: 5.262289
 96237/100000: episode: 9820, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.192, mean reward: 0.419 [0.287, 0.550], mean action: 45.900 [2.000, 81.000], mean observation: 3.169 [-1.222, 10.268], loss: 1.579547, mae: 5.071828, mean_q: 5.264499
 96247/100000: episode: 9821, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 3.979, mean reward: 0.398 [0.344, 0.510], mean action: 46.400 [2.000, 96.000], mean observation: 3.153 [-1.812, 10.337], loss: 1.395021, mae: 5.070835, mean_q: 5.266031
 96257/100000: episode: 9822, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.307, mean reward: 0.431 [0.412, 0.444], mean action: 43.900 [13.000, 85.000], mean observation: 3.159 [-1.324, 10.365], loss: 0.811799, mae: 5.068464, mean_q: 5.267540
 96267/100000: episode: 9823, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.066, mean reward: 0.407 [0.276, 0.491], mean action: 53.900 [20.000, 98.000], mean observation: 3.162 [-2.430, 10.350], loss: 1.257345, mae: 5.070577, mean_q: 5.265607
 96277/100000: episode: 9824, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.081, mean reward: 0.408 [0.334, 0.466], mean action: 43.600 [40.000, 75.000], mean observation: 3.161 [-1.657, 10.375], loss: 1.181252, mae: 5.070422, mean_q: 5.264716
 96287/100000: episode: 9825, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.830, mean reward: 0.383 [0.382, 0.386], mean action: 47.900 [40.000, 79.000], mean observation: 3.165 [-1.325, 10.389], loss: 1.257152, mae: 5.070939, mean_q: 5.265573
 96297/100000: episode: 9826, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.099, mean reward: 0.410 [0.343, 0.492], mean action: 51.800 [13.000, 95.000], mean observation: 3.172 [-1.160, 10.359], loss: 0.988260, mae: 5.069745, mean_q: 5.266945
 96307/100000: episode: 9827, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.250, mean reward: 0.425 [0.419, 0.454], mean action: 41.100 [26.000, 67.000], mean observation: 3.167 [-1.690, 10.455], loss: 0.973670, mae: 5.070045, mean_q: 5.266653
 96317/100000: episode: 9828, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.028, mean reward: 0.403 [0.318, 0.496], mean action: 29.600 [8.000, 46.000], mean observation: 3.159 [-1.021, 10.369], loss: 1.123681, mae: 5.070657, mean_q: 5.263639
 96327/100000: episode: 9829, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.055, mean reward: 0.405 [0.336, 0.521], mean action: 48.200 [27.000, 73.000], mean observation: 3.147 [-1.493, 10.304], loss: 1.441502, mae: 5.072003, mean_q: 5.260648
 96337/100000: episode: 9830, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.108, mean reward: 0.411 [0.375, 0.494], mean action: 47.100 [30.000, 101.000], mean observation: 3.156 [-1.317, 10.361], loss: 1.488476, mae: 5.072024, mean_q: 5.261240
 96347/100000: episode: 9831, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.808, mean reward: 0.381 [0.329, 0.421], mean action: 45.200 [2.000, 95.000], mean observation: 3.163 [-1.455, 10.338], loss: 1.333350, mae: 5.071132, mean_q: 5.262222
 96357/100000: episode: 9832, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.767, mean reward: 0.377 [0.321, 0.450], mean action: 45.100 [40.000, 91.000], mean observation: 3.172 [-1.596, 10.357], loss: 1.245912, mae: 5.070562, mean_q: 5.262975
 96367/100000: episode: 9833, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.199, mean reward: 0.420 [0.373, 0.465], mean action: 49.000 [40.000, 80.000], mean observation: 3.176 [-1.079, 10.370], loss: 1.298619, mae: 5.070341, mean_q: 5.263840
 96377/100000: episode: 9834, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 4.463, mean reward: 0.446 [0.335, 0.553], mean action: 50.900 [6.000, 93.000], mean observation: 3.162 [-1.425, 10.368], loss: 1.355560, mae: 5.070605, mean_q: 5.263969
 96387/100000: episode: 9835, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.568, mean reward: 0.457 [0.381, 0.497], mean action: 54.100 [26.000, 92.000], mean observation: 3.163 [-1.243, 10.306], loss: 1.464586, mae: 5.070819, mean_q: 5.261945
 96397/100000: episode: 9836, duration: 0.126s, episode steps: 10, steps per second: 80, episode reward: 3.778, mean reward: 0.378 [0.353, 0.407], mean action: 68.400 [11.000, 92.000], mean observation: 3.159 [-0.696, 10.351], loss: 1.225212, mae: 5.069663, mean_q: 5.262501
 96407/100000: episode: 9837, duration: 0.114s, episode steps: 10, steps per second: 87, episode reward: 4.575, mean reward: 0.458 [0.394, 0.467], mean action: 75.500 [14.000, 100.000], mean observation: 3.157 [-1.141, 10.149], loss: 1.214556, mae: 5.069483, mean_q: 5.264091
 96417/100000: episode: 9838, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 3.946, mean reward: 0.395 [0.342, 0.465], mean action: 61.600 [4.000, 101.000], mean observation: 3.153 [-1.372, 10.309], loss: 0.831402, mae: 5.067975, mean_q: 5.265415
 96427/100000: episode: 9839, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 4.085, mean reward: 0.409 [0.356, 0.431], mean action: 84.400 [20.000, 92.000], mean observation: 3.161 [-1.209, 10.338], loss: 1.022269, mae: 5.069123, mean_q: 5.266303
 96437/100000: episode: 9840, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 4.014, mean reward: 0.401 [0.368, 0.511], mean action: 83.300 [34.000, 92.000], mean observation: 3.155 [-2.055, 10.239], loss: 0.993350, mae: 5.069323, mean_q: 5.267084
 96447/100000: episode: 9841, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 3.901, mean reward: 0.390 [0.304, 0.444], mean action: 58.700 [1.000, 98.000], mean observation: 3.152 [-1.190, 10.370], loss: 1.116230, mae: 5.070018, mean_q: 5.268047
 96457/100000: episode: 9842, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 13.775, mean reward: 1.378 [0.352, 10.000], mean action: 46.500 [0.000, 92.000], mean observation: 3.171 [-1.088, 10.372], loss: 1.177525, mae: 5.070717, mean_q: 5.269586
 96467/100000: episode: 9843, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 3.709, mean reward: 0.371 [0.297, 0.397], mean action: 69.200 [5.000, 92.000], mean observation: 3.152 [-1.569, 10.238], loss: 1.737351, mae: 5.072917, mean_q: 5.270688
 96477/100000: episode: 9844, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 3.753, mean reward: 0.375 [0.359, 0.439], mean action: 75.000 [33.000, 98.000], mean observation: 3.159 [-1.029, 10.319], loss: 1.362366, mae: 5.071265, mean_q: 5.272170
 96487/100000: episode: 9845, duration: 0.098s, episode steps: 10, steps per second: 102, episode reward: 4.095, mean reward: 0.410 [0.408, 0.411], mean action: 77.900 [33.000, 92.000], mean observation: 3.162 [-1.235, 10.239], loss: 1.079547, mae: 5.070084, mean_q: 5.268868
 96497/100000: episode: 9846, duration: 0.138s, episode steps: 10, steps per second: 72, episode reward: 4.134, mean reward: 0.413 [0.385, 0.518], mean action: 62.200 [3.000, 92.000], mean observation: 3.160 [-1.547, 10.400], loss: 1.020015, mae: 5.069923, mean_q: 5.265605
 96507/100000: episode: 9847, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.643, mean reward: 0.464 [0.433, 0.494], mean action: 69.100 [19.000, 92.000], mean observation: 3.129 [-1.018, 10.397], loss: 1.140500, mae: 5.070558, mean_q: 5.265071
 96517/100000: episode: 9848, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 3.973, mean reward: 0.397 [0.390, 0.449], mean action: 81.800 [58.000, 92.000], mean observation: 3.161 [-1.134, 10.382], loss: 1.179814, mae: 5.070566, mean_q: 5.266917
 96527/100000: episode: 9849, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.872, mean reward: 0.387 [0.295, 0.461], mean action: 71.600 [11.000, 92.000], mean observation: 3.162 [-2.238, 10.400], loss: 0.868336, mae: 5.069448, mean_q: 5.268435
 96537/100000: episode: 9850, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.680, mean reward: 0.368 [0.349, 0.460], mean action: 61.400 [0.000, 92.000], mean observation: 3.158 [-1.359, 10.421], loss: 1.418263, mae: 5.071866, mean_q: 5.269587
 96547/100000: episode: 9851, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 4.354, mean reward: 0.435 [0.435, 0.435], mean action: 87.700 [65.000, 98.000], mean observation: 3.159 [-1.376, 10.387], loss: 1.233950, mae: 5.071213, mean_q: 5.271542
 96557/100000: episode: 9852, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 4.503, mean reward: 0.450 [0.338, 0.463], mean action: 67.900 [7.000, 92.000], mean observation: 3.152 [-1.372, 10.315], loss: 1.600098, mae: 5.072522, mean_q: 5.272679
 96567/100000: episode: 9853, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 4.117, mean reward: 0.412 [0.335, 0.448], mean action: 70.400 [3.000, 93.000], mean observation: 3.170 [-0.792, 10.350], loss: 1.108735, mae: 5.070580, mean_q: 5.274157
 96577/100000: episode: 9854, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 4.002, mean reward: 0.400 [0.354, 0.489], mean action: 73.500 [15.000, 92.000], mean observation: 3.163 [-1.501, 10.474], loss: 0.992466, mae: 5.070123, mean_q: 5.274797
 96587/100000: episode: 9855, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 4.260, mean reward: 0.426 [0.389, 0.513], mean action: 80.700 [27.000, 92.000], mean observation: 3.148 [-1.888, 10.333], loss: 1.576445, mae: 5.072484, mean_q: 5.271017
 96597/100000: episode: 9856, duration: 0.112s, episode steps: 10, steps per second: 90, episode reward: 5.078, mean reward: 0.508 [0.504, 0.546], mean action: 80.800 [32.000, 94.000], mean observation: 3.155 [-1.095, 10.346], loss: 1.294123, mae: 5.071052, mean_q: 5.264946
 96607/100000: episode: 9857, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.475, mean reward: 0.447 [0.397, 0.551], mean action: 45.700 [7.000, 92.000], mean observation: 3.158 [-1.212, 10.423], loss: 1.238522, mae: 5.070395, mean_q: 5.262744
 96617/100000: episode: 9858, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.389, mean reward: 0.439 [0.431, 0.440], mean action: 42.100 [17.000, 72.000], mean observation: 3.152 [-1.198, 10.335], loss: 1.077929, mae: 5.069751, mean_q: 5.264008
 96627/100000: episode: 9859, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.394, mean reward: 0.439 [0.312, 0.548], mean action: 45.300 [6.000, 99.000], mean observation: 3.161 [-0.926, 10.422], loss: 1.258865, mae: 5.070229, mean_q: 5.262529
 96637/100000: episode: 9860, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.116, mean reward: 0.412 [0.382, 0.494], mean action: 45.200 [14.000, 80.000], mean observation: 3.158 [-2.109, 10.393], loss: 1.219054, mae: 5.069953, mean_q: 5.260975
 96647/100000: episode: 9861, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.205, mean reward: 0.420 [0.419, 0.431], mean action: 59.000 [24.000, 99.000], mean observation: 3.144 [-1.136, 10.212], loss: 1.387806, mae: 5.070269, mean_q: 5.262505
 96657/100000: episode: 9862, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 4.244, mean reward: 0.424 [0.401, 0.524], mean action: 39.800 [4.000, 67.000], mean observation: 3.153 [-1.715, 10.168], loss: 1.252419, mae: 5.069620, mean_q: 5.261649
 96667/100000: episode: 9863, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 4.003, mean reward: 0.400 [0.336, 0.491], mean action: 43.700 [13.000, 90.000], mean observation: 3.157 [-1.226, 10.313], loss: 1.183851, mae: 5.069388, mean_q: 5.260170
 96677/100000: episode: 9864, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 4.623, mean reward: 0.462 [0.410, 0.508], mean action: 41.700 [3.000, 88.000], mean observation: 3.165 [-1.603, 10.372], loss: 1.221816, mae: 5.069602, mean_q: 5.261200
 96687/100000: episode: 9865, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.850, mean reward: 0.385 [0.305, 0.472], mean action: 39.300 [4.000, 67.000], mean observation: 3.150 [-1.994, 10.222], loss: 0.852538, mae: 5.068529, mean_q: 5.262150
 96697/100000: episode: 9866, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.446, mean reward: 0.445 [0.371, 0.543], mean action: 46.500 [34.000, 80.000], mean observation: 3.153 [-1.382, 10.359], loss: 0.949212, mae: 5.069537, mean_q: 5.263809
 96707/100000: episode: 9867, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.303, mean reward: 0.430 [0.395, 0.458], mean action: 51.400 [0.000, 87.000], mean observation: 3.159 [-1.256, 10.458], loss: 1.338591, mae: 5.071208, mean_q: 5.266219
 96717/100000: episode: 9868, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.906, mean reward: 0.391 [0.323, 0.571], mean action: 43.700 [4.000, 94.000], mean observation: 3.154 [-1.825, 10.158], loss: 1.088813, mae: 5.070302, mean_q: 5.269221
 96727/100000: episode: 9869, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 5.014, mean reward: 0.501 [0.501, 0.501], mean action: 57.300 [34.000, 100.000], mean observation: 3.143 [-1.492, 10.467], loss: 1.257026, mae: 5.070906, mean_q: 5.271060
 96737/100000: episode: 9870, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.591, mean reward: 0.459 [0.319, 0.558], mean action: 47.100 [22.000, 97.000], mean observation: 3.160 [-1.419, 10.364], loss: 1.267542, mae: 5.071135, mean_q: 5.272794
 96747/100000: episode: 9871, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.452, mean reward: 0.445 [0.426, 0.522], mean action: 53.000 [7.000, 98.000], mean observation: 3.160 [-1.930, 10.312], loss: 1.163546, mae: 5.070971, mean_q: 5.274581
 96757/100000: episode: 9872, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.923, mean reward: 0.392 [0.386, 0.450], mean action: 48.400 [28.000, 99.000], mean observation: 3.159 [-1.524, 10.322], loss: 1.502338, mae: 5.072158, mean_q: 5.277011
 96767/100000: episode: 9873, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.965, mean reward: 0.397 [0.327, 0.496], mean action: 42.800 [6.000, 99.000], mean observation: 3.157 [-1.226, 10.384], loss: 1.055739, mae: 5.070337, mean_q: 5.279790
 96777/100000: episode: 9874, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.178, mean reward: 0.418 [0.345, 0.461], mean action: 45.600 [14.000, 94.000], mean observation: 3.158 [-1.858, 10.319], loss: 0.991978, mae: 5.070502, mean_q: 5.282556
 96787/100000: episode: 9875, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.178, mean reward: 0.418 [0.353, 0.460], mean action: 46.700 [40.000, 70.000], mean observation: 3.163 [-0.987, 10.372], loss: 1.118096, mae: 5.071159, mean_q: 5.284865
 96797/100000: episode: 9876, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 4.035, mean reward: 0.404 [0.355, 0.543], mean action: 53.600 [6.000, 94.000], mean observation: 3.156 [-1.950, 10.541], loss: 1.093954, mae: 5.071322, mean_q: 5.287508
 96807/100000: episode: 9877, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.668, mean reward: 0.467 [0.454, 0.519], mean action: 45.800 [38.000, 74.000], mean observation: 3.160 [-0.744, 10.278], loss: 1.405455, mae: 5.072907, mean_q: 5.290058
 96817/100000: episode: 9878, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.699, mean reward: 0.370 [0.303, 0.467], mean action: 48.100 [18.000, 101.000], mean observation: 3.157 [-1.300, 10.283], loss: 1.735243, mae: 5.073956, mean_q: 5.289006
 96827/100000: episode: 9879, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.697, mean reward: 0.470 [0.458, 0.517], mean action: 45.200 [32.000, 79.000], mean observation: 3.152 [-1.046, 10.352], loss: 1.122947, mae: 5.071537, mean_q: 5.286478
 96837/100000: episode: 9880, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.558, mean reward: 0.456 [0.405, 0.493], mean action: 56.500 [40.000, 95.000], mean observation: 3.162 [-1.274, 10.281], loss: 1.301468, mae: 5.072089, mean_q: 5.284548
 96847/100000: episode: 9881, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.251, mean reward: 0.425 [0.342, 0.523], mean action: 63.200 [38.000, 90.000], mean observation: 3.154 [-0.872, 10.401], loss: 1.028471, mae: 5.071063, mean_q: 5.284517
 96857/100000: episode: 9882, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 3.626, mean reward: 0.363 [0.331, 0.421], mean action: 43.600 [7.000, 98.000], mean observation: 3.173 [-0.927, 10.357], loss: 1.029109, mae: 5.071406, mean_q: 5.283832
 96867/100000: episode: 9883, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.307, mean reward: 0.431 [0.385, 0.521], mean action: 46.700 [0.000, 96.000], mean observation: 3.155 [-1.453, 10.433], loss: 1.100440, mae: 5.071906, mean_q: 5.281177
 96877/100000: episode: 9884, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.414, mean reward: 0.441 [0.292, 0.491], mean action: 43.700 [6.000, 93.000], mean observation: 3.145 [-1.601, 10.282], loss: 1.098050, mae: 5.072051, mean_q: 5.281364
 96887/100000: episode: 9885, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 4.530, mean reward: 0.453 [0.453, 0.453], mean action: 54.100 [40.000, 100.000], mean observation: 3.153 [-1.851, 10.326], loss: 1.222543, mae: 5.072826, mean_q: 5.283658
 96897/100000: episode: 9886, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.394, mean reward: 0.439 [0.429, 0.505], mean action: 42.200 [0.000, 75.000], mean observation: 3.147 [-1.138, 10.406], loss: 1.385279, mae: 5.073400, mean_q: 5.285038
 96907/100000: episode: 9887, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.850, mean reward: 0.385 [0.331, 0.444], mean action: 41.300 [4.000, 92.000], mean observation: 3.160 [-1.488, 10.245], loss: 1.507421, mae: 5.073637, mean_q: 5.280209
 96917/100000: episode: 9888, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 3.941, mean reward: 0.394 [0.367, 0.488], mean action: 40.700 [2.000, 87.000], mean observation: 3.155 [-1.094, 10.311], loss: 1.282603, mae: 5.072871, mean_q: 5.277617
 96927/100000: episode: 9889, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.942, mean reward: 0.394 [0.299, 0.515], mean action: 45.800 [6.000, 89.000], mean observation: 3.156 [-1.488, 10.347], loss: 1.133041, mae: 5.072273, mean_q: 5.270833
 96937/100000: episode: 9890, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.184, mean reward: 0.418 [0.322, 0.558], mean action: 43.100 [26.000, 86.000], mean observation: 3.142 [-1.417, 10.264], loss: 1.101122, mae: 5.072598, mean_q: 5.265526
 96947/100000: episode: 9891, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 3.808, mean reward: 0.381 [0.317, 0.460], mean action: 50.800 [32.000, 93.000], mean observation: 3.167 [-1.227, 10.212], loss: 1.099429, mae: 5.072907, mean_q: 5.265679
 96957/100000: episode: 9892, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.072, mean reward: 0.407 [0.318, 0.523], mean action: 50.600 [40.000, 97.000], mean observation: 3.165 [-2.125, 10.317], loss: 1.334901, mae: 5.074074, mean_q: 5.261646
 96967/100000: episode: 9893, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 4.264, mean reward: 0.426 [0.389, 0.451], mean action: 76.200 [9.000, 98.000], mean observation: 3.159 [-1.484, 10.213], loss: 1.235363, mae: 5.073721, mean_q: 5.258757
 96977/100000: episode: 9894, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 4.132, mean reward: 0.413 [0.373, 0.479], mean action: 63.200 [5.000, 98.000], mean observation: 3.153 [-1.235, 10.360], loss: 1.148248, mae: 5.073545, mean_q: 5.259994
 96987/100000: episode: 9895, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.044, mean reward: 0.404 [0.386, 0.422], mean action: 42.700 [15.000, 98.000], mean observation: 3.163 [-1.209, 10.351], loss: 1.334418, mae: 5.074481, mean_q: 5.261685
 96997/100000: episode: 9896, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.151, mean reward: 0.415 [0.408, 0.434], mean action: 53.900 [1.000, 95.000], mean observation: 3.154 [-1.318, 10.175], loss: 1.267062, mae: 5.074211, mean_q: 5.262503
 97007/100000: episode: 9897, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.328, mean reward: 0.433 [0.311, 0.500], mean action: 41.700 [5.000, 91.000], mean observation: 3.158 [-1.907, 10.265], loss: 1.200666, mae: 5.073831, mean_q: 5.263887
 97017/100000: episode: 9898, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.986, mean reward: 0.399 [0.317, 0.495], mean action: 42.400 [23.000, 95.000], mean observation: 3.162 [-1.442, 10.197], loss: 1.110484, mae: 5.073594, mean_q: 5.260418
 97027/100000: episode: 9899, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.067, mean reward: 0.407 [0.355, 0.480], mean action: 27.800 [0.000, 60.000], mean observation: 3.155 [-1.525, 10.346], loss: 1.081958, mae: 5.073580, mean_q: 5.259671
 97037/100000: episode: 9900, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 3.777, mean reward: 0.378 [0.362, 0.412], mean action: 54.900 [40.000, 90.000], mean observation: 3.176 [-1.020, 10.382], loss: 0.922625, mae: 5.073552, mean_q: 5.261909
 97047/100000: episode: 9901, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.220, mean reward: 0.422 [0.352, 0.584], mean action: 56.700 [28.000, 101.000], mean observation: 3.145 [-1.103, 10.374], loss: 1.087530, mae: 5.074830, mean_q: 5.263937
 97057/100000: episode: 9902, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.030, mean reward: 0.403 [0.262, 0.525], mean action: 50.300 [26.000, 100.000], mean observation: 3.146 [-1.664, 10.225], loss: 1.114031, mae: 5.075446, mean_q: 5.265830
 97067/100000: episode: 9903, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.660, mean reward: 0.366 [0.321, 0.453], mean action: 50.500 [10.000, 98.000], mean observation: 3.157 [-1.804, 10.267], loss: 1.183858, mae: 5.075760, mean_q: 5.267142
 97077/100000: episode: 9904, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.137, mean reward: 0.414 [0.322, 0.564], mean action: 35.100 [2.000, 73.000], mean observation: 3.152 [-1.912, 10.605], loss: 1.297811, mae: 5.076537, mean_q: 5.266419
 97087/100000: episode: 9905, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 3.868, mean reward: 0.387 [0.319, 0.506], mean action: 57.500 [23.000, 96.000], mean observation: 3.150 [-1.342, 10.551], loss: 1.093939, mae: 5.075808, mean_q: 5.264539
 97097/100000: episode: 9906, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.843, mean reward: 0.384 [0.328, 0.425], mean action: 33.200 [15.000, 81.000], mean observation: 3.157 [-1.606, 10.353], loss: 1.044869, mae: 5.075639, mean_q: 5.264718
 97107/100000: episode: 9907, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.251, mean reward: 0.425 [0.351, 0.487], mean action: 31.500 [6.000, 77.000], mean observation: 3.156 [-1.705, 10.562], loss: 1.211329, mae: 5.076224, mean_q: 5.266398
 97117/100000: episode: 9908, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.162, mean reward: 0.416 [0.368, 0.475], mean action: 45.100 [23.000, 84.000], mean observation: 3.156 [-1.502, 10.413], loss: 1.088689, mae: 5.076047, mean_q: 5.267818
 97127/100000: episode: 9909, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.076, mean reward: 0.408 [0.324, 0.473], mean action: 38.900 [7.000, 88.000], mean observation: 3.147 [-0.989, 10.390], loss: 1.054296, mae: 5.076061, mean_q: 5.268620
 97137/100000: episode: 9910, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.334, mean reward: 0.433 [0.324, 0.500], mean action: 41.500 [17.000, 87.000], mean observation: 3.158 [-1.076, 10.385], loss: 0.981473, mae: 5.076003, mean_q: 5.269029
 97147/100000: episode: 9911, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.910, mean reward: 0.391 [0.291, 0.492], mean action: 19.200 [2.000, 60.000], mean observation: 3.157 [-2.556, 10.308], loss: 1.298793, mae: 5.077450, mean_q: 5.265733
 97157/100000: episode: 9912, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.458, mean reward: 0.346 [0.320, 0.407], mean action: 45.000 [23.000, 98.000], mean observation: 3.163 [-1.850, 10.342], loss: 0.913561, mae: 5.075955, mean_q: 5.261185
 97167/100000: episode: 9913, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 3.907, mean reward: 0.391 [0.354, 0.452], mean action: 69.100 [11.000, 100.000], mean observation: 3.150 [-1.215, 10.256], loss: 1.345107, mae: 5.077954, mean_q: 5.258994
 97177/100000: episode: 9914, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 4.363, mean reward: 0.436 [0.338, 0.502], mean action: 50.400 [23.000, 95.000], mean observation: 3.161 [-1.151, 10.267], loss: 1.063154, mae: 5.076987, mean_q: 5.259624
 97187/100000: episode: 9915, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 3.977, mean reward: 0.398 [0.375, 0.493], mean action: 66.300 [23.000, 98.000], mean observation: 3.139 [-0.756, 10.438], loss: 1.168485, mae: 5.077109, mean_q: 5.260241
 97197/100000: episode: 9916, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 4.186, mean reward: 0.419 [0.384, 0.446], mean action: 81.000 [26.000, 98.000], mean observation: 3.170 [-1.655, 10.372], loss: 1.253744, mae: 5.077904, mean_q: 5.261302
 97207/100000: episode: 9917, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 3.725, mean reward: 0.372 [0.350, 0.386], mean action: 91.200 [62.000, 98.000], mean observation: 3.151 [-1.190, 10.420], loss: 1.010325, mae: 5.076905, mean_q: 5.262627
 97217/100000: episode: 9918, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.303, mean reward: 0.430 [0.343, 0.535], mean action: 60.700 [6.000, 98.000], mean observation: 3.154 [-1.084, 10.309], loss: 1.192980, mae: 5.077876, mean_q: 5.264392
 97227/100000: episode: 9919, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.265, mean reward: 0.426 [0.380, 0.482], mean action: 73.000 [10.000, 101.000], mean observation: 3.156 [-1.193, 10.283], loss: 1.361780, mae: 5.078625, mean_q: 5.266647
 97237/100000: episode: 9920, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 4.137, mean reward: 0.414 [0.346, 0.497], mean action: 81.300 [23.000, 98.000], mean observation: 3.162 [-1.208, 10.334], loss: 1.127093, mae: 5.077765, mean_q: 5.269034
 97247/100000: episode: 9921, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 4.260, mean reward: 0.426 [0.343, 0.551], mean action: 74.000 [8.000, 98.000], mean observation: 3.156 [-1.463, 10.476], loss: 1.275242, mae: 5.078416, mean_q: 5.270513
 97257/100000: episode: 9922, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.124, mean reward: 0.412 [0.394, 0.465], mean action: 67.000 [18.000, 98.000], mean observation: 3.171 [-1.437, 10.430], loss: 0.973757, mae: 5.077063, mean_q: 5.268809
 97267/100000: episode: 9923, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 3.919, mean reward: 0.392 [0.352, 0.471], mean action: 81.300 [4.000, 98.000], mean observation: 3.150 [-1.534, 10.266], loss: 1.163814, mae: 5.077927, mean_q: 5.269381
 97277/100000: episode: 9924, duration: 0.108s, episode steps: 10, steps per second: 92, episode reward: 3.757, mean reward: 0.376 [0.334, 0.499], mean action: 76.700 [13.000, 101.000], mean observation: 3.160 [-1.807, 10.396], loss: 1.263418, mae: 5.078334, mean_q: 5.270890
 97278/100000: episode: 9925, duration: 0.026s, episode steps: 1, steps per second: 39, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 56.000 [56.000, 56.000], mean observation: 3.179 [-0.959, 10.488], loss: 0.819645, mae: 5.076632, mean_q: 5.272429
 97288/100000: episode: 9926, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.540, mean reward: 0.454 [0.419, 0.557], mean action: 60.500 [3.000, 98.000], mean observation: 3.159 [-1.376, 10.445], loss: 1.092483, mae: 5.077867, mean_q: 5.274176
 97298/100000: episode: 9927, duration: 0.094s, episode steps: 10, steps per second: 107, episode reward: 3.889, mean reward: 0.389 [0.312, 0.440], mean action: 87.000 [6.000, 101.000], mean observation: 3.157 [-1.180, 10.294], loss: 1.255803, mae: 5.078405, mean_q: 5.277542
 97308/100000: episode: 9928, duration: 0.141s, episode steps: 10, steps per second: 71, episode reward: 4.119, mean reward: 0.412 [0.372, 0.477], mean action: 71.500 [35.000, 98.000], mean observation: 3.147 [-1.538, 10.288], loss: 1.347388, mae: 5.078681, mean_q: 5.277672
 97318/100000: episode: 9929, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: 3.547, mean reward: 0.355 [0.293, 0.429], mean action: 68.900 [4.000, 98.000], mean observation: 3.149 [-1.520, 10.327], loss: 1.418088, mae: 5.078604, mean_q: 5.275578
 97328/100000: episode: 9930, duration: 0.107s, episode steps: 10, steps per second: 94, episode reward: 4.258, mean reward: 0.426 [0.426, 0.426], mean action: 89.400 [58.000, 98.000], mean observation: 3.144 [-0.850, 10.351], loss: 1.560197, mae: 5.079076, mean_q: 5.267897
 97338/100000: episode: 9931, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 3.780, mean reward: 0.378 [0.318, 0.423], mean action: 86.700 [32.000, 98.000], mean observation: 3.155 [-0.662, 10.223], loss: 1.408012, mae: 5.078286, mean_q: 5.265356
 97348/100000: episode: 9932, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 4.259, mean reward: 0.426 [0.426, 0.426], mean action: 88.900 [28.000, 100.000], mean observation: 3.166 [-1.630, 10.379], loss: 1.604588, mae: 5.078680, mean_q: 5.266043
 97358/100000: episode: 9933, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 4.103, mean reward: 0.410 [0.384, 0.451], mean action: 70.500 [11.000, 98.000], mean observation: 3.158 [-1.781, 10.375], loss: 1.335363, mae: 5.077523, mean_q: 5.267484
 97368/100000: episode: 9934, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.717, mean reward: 0.372 [0.328, 0.411], mean action: 56.600 [10.000, 98.000], mean observation: 3.154 [-2.661, 10.421], loss: 1.327449, mae: 5.077252, mean_q: 5.269238
 97378/100000: episode: 9935, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.351, mean reward: 0.435 [0.423, 0.483], mean action: 70.700 [17.000, 101.000], mean observation: 3.150 [-1.654, 10.430], loss: 1.285922, mae: 5.076908, mean_q: 5.270272
 97388/100000: episode: 9936, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 4.062, mean reward: 0.406 [0.384, 0.447], mean action: 80.600 [13.000, 98.000], mean observation: 3.177 [-1.305, 10.442], loss: 1.302669, mae: 5.076965, mean_q: 5.267856
 97398/100000: episode: 9937, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 3.915, mean reward: 0.391 [0.373, 0.434], mean action: 79.800 [6.000, 98.000], mean observation: 3.143 [-0.899, 10.269], loss: 1.372474, mae: 5.077255, mean_q: 5.264225
 97408/100000: episode: 9938, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 3.904, mean reward: 0.390 [0.367, 0.414], mean action: 82.300 [26.000, 98.000], mean observation: 3.149 [-1.562, 10.306], loss: 1.398196, mae: 5.077230, mean_q: 5.261346
 97413/100000: episode: 9939, duration: 0.079s, episode steps: 5, steps per second: 63, episode reward: 11.922, mean reward: 2.384 [0.435, 10.000], mean action: 66.600 [23.000, 98.000], mean observation: 3.156 [-0.764, 10.242], loss: 1.378424, mae: 5.077195, mean_q: 5.256402
 97423/100000: episode: 9940, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.047, mean reward: 0.405 [0.331, 0.526], mean action: 32.300 [18.000, 79.000], mean observation: 3.155 [-1.497, 10.383], loss: 1.399211, mae: 5.077106, mean_q: 5.256618
 97433/100000: episode: 9941, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.400, mean reward: 0.440 [0.401, 0.508], mean action: 28.900 [2.000, 80.000], mean observation: 3.151 [-1.550, 10.439], loss: 1.397072, mae: 5.076922, mean_q: 5.257272
 97443/100000: episode: 9942, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.513, mean reward: 0.451 [0.412, 0.554], mean action: 28.400 [23.000, 72.000], mean observation: 3.170 [-1.500, 10.244], loss: 1.007877, mae: 5.075152, mean_q: 5.255820
 97453/100000: episode: 9943, duration: 0.201s, episode steps: 10, steps per second: 50, episode reward: 3.968, mean reward: 0.397 [0.313, 0.480], mean action: 29.300 [17.000, 92.000], mean observation: 3.165 [-0.938, 10.565], loss: 1.041508, mae: 5.075506, mean_q: 5.252964
 97463/100000: episode: 9944, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.280, mean reward: 0.428 [0.348, 0.533], mean action: 31.000 [1.000, 84.000], mean observation: 3.153 [-1.948, 10.206], loss: 1.251661, mae: 5.076467, mean_q: 5.252680
 97473/100000: episode: 9945, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.402, mean reward: 0.440 [0.389, 0.482], mean action: 46.500 [23.000, 99.000], mean observation: 3.164 [-1.136, 10.580], loss: 1.594495, mae: 5.077731, mean_q: 5.252819
 97483/100000: episode: 9946, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 4.061, mean reward: 0.406 [0.301, 0.461], mean action: 45.300 [21.000, 89.000], mean observation: 3.151 [-2.044, 10.351], loss: 1.220303, mae: 5.075878, mean_q: 5.253594
 97493/100000: episode: 9947, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.476, mean reward: 0.448 [0.375, 0.582], mean action: 42.700 [8.000, 79.000], mean observation: 3.151 [-1.027, 10.316], loss: 1.163391, mae: 5.075683, mean_q: 5.254987
 97503/100000: episode: 9948, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.331, mean reward: 0.433 [0.324, 0.506], mean action: 26.100 [23.000, 54.000], mean observation: 3.156 [-1.714, 10.321], loss: 1.564546, mae: 5.077127, mean_q: 5.256971
 97513/100000: episode: 9949, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.920, mean reward: 0.392 [0.299, 0.510], mean action: 27.200 [5.000, 50.000], mean observation: 3.150 [-1.758, 10.311], loss: 1.286397, mae: 5.076032, mean_q: 5.258465
 97514/100000: episode: 9950, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 23.000 [23.000, 23.000], mean observation: 3.140 [-0.849, 10.100], loss: 1.848952, mae: 5.078024, mean_q: 5.259317
 97524/100000: episode: 9951, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 3.945, mean reward: 0.394 [0.361, 0.451], mean action: 43.800 [23.000, 84.000], mean observation: 3.166 [-1.447, 10.309], loss: 1.469349, mae: 5.076363, mean_q: 5.259870
 97534/100000: episode: 9952, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.186, mean reward: 0.419 [0.378, 0.453], mean action: 30.100 [6.000, 66.000], mean observation: 3.154 [-2.196, 10.294], loss: 1.105935, mae: 5.074672, mean_q: 5.260423
 97544/100000: episode: 9953, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.269, mean reward: 0.427 [0.300, 0.494], mean action: 46.200 [23.000, 94.000], mean observation: 3.164 [-1.963, 10.314], loss: 1.498000, mae: 5.076170, mean_q: 5.261359
 97554/100000: episode: 9954, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.325, mean reward: 0.432 [0.419, 0.525], mean action: 38.700 [15.000, 95.000], mean observation: 3.162 [-1.392, 10.289], loss: 1.109699, mae: 5.074583, mean_q: 5.262525
 97564/100000: episode: 9955, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.082, mean reward: 0.408 [0.324, 0.516], mean action: 27.400 [8.000, 82.000], mean observation: 3.162 [-1.413, 10.416], loss: 1.565700, mae: 5.075953, mean_q: 5.264103
 97574/100000: episode: 9956, duration: 0.229s, episode steps: 10, steps per second: 44, episode reward: 4.336, mean reward: 0.434 [0.401, 0.516], mean action: 28.800 [1.000, 63.000], mean observation: 3.150 [-1.400, 10.290], loss: 1.048421, mae: 5.073833, mean_q: 5.265192
 97584/100000: episode: 9957, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.810, mean reward: 0.381 [0.337, 0.463], mean action: 34.600 [18.000, 97.000], mean observation: 3.150 [-1.079, 10.349], loss: 1.295452, mae: 5.074708, mean_q: 5.264905
 97594/100000: episode: 9958, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.823, mean reward: 0.382 [0.303, 0.476], mean action: 46.900 [11.000, 98.000], mean observation: 3.159 [-1.344, 10.317], loss: 1.257515, mae: 5.074555, mean_q: 5.261397
 97604/100000: episode: 9959, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.828, mean reward: 0.383 [0.325, 0.431], mean action: 37.800 [21.000, 82.000], mean observation: 3.152 [-1.172, 10.336], loss: 1.190392, mae: 5.074250, mean_q: 5.260636
 97614/100000: episode: 9960, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.902, mean reward: 0.390 [0.306, 0.560], mean action: 25.400 [11.000, 53.000], mean observation: 3.149 [-1.450, 10.327], loss: 1.077733, mae: 5.073938, mean_q: 5.261449
 97624/100000: episode: 9961, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.919, mean reward: 0.392 [0.367, 0.461], mean action: 45.300 [17.000, 96.000], mean observation: 3.152 [-0.876, 10.536], loss: 1.467490, mae: 5.075614, mean_q: 5.263204
 97634/100000: episode: 9962, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 3.993, mean reward: 0.399 [0.326, 0.470], mean action: 41.700 [7.000, 91.000], mean observation: 3.157 [-1.389, 10.324], loss: 1.183797, mae: 5.074265, mean_q: 5.261187
 97644/100000: episode: 9963, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.667, mean reward: 0.467 [0.336, 0.610], mean action: 34.200 [23.000, 68.000], mean observation: 3.159 [-1.484, 10.240], loss: 1.264536, mae: 5.074927, mean_q: 5.259141
 97654/100000: episode: 9964, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.296, mean reward: 0.430 [0.355, 0.545], mean action: 49.900 [20.000, 99.000], mean observation: 3.164 [-0.981, 10.256], loss: 1.153132, mae: 5.074550, mean_q: 5.259387
 97664/100000: episode: 9965, duration: 0.228s, episode steps: 10, steps per second: 44, episode reward: 4.360, mean reward: 0.436 [0.419, 0.560], mean action: 24.500 [23.000, 38.000], mean observation: 3.148 [-1.696, 10.223], loss: 1.275107, mae: 5.075144, mean_q: 5.260979
 97674/100000: episode: 9966, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.078, mean reward: 0.408 [0.329, 0.556], mean action: 30.200 [15.000, 71.000], mean observation: 3.147 [-1.157, 10.408], loss: 1.297723, mae: 5.075056, mean_q: 5.262379
 97684/100000: episode: 9967, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.720, mean reward: 0.472 [0.458, 0.529], mean action: 38.800 [18.000, 91.000], mean observation: 3.160 [-1.174, 10.254], loss: 1.319012, mae: 5.075154, mean_q: 5.263597
 97694/100000: episode: 9968, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.127, mean reward: 0.413 [0.317, 0.519], mean action: 24.800 [6.000, 58.000], mean observation: 3.156 [-1.668, 10.323], loss: 0.973718, mae: 5.073584, mean_q: 5.264308
 97704/100000: episode: 9969, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.794, mean reward: 0.379 [0.308, 0.488], mean action: 38.200 [0.000, 95.000], mean observation: 3.153 [-1.027, 10.235], loss: 1.063286, mae: 5.074084, mean_q: 5.265699
 97714/100000: episode: 9970, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.063, mean reward: 0.406 [0.358, 0.462], mean action: 31.400 [1.000, 84.000], mean observation: 3.163 [-1.736, 10.522], loss: 1.268626, mae: 5.075061, mean_q: 5.266384
 97724/100000: episode: 9971, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.093, mean reward: 0.409 [0.336, 0.508], mean action: 32.200 [15.000, 67.000], mean observation: 3.151 [-2.094, 10.193], loss: 1.242679, mae: 5.075018, mean_q: 5.262853
 97734/100000: episode: 9972, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.173, mean reward: 0.417 [0.346, 0.552], mean action: 35.900 [20.000, 66.000], mean observation: 3.151 [-2.133, 10.289], loss: 0.932588, mae: 5.073534, mean_q: 5.259225
 97744/100000: episode: 9973, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.144, mean reward: 0.414 [0.383, 0.534], mean action: 55.100 [46.000, 73.000], mean observation: 3.163 [-1.271, 10.510], loss: 1.418778, mae: 5.075486, mean_q: 5.260523
 97754/100000: episode: 9974, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.251, mean reward: 0.425 [0.349, 0.564], mean action: 35.800 [7.000, 57.000], mean observation: 3.151 [-1.280, 10.264], loss: 1.182318, mae: 5.074591, mean_q: 5.262636
 97764/100000: episode: 9975, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 5.367, mean reward: 0.537 [0.413, 0.590], mean action: 45.100 [3.000, 71.000], mean observation: 3.159 [-1.107, 10.311], loss: 1.194598, mae: 5.074506, mean_q: 5.264107
 97774/100000: episode: 9976, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.820, mean reward: 0.382 [0.309, 0.519], mean action: 48.200 [23.000, 87.000], mean observation: 3.161 [-1.791, 10.351], loss: 1.620522, mae: 5.075823, mean_q: 5.265155
 97784/100000: episode: 9977, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.880, mean reward: 0.388 [0.324, 0.465], mean action: 33.400 [1.000, 88.000], mean observation: 3.154 [-1.373, 10.369], loss: 1.301029, mae: 5.074739, mean_q: 5.266651
 97794/100000: episode: 9978, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.163, mean reward: 0.416 [0.384, 0.509], mean action: 24.900 [13.000, 51.000], mean observation: 3.160 [-1.794, 10.275], loss: 1.470656, mae: 5.075149, mean_q: 5.268441
 97804/100000: episode: 9979, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.299, mean reward: 0.430 [0.351, 0.522], mean action: 40.000 [13.000, 101.000], mean observation: 3.154 [-1.697, 10.317], loss: 1.367234, mae: 5.074725, mean_q: 5.270404
 97814/100000: episode: 9980, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 3.808, mean reward: 0.381 [0.302, 0.431], mean action: 36.800 [21.000, 101.000], mean observation: 3.161 [-1.691, 10.299], loss: 1.127685, mae: 5.073877, mean_q: 5.272654
 97824/100000: episode: 9981, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.020, mean reward: 0.402 [0.380, 0.441], mean action: 28.800 [23.000, 68.000], mean observation: 3.149 [-1.525, 10.430], loss: 1.196562, mae: 5.074367, mean_q: 5.275192
 97834/100000: episode: 9982, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.845, mean reward: 0.385 [0.302, 0.461], mean action: 32.800 [2.000, 91.000], mean observation: 3.163 [-1.531, 10.388], loss: 0.925812, mae: 5.073607, mean_q: 5.271955
 97844/100000: episode: 9983, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.130, mean reward: 0.413 [0.338, 0.528], mean action: 43.800 [20.000, 100.000], mean observation: 3.153 [-1.612, 10.271], loss: 1.111909, mae: 5.074534, mean_q: 5.271091
 97854/100000: episode: 9984, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.147, mean reward: 0.415 [0.372, 0.455], mean action: 34.000 [2.000, 99.000], mean observation: 3.156 [-1.808, 10.229], loss: 0.882041, mae: 5.073970, mean_q: 5.272143
 97864/100000: episode: 9985, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.366, mean reward: 0.437 [0.417, 0.520], mean action: 42.000 [6.000, 98.000], mean observation: 3.151 [-2.006, 10.208], loss: 1.814580, mae: 5.077873, mean_q: 5.273192
 97874/100000: episode: 9986, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.131, mean reward: 0.413 [0.373, 0.506], mean action: 22.800 [12.000, 42.000], mean observation: 3.152 [-2.396, 10.444], loss: 1.053204, mae: 5.074827, mean_q: 5.274722
 97884/100000: episode: 9987, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.329, mean reward: 0.433 [0.424, 0.478], mean action: 38.000 [23.000, 83.000], mean observation: 3.160 [-1.768, 10.547], loss: 1.137443, mae: 5.075356, mean_q: 5.272420
 97894/100000: episode: 9988, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 3.632, mean reward: 0.363 [0.332, 0.420], mean action: 46.300 [22.000, 81.000], mean observation: 3.163 [-2.145, 10.306], loss: 1.021421, mae: 5.075171, mean_q: 5.271736
 97904/100000: episode: 9989, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.203, mean reward: 0.420 [0.400, 0.486], mean action: 38.800 [7.000, 92.000], mean observation: 3.162 [-1.503, 10.414], loss: 0.961515, mae: 5.075237, mean_q: 5.272793
 97914/100000: episode: 9990, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 4.468, mean reward: 0.447 [0.437, 0.499], mean action: 42.700 [20.000, 97.000], mean observation: 3.154 [-1.821, 10.314], loss: 1.111557, mae: 5.076108, mean_q: 5.276276
 97924/100000: episode: 9991, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.289, mean reward: 0.429 [0.421, 0.475], mean action: 34.600 [23.000, 48.000], mean observation: 3.162 [-1.548, 10.204], loss: 1.305593, mae: 5.076905, mean_q: 5.282883
 97934/100000: episode: 9992, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.558, mean reward: 0.456 [0.330, 0.531], mean action: 33.100 [9.000, 66.000], mean observation: 3.171 [-0.877, 10.245], loss: 1.329270, mae: 5.077185, mean_q: 5.287805
 97944/100000: episode: 9993, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.937, mean reward: 0.394 [0.297, 0.500], mean action: 26.200 [17.000, 46.000], mean observation: 3.154 [-1.174, 10.280], loss: 1.061630, mae: 5.076074, mean_q: 5.290065
 97954/100000: episode: 9994, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.224, mean reward: 0.422 [0.388, 0.539], mean action: 31.100 [0.000, 83.000], mean observation: 3.163 [-1.459, 10.484], loss: 1.469238, mae: 5.077723, mean_q: 5.291886
 97964/100000: episode: 9995, duration: 0.202s, episode steps: 10, steps per second: 50, episode reward: 4.021, mean reward: 0.402 [0.288, 0.485], mean action: 29.800 [15.000, 75.000], mean observation: 3.139 [-1.096, 10.269], loss: 0.934889, mae: 5.075778, mean_q: 5.293609
 97974/100000: episode: 9996, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.608, mean reward: 0.461 [0.443, 0.528], mean action: 36.800 [23.000, 96.000], mean observation: 3.163 [-1.073, 10.339], loss: 1.081716, mae: 5.076797, mean_q: 5.294738
 97984/100000: episode: 9997, duration: 0.187s, episode steps: 10, steps per second: 54, episode reward: 3.914, mean reward: 0.391 [0.310, 0.487], mean action: 30.100 [18.000, 66.000], mean observation: 3.153 [-1.429, 10.351], loss: 0.860394, mae: 5.076180, mean_q: 5.292251
 97994/100000: episode: 9998, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.865, mean reward: 0.387 [0.337, 0.470], mean action: 47.600 [17.000, 95.000], mean observation: 3.160 [-1.934, 10.305], loss: 1.334627, mae: 5.078534, mean_q: 5.290982
 98004/100000: episode: 9999, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.964, mean reward: 0.396 [0.361, 0.474], mean action: 33.700 [3.000, 93.000], mean observation: 3.154 [-1.656, 10.375], loss: 1.190063, mae: 5.078189, mean_q: 5.285153
 98014/100000: episode: 10000, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.142, mean reward: 0.414 [0.324, 0.485], mean action: 39.300 [23.000, 84.000], mean observation: 3.155 [-1.173, 10.264], loss: 1.201945, mae: 5.078092, mean_q: 5.282269
 98024/100000: episode: 10001, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.179, mean reward: 0.418 [0.355, 0.545], mean action: 48.400 [23.000, 101.000], mean observation: 3.152 [-1.634, 10.384], loss: 1.614124, mae: 5.079563, mean_q: 5.279194
 98034/100000: episode: 10002, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.803, mean reward: 0.380 [0.293, 0.547], mean action: 37.300 [9.000, 98.000], mean observation: 3.159 [-1.524, 10.372], loss: 1.347532, mae: 5.078078, mean_q: 5.272151
 98044/100000: episode: 10003, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.501, mean reward: 0.450 [0.333, 0.512], mean action: 30.200 [17.000, 86.000], mean observation: 3.153 [-1.326, 10.153], loss: 1.339004, mae: 5.077904, mean_q: 5.269174
 98054/100000: episode: 10004, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.156, mean reward: 0.416 [0.346, 0.560], mean action: 39.100 [2.000, 67.000], mean observation: 3.159 [-1.920, 10.463], loss: 1.332399, mae: 5.077632, mean_q: 5.268752
 98064/100000: episode: 10005, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 3.616, mean reward: 0.362 [0.332, 0.409], mean action: 57.800 [20.000, 98.000], mean observation: 3.153 [-1.560, 10.396], loss: 1.199618, mae: 5.076929, mean_q: 5.269468
 98074/100000: episode: 10006, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.347, mean reward: 0.435 [0.349, 0.519], mean action: 32.400 [1.000, 70.000], mean observation: 3.162 [-1.119, 10.188], loss: 1.430425, mae: 5.077787, mean_q: 5.270816
 98084/100000: episode: 10007, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.257, mean reward: 0.426 [0.355, 0.526], mean action: 29.800 [2.000, 70.000], mean observation: 3.145 [-1.130, 10.305], loss: 1.231376, mae: 5.076874, mean_q: 5.272428
 98094/100000: episode: 10008, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 4.081, mean reward: 0.408 [0.353, 0.557], mean action: 39.800 [17.000, 96.000], mean observation: 3.160 [-1.690, 10.330], loss: 0.743288, mae: 5.075202, mean_q: 5.273990
 98104/100000: episode: 10009, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 4.123, mean reward: 0.412 [0.320, 0.525], mean action: 38.300 [20.000, 93.000], mean observation: 3.149 [-1.610, 10.359], loss: 1.151735, mae: 5.076987, mean_q: 5.274190
 98114/100000: episode: 10010, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.843, mean reward: 0.384 [0.345, 0.423], mean action: 36.600 [5.000, 82.000], mean observation: 3.170 [-1.774, 10.276], loss: 1.262764, mae: 5.077655, mean_q: 5.271819
 98124/100000: episode: 10011, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.143, mean reward: 0.414 [0.329, 0.472], mean action: 43.500 [20.000, 93.000], mean observation: 3.166 [-0.845, 10.283], loss: 1.097677, mae: 5.077083, mean_q: 5.272883
 98134/100000: episode: 10012, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.077, mean reward: 0.408 [0.352, 0.508], mean action: 44.300 [20.000, 90.000], mean observation: 3.155 [-1.007, 10.272], loss: 1.160877, mae: 5.077676, mean_q: 5.273812
 98144/100000: episode: 10013, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 4.471, mean reward: 0.447 [0.366, 0.556], mean action: 33.500 [19.000, 78.000], mean observation: 3.159 [-1.681, 10.435], loss: 1.787499, mae: 5.080009, mean_q: 5.275306
 98154/100000: episode: 10014, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.127, mean reward: 0.413 [0.391, 0.443], mean action: 48.600 [11.000, 89.000], mean observation: 3.146 [-1.588, 10.345], loss: 1.229558, mae: 5.077671, mean_q: 5.277402
 98164/100000: episode: 10015, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.136, mean reward: 0.414 [0.337, 0.523], mean action: 23.400 [2.000, 52.000], mean observation: 3.162 [-0.931, 10.274], loss: 0.923276, mae: 5.076344, mean_q: 5.278701
 98168/100000: episode: 10016, duration: 0.089s, episode steps: 4, steps per second: 45, episode reward: 11.257, mean reward: 2.814 [0.366, 10.000], mean action: 20.000 [20.000, 20.000], mean observation: 3.169 [-1.528, 10.438], loss: 1.163622, mae: 5.077488, mean_q: 5.281902
 98178/100000: episode: 10017, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 3.836, mean reward: 0.384 [0.316, 0.413], mean action: 48.000 [2.000, 93.000], mean observation: 3.159 [-1.476, 10.434], loss: 1.410244, mae: 5.078652, mean_q: 5.281854
 98188/100000: episode: 10018, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.034, mean reward: 0.403 [0.380, 0.461], mean action: 36.500 [17.000, 98.000], mean observation: 3.147 [-1.081, 10.242], loss: 1.230975, mae: 5.078037, mean_q: 5.282527
 98190/100000: episode: 10019, duration: 0.052s, episode steps: 2, steps per second: 38, episode reward: 10.447, mean reward: 5.224 [0.447, 10.000], mean action: 9.500 [5.000, 14.000], mean observation: 3.155 [-1.289, 10.449], loss: 1.503299, mae: 5.079032, mean_q: 5.283319
 98200/100000: episode: 10020, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.038, mean reward: 0.404 [0.291, 0.498], mean action: 32.800 [20.000, 79.000], mean observation: 3.155 [-1.393, 10.372], loss: 1.120866, mae: 5.077734, mean_q: 5.284348
 98210/100000: episode: 10021, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.102, mean reward: 0.410 [0.310, 0.503], mean action: 30.800 [2.000, 65.000], mean observation: 3.157 [-1.171, 10.337], loss: 1.228195, mae: 5.078628, mean_q: 5.288660
 98220/100000: episode: 10022, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.591, mean reward: 0.359 [0.311, 0.442], mean action: 26.600 [5.000, 60.000], mean observation: 3.150 [-1.664, 10.298], loss: 1.336274, mae: 5.079024, mean_q: 5.291721
 98230/100000: episode: 10023, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.017, mean reward: 0.402 [0.327, 0.470], mean action: 21.300 [3.000, 68.000], mean observation: 3.158 [-1.392, 10.417], loss: 1.107165, mae: 5.078225, mean_q: 5.294463
 98240/100000: episode: 10024, duration: 0.230s, episode steps: 10, steps per second: 44, episode reward: 3.736, mean reward: 0.374 [0.311, 0.523], mean action: 24.900 [5.000, 78.000], mean observation: 3.158 [-1.136, 10.243], loss: 1.137998, mae: 5.078490, mean_q: 5.293330
 98250/100000: episode: 10025, duration: 0.203s, episode steps: 10, steps per second: 49, episode reward: 4.275, mean reward: 0.427 [0.355, 0.524], mean action: 29.900 [19.000, 80.000], mean observation: 3.150 [-1.782, 10.372], loss: 1.165107, mae: 5.078882, mean_q: 5.292589
 98260/100000: episode: 10026, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.391, mean reward: 0.439 [0.391, 0.504], mean action: 43.000 [1.000, 97.000], mean observation: 3.152 [-1.493, 10.295], loss: 1.407815, mae: 5.079990, mean_q: 5.293593
 98270/100000: episode: 10027, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 5.017, mean reward: 0.502 [0.499, 0.511], mean action: 46.200 [20.000, 94.000], mean observation: 3.163 [-1.726, 10.328], loss: 0.996029, mae: 5.078490, mean_q: 5.294065
 98280/100000: episode: 10028, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 3.759, mean reward: 0.376 [0.300, 0.498], mean action: 29.700 [0.000, 83.000], mean observation: 3.152 [-1.058, 10.306], loss: 1.580834, mae: 5.080895, mean_q: 5.295007
 98290/100000: episode: 10029, duration: 0.228s, episode steps: 10, steps per second: 44, episode reward: 3.989, mean reward: 0.399 [0.301, 0.488], mean action: 21.800 [0.000, 34.000], mean observation: 3.166 [-1.852, 10.291], loss: 1.393748, mae: 5.080375, mean_q: 5.296954
 98300/100000: episode: 10030, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.088, mean reward: 0.409 [0.369, 0.465], mean action: 36.600 [6.000, 95.000], mean observation: 3.160 [-1.968, 10.375], loss: 1.157115, mae: 5.079558, mean_q: 5.298874
 98310/100000: episode: 10031, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.382, mean reward: 0.438 [0.360, 0.505], mean action: 37.600 [19.000, 84.000], mean observation: 3.147 [-1.457, 10.396], loss: 1.193776, mae: 5.079550, mean_q: 5.296149
 98320/100000: episode: 10032, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.170, mean reward: 0.417 [0.330, 0.522], mean action: 30.800 [4.000, 75.000], mean observation: 3.153 [-1.268, 10.301], loss: 1.491371, mae: 5.080874, mean_q: 5.291074
 98330/100000: episode: 10033, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 3.878, mean reward: 0.388 [0.353, 0.452], mean action: 36.800 [0.000, 98.000], mean observation: 3.150 [-1.387, 10.264], loss: 1.284102, mae: 5.079955, mean_q: 5.290664
 98340/100000: episode: 10034, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.110, mean reward: 0.411 [0.351, 0.494], mean action: 31.600 [12.000, 89.000], mean observation: 3.161 [-1.270, 10.302], loss: 1.299294, mae: 5.079922, mean_q: 5.288416
 98350/100000: episode: 10035, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.663, mean reward: 0.366 [0.298, 0.422], mean action: 35.200 [20.000, 80.000], mean observation: 3.153 [-0.963, 10.442], loss: 1.441166, mae: 5.080490, mean_q: 5.283620
 98360/100000: episode: 10036, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 3.883, mean reward: 0.388 [0.329, 0.493], mean action: 37.200 [20.000, 82.000], mean observation: 3.164 [-1.344, 10.428], loss: 1.649135, mae: 5.081091, mean_q: 5.282310
 98370/100000: episode: 10037, duration: 0.179s, episode steps: 10, steps per second: 56, episode reward: 3.608, mean reward: 0.361 [0.311, 0.464], mean action: 29.600 [18.000, 74.000], mean observation: 3.158 [-1.355, 10.420], loss: 1.031249, mae: 5.078723, mean_q: 5.279838
 98380/100000: episode: 10038, duration: 0.211s, episode steps: 10, steps per second: 47, episode reward: 4.166, mean reward: 0.417 [0.360, 0.498], mean action: 34.100 [8.000, 81.000], mean observation: 3.151 [-2.153, 10.211], loss: 1.064903, mae: 5.078920, mean_q: 5.279931
 98390/100000: episode: 10039, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.747, mean reward: 0.375 [0.325, 0.451], mean action: 28.900 [4.000, 90.000], mean observation: 3.155 [-1.146, 10.358], loss: 1.028558, mae: 5.078839, mean_q: 5.279023
 98400/100000: episode: 10040, duration: 0.202s, episode steps: 10, steps per second: 49, episode reward: 4.011, mean reward: 0.401 [0.375, 0.476], mean action: 22.500 [8.000, 47.000], mean observation: 3.150 [-1.459, 10.229], loss: 0.960830, mae: 5.079035, mean_q: 5.276183
 98410/100000: episode: 10041, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.965, mean reward: 0.396 [0.355, 0.481], mean action: 36.900 [14.000, 88.000], mean observation: 3.152 [-0.949, 10.390], loss: 1.500462, mae: 5.081448, mean_q: 5.273267
 98420/100000: episode: 10042, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 4.223, mean reward: 0.422 [0.335, 0.523], mean action: 44.100 [10.000, 96.000], mean observation: 3.155 [-1.596, 10.366], loss: 1.315381, mae: 5.080670, mean_q: 5.273086
 98430/100000: episode: 10043, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.220, mean reward: 0.422 [0.381, 0.534], mean action: 31.300 [20.000, 82.000], mean observation: 3.161 [-1.674, 10.321], loss: 1.064540, mae: 5.079911, mean_q: 5.274690
 98440/100000: episode: 10044, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.533, mean reward: 0.453 [0.396, 0.534], mean action: 48.800 [2.000, 91.000], mean observation: 3.154 [-1.217, 10.403], loss: 1.050042, mae: 5.080215, mean_q: 5.276545
 98450/100000: episode: 10045, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 4.013, mean reward: 0.401 [0.371, 0.470], mean action: 42.900 [20.000, 101.000], mean observation: 3.159 [-1.151, 10.311], loss: 1.337490, mae: 5.081644, mean_q: 5.277885
 98460/100000: episode: 10046, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.292, mean reward: 0.429 [0.327, 0.557], mean action: 22.700 [20.000, 42.000], mean observation: 3.152 [-2.140, 10.277], loss: 1.081968, mae: 5.080534, mean_q: 5.277389
 98470/100000: episode: 10047, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 5.071, mean reward: 0.507 [0.424, 0.541], mean action: 45.000 [6.000, 94.000], mean observation: 3.158 [-1.451, 10.225], loss: 1.198021, mae: 5.081120, mean_q: 5.276151
 98480/100000: episode: 10048, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.065, mean reward: 0.407 [0.351, 0.510], mean action: 27.100 [1.000, 87.000], mean observation: 3.161 [-1.581, 10.381], loss: 1.059549, mae: 5.080585, mean_q: 5.276359
 98490/100000: episode: 10049, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.491, mean reward: 0.449 [0.359, 0.552], mean action: 35.200 [20.000, 69.000], mean observation: 3.157 [-1.474, 10.230], loss: 1.297765, mae: 5.081403, mean_q: 5.276800
 98500/100000: episode: 10050, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.966, mean reward: 0.397 [0.362, 0.524], mean action: 54.800 [17.000, 81.000], mean observation: 3.156 [-1.884, 10.247], loss: 1.293502, mae: 5.081450, mean_q: 5.277423
 98510/100000: episode: 10051, duration: 0.160s, episode steps: 10, steps per second: 63, episode reward: 3.950, mean reward: 0.395 [0.389, 0.413], mean action: 57.500 [21.000, 93.000], mean observation: 3.149 [-1.350, 10.307], loss: 1.081116, mae: 5.080883, mean_q: 5.278871
 98520/100000: episode: 10052, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.035, mean reward: 0.404 [0.376, 0.480], mean action: 46.700 [13.000, 74.000], mean observation: 3.159 [-0.987, 10.418], loss: 0.960981, mae: 5.080532, mean_q: 5.279872
 98530/100000: episode: 10053, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.180, mean reward: 0.418 [0.288, 0.542], mean action: 62.700 [15.000, 94.000], mean observation: 3.147 [-1.211, 10.231], loss: 0.951607, mae: 5.080817, mean_q: 5.281525
 98540/100000: episode: 10054, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.535, mean reward: 0.453 [0.379, 0.547], mean action: 36.200 [3.000, 91.000], mean observation: 3.161 [-2.548, 10.285], loss: 1.165777, mae: 5.082322, mean_q: 5.283996
 98550/100000: episode: 10055, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.479, mean reward: 0.448 [0.378, 0.526], mean action: 25.600 [4.000, 74.000], mean observation: 3.152 [-1.214, 10.385], loss: 1.227966, mae: 5.082658, mean_q: 5.287438
 98560/100000: episode: 10056, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 3.964, mean reward: 0.396 [0.316, 0.514], mean action: 35.600 [20.000, 90.000], mean observation: 3.159 [-1.586, 10.347], loss: 1.296290, mae: 5.083158, mean_q: 5.289361
 98570/100000: episode: 10057, duration: 0.227s, episode steps: 10, steps per second: 44, episode reward: 4.303, mean reward: 0.430 [0.389, 0.493], mean action: 30.600 [3.000, 78.000], mean observation: 3.152 [-2.349, 10.473], loss: 1.155194, mae: 5.082618, mean_q: 5.291545
 98580/100000: episode: 10058, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.147, mean reward: 0.415 [0.338, 0.507], mean action: 35.900 [8.000, 75.000], mean observation: 3.164 [-1.356, 10.324], loss: 1.234968, mae: 5.083088, mean_q: 5.293604
 98590/100000: episode: 10059, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 3.752, mean reward: 0.375 [0.321, 0.469], mean action: 39.300 [20.000, 92.000], mean observation: 3.169 [-1.197, 10.288], loss: 1.203564, mae: 5.082887, mean_q: 5.294459
 98600/100000: episode: 10060, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 3.867, mean reward: 0.387 [0.324, 0.532], mean action: 36.700 [20.000, 99.000], mean observation: 3.162 [-2.704, 10.351], loss: 1.441990, mae: 5.083819, mean_q: 5.293035
 98610/100000: episode: 10061, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 3.861, mean reward: 0.386 [0.336, 0.450], mean action: 35.900 [20.000, 85.000], mean observation: 3.163 [-1.335, 10.326], loss: 1.346760, mae: 5.083344, mean_q: 5.293311
 98620/100000: episode: 10062, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.050, mean reward: 0.405 [0.390, 0.468], mean action: 39.600 [12.000, 88.000], mean observation: 3.160 [-1.749, 10.364], loss: 0.926446, mae: 5.081716, mean_q: 5.294206
 98630/100000: episode: 10063, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 4.167, mean reward: 0.417 [0.360, 0.517], mean action: 44.900 [20.000, 99.000], mean observation: 3.162 [-1.570, 10.535], loss: 1.012231, mae: 5.082060, mean_q: 5.294725
 98640/100000: episode: 10064, duration: 0.186s, episode steps: 10, steps per second: 54, episode reward: 4.829, mean reward: 0.483 [0.478, 0.529], mean action: 29.000 [2.000, 72.000], mean observation: 3.163 [-1.439, 10.390], loss: 1.269911, mae: 5.083438, mean_q: 5.296605
 98650/100000: episode: 10065, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.995, mean reward: 0.399 [0.344, 0.498], mean action: 44.700 [20.000, 100.000], mean observation: 3.155 [-1.512, 10.265], loss: 0.999359, mae: 5.082633, mean_q: 5.298595
 98660/100000: episode: 10066, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 3.950, mean reward: 0.395 [0.332, 0.564], mean action: 38.300 [19.000, 95.000], mean observation: 3.166 [-1.333, 10.298], loss: 1.299309, mae: 5.084091, mean_q: 5.298254
 98670/100000: episode: 10067, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.336, mean reward: 0.434 [0.366, 0.564], mean action: 40.800 [2.000, 75.000], mean observation: 3.157 [-1.460, 10.509], loss: 1.021383, mae: 5.083297, mean_q: 5.298053
 98680/100000: episode: 10068, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 3.994, mean reward: 0.399 [0.346, 0.477], mean action: 47.700 [20.000, 101.000], mean observation: 3.161 [-1.082, 10.448], loss: 1.181311, mae: 5.084105, mean_q: 5.299491
 98690/100000: episode: 10069, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 4.493, mean reward: 0.449 [0.432, 0.528], mean action: 45.700 [17.000, 97.000], mean observation: 3.167 [-1.666, 10.268], loss: 1.072538, mae: 5.084264, mean_q: 5.301344
 98697/100000: episode: 10070, duration: 0.122s, episode steps: 7, steps per second: 58, episode reward: 12.285, mean reward: 1.755 [0.353, 10.000], mean action: 36.429 [20.000, 98.000], mean observation: 3.157 [-1.840, 10.221], loss: 1.075123, mae: 5.084592, mean_q: 5.303361
 98707/100000: episode: 10071, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 4.857, mean reward: 0.486 [0.417, 0.518], mean action: 52.000 [14.000, 91.000], mean observation: 3.143 [-2.133, 10.175], loss: 1.185133, mae: 5.085390, mean_q: 5.305362
 98717/100000: episode: 10072, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 4.197, mean reward: 0.420 [0.314, 0.596], mean action: 46.000 [20.000, 98.000], mean observation: 3.150 [-1.678, 10.616], loss: 1.311046, mae: 5.086095, mean_q: 5.307756
 98727/100000: episode: 10073, duration: 0.188s, episode steps: 10, steps per second: 53, episode reward: 4.216, mean reward: 0.422 [0.345, 0.554], mean action: 34.600 [18.000, 67.000], mean observation: 3.164 [-1.421, 10.349], loss: 1.305781, mae: 5.086195, mean_q: 5.310682
 98737/100000: episode: 10074, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.847, mean reward: 0.385 [0.276, 0.450], mean action: 28.800 [10.000, 88.000], mean observation: 3.157 [-1.128, 10.356], loss: 1.204327, mae: 5.086056, mean_q: 5.314517
 98747/100000: episode: 10075, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.675, mean reward: 0.368 [0.321, 0.434], mean action: 46.100 [19.000, 86.000], mean observation: 3.157 [-1.315, 10.332], loss: 1.481970, mae: 5.087173, mean_q: 5.317524
 98757/100000: episode: 10076, duration: 0.174s, episode steps: 10, steps per second: 57, episode reward: 4.070, mean reward: 0.407 [0.325, 0.553], mean action: 35.000 [1.000, 93.000], mean observation: 3.151 [-1.078, 10.351], loss: 1.498585, mae: 5.087274, mean_q: 5.320518
 98767/100000: episode: 10077, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.161, mean reward: 0.416 [0.401, 0.526], mean action: 32.200 [20.000, 82.000], mean observation: 3.155 [-1.616, 10.452], loss: 1.093775, mae: 5.085825, mean_q: 5.323108
 98777/100000: episode: 10078, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 3.982, mean reward: 0.398 [0.301, 0.504], mean action: 34.800 [10.000, 85.000], mean observation: 3.157 [-1.374, 10.492], loss: 1.269799, mae: 5.086725, mean_q: 5.325084
 98783/100000: episode: 10079, duration: 0.115s, episode steps: 6, steps per second: 52, episode reward: 11.967, mean reward: 1.994 [0.363, 10.000], mean action: 30.667 [20.000, 84.000], mean observation: 3.149 [-1.658, 10.266], loss: 1.005397, mae: 5.085547, mean_q: 5.323928
 98793/100000: episode: 10080, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.677, mean reward: 0.368 [0.300, 0.476], mean action: 32.200 [20.000, 85.000], mean observation: 3.141 [-1.628, 10.259], loss: 1.276063, mae: 5.087153, mean_q: 5.326811
 98803/100000: episode: 10081, duration: 0.198s, episode steps: 10, steps per second: 50, episode reward: 4.009, mean reward: 0.401 [0.372, 0.499], mean action: 23.000 [20.000, 50.000], mean observation: 3.163 [-1.703, 10.477], loss: 1.112660, mae: 5.086945, mean_q: 5.330865
 98813/100000: episode: 10082, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 3.803, mean reward: 0.380 [0.323, 0.427], mean action: 51.300 [20.000, 92.000], mean observation: 3.157 [-0.964, 10.323], loss: 1.215380, mae: 5.087844, mean_q: 5.333404
 98823/100000: episode: 10083, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.395, mean reward: 0.440 [0.410, 0.509], mean action: 29.700 [20.000, 60.000], mean observation: 3.164 [-1.695, 10.373], loss: 1.285925, mae: 5.088243, mean_q: 5.335066
 98833/100000: episode: 10084, duration: 0.187s, episode steps: 10, steps per second: 53, episode reward: 4.181, mean reward: 0.418 [0.356, 0.467], mean action: 33.300 [1.000, 74.000], mean observation: 3.155 [-1.699, 10.280], loss: 1.249371, mae: 5.088257, mean_q: 5.336974
 98843/100000: episode: 10085, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 3.812, mean reward: 0.381 [0.328, 0.483], mean action: 50.100 [20.000, 94.000], mean observation: 3.152 [-1.574, 10.215], loss: 1.210563, mae: 5.087951, mean_q: 5.339881
 98847/100000: episode: 10086, duration: 0.068s, episode steps: 4, steps per second: 59, episode reward: 11.302, mean reward: 2.825 [0.374, 10.000], mean action: 65.500 [20.000, 98.000], mean observation: 3.169 [-1.321, 10.283], loss: 1.181565, mae: 5.087855, mean_q: 5.341900
 98857/100000: episode: 10087, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.089, mean reward: 0.409 [0.303, 0.516], mean action: 31.800 [20.000, 84.000], mean observation: 3.158 [-1.353, 10.460], loss: 0.938363, mae: 5.087005, mean_q: 5.337138
 98867/100000: episode: 10088, duration: 0.196s, episode steps: 10, steps per second: 51, episode reward: 4.432, mean reward: 0.443 [0.357, 0.481], mean action: 27.500 [4.000, 99.000], mean observation: 3.159 [-1.790, 10.302], loss: 1.185641, mae: 5.088373, mean_q: 5.334641
 98877/100000: episode: 10089, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 4.069, mean reward: 0.407 [0.357, 0.523], mean action: 29.600 [3.000, 67.000], mean observation: 3.162 [-1.674, 10.356], loss: 1.132590, mae: 5.088686, mean_q: 5.335675
 98887/100000: episode: 10090, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 4.313, mean reward: 0.431 [0.354, 0.590], mean action: 44.400 [16.000, 93.000], mean observation: 3.153 [-1.228, 10.243], loss: 1.299779, mae: 5.089698, mean_q: 5.337523
 98897/100000: episode: 10091, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.203, mean reward: 0.420 [0.331, 0.485], mean action: 31.300 [12.000, 95.000], mean observation: 3.168 [-1.279, 10.379], loss: 1.360891, mae: 5.090080, mean_q: 5.337152
 98907/100000: episode: 10092, duration: 0.194s, episode steps: 10, steps per second: 51, episode reward: 4.066, mean reward: 0.407 [0.308, 0.506], mean action: 29.800 [10.000, 81.000], mean observation: 3.154 [-1.611, 10.271], loss: 1.329544, mae: 5.089820, mean_q: 5.336825
 98917/100000: episode: 10093, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.653, mean reward: 0.365 [0.309, 0.467], mean action: 34.300 [20.000, 93.000], mean observation: 3.159 [-1.339, 10.299], loss: 1.347705, mae: 5.089818, mean_q: 5.337046
 98927/100000: episode: 10094, duration: 0.205s, episode steps: 10, steps per second: 49, episode reward: 4.666, mean reward: 0.467 [0.467, 0.467], mean action: 27.900 [8.000, 52.000], mean observation: 3.148 [-0.979, 10.412], loss: 1.390793, mae: 5.089793, mean_q: 5.338626
 98932/100000: episode: 10095, duration: 0.084s, episode steps: 5, steps per second: 60, episode reward: 11.695, mean reward: 2.339 [0.416, 10.000], mean action: 45.200 [20.000, 93.000], mean observation: 3.153 [-1.184, 10.379], loss: 0.778492, mae: 5.087495, mean_q: 5.337624
 98942/100000: episode: 10096, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.540, mean reward: 0.454 [0.451, 0.469], mean action: 35.300 [14.000, 79.000], mean observation: 3.156 [-1.558, 10.386], loss: 1.370667, mae: 5.089839, mean_q: 5.337566
 98952/100000: episode: 10097, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 3.850, mean reward: 0.385 [0.290, 0.484], mean action: 33.400 [20.000, 96.000], mean observation: 3.150 [-1.090, 10.173], loss: 1.154767, mae: 5.089380, mean_q: 5.336672
 98962/100000: episode: 10098, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.056, mean reward: 0.406 [0.361, 0.496], mean action: 36.500 [6.000, 95.000], mean observation: 3.142 [-1.896, 10.285], loss: 1.286197, mae: 5.089707, mean_q: 5.338097
 98972/100000: episode: 10099, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 3.617, mean reward: 0.362 [0.318, 0.511], mean action: 30.900 [20.000, 96.000], mean observation: 3.152 [-2.622, 10.325], loss: 1.656263, mae: 5.091255, mean_q: 5.339402
 98982/100000: episode: 10100, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.013, mean reward: 0.401 [0.294, 0.504], mean action: 26.400 [20.000, 60.000], mean observation: 3.169 [-1.735, 10.400], loss: 1.499712, mae: 5.090731, mean_q: 5.338285
 98992/100000: episode: 10101, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 5.138, mean reward: 0.514 [0.514, 0.514], mean action: 35.400 [20.000, 80.000], mean observation: 3.165 [-1.387, 10.366], loss: 1.409216, mae: 5.090034, mean_q: 5.340087
 99002/100000: episode: 10102, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 13.669, mean reward: 1.367 [0.334, 10.000], mean action: 27.200 [13.000, 62.000], mean observation: 3.149 [-1.880, 10.271], loss: 1.154339, mae: 5.088927, mean_q: 5.339797
 99012/100000: episode: 10103, duration: 0.175s, episode steps: 10, steps per second: 57, episode reward: 4.628, mean reward: 0.463 [0.447, 0.501], mean action: 48.000 [20.000, 93.000], mean observation: 3.158 [-1.184, 10.365], loss: 0.799610, mae: 5.088136, mean_q: 5.338805
 99022/100000: episode: 10104, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.325, mean reward: 0.433 [0.329, 0.544], mean action: 48.500 [17.000, 101.000], mean observation: 3.140 [-1.195, 10.498], loss: 1.327983, mae: 5.090350, mean_q: 5.339979
 99032/100000: episode: 10105, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 3.809, mean reward: 0.381 [0.350, 0.420], mean action: 48.400 [20.000, 101.000], mean observation: 3.159 [-1.039, 10.323], loss: 1.121740, mae: 5.089911, mean_q: 5.342460
 99042/100000: episode: 10106, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.928, mean reward: 0.393 [0.309, 0.497], mean action: 29.700 [4.000, 90.000], mean observation: 3.142 [-1.256, 10.274], loss: 1.464689, mae: 5.091611, mean_q: 5.344065
 99052/100000: episode: 10107, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 3.750, mean reward: 0.375 [0.290, 0.441], mean action: 47.000 [6.000, 87.000], mean observation: 3.168 [-1.219, 10.253], loss: 1.429942, mae: 5.091519, mean_q: 5.340044
 99054/100000: episode: 10108, duration: 0.053s, episode steps: 2, steps per second: 38, episode reward: 10.412, mean reward: 5.206 [0.412, 10.000], mean action: 17.000 [14.000, 20.000], mean observation: 3.134 [-0.627, 10.100], loss: 2.058978, mae: 5.094079, mean_q: 5.339719
 99064/100000: episode: 10109, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.373, mean reward: 0.437 [0.421, 0.488], mean action: 35.100 [17.000, 95.000], mean observation: 3.149 [-1.769, 10.402], loss: 1.123063, mae: 5.090252, mean_q: 5.340394
 99074/100000: episode: 10110, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.919, mean reward: 0.392 [0.362, 0.461], mean action: 31.200 [20.000, 74.000], mean observation: 3.147 [-0.898, 10.487], loss: 1.036732, mae: 5.090217, mean_q: 5.339357
 99084/100000: episode: 10111, duration: 0.185s, episode steps: 10, steps per second: 54, episode reward: 3.923, mean reward: 0.392 [0.301, 0.518], mean action: 26.700 [20.000, 87.000], mean observation: 3.160 [-1.775, 10.258], loss: 1.110906, mae: 5.090807, mean_q: 5.333349
 99094/100000: episode: 10112, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.229, mean reward: 0.423 [0.352, 0.544], mean action: 30.200 [7.000, 68.000], mean observation: 3.165 [-1.535, 10.386], loss: 1.580703, mae: 5.092974, mean_q: 5.329028
 99104/100000: episode: 10113, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 4.137, mean reward: 0.414 [0.307, 0.495], mean action: 33.400 [18.000, 89.000], mean observation: 3.151 [-1.725, 10.405], loss: 1.087183, mae: 5.091035, mean_q: 5.323722
 99114/100000: episode: 10114, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.723, mean reward: 0.372 [0.346, 0.463], mean action: 46.900 [20.000, 85.000], mean observation: 3.164 [-1.525, 10.234], loss: 1.406226, mae: 5.092702, mean_q: 5.324069
 99124/100000: episode: 10115, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 4.260, mean reward: 0.426 [0.366, 0.481], mean action: 32.700 [14.000, 82.000], mean observation: 3.150 [-1.184, 10.257], loss: 1.294472, mae: 5.092433, mean_q: 5.327833
 99134/100000: episode: 10116, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.033, mean reward: 0.403 [0.320, 0.489], mean action: 37.100 [20.000, 85.000], mean observation: 3.155 [-2.027, 10.429], loss: 1.428237, mae: 5.093060, mean_q: 5.328286
 99144/100000: episode: 10117, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.175, mean reward: 0.418 [0.298, 0.522], mean action: 27.400 [4.000, 61.000], mean observation: 3.157 [-1.317, 10.484], loss: 1.740528, mae: 5.094110, mean_q: 5.325001
 99145/100000: episode: 10118, duration: 0.030s, episode steps: 1, steps per second: 34, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 88.000 [88.000, 88.000], mean observation: 3.181 [-1.274, 10.149], loss: 0.840009, mae: 5.090408, mean_q: 5.324113
 99155/100000: episode: 10119, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 3.983, mean reward: 0.398 [0.328, 0.466], mean action: 30.100 [1.000, 87.000], mean observation: 3.165 [-0.944, 10.321], loss: 0.910146, mae: 5.090696, mean_q: 5.324768
 99165/100000: episode: 10120, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.422, mean reward: 0.442 [0.425, 0.595], mean action: 31.800 [20.000, 80.000], mean observation: 3.164 [-1.301, 10.404], loss: 1.599418, mae: 5.093377, mean_q: 5.326972
 99175/100000: episode: 10121, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 4.708, mean reward: 0.471 [0.429, 0.489], mean action: 40.300 [11.000, 93.000], mean observation: 3.151 [-1.493, 10.356], loss: 0.836065, mae: 5.090599, mean_q: 5.329047
 99185/100000: episode: 10122, duration: 0.206s, episode steps: 10, steps per second: 48, episode reward: 4.472, mean reward: 0.447 [0.338, 0.528], mean action: 30.400 [0.000, 93.000], mean observation: 3.167 [-1.303, 10.356], loss: 1.180918, mae: 5.092363, mean_q: 5.329338
 99195/100000: episode: 10123, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 4.055, mean reward: 0.405 [0.319, 0.559], mean action: 28.200 [14.000, 77.000], mean observation: 3.157 [-2.010, 10.377], loss: 1.242920, mae: 5.093052, mean_q: 5.324508
 99205/100000: episode: 10124, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.164, mean reward: 0.416 [0.336, 0.481], mean action: 30.000 [14.000, 98.000], mean observation: 3.153 [-1.731, 10.358], loss: 1.412364, mae: 5.093882, mean_q: 5.320699
 99215/100000: episode: 10125, duration: 0.191s, episode steps: 10, steps per second: 52, episode reward: 4.009, mean reward: 0.401 [0.309, 0.532], mean action: 36.200 [4.000, 81.000], mean observation: 3.144 [-1.108, 10.366], loss: 1.039238, mae: 5.092680, mean_q: 5.322453
 99225/100000: episode: 10126, duration: 0.218s, episode steps: 10, steps per second: 46, episode reward: 4.001, mean reward: 0.400 [0.322, 0.520], mean action: 23.500 [20.000, 34.000], mean observation: 3.142 [-2.446, 10.286], loss: 1.268800, mae: 5.093784, mean_q: 5.324060
 99235/100000: episode: 10127, duration: 0.195s, episode steps: 10, steps per second: 51, episode reward: 4.134, mean reward: 0.413 [0.349, 0.551], mean action: 26.500 [10.000, 95.000], mean observation: 3.147 [-1.919, 10.405], loss: 1.211946, mae: 5.093774, mean_q: 5.323202
 99245/100000: episode: 10128, duration: 0.192s, episode steps: 10, steps per second: 52, episode reward: 4.405, mean reward: 0.441 [0.424, 0.515], mean action: 33.400 [20.000, 87.000], mean observation: 3.164 [-1.500, 10.396], loss: 1.112735, mae: 5.093493, mean_q: 5.324090
 99255/100000: episode: 10129, duration: 0.198s, episode steps: 10, steps per second: 51, episode reward: 4.340, mean reward: 0.434 [0.322, 0.522], mean action: 19.600 [16.000, 20.000], mean observation: 3.154 [-1.537, 10.270], loss: 1.604979, mae: 5.095442, mean_q: 5.324466
 99265/100000: episode: 10130, duration: 0.204s, episode steps: 10, steps per second: 49, episode reward: 4.214, mean reward: 0.421 [0.393, 0.527], mean action: 39.500 [20.000, 94.000], mean observation: 3.162 [-1.264, 10.278], loss: 1.175627, mae: 5.093578, mean_q: 5.325048
 99275/100000: episode: 10131, duration: 0.189s, episode steps: 10, steps per second: 53, episode reward: 4.070, mean reward: 0.407 [0.325, 0.513], mean action: 29.100 [2.000, 73.000], mean observation: 3.151 [-2.174, 10.316], loss: 1.320651, mae: 5.094056, mean_q: 5.322803
 99285/100000: episode: 10132, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 4.136, mean reward: 0.414 [0.346, 0.582], mean action: 42.400 [2.000, 96.000], mean observation: 3.168 [-1.469, 10.223], loss: 1.431046, mae: 5.094602, mean_q: 5.320354
 99295/100000: episode: 10133, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.468, mean reward: 0.447 [0.288, 0.529], mean action: 32.500 [20.000, 100.000], mean observation: 3.166 [-1.436, 10.342], loss: 1.288138, mae: 5.093951, mean_q: 5.319870
 99305/100000: episode: 10134, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 3.866, mean reward: 0.387 [0.308, 0.470], mean action: 20.800 [1.000, 33.000], mean observation: 3.150 [-2.197, 10.315], loss: 1.320525, mae: 5.094323, mean_q: 5.321597
 99315/100000: episode: 10135, duration: 0.194s, episode steps: 10, steps per second: 52, episode reward: 3.971, mean reward: 0.397 [0.315, 0.544], mean action: 34.400 [20.000, 84.000], mean observation: 3.163 [-1.955, 10.526], loss: 1.267767, mae: 5.094417, mean_q: 5.323668
 99325/100000: episode: 10136, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.147, mean reward: 0.415 [0.363, 0.491], mean action: 42.500 [20.000, 91.000], mean observation: 3.147 [-1.225, 10.509], loss: 1.604397, mae: 5.095748, mean_q: 5.325836
 99335/100000: episode: 10137, duration: 0.153s, episode steps: 10, steps per second: 66, episode reward: 4.057, mean reward: 0.406 [0.374, 0.482], mean action: 50.100 [0.000, 100.000], mean observation: 3.176 [-1.802, 10.408], loss: 1.527327, mae: 5.095091, mean_q: 5.325242
 99345/100000: episode: 10138, duration: 0.193s, episode steps: 10, steps per second: 52, episode reward: 3.988, mean reward: 0.399 [0.369, 0.450], mean action: 31.700 [0.000, 94.000], mean observation: 3.161 [-1.817, 10.333], loss: 1.456924, mae: 5.094796, mean_q: 5.322961
 99355/100000: episode: 10139, duration: 0.182s, episode steps: 10, steps per second: 55, episode reward: 3.941, mean reward: 0.394 [0.344, 0.511], mean action: 44.300 [20.000, 78.000], mean observation: 3.151 [-2.626, 10.330], loss: 1.255670, mae: 5.093681, mean_q: 5.324203
 99365/100000: episode: 10140, duration: 0.171s, episode steps: 10, steps per second: 59, episode reward: 4.328, mean reward: 0.433 [0.375, 0.492], mean action: 35.500 [17.000, 101.000], mean observation: 3.167 [-1.691, 10.407], loss: 1.061803, mae: 5.093011, mean_q: 5.323391
 99375/100000: episode: 10141, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 4.098, mean reward: 0.410 [0.347, 0.554], mean action: 27.500 [0.000, 94.000], mean observation: 3.163 [-1.053, 10.513], loss: 1.328639, mae: 5.093893, mean_q: 5.323365
 99385/100000: episode: 10142, duration: 0.206s, episode steps: 10, steps per second: 49, episode reward: 4.971, mean reward: 0.497 [0.444, 0.503], mean action: 26.900 [3.000, 64.000], mean observation: 3.136 [-1.659, 10.220], loss: 1.370062, mae: 5.093908, mean_q: 5.325096
 99395/100000: episode: 10143, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 4.281, mean reward: 0.428 [0.334, 0.533], mean action: 30.600 [5.000, 87.000], mean observation: 3.159 [-1.286, 10.411], loss: 1.187423, mae: 5.093243, mean_q: 5.327230
 99405/100000: episode: 10144, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 4.152, mean reward: 0.415 [0.356, 0.471], mean action: 35.900 [16.000, 91.000], mean observation: 3.146 [-1.276, 10.408], loss: 1.460637, mae: 5.094250, mean_q: 5.325301
 99415/100000: episode: 10145, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.841, mean reward: 0.384 [0.275, 0.450], mean action: 25.000 [19.000, 71.000], mean observation: 3.156 [-1.530, 10.295], loss: 1.490860, mae: 5.094237, mean_q: 5.323467
 99425/100000: episode: 10146, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 4.398, mean reward: 0.440 [0.347, 0.471], mean action: 30.000 [3.000, 62.000], mean observation: 3.161 [-1.786, 10.291], loss: 1.382808, mae: 5.093863, mean_q: 5.323645
 99435/100000: episode: 10147, duration: 0.163s, episode steps: 10, steps per second: 62, episode reward: 3.865, mean reward: 0.387 [0.283, 0.581], mean action: 47.600 [9.000, 81.000], mean observation: 3.147 [-1.190, 10.312], loss: 1.272885, mae: 5.093244, mean_q: 5.323581
 99445/100000: episode: 10148, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 3.825, mean reward: 0.383 [0.334, 0.427], mean action: 39.600 [20.000, 89.000], mean observation: 3.160 [-1.140, 10.435], loss: 1.191815, mae: 5.093041, mean_q: 5.325461
 99455/100000: episode: 10149, duration: 0.177s, episode steps: 10, steps per second: 57, episode reward: 4.202, mean reward: 0.420 [0.297, 0.535], mean action: 30.100 [2.000, 96.000], mean observation: 3.153 [-0.858, 10.341], loss: 1.560416, mae: 5.094533, mean_q: 5.324207
 99465/100000: episode: 10150, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 4.366, mean reward: 0.437 [0.437, 0.437], mean action: 59.400 [12.000, 98.000], mean observation: 3.146 [-0.778, 10.216], loss: 1.474414, mae: 5.094376, mean_q: 5.322044
 99475/100000: episode: 10151, duration: 0.183s, episode steps: 10, steps per second: 55, episode reward: 4.255, mean reward: 0.425 [0.345, 0.546], mean action: 32.800 [5.000, 90.000], mean observation: 3.153 [-1.241, 10.303], loss: 1.377888, mae: 5.093976, mean_q: 5.324516
 99485/100000: episode: 10152, duration: 0.184s, episode steps: 10, steps per second: 54, episode reward: 3.852, mean reward: 0.385 [0.289, 0.483], mean action: 28.100 [18.000, 71.000], mean observation: 3.171 [-1.484, 10.351], loss: 1.148460, mae: 5.093111, mean_q: 5.324586
 99495/100000: episode: 10153, duration: 0.178s, episode steps: 10, steps per second: 56, episode reward: 4.705, mean reward: 0.471 [0.392, 0.541], mean action: 39.700 [0.000, 88.000], mean observation: 3.163 [-1.148, 10.342], loss: 1.312844, mae: 5.094094, mean_q: 5.323426
 99505/100000: episode: 10154, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.299, mean reward: 0.430 [0.426, 0.466], mean action: 44.500 [20.000, 100.000], mean observation: 3.154 [-1.532, 10.385], loss: 1.252702, mae: 5.093942, mean_q: 5.320423
 99515/100000: episode: 10155, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 3.742, mean reward: 0.374 [0.316, 0.404], mean action: 41.500 [2.000, 94.000], mean observation: 3.162 [-2.221, 10.338], loss: 1.480522, mae: 5.094735, mean_q: 5.320675
 99525/100000: episode: 10156, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 3.792, mean reward: 0.379 [0.335, 0.474], mean action: 54.500 [0.000, 95.000], mean observation: 3.150 [-0.990, 10.343], loss: 1.215227, mae: 5.094127, mean_q: 5.322095
 99535/100000: episode: 10157, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.722, mean reward: 0.472 [0.380, 0.559], mean action: 52.100 [6.000, 87.000], mean observation: 3.154 [-1.747, 10.403], loss: 1.540396, mae: 5.095438, mean_q: 5.322947
 99545/100000: episode: 10158, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 4.610, mean reward: 0.461 [0.371, 0.529], mean action: 41.800 [1.000, 84.000], mean observation: 3.151 [-1.644, 10.316], loss: 1.313082, mae: 5.094763, mean_q: 5.324360
 99555/100000: episode: 10159, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.182, mean reward: 0.418 [0.392, 0.479], mean action: 53.000 [16.000, 83.000], mean observation: 3.165 [-1.452, 10.164], loss: 1.220433, mae: 5.094405, mean_q: 5.325073
 99565/100000: episode: 10160, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 4.013, mean reward: 0.401 [0.355, 0.453], mean action: 62.200 [27.000, 94.000], mean observation: 3.149 [-0.810, 10.296], loss: 1.468646, mae: 5.095599, mean_q: 5.325551
 99575/100000: episode: 10161, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 4.090, mean reward: 0.409 [0.377, 0.455], mean action: 54.100 [24.000, 95.000], mean observation: 3.166 [-0.912, 10.391], loss: 1.140480, mae: 5.094468, mean_q: 5.322145
 99585/100000: episode: 10162, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.231, mean reward: 0.423 [0.348, 0.461], mean action: 49.800 [5.000, 96.000], mean observation: 3.166 [-1.125, 10.354], loss: 1.214994, mae: 5.095107, mean_q: 5.321624
 99593/100000: episode: 10163, duration: 0.132s, episode steps: 8, steps per second: 61, episode reward: 13.134, mean reward: 1.642 [0.374, 10.000], mean action: 51.000 [8.000, 80.000], mean observation: 3.144 [-1.074, 10.419], loss: 1.240314, mae: 5.095456, mean_q: 5.321597
 99603/100000: episode: 10164, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.022, mean reward: 0.402 [0.323, 0.487], mean action: 53.300 [20.000, 96.000], mean observation: 3.164 [-1.371, 10.436], loss: 1.361293, mae: 5.096096, mean_q: 5.322767
 99613/100000: episode: 10165, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 4.076, mean reward: 0.408 [0.362, 0.599], mean action: 54.200 [4.000, 89.000], mean observation: 3.157 [-0.687, 10.388], loss: 1.130447, mae: 5.095172, mean_q: 5.324602
 99623/100000: episode: 10166, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 3.761, mean reward: 0.376 [0.335, 0.426], mean action: 60.000 [33.000, 101.000], mean observation: 3.150 [-0.962, 10.225], loss: 1.410060, mae: 5.096465, mean_q: 5.326688
 99633/100000: episode: 10167, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 4.146, mean reward: 0.415 [0.324, 0.498], mean action: 53.900 [28.000, 98.000], mean observation: 3.143 [-1.077, 10.380], loss: 1.381937, mae: 5.096378, mean_q: 5.329049
 99643/100000: episode: 10168, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 4.253, mean reward: 0.425 [0.413, 0.451], mean action: 42.300 [14.000, 54.000], mean observation: 3.160 [-1.211, 10.308], loss: 1.003798, mae: 5.094937, mean_q: 5.331341
 99653/100000: episode: 10169, duration: 0.207s, episode steps: 10, steps per second: 48, episode reward: 4.094, mean reward: 0.409 [0.368, 0.548], mean action: 42.100 [1.000, 80.000], mean observation: 3.161 [-1.837, 10.301], loss: 1.319128, mae: 5.096549, mean_q: 5.332894
 99663/100000: episode: 10170, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.715, mean reward: 0.372 [0.323, 0.405], mean action: 48.400 [9.000, 93.000], mean observation: 3.155 [-1.578, 10.373], loss: 1.274103, mae: 5.096436, mean_q: 5.329688
 99673/100000: episode: 10171, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.323, mean reward: 0.432 [0.370, 0.487], mean action: 48.600 [0.000, 96.000], mean observation: 3.155 [-1.228, 10.381], loss: 1.340271, mae: 5.096858, mean_q: 5.329237
 99683/100000: episode: 10172, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 3.811, mean reward: 0.381 [0.362, 0.437], mean action: 50.900 [6.000, 86.000], mean observation: 3.145 [-1.840, 10.309], loss: 1.247262, mae: 5.096367, mean_q: 5.330306
 99693/100000: episode: 10173, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.085, mean reward: 0.408 [0.313, 0.552], mean action: 51.300 [7.000, 71.000], mean observation: 3.159 [-1.184, 10.292], loss: 1.176939, mae: 5.096319, mean_q: 5.333177
 99703/100000: episode: 10174, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.318, mean reward: 0.432 [0.366, 0.480], mean action: 41.500 [5.000, 54.000], mean observation: 3.165 [-1.568, 10.397], loss: 1.079007, mae: 5.096029, mean_q: 5.334994
 99713/100000: episode: 10175, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 4.130, mean reward: 0.413 [0.397, 0.475], mean action: 57.600 [10.000, 99.000], mean observation: 3.150 [-1.583, 10.281], loss: 1.313280, mae: 5.097497, mean_q: 5.336969
 99723/100000: episode: 10176, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 3.999, mean reward: 0.400 [0.343, 0.483], mean action: 44.100 [2.000, 90.000], mean observation: 3.155 [-1.148, 10.294], loss: 1.530230, mae: 5.098410, mean_q: 5.338568
 99733/100000: episode: 10177, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 3.800, mean reward: 0.380 [0.340, 0.434], mean action: 56.800 [27.000, 98.000], mean observation: 3.144 [-1.174, 10.397], loss: 1.177390, mae: 5.096773, mean_q: 5.340569
 99743/100000: episode: 10178, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.620, mean reward: 0.462 [0.425, 0.558], mean action: 59.600 [40.000, 86.000], mean observation: 3.162 [-1.471, 10.332], loss: 1.426857, mae: 5.097893, mean_q: 5.343421
 99753/100000: episode: 10179, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 3.952, mean reward: 0.395 [0.311, 0.516], mean action: 51.000 [1.000, 88.000], mean observation: 3.170 [-1.698, 10.348], loss: 1.403322, mae: 5.097678, mean_q: 5.345558
 99763/100000: episode: 10180, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 3.860, mean reward: 0.386 [0.356, 0.454], mean action: 34.900 [8.000, 56.000], mean observation: 3.154 [-1.752, 10.299], loss: 1.427230, mae: 5.097936, mean_q: 5.344079
 99773/100000: episode: 10181, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 4.460, mean reward: 0.446 [0.370, 0.528], mean action: 47.300 [4.000, 84.000], mean observation: 3.153 [-1.519, 10.335], loss: 1.319978, mae: 5.097377, mean_q: 5.340707
 99783/100000: episode: 10182, duration: 0.153s, episode steps: 10, steps per second: 66, episode reward: 4.254, mean reward: 0.425 [0.422, 0.453], mean action: 59.900 [46.000, 89.000], mean observation: 3.152 [-1.124, 10.290], loss: 1.124268, mae: 5.096671, mean_q: 5.340565
 99793/100000: episode: 10183, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 4.004, mean reward: 0.400 [0.322, 0.474], mean action: 43.500 [3.000, 89.000], mean observation: 3.158 [-1.743, 10.343], loss: 1.016649, mae: 5.096570, mean_q: 5.341527
 99803/100000: episode: 10184, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 3.936, mean reward: 0.394 [0.318, 0.487], mean action: 59.600 [17.000, 98.000], mean observation: 3.152 [-1.503, 10.318], loss: 1.518150, mae: 5.098860, mean_q: 5.342443
 99813/100000: episode: 10185, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 3.996, mean reward: 0.400 [0.385, 0.454], mean action: 56.900 [28.000, 94.000], mean observation: 3.148 [-1.618, 10.291], loss: 1.371916, mae: 5.098672, mean_q: 5.344313
 99823/100000: episode: 10186, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 4.443, mean reward: 0.444 [0.395, 0.474], mean action: 50.200 [17.000, 80.000], mean observation: 3.158 [-1.089, 10.341], loss: 1.230270, mae: 5.098415, mean_q: 5.346128
 99833/100000: episode: 10187, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 3.557, mean reward: 0.356 [0.332, 0.371], mean action: 65.500 [42.000, 100.000], mean observation: 3.155 [-1.031, 10.266], loss: 1.255868, mae: 5.098788, mean_q: 5.347526
 99843/100000: episode: 10188, duration: 0.180s, episode steps: 10, steps per second: 56, episode reward: 3.855, mean reward: 0.386 [0.373, 0.412], mean action: 46.700 [19.000, 100.000], mean observation: 3.163 [-1.272, 10.405], loss: 0.950734, mae: 5.097787, mean_q: 5.349251
 99844/100000: episode: 10189, duration: 0.034s, episode steps: 1, steps per second: 29, episode reward: 10.000, mean reward: 10.000 [10.000, 10.000], mean action: 38.000 [38.000, 38.000], mean observation: 3.139 [-1.268, 10.201], loss: 0.828094, mae: 5.097121, mean_q: 5.350326
 99854/100000: episode: 10190, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 3.962, mean reward: 0.396 [0.369, 0.485], mean action: 50.700 [5.000, 94.000], mean observation: 3.149 [-1.421, 10.335], loss: 1.309036, mae: 5.099809, mean_q: 5.351726
 99864/100000: episode: 10191, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 4.276, mean reward: 0.428 [0.392, 0.487], mean action: 40.100 [4.000, 54.000], mean observation: 3.146 [-1.778, 10.340], loss: 1.234411, mae: 5.100019, mean_q: 5.353854
 99874/100000: episode: 10192, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 3.730, mean reward: 0.373 [0.294, 0.450], mean action: 49.700 [10.000, 71.000], mean observation: 3.150 [-1.337, 10.336], loss: 1.221811, mae: 5.100161, mean_q: 5.355116
 99884/100000: episode: 10193, duration: 0.144s, episode steps: 10, steps per second: 70, episode reward: 4.207, mean reward: 0.421 [0.386, 0.481], mean action: 58.400 [27.000, 96.000], mean observation: 3.163 [-1.454, 10.334], loss: 1.151921, mae: 5.100271, mean_q: 5.357028
 99889/100000: episode: 10194, duration: 0.102s, episode steps: 5, steps per second: 49, episode reward: 11.931, mean reward: 2.386 [0.404, 10.000], mean action: 35.800 [0.000, 54.000], mean observation: 3.151 [-1.427, 10.273], loss: 0.997179, mae: 5.099910, mean_q: 5.358442
 99899/100000: episode: 10195, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 4.139, mean reward: 0.414 [0.402, 0.481], mean action: 55.400 [19.000, 91.000], mean observation: 3.148 [-1.214, 10.341], loss: 1.222390, mae: 5.101092, mean_q: 5.360237
 99909/100000: episode: 10196, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 4.030, mean reward: 0.403 [0.391, 0.490], mean action: 47.600 [9.000, 101.000], mean observation: 3.152 [-0.938, 10.387], loss: 1.466715, mae: 5.102296, mean_q: 5.362180
 99919/100000: episode: 10197, duration: 0.140s, episode steps: 10, steps per second: 71, episode reward: 3.958, mean reward: 0.396 [0.309, 0.545], mean action: 55.200 [7.000, 100.000], mean observation: 3.149 [-1.206, 10.337], loss: 0.960653, mae: 5.100778, mean_q: 5.364362
 99929/100000: episode: 10198, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 4.358, mean reward: 0.436 [0.388, 0.482], mean action: 60.500 [32.000, 97.000], mean observation: 3.161 [-1.991, 10.278], loss: 1.486887, mae: 5.103206, mean_q: 5.366018
 99939/100000: episode: 10199, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 3.781, mean reward: 0.378 [0.307, 0.457], mean action: 58.200 [45.000, 76.000], mean observation: 3.159 [-1.287, 10.285], loss: 1.313335, mae: 5.102563, mean_q: 5.366993
 99949/100000: episode: 10200, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.315, mean reward: 0.432 [0.323, 0.549], mean action: 57.100 [30.000, 101.000], mean observation: 3.147 [-1.525, 10.405], loss: 1.503720, mae: 5.103429, mean_q: 5.368259
 99959/100000: episode: 10201, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 3.868, mean reward: 0.387 [0.375, 0.436], mean action: 64.800 [33.000, 98.000], mean observation: 3.165 [-2.498, 10.387], loss: 1.805975, mae: 5.104631, mean_q: 5.369599
 99969/100000: episode: 10202, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 4.122, mean reward: 0.412 [0.370, 0.484], mean action: 45.800 [1.000, 54.000], mean observation: 3.161 [-1.595, 10.294], loss: 0.854874, mae: 5.100961, mean_q: 5.370999
 99979/100000: episode: 10203, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.849, mean reward: 0.485 [0.478, 0.547], mean action: 50.900 [12.000, 93.000], mean observation: 3.144 [-0.959, 10.246], loss: 1.528508, mae: 5.103439, mean_q: 5.368659
 99989/100000: episode: 10204, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 4.062, mean reward: 0.406 [0.282, 0.537], mean action: 51.900 [7.000, 80.000], mean observation: 3.164 [-1.865, 10.509], loss: 1.061722, mae: 5.101967, mean_q: 5.366262
 99999/100000: episode: 10205, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 4.417, mean reward: 0.442 [0.363, 0.483], mean action: 46.800 [3.000, 66.000], mean observation: 3.153 [-1.893, 10.449], loss: 1.234443, mae: 5.102696, mean_q: 5.363605
done, took 1648.508 seconds
TESTING OVER 10000 TRACES
Testing for 1 episodes ...
Episode 1: reward: 3.574, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.838, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.908, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.063, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.050, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.320, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.325, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.676, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.449, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.565, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.570, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.722, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.310, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.820, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.691, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.913, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.853, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.253, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.461, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.140, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.341, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.990, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.652, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.679, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.190, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.008, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.888, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.446, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.262, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.644, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.212, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.036, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.137, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.024, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.991, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.036, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.101, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.777, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.058, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.809, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.745, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.194, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.047, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.423, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.708, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.153, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.873, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.912, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.434, steps: 2
Testing for 1 episodes ...
Episode 1: reward: 4.556, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.652, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.359, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.777, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.023, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.928, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.940, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.105, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 14.080, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.809, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.002, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.008, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.749, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.997, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.569, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.616, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.705, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.802, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.298, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.851, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.547, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.295, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.263, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.830, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.069, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.699, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.118, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.097, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.007, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.989, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.420, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.183, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.932, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.983, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.687, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.129, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.377, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.167, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.806, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.110, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.799, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.875, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.703, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.666, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.893, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.552, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.269, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.111, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.600, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.078, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.134, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.286, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.015, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.685, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.326, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.031, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.906, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.854, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.984, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.054, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.745, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.438, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.320, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.514, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.809, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.895, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.790, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.105, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.464, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.976, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.002, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.081, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.048, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.753, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.896, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.631, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.847, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.985, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.554, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.180, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.750, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.222, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.149, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.131, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.089, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.034, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.819, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.933, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.649, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.891, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.801, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.908, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.261, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.172, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.527, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.845, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.639, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.034, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.784, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.904, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.561, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.837, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.469, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.765, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.763, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.036, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.138, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.541, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.617, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.933, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.135, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.717, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.293, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 11.441, steps: 5
Testing for 1 episodes ...
Episode 1: reward: 4.230, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.005, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.706, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.324, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.106, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.417, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.275, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.757, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.703, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.722, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.167, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.914, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.060, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 13.482, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.860, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.205, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.137, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.988, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.816, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.289, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.738, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.379, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.473, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.924, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.806, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.484, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.417, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.512, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.000, steps: 1
Testing for 1 episodes ...
Episode 1: reward: 3.815, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.088, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.761, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.064, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.807, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.793, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.082, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.051, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.636, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.941, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.825, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.060, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.864, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.302, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.655, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.100, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.675, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.245, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.489, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.547, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.842, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.278, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.212, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.990, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.071, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.006, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.845, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.815, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.912, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.326, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.869, steps: 3
Testing for 1 episodes ...
Episode 1: reward: 4.188, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.518, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.754, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.957, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.907, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.519, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.485, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.664, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.709, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.038, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.849, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.050, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.869, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.076, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.117, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.062, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.534, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.687, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.117, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.010, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.654, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.624, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.398, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.407, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.454, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.818, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.802, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 12.911, steps: 8
Testing for 1 episodes ...
Episode 1: reward: 4.474, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.180, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.224, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.132, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.989, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.068, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.597, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.985, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.745, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.031, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.064, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.531, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.090, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.586, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.454, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.317, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.113, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.528, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.430, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.818, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.953, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.685, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.753, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.189, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.947, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.128, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.531, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.688, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.792, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.922, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.678, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.178, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.478, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.803, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.986, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.030, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.120, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.843, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.772, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.614, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.845, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.777, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 11.512, steps: 5
Testing for 1 episodes ...
Episode 1: reward: 4.026, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.446, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.193, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.396, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.207, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.614, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.177, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.727, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.346, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.200, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.001, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.363, steps: 2
Testing for 1 episodes ...
Episode 1: reward: 4.015, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 13.065, steps: 8
Testing for 1 episodes ...
Episode 1: reward: 3.918, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.907, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.265, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.727, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.248, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.401, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.359, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.687, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.767, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.162, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.038, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.983, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.258, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.319, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.918, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.697, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.216, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.465, steps: 2
Testing for 1 episodes ...
Episode 1: reward: 5.130, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.940, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.738, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.366, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.560, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.889, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.878, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.000, steps: 1
Testing for 1 episodes ...
Episode 1: reward: 4.567, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.070, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.531, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.652, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.996, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.284, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.698, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.887, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.997, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.627, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.815, steps: 3
Testing for 1 episodes ...
Episode 1: reward: 4.365, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.701, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.974, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.848, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.673, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.859, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.353, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.225, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.934, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.899, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 12.006, steps: 5
Testing for 1 episodes ...
Episode 1: reward: 4.029, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.507, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.807, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.760, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.993, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.465, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.807, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.686, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.017, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 11.211, steps: 4
Testing for 1 episodes ...
Episode 1: reward: 3.806, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.170, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.993, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.964, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.964, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.184, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.827, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.125, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.573, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.899, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.675, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.824, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.467, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.792, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.607, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.009, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.201, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.147, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.262, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.631, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.722, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.935, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.760, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.854, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.268, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.227, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.853, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.386, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.485, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.213, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.341, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.848, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.938, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.803, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.195, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.000, steps: 1
Testing for 1 episodes ...
Episode 1: reward: 4.353, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.874, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.961, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.765, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.081, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.317, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.991, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.818, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.754, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.001, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.047, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.163, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.007, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.044, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.977, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.825, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.862, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.827, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.756, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.639, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.600, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.917, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.292, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.237, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 11.690, steps: 5
Testing for 1 episodes ...
Episode 1: reward: 4.704, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.751, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 11.021, steps: 4
Testing for 1 episodes ...
Episode 1: reward: 4.121, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.317, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.091, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.041, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.907, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.971, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 12.239, steps: 6
Testing for 1 episodes ...
Episode 1: reward: 3.748, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.323, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.316, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.535, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.018, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.372, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.972, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.257, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.614, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.381, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.052, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.105, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.982, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.544, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.977, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.857, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.079, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.908, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.645, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.347, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.637, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.105, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.004, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.530, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.869, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.386, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.688, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.930, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.211, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.988, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.003, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.538, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.034, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.468, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.972, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.192, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.722, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.829, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.592, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.320, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.702, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.824, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.349, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.147, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.851, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.335, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.985, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.825, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.756, steps: 3
Testing for 1 episodes ...
Episode 1: reward: 3.696, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.988, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.802, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.464, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.959, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.272, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.289, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.028, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.747, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.636, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.942, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.478, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.672, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.133, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.849, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.565, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.759, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.032, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.883, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.896, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.957, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.509, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.247, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.224, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 13.094, steps: 7
Testing for 1 episodes ...
Episode 1: reward: 3.924, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.685, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.193, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.274, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.882, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.892, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.943, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.495, steps: 2
Testing for 1 episodes ...
Episode 1: reward: 3.886, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.853, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.919, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.453, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.022, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.073, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.357, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.731, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.242, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.532, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.412, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.953, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.604, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.217, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.414, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.127, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.739, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.911, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.716, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.584, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.031, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.872, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.739, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.084, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.071, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.065, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.131, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.760, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.029, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.084, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.952, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.930, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.879, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.967, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.074, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.058, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.606, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.664, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.146, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.422, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.359, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.040, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.838, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.768, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.860, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.133, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.527, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.668, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.430, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.216, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.928, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.674, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.148, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.878, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.574, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.399, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.521, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.629, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.911, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.884, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.868, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.218, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.198, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.896, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.931, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.268, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 13.776, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.873, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.747, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.565, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.435, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.411, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.807, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.623, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.083, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.638, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.204, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.144, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.936, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.063, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.873, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.000, steps: 1
Testing for 1 episodes ...
Episode 1: reward: 4.669, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.842, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.959, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.994, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.062, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.883, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.175, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.674, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.831, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.404, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.934, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.940, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.716, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.971, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.974, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.169, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.779, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.059, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.669, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.931, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.136, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.073, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.690, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.789, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.038, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.068, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.538, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.063, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.885, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.665, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.612, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.942, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.246, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.735, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.701, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.663, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.596, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.324, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.984, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.022, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.032, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.164, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.006, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.150, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.998, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.801, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.976, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.859, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.148, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.786, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.599, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.045, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.000, steps: 1
Testing for 1 episodes ...
Episode 1: reward: 4.107, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.551, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.009, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.681, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.517, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.663, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.167, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.302, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.840, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 12.646, steps: 7
Testing for 1 episodes ...
Episode 1: reward: 3.888, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.354, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.616, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.662, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.732, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.135, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.769, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.002, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.922, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.802, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.873, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.240, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.466, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.108, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.229, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.551, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.182, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.856, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.208, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.786, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.200, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.472, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.656, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.888, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.572, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.833, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.996, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.689, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.306, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.861, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.912, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.875, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.689, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.938, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.386, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 13.889, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.697, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.906, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.006, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.065, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.784, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.029, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.893, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.905, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.662, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.767, steps: 3
Testing for 1 episodes ...
Episode 1: reward: 3.865, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.481, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.931, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.023, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.136, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.279, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.670, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.599, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.921, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.138, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.752, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.087, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.984, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.138, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.437, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.034, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.562, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.588, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.468, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.727, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.220, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.597, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.274, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.242, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.940, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.170, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.308, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.581, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.804, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.628, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.627, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.522, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.367, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.040, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.873, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.620, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.207, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.817, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.099, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.822, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.777, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.885, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.606, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.293, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.566, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.794, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.864, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.123, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.606, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.525, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.842, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.175, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.737, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.849, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.785, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 13.859, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.487, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 12.793, steps: 7
Testing for 1 episodes ...
Episode 1: reward: 4.356, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.783, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.052, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.162, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.470, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.678, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.027, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.924, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.233, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.935, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.838, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.196, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.849, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.210, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.193, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.149, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.804, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.365, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.898, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.743, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.495, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.801, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.240, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.802, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.733, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.896, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.750, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.214, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.260, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.904, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.640, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.806, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.715, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.019, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.257, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.678, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.762, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.092, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.431, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.514, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.516, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.606, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.323, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.098, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.811, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.598, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.916, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.855, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.104, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.656, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.438, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.518, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.474, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.984, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.128, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.249, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.000, steps: 1
Testing for 1 episodes ...
Episode 1: reward: 3.973, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.247, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.256, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.047, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.629, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.735, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.850, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.880, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.072, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.648, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.185, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.114, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.024, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.451, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.490, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.836, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.669, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.940, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.255, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.033, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.805, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.793, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.822, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.213, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.807, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.245, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.956, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.795, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.847, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.827, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 11.469, steps: 4
Testing for 1 episodes ...
Episode 1: reward: 4.057, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.121, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.136, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.011, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.657, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.885, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.027, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.812, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.094, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.662, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.637, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.931, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.923, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.971, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.694, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.430, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.077, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.751, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.136, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.055, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.000, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.009, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.482, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.200, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.861, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.154, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.629, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.000, steps: 1
Testing for 1 episodes ...
Episode 1: reward: 4.205, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.762, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.793, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.504, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.248, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.418, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.957, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.257, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.934, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.777, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.058, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.874, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.438, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.765, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.530, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.932, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.903, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.895, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.207, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.231, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.099, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.466, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.664, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.195, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.142, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.792, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.080, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.903, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.038, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.855, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.521, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.077, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.193, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.519, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.373, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.041, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.375, steps: 2
Testing for 1 episodes ...
Episode 1: reward: 4.221, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.757, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.981, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.803, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.254, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.547, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.602, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.988, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.482, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.918, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.849, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.980, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.676, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.385, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.919, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.114, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.910, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.540, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.127, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.803, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.625, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.015, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.042, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.348, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.637, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.666, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.819, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.241, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.698, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.219, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.069, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.964, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.348, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.883, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.138, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.378, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.928, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.878, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.941, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.238, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.487, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.949, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.778, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.211, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.000, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 10.000, steps: 1
Testing for 1 episodes ...
Episode 1: reward: 4.260, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.696, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.886, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.745, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.239, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.780, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.059, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.635, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.899, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.825, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.754, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.570, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.920, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.761, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.359, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.345, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 12.030, steps: 7
Testing for 1 episodes ...
Episode 1: reward: 3.875, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.320, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.884, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.858, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.881, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.322, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.305, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.445, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.152, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.150, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.804, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.333, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.354, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 4.142, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 5.224, steps: 10
Testing for 1 episodes ...
Episode 1: reward: 3.971, steps: 10
[Result] Frequency MC: 62 | Frequency RL: 36
--Return--
None
> [0;32m/home/luigi/Development/StatisticalSystemChecking/RL/rl_agent_ekf.py[0m(87)[0;36mmain[0;34m()[0m
[0;32m     86 [0;31m    [0;32mimport[0m [0mipdb[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m---> 87 [0;31m    [0mipdb[0m[0;34m.[0m[0mset_trace[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m     88 [0;31m[0;34m[0m[0m
[0m
ipdb> 